# Comparing `tmp/deepspeed-0.8.3.tar.gz` & `tmp/deepspeed-0.9.0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "deepspeed-0.8.3.tar", last modified: Mon Mar 20 17:59:48 2023, max compression
+gzip compressed data, was "deepspeed-0.9.0.tar", last modified: Thu Apr 13 15:30:33 2023, max compression
```

## Comparing `deepspeed-0.8.3.tar` & `deepspeed-0.9.0.tar`

### file list

```diff
@@ -1,617 +1,602 @@
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:48.079686 deepspeed-0.8.3/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1141 2022-07-06 20:47:37.000000 deepspeed-0.8.3/LICENSE
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      290 2023-02-28 00:54:52.000000 deepspeed-0.8.3/MANIFEST.in
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    24840 2023-03-20 17:59:48.088853 deepspeed-0.8.3/PKG-INFO
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)    23957 2023-03-20 17:58:16.000000 deepspeed-0.8.3/README.md
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:46.686353 deepspeed-0.8.3/accelerator/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      164 2023-02-28 00:58:40.000000 deepspeed-0.8.3/accelerator/__init__.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4481 2023-02-28 00:58:40.000000 deepspeed-0.8.3/accelerator/abstract_accelerator.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     8020 2023-03-20 17:58:16.000000 deepspeed-0.8.3/accelerator/cuda_accelerator.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     3734 2023-02-28 00:58:40.000000 deepspeed-0.8.3/accelerator/real_accelerator.py
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:46.695519 deepspeed-0.8.3/benchmarks/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)       45 2023-02-28 00:58:40.000000 deepspeed-0.8.3/benchmarks/__init__.py
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:46.704686 deepspeed-0.8.3/benchmarks/communication/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)       45 2023-02-28 00:58:40.000000 deepspeed-0.8.3/benchmarks/communication/__init__.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     6424 2023-03-07 18:15:12.000000 deepspeed-0.8.3/benchmarks/communication/all_gather.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4074 2023-03-07 18:15:12.000000 deepspeed-0.8.3/benchmarks/communication/all_reduce.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     5082 2023-03-07 18:15:12.000000 deepspeed-0.8.3/benchmarks/communication/all_to_all.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4083 2023-03-07 18:15:12.000000 deepspeed-0.8.3/benchmarks/communication/broadcast.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      294 2023-03-07 18:15:12.000000 deepspeed-0.8.3/benchmarks/communication/constants.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4592 2023-03-07 18:15:12.000000 deepspeed-0.8.3/benchmarks/communication/pt2pt.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1604 2023-02-28 00:58:40.000000 deepspeed-0.8.3/benchmarks/communication/run_all.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     7980 2023-03-07 18:15:12.000000 deepspeed-0.8.3/benchmarks/communication/utils.py
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:46.713853 deepspeed-0.8.3/benchmarks/inference/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     3159 2023-03-07 18:15:12.000000 deepspeed-0.8.3/benchmarks/inference/bert-bench.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4625 2023-02-28 00:58:40.000000 deepspeed-0.8.3/benchmarks/inference/collect_results.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4164 2023-03-07 18:15:12.000000 deepspeed-0.8.3/benchmarks/inference/gpt-bench.py
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:46.723019 deepspeed-0.8.3/bin/
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)      106 2022-07-06 20:47:37.000000 deepspeed-0.8.3/bin/deepspeed
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)      106 2022-07-06 20:47:37.000000 deepspeed-0.8.3/bin/deepspeed.pt
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)      106 2022-07-06 20:47:37.000000 deepspeed-0.8.3/bin/ds
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)      757 2022-08-05 18:43:48.000000 deepspeed-0.8.3/bin/ds_bench
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)     1805 2022-07-06 20:47:37.000000 deepspeed-0.8.3/bin/ds_elastic
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)      109 2022-07-06 20:47:37.000000 deepspeed-0.8.3/bin/ds_report
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)      680 2022-07-06 20:47:37.000000 deepspeed-0.8.3/bin/ds_ssh
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)      109 2022-07-06 20:47:37.000000 deepspeed-0.8.3/bin/dsr
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:44.000000 deepspeed-0.8.3/build.txt
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:46.594686 deepspeed-0.8.3/csrc/
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:46.723019 deepspeed-0.8.3/csrc/adagrad/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     8339 2023-02-28 00:54:52.000000 deepspeed-0.8.3/csrc/adagrad/cpu_adagrad.cpp
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:46.732186 deepspeed-0.8.3/csrc/adam/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    10329 2023-02-28 00:54:52.000000 deepspeed-0.8.3/csrc/adam/cpu_adam.cpp
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      781 2021-06-23 17:42:12.000000 deepspeed-0.8.3/csrc/adam/fused_adam_frontend.cpp
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     6524 2021-06-23 17:42:12.000000 deepspeed-0.8.3/csrc/adam/multi_tensor_adam.cu
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     5546 2021-06-23 17:42:12.000000 deepspeed-0.8.3/csrc/adam/multi_tensor_apply.cuh
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:46.576352 deepspeed-0.8.3/csrc/aio/
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:46.732186 deepspeed-0.8.3/csrc/aio/common/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    13022 2022-07-06 20:47:37.000000 deepspeed-0.8.3/csrc/aio/common/deepspeed_aio_common.cpp
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1342 2022-07-06 20:47:37.000000 deepspeed-0.8.3/csrc/aio/common/deepspeed_aio_common.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2011 2022-07-06 20:47:37.000000 deepspeed-0.8.3/csrc/aio/common/deepspeed_aio_types.cpp
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1380 2022-07-06 20:47:37.000000 deepspeed-0.8.3/csrc/aio/common/deepspeed_aio_types.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4308 2023-02-28 00:54:52.000000 deepspeed-0.8.3/csrc/aio/common/deepspeed_aio_utils.cpp
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2064 2022-07-06 20:47:37.000000 deepspeed-0.8.3/csrc/aio/common/deepspeed_aio_utils.h
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:46.759686 deepspeed-0.8.3/csrc/aio/py_lib/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2685 2022-07-06 20:47:37.000000 deepspeed-0.8.3/csrc/aio/py_lib/deepspeed_aio_thread.cpp
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1405 2022-07-06 20:47:37.000000 deepspeed-0.8.3/csrc/aio/py_lib/deepspeed_aio_thread.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1218 2023-02-28 00:54:52.000000 deepspeed-0.8.3/csrc/aio/py_lib/deepspeed_pin_tensor.cpp
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      633 2023-02-28 00:54:52.000000 deepspeed-0.8.3/csrc/aio/py_lib/deepspeed_pin_tensor.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4228 2022-07-06 20:47:37.000000 deepspeed-0.8.3/csrc/aio/py_lib/deepspeed_py_aio.cpp
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      954 2022-07-06 20:47:37.000000 deepspeed-0.8.3/csrc/aio/py_lib/deepspeed_py_aio.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    10015 2023-02-28 00:54:52.000000 deepspeed-0.8.3/csrc/aio/py_lib/deepspeed_py_aio_handle.cpp
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2437 2023-02-28 00:54:52.000000 deepspeed-0.8.3/csrc/aio/py_lib/deepspeed_py_aio_handle.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4400 2022-07-06 20:47:37.000000 deepspeed-0.8.3/csrc/aio/py_lib/deepspeed_py_copy.cpp
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1148 2022-07-06 20:47:37.000000 deepspeed-0.8.3/csrc/aio/py_lib/deepspeed_py_copy.h
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)     1783 2023-02-28 00:54:52.000000 deepspeed-0.8.3/csrc/aio/py_lib/py_ds_aio.cpp
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:46.759686 deepspeed-0.8.3/csrc/common/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1225 2023-02-28 00:58:40.000000 deepspeed-0.8.3/csrc/common/custom_cuda_kernel.cu
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:46.860519 deepspeed-0.8.3/csrc/includes/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1928 2023-02-28 00:58:40.000000 deepspeed-0.8.3/csrc/includes/StopWatch.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1127 2023-02-28 00:58:40.000000 deepspeed-0.8.3/csrc/includes/Timer.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      287 2022-07-06 20:47:37.000000 deepspeed-0.8.3/csrc/includes/compat.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     6864 2023-02-28 00:58:40.000000 deepspeed-0.8.3/csrc/includes/context.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    12060 2022-11-14 17:49:34.000000 deepspeed-0.8.3/csrc/includes/conversion_utils.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     5546 2023-02-28 00:58:40.000000 deepspeed-0.8.3/csrc/includes/cpu_adagrad.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     8068 2023-02-28 00:58:40.000000 deepspeed-0.8.3/csrc/includes/cpu_adam.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     3224 2023-02-28 00:58:40.000000 deepspeed-0.8.3/csrc/includes/cublas_wrappers.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    13108 2022-12-20 18:47:22.000000 deepspeed-0.8.3/csrc/includes/custom_cuda_layers.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     7253 2023-02-28 00:54:52.000000 deepspeed-0.8.3/csrc/includes/dequantization_utils.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2142 2023-02-28 00:58:40.000000 deepspeed-0.8.3/csrc/includes/dropout.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1124 2022-12-20 18:47:22.000000 deepspeed-0.8.3/csrc/includes/ds_kernel_utils.h
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)     6111 2023-02-28 00:58:40.000000 deepspeed-0.8.3/csrc/includes/ds_transformer_cuda.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     3136 2023-02-28 00:58:40.000000 deepspeed-0.8.3/csrc/includes/feed_forward.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      965 2023-02-28 00:58:40.000000 deepspeed-0.8.3/csrc/includes/gelu.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    10273 2023-02-28 00:58:40.000000 deepspeed-0.8.3/csrc/includes/gemm_test.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1454 2023-02-28 00:58:40.000000 deepspeed-0.8.3/csrc/includes/general_kernels.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    33918 2023-02-28 00:54:52.000000 deepspeed-0.8.3/csrc/includes/memory_access_utils.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     7036 2023-02-28 00:58:40.000000 deepspeed-0.8.3/csrc/includes/normalize_layer.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2240 2023-02-28 00:58:40.000000 deepspeed-0.8.3/csrc/includes/quantization.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    17336 2023-02-28 00:54:52.000000 deepspeed-0.8.3/csrc/includes/quantization_utils.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      211 2023-02-28 00:58:40.000000 deepspeed-0.8.3/csrc/includes/quantizer.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    18593 2022-11-23 21:50:53.000000 deepspeed-0.8.3/csrc/includes/reduction_utils.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4473 2023-02-28 00:58:40.000000 deepspeed-0.8.3/csrc/includes/simd.h
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)     1589 2023-02-28 00:58:40.000000 deepspeed-0.8.3/csrc/includes/softmax.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     6754 2023-02-28 00:58:40.000000 deepspeed-0.8.3/csrc/includes/strided_batch_gemm.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     6335 2023-02-28 00:58:40.000000 deepspeed-0.8.3/csrc/includes/type_shim.h
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:46.869686 deepspeed-0.8.3/csrc/lamb/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     3953 2022-07-06 20:47:37.000000 deepspeed-0.8.3/csrc/lamb/fused_lamb_cuda.cpp
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    15242 2022-11-14 17:49:34.000000 deepspeed-0.8.3/csrc/lamb/fused_lamb_cuda_kernel.cu
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:46.878852 deepspeed-0.8.3/csrc/quantization/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     3254 2023-02-28 00:54:52.000000 deepspeed-0.8.3/csrc/quantization/dequantize.cu
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    37512 2023-02-28 00:58:40.000000 deepspeed-0.8.3/csrc/quantization/fake_quantizer.cu
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     5685 2023-02-28 00:54:52.000000 deepspeed-0.8.3/csrc/quantization/pt_binding.cpp
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     6940 2023-02-28 00:54:52.000000 deepspeed-0.8.3/csrc/quantization/quantize.cu
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:46.897186 deepspeed-0.8.3/csrc/random_ltd/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     8360 2022-12-20 18:47:22.000000 deepspeed-0.8.3/csrc/random_ltd/gather_scatter.cu
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     9542 2022-12-20 18:47:22.000000 deepspeed-0.8.3/csrc/random_ltd/pt_binding.cpp
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     5028 2022-12-20 18:47:22.000000 deepspeed-0.8.3/csrc/random_ltd/slice_attn_masks.cu
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     6979 2022-12-20 18:47:22.000000 deepspeed-0.8.3/csrc/random_ltd/token_sort.cu
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:46.897186 deepspeed-0.8.3/csrc/sparse_attention/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4425 2022-07-06 20:47:37.000000 deepspeed-0.8.3/csrc/sparse_attention/utils.cpp
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:46.585519 deepspeed-0.8.3/csrc/spatial/
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:46.897186 deepspeed-0.8.3/csrc/spatial/csrc/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     6225 2022-11-14 17:49:34.000000 deepspeed-0.8.3/csrc/spatial/csrc/opt_bias_add.cu
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     3815 2022-11-14 17:49:34.000000 deepspeed-0.8.3/csrc/spatial/csrc/pt_binding.cpp
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:46.897186 deepspeed-0.8.3/csrc/spatial/includes/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      785 2022-11-14 17:49:34.000000 deepspeed-0.8.3/csrc/spatial/includes/spatial_cuda_layers.h
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:46.933853 deepspeed-0.8.3/csrc/transformer/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    17519 2023-02-28 00:58:40.000000 deepspeed-0.8.3/csrc/transformer/cublas_wrappers.cu
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)    29758 2023-02-28 00:58:40.000000 deepspeed-0.8.3/csrc/transformer/dropout_kernels.cu
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    47536 2022-07-06 20:47:37.000000 deepspeed-0.8.3/csrc/transformer/ds_transformer_cuda.cpp
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    12138 2023-02-28 00:58:40.000000 deepspeed-0.8.3/csrc/transformer/gelu_kernels.cu
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    14467 2023-02-28 00:58:40.000000 deepspeed-0.8.3/csrc/transformer/general_kernels.cu
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:46.594686 deepspeed-0.8.3/csrc/transformer/inference/
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:46.961352 deepspeed-0.8.3/csrc/transformer/inference/csrc/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    16290 2023-02-28 00:54:52.000000 deepspeed-0.8.3/csrc/transformer/inference/csrc/apply_rotary_pos_emb.cu
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     6860 2022-12-20 18:47:22.000000 deepspeed-0.8.3/csrc/transformer/inference/csrc/dequantize.cu
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    28997 2022-12-20 18:47:22.000000 deepspeed-0.8.3/csrc/transformer/inference/csrc/gelu.cu
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    22637 2022-12-20 18:47:22.000000 deepspeed-0.8.3/csrc/transformer/inference/csrc/layer_norm.cu
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    74030 2023-02-28 00:54:52.000000 deepspeed-0.8.3/csrc/transformer/inference/csrc/pt_binding.cpp
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2147 2022-12-20 18:47:22.000000 deepspeed-0.8.3/csrc/transformer/inference/csrc/relu.cu
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    22413 2022-12-20 18:47:22.000000 deepspeed-0.8.3/csrc/transformer/inference/csrc/softmax.cu
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    29483 2022-12-20 18:47:22.000000 deepspeed-0.8.3/csrc/transformer/inference/csrc/transform.cu
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:46.961352 deepspeed-0.8.3/csrc/transformer/inference/includes/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     9286 2022-11-14 17:49:34.000000 deepspeed-0.8.3/csrc/transformer/inference/includes/inference_context.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    17691 2022-11-14 17:49:34.000000 deepspeed-0.8.3/csrc/transformer/inference/includes/inference_cublas_wrappers.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     8217 2022-12-20 18:47:22.000000 deepspeed-0.8.3/csrc/transformer/inference/includes/inference_cuda_layers.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    74847 2023-02-28 00:58:40.000000 deepspeed-0.8.3/csrc/transformer/normalize_kernels.cu
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    26705 2023-02-28 00:58:40.000000 deepspeed-0.8.3/csrc/transformer/softmax_kernels.cu
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)    22656 2023-02-28 00:58:40.000000 deepspeed-0.8.3/csrc/transformer/transform_kernels.cu
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:46.961352 deepspeed-0.8.3/csrc/utils/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      742 2021-06-23 17:42:12.000000 deepspeed-0.8.3/csrc/utils/flatten_unflatten.cpp
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:46.970519 deepspeed-0.8.3/deepspeed/
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)    11822 2022-11-23 21:50:53.000000 deepspeed-0.8.3/deepspeed/__init__.py
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.007186 deepspeed-0.8.3/deepspeed/accelerator/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      164 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/accelerator/__init__.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4481 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/accelerator/abstract_accelerator.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     8020 2023-03-20 17:58:16.000000 deepspeed-0.8.3/deepspeed/accelerator/cuda_accelerator.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     3734 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/accelerator/real_accelerator.py
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.025519 deepspeed-0.8.3/deepspeed/autotuning/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)       79 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/autotuning/__init__.py
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)    55386 2023-03-20 17:58:16.000000 deepspeed-0.8.3/deepspeed/autotuning/autotuner.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     5682 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/autotuning/config.py
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.025519 deepspeed-0.8.3/deepspeed/autotuning/config_templates/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)       48 2022-07-06 20:47:37.000000 deepspeed-0.8.3/deepspeed/autotuning/config_templates/template_zero0.json
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      113 2022-07-06 20:47:37.000000 deepspeed-0.8.3/deepspeed/autotuning/config_templates/template_zero1.json
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      237 2022-07-06 20:47:37.000000 deepspeed-0.8.3/deepspeed/autotuning/config_templates/template_zero2.json
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      485 2022-07-06 20:47:37.000000 deepspeed-0.8.3/deepspeed/autotuning/config_templates/template_zero3.json
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     6740 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/autotuning/constants.py
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)    15993 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/autotuning/scheduler.py
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.034686 deepspeed-0.8.3/deepspeed/autotuning/tuner/
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)      185 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/autotuning/tuner/__init__.py
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)     2653 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/autotuning/tuner/base_tuner.py
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)     1769 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/autotuning/tuner/cost_model.py
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)     1106 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/autotuning/tuner/index_based_tuner.py
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)     5778 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/autotuning/tuner/model_based_tuner.py
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)     2357 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/autotuning/tuner/utils.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    15156 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/autotuning/utils.py
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.062186 deepspeed-0.8.3/deepspeed/checkpoint/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      551 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/checkpoint/__init__.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2289 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/checkpoint/constants.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    12550 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/checkpoint/deepspeed_checkpoint.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4844 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/checkpoint/reshape_3d_utils.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     8069 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/checkpoint/reshape_meg_2d.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2997 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/checkpoint/reshape_utils.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     5439 2022-11-14 17:49:34.000000 deepspeed-0.8.3/deepspeed/checkpoint/universal_checkpoint.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      958 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/checkpoint/utils.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     5707 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/checkpoint/zero_checkpoint.py
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.071353 deepspeed-0.8.3/deepspeed/comm/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2159 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/comm/__init__.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1371 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/comm/backend.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    27378 2023-03-20 17:58:16.000000 deepspeed-0.8.3/deepspeed/comm/comm.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      879 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/comm/config.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1206 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/comm/constants.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    10654 2023-03-07 18:15:12.000000 deepspeed-0.8.3/deepspeed/comm/torch.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4513 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/comm/utils.py
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.135519 deepspeed-0.8.3/deepspeed/compression/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      193 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/compression/__init__.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    39248 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/compression/basic_layer.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    10618 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/compression/compress.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    22941 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/compression/config.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4909 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/compression/constants.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    12859 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/compression/helper.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     7911 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/compression/scheduler.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     7824 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/compression/utils.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      689 2022-08-05 18:43:48.000000 deepspeed-0.8.3/deepspeed/constants.py
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.135519 deepspeed-0.8.3/deepspeed/elasticity/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      333 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/elasticity/__init__.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4757 2022-08-05 18:43:48.000000 deepspeed-0.8.3/deepspeed/elasticity/config.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2409 2022-08-05 18:43:48.000000 deepspeed-0.8.3/deepspeed/elasticity/constants.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     7931 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/elasticity/elastic_agent.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    18076 2022-08-05 18:43:48.000000 deepspeed-0.8.3/deepspeed/elasticity/elasticity.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      409 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/elasticity/utils.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     5097 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/env_report.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      706 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/git_version_info.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      790 2023-03-20 17:59:44.000000 deepspeed-0.8.3/deepspeed/git_version_info_installed.py
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.144686 deepspeed-0.8.3/deepspeed/inference/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)       82 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/inference/__init__.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     9148 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/inference/config.py
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)    26201 2023-03-07 18:15:12.000000 deepspeed-0.8.3/deepspeed/inference/engine.py
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.163019 deepspeed-0.8.3/deepspeed/launcher/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)       45 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/launcher/__init__.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      303 2023-03-02 00:27:13.000000 deepspeed-0.8.3/deepspeed/launcher/constants.py
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)    14179 2022-11-14 17:49:34.000000 deepspeed-0.8.3/deepspeed/launcher/launch.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    12414 2023-03-02 00:27:13.000000 deepspeed-0.8.3/deepspeed/launcher/multinode_runner.py
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)    22625 2023-03-02 00:27:13.000000 deepspeed-0.8.3/deepspeed/launcher/runner.py
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.172186 deepspeed-0.8.3/deepspeed/model_implementations/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      170 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/model_implementations/__init__.py
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.181352 deepspeed-0.8.3/deepspeed/model_implementations/diffusers/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)       45 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/model_implementations/diffusers/__init__.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2418 2023-03-07 18:15:12.000000 deepspeed-0.8.3/deepspeed/model_implementations/diffusers/unet.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     6055 2023-03-07 18:15:12.000000 deepspeed-0.8.3/deepspeed/model_implementations/diffusers/vae.py
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.199686 deepspeed-0.8.3/deepspeed/model_implementations/features/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)       45 2023-03-07 18:15:12.000000 deepspeed-0.8.3/deepspeed/model_implementations/features/__init__.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      518 2023-03-07 18:15:12.000000 deepspeed-0.8.3/deepspeed/model_implementations/features/cuda_graph.py
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.208852 deepspeed-0.8.3/deepspeed/model_implementations/transformers/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)       45 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/model_implementations/transformers/__init__.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     3083 2023-03-07 18:15:12.000000 deepspeed-0.8.3/deepspeed/model_implementations/transformers/clip_encoder.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      337 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/model_implementations/transformers/ds_base.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      748 2023-02-28 00:54:52.000000 deepspeed-0.8.3/deepspeed/model_implementations/transformers/ds_bert.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      750 2023-02-28 00:54:52.000000 deepspeed-0.8.3/deepspeed/model_implementations/transformers/ds_bloom.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      746 2023-02-28 00:54:52.000000 deepspeed-0.8.3/deepspeed/model_implementations/transformers/ds_gpt.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      763 2023-02-28 00:54:52.000000 deepspeed-0.8.3/deepspeed/model_implementations/transformers/ds_megatron_gpt.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      746 2023-02-28 00:54:52.000000 deepspeed-0.8.3/deepspeed/model_implementations/transformers/ds_opt.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     8392 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/model_implementations/transformers/ds_transformer.py
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.254686 deepspeed-0.8.3/deepspeed/module_inject/
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)      394 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/module_inject/__init__.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4980 2023-03-07 18:15:12.000000 deepspeed-0.8.3/deepspeed/module_inject/auto_tp.py
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.291352 deepspeed-0.8.3/deepspeed/module_inject/containers/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      788 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/module_inject/containers/__init__.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    10229 2023-03-07 18:15:12.000000 deepspeed-0.8.3/deepspeed/module_inject/containers/base.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     5951 2023-03-07 18:15:12.000000 deepspeed-0.8.3/deepspeed/module_inject/containers/base_moe.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     3283 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/module_inject/containers/bert.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     5053 2023-03-02 00:27:13.000000 deepspeed-0.8.3/deepspeed/module_inject/containers/bloom.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2605 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/module_inject/containers/clip.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2849 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/module_inject/containers/distil_bert.py
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.300519 deepspeed-0.8.3/deepspeed/module_inject/containers/features/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      131 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/module_inject/containers/features/__init__.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1458 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/module_inject/containers/features/megatron.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2341 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/module_inject/containers/features/meta_tensor.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2032 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/module_inject/containers/gpt2.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     3878 2023-03-02 00:27:13.000000 deepspeed-0.8.3/deepspeed/module_inject/containers/gptj.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4151 2023-03-02 00:27:13.000000 deepspeed-0.8.3/deepspeed/module_inject/containers/gptneo.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     5128 2023-03-02 00:27:13.000000 deepspeed-0.8.3/deepspeed/module_inject/containers/gptneox.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4867 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/module_inject/containers/megatron_gpt.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     3862 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/module_inject/containers/megatron_gpt_moe.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     5255 2023-03-02 00:27:13.000000 deepspeed-0.8.3/deepspeed/module_inject/containers/opt.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1687 2023-03-07 18:15:12.000000 deepspeed-0.8.3/deepspeed/module_inject/containers/unet.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1118 2023-03-07 18:15:12.000000 deepspeed-0.8.3/deepspeed/module_inject/containers/vae.py
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)     4624 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/module_inject/inject.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     3542 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/module_inject/layers.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    16201 2023-03-20 17:58:16.000000 deepspeed-0.8.3/deepspeed/module_inject/load_checkpoint.py
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)     3248 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/module_inject/module_quantize.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     7423 2023-03-07 18:15:12.000000 deepspeed-0.8.3/deepspeed/module_inject/policy.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    36778 2023-03-02 00:27:13.000000 deepspeed-0.8.3/deepspeed/module_inject/replace_module.py
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)      918 2023-02-28 00:54:52.000000 deepspeed-0.8.3/deepspeed/module_inject/replace_policy.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1603 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/module_inject/utils.py
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.300519 deepspeed-0.8.3/deepspeed/moe/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)       45 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/moe/__init__.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1192 2022-07-06 20:47:37.000000 deepspeed-0.8.3/deepspeed/moe/experts.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     6472 2022-12-20 18:47:22.000000 deepspeed-0.8.3/deepspeed/moe/layer.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     3558 2022-08-05 18:43:48.000000 deepspeed-0.8.3/deepspeed/moe/mappings.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    21567 2022-08-25 19:18:51.000000 deepspeed-0.8.3/deepspeed/moe/sharded_moe.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     5352 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/moe/utils.py
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.309686 deepspeed-0.8.3/deepspeed/monitor/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)       45 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/monitor/__init__.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2660 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/monitor/config.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2856 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/monitor/csv_monitor.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1553 2023-03-02 00:27:13.000000 deepspeed-0.8.3/deepspeed/monitor/monitor.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2248 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/monitor/tensorboard.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      704 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/monitor/utils.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1099 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/monitor/wandb.py
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.318853 deepspeed-0.8.3/deepspeed/nebula/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)       45 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/nebula/__init__.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1942 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/nebula/config.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2864 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/nebula/constants.py
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.318853 deepspeed-0.8.3/deepspeed/ops/
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)      427 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/ops/__init__.py
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.318853 deepspeed-0.8.3/deepspeed/ops/adagrad/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)       91 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/ops/adagrad/__init__.py
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)     6123 2023-02-28 00:54:52.000000 deepspeed-0.8.3/deepspeed/ops/adagrad/cpu_adagrad.py
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.337186 deepspeed-0.8.3/deepspeed/ops/adam/
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)      119 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/ops/adam/__init__.py
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)     9609 2023-03-02 00:27:13.000000 deepspeed-0.8.3/deepspeed/ops/adam/cpu_adam.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     7853 2023-02-28 00:54:52.000000 deepspeed-0.8.3/deepspeed/ops/adam/fused_adam.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      378 2022-08-05 18:43:48.000000 deepspeed-0.8.3/deepspeed/ops/adam/multi_tensor_apply.py
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.337186 deepspeed-0.8.3/deepspeed/ops/aio/
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)      126 2022-07-06 20:47:37.000000 deepspeed-0.8.3/deepspeed/ops/aio/__init__.py
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:46.640519 deepspeed-0.8.3/deepspeed/ops/csrc/
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.355519 deepspeed-0.8.3/deepspeed/ops/csrc/adagrad/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     8339 2023-02-28 00:54:52.000000 deepspeed-0.8.3/deepspeed/ops/csrc/adagrad/cpu_adagrad.cpp
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.355519 deepspeed-0.8.3/deepspeed/ops/csrc/adam/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    10329 2023-02-28 00:54:52.000000 deepspeed-0.8.3/deepspeed/ops/csrc/adam/cpu_adam.cpp
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      781 2021-06-23 17:42:12.000000 deepspeed-0.8.3/deepspeed/ops/csrc/adam/fused_adam_frontend.cpp
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     6524 2021-06-23 17:42:12.000000 deepspeed-0.8.3/deepspeed/ops/csrc/adam/multi_tensor_adam.cu
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     5546 2021-06-23 17:42:12.000000 deepspeed-0.8.3/deepspeed/ops/csrc/adam/multi_tensor_apply.cuh
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:46.631353 deepspeed-0.8.3/deepspeed/ops/csrc/aio/
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.364686 deepspeed-0.8.3/deepspeed/ops/csrc/aio/common/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    13022 2022-07-06 20:47:37.000000 deepspeed-0.8.3/deepspeed/ops/csrc/aio/common/deepspeed_aio_common.cpp
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1342 2022-07-06 20:47:37.000000 deepspeed-0.8.3/deepspeed/ops/csrc/aio/common/deepspeed_aio_common.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2011 2022-07-06 20:47:37.000000 deepspeed-0.8.3/deepspeed/ops/csrc/aio/common/deepspeed_aio_types.cpp
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1380 2022-07-06 20:47:37.000000 deepspeed-0.8.3/deepspeed/ops/csrc/aio/common/deepspeed_aio_types.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4308 2023-02-28 00:54:52.000000 deepspeed-0.8.3/deepspeed/ops/csrc/aio/common/deepspeed_aio_utils.cpp
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2064 2022-07-06 20:47:37.000000 deepspeed-0.8.3/deepspeed/ops/csrc/aio/common/deepspeed_aio_utils.h
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.373852 deepspeed-0.8.3/deepspeed/ops/csrc/aio/py_lib/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2685 2022-07-06 20:47:37.000000 deepspeed-0.8.3/deepspeed/ops/csrc/aio/py_lib/deepspeed_aio_thread.cpp
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1405 2022-07-06 20:47:37.000000 deepspeed-0.8.3/deepspeed/ops/csrc/aio/py_lib/deepspeed_aio_thread.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1218 2023-02-28 00:54:52.000000 deepspeed-0.8.3/deepspeed/ops/csrc/aio/py_lib/deepspeed_pin_tensor.cpp
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      633 2023-02-28 00:54:52.000000 deepspeed-0.8.3/deepspeed/ops/csrc/aio/py_lib/deepspeed_pin_tensor.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4228 2022-07-06 20:47:37.000000 deepspeed-0.8.3/deepspeed/ops/csrc/aio/py_lib/deepspeed_py_aio.cpp
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      954 2022-07-06 20:47:37.000000 deepspeed-0.8.3/deepspeed/ops/csrc/aio/py_lib/deepspeed_py_aio.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    10015 2023-02-28 00:54:52.000000 deepspeed-0.8.3/deepspeed/ops/csrc/aio/py_lib/deepspeed_py_aio_handle.cpp
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2437 2023-02-28 00:54:52.000000 deepspeed-0.8.3/deepspeed/ops/csrc/aio/py_lib/deepspeed_py_aio_handle.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4400 2022-07-06 20:47:37.000000 deepspeed-0.8.3/deepspeed/ops/csrc/aio/py_lib/deepspeed_py_copy.cpp
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1148 2022-07-06 20:47:37.000000 deepspeed-0.8.3/deepspeed/ops/csrc/aio/py_lib/deepspeed_py_copy.h
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)     1783 2023-02-28 00:54:52.000000 deepspeed-0.8.3/deepspeed/ops/csrc/aio/py_lib/py_ds_aio.cpp
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.373852 deepspeed-0.8.3/deepspeed/ops/csrc/aio/py_test/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      359 2021-08-17 18:54:40.000000 deepspeed-0.8.3/deepspeed/ops/csrc/aio/py_test/single_process_config.json
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.383019 deepspeed-0.8.3/deepspeed/ops/csrc/common/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1225 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/ops/csrc/common/custom_cuda_kernel.cu
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.410519 deepspeed-0.8.3/deepspeed/ops/csrc/includes/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1928 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/ops/csrc/includes/StopWatch.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1127 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/ops/csrc/includes/Timer.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      287 2022-07-06 20:47:37.000000 deepspeed-0.8.3/deepspeed/ops/csrc/includes/compat.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     6864 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/ops/csrc/includes/context.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    12060 2022-11-14 17:49:34.000000 deepspeed-0.8.3/deepspeed/ops/csrc/includes/conversion_utils.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     5546 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/ops/csrc/includes/cpu_adagrad.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     8068 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/ops/csrc/includes/cpu_adam.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     3224 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/ops/csrc/includes/cublas_wrappers.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    13108 2022-12-20 18:47:22.000000 deepspeed-0.8.3/deepspeed/ops/csrc/includes/custom_cuda_layers.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     7253 2023-02-28 00:54:52.000000 deepspeed-0.8.3/deepspeed/ops/csrc/includes/dequantization_utils.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2142 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/ops/csrc/includes/dropout.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1124 2022-12-20 18:47:22.000000 deepspeed-0.8.3/deepspeed/ops/csrc/includes/ds_kernel_utils.h
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)     6111 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/ops/csrc/includes/ds_transformer_cuda.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     3136 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/ops/csrc/includes/feed_forward.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      965 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/ops/csrc/includes/gelu.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    10273 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/ops/csrc/includes/gemm_test.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1454 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/ops/csrc/includes/general_kernels.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    33918 2023-02-28 00:54:52.000000 deepspeed-0.8.3/deepspeed/ops/csrc/includes/memory_access_utils.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     7036 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/ops/csrc/includes/normalize_layer.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2240 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/ops/csrc/includes/quantization.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    17336 2023-02-28 00:54:52.000000 deepspeed-0.8.3/deepspeed/ops/csrc/includes/quantization_utils.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      211 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/ops/csrc/includes/quantizer.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    18593 2022-11-23 21:50:53.000000 deepspeed-0.8.3/deepspeed/ops/csrc/includes/reduction_utils.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4473 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/ops/csrc/includes/simd.h
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)     1589 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/ops/csrc/includes/softmax.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     6754 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/ops/csrc/includes/strided_batch_gemm.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     6335 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/ops/csrc/includes/type_shim.h
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.410519 deepspeed-0.8.3/deepspeed/ops/csrc/lamb/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     3953 2022-07-06 20:47:37.000000 deepspeed-0.8.3/deepspeed/ops/csrc/lamb/fused_lamb_cuda.cpp
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    15242 2022-11-14 17:49:34.000000 deepspeed-0.8.3/deepspeed/ops/csrc/lamb/fused_lamb_cuda_kernel.cu
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.438019 deepspeed-0.8.3/deepspeed/ops/csrc/quantization/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     3254 2023-02-28 00:54:52.000000 deepspeed-0.8.3/deepspeed/ops/csrc/quantization/dequantize.cu
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    37512 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/ops/csrc/quantization/fake_quantizer.cu
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     5685 2023-02-28 00:54:52.000000 deepspeed-0.8.3/deepspeed/ops/csrc/quantization/pt_binding.cpp
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     6940 2023-02-28 00:54:52.000000 deepspeed-0.8.3/deepspeed/ops/csrc/quantization/quantize.cu
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.447186 deepspeed-0.8.3/deepspeed/ops/csrc/random_ltd/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     8360 2022-12-20 18:47:22.000000 deepspeed-0.8.3/deepspeed/ops/csrc/random_ltd/gather_scatter.cu
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     9542 2022-12-20 18:47:22.000000 deepspeed-0.8.3/deepspeed/ops/csrc/random_ltd/pt_binding.cpp
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     5028 2022-12-20 18:47:22.000000 deepspeed-0.8.3/deepspeed/ops/csrc/random_ltd/slice_attn_masks.cu
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     6979 2022-12-20 18:47:22.000000 deepspeed-0.8.3/deepspeed/ops/csrc/random_ltd/token_sort.cu
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.447186 deepspeed-0.8.3/deepspeed/ops/csrc/sparse_attention/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4425 2022-07-06 20:47:37.000000 deepspeed-0.8.3/deepspeed/ops/csrc/sparse_attention/utils.cpp
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:46.640519 deepspeed-0.8.3/deepspeed/ops/csrc/spatial/
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.447186 deepspeed-0.8.3/deepspeed/ops/csrc/spatial/csrc/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     6225 2022-11-14 17:49:34.000000 deepspeed-0.8.3/deepspeed/ops/csrc/spatial/csrc/opt_bias_add.cu
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     3815 2022-11-14 17:49:34.000000 deepspeed-0.8.3/deepspeed/ops/csrc/spatial/csrc/pt_binding.cpp
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.447186 deepspeed-0.8.3/deepspeed/ops/csrc/spatial/includes/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      785 2022-11-14 17:49:34.000000 deepspeed-0.8.3/deepspeed/ops/csrc/spatial/includes/spatial_cuda_layers.h
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.465519 deepspeed-0.8.3/deepspeed/ops/csrc/transformer/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    17519 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/ops/csrc/transformer/cublas_wrappers.cu
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)    29758 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/ops/csrc/transformer/dropout_kernels.cu
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    47536 2022-07-06 20:47:37.000000 deepspeed-0.8.3/deepspeed/ops/csrc/transformer/ds_transformer_cuda.cpp
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    12138 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/ops/csrc/transformer/gelu_kernels.cu
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    14467 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/ops/csrc/transformer/general_kernels.cu
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:46.640519 deepspeed-0.8.3/deepspeed/ops/csrc/transformer/inference/
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.474686 deepspeed-0.8.3/deepspeed/ops/csrc/transformer/inference/csrc/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    16290 2023-02-28 00:54:52.000000 deepspeed-0.8.3/deepspeed/ops/csrc/transformer/inference/csrc/apply_rotary_pos_emb.cu
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     6860 2022-12-20 18:47:22.000000 deepspeed-0.8.3/deepspeed/ops/csrc/transformer/inference/csrc/dequantize.cu
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    28997 2022-12-20 18:47:22.000000 deepspeed-0.8.3/deepspeed/ops/csrc/transformer/inference/csrc/gelu.cu
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    22637 2022-12-20 18:47:22.000000 deepspeed-0.8.3/deepspeed/ops/csrc/transformer/inference/csrc/layer_norm.cu
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    74030 2023-02-28 00:54:52.000000 deepspeed-0.8.3/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2147 2022-12-20 18:47:22.000000 deepspeed-0.8.3/deepspeed/ops/csrc/transformer/inference/csrc/relu.cu
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    22413 2022-12-20 18:47:22.000000 deepspeed-0.8.3/deepspeed/ops/csrc/transformer/inference/csrc/softmax.cu
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    29483 2022-12-20 18:47:22.000000 deepspeed-0.8.3/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.474686 deepspeed-0.8.3/deepspeed/ops/csrc/transformer/inference/includes/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     9286 2022-11-14 17:49:34.000000 deepspeed-0.8.3/deepspeed/ops/csrc/transformer/inference/includes/inference_context.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    17691 2022-11-14 17:49:34.000000 deepspeed-0.8.3/deepspeed/ops/csrc/transformer/inference/includes/inference_cublas_wrappers.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     8217 2022-12-20 18:47:22.000000 deepspeed-0.8.3/deepspeed/ops/csrc/transformer/inference/includes/inference_cuda_layers.h
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    74847 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/ops/csrc/transformer/normalize_kernels.cu
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    26705 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/ops/csrc/transformer/softmax_kernels.cu
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)    22656 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/ops/csrc/transformer/transform_kernels.cu
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.474686 deepspeed-0.8.3/deepspeed/ops/csrc/utils/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      742 2021-06-23 17:42:12.000000 deepspeed-0.8.3/deepspeed/ops/csrc/utils/flatten_unflatten.cpp
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.483852 deepspeed-0.8.3/deepspeed/ops/lamb/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)       80 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/ops/lamb/__init__.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     8481 2023-02-28 00:54:52.000000 deepspeed-0.8.3/deepspeed/ops/lamb/fused_lamb.py
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.538853 deepspeed-0.8.3/deepspeed/ops/op_builder/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1980 2023-03-20 17:58:16.000000 deepspeed-0.8.3/deepspeed/ops/op_builder/__init__.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1136 2023-02-28 00:54:52.000000 deepspeed-0.8.3/deepspeed/ops/op_builder/all_ops.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     3628 2023-02-28 00:54:52.000000 deepspeed-0.8.3/deepspeed/ops/op_builder/async_io.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    28817 2023-02-28 00:54:52.000000 deepspeed-0.8.3/deepspeed/ops/op_builder/builder.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1521 2023-02-28 00:54:52.000000 deepspeed-0.8.3/deepspeed/ops/op_builder/cpu_adagrad.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1498 2023-02-28 00:54:52.000000 deepspeed-0.8.3/deepspeed/ops/op_builder/cpu_adam.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1029 2022-11-14 17:49:34.000000 deepspeed-0.8.3/deepspeed/ops/op_builder/fused_adam.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1247 2022-11-14 17:49:34.000000 deepspeed-0.8.3/deepspeed/ops/op_builder/fused_lamb.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      755 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/ops/op_builder/quantizer.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1105 2022-12-20 18:47:22.000000 deepspeed-0.8.3/deepspeed/ops/op_builder/random_ltd.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     3029 2022-08-05 18:43:49.000000 deepspeed-0.8.3/deepspeed/ops/op_builder/sparse_attn.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1532 2022-11-14 17:49:34.000000 deepspeed-0.8.3/deepspeed/ops/op_builder/spatial_inference.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      521 2022-04-26 17:01:33.000000 deepspeed-0.8.3/deepspeed/ops/op_builder/stochastic_transformer.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1344 2022-08-05 18:43:49.000000 deepspeed-0.8.3/deepspeed/ops/op_builder/transformer.py
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)     2652 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/ops/op_builder/transformer_inference.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      387 2021-06-23 17:42:12.000000 deepspeed-0.8.3/deepspeed/ops/op_builder/utils.py
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.538853 deepspeed-0.8.3/deepspeed/ops/quantizer/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)       82 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/ops/quantizer/__init__.py
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)     1149 2023-02-28 00:54:52.000000 deepspeed-0.8.3/deepspeed/ops/quantizer/quantizer.py
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.538853 deepspeed-0.8.3/deepspeed/ops/random_ltd/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      141 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/ops/random_ltd/__init__.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     5495 2023-02-28 00:54:52.000000 deepspeed-0.8.3/deepspeed/ops/random_ltd/dropping_utils.py
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.566352 deepspeed-0.8.3/deepspeed/ops/sparse_attention/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      417 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/ops/sparse_attention/__init__.py
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)     3494 2022-07-06 20:47:37.000000 deepspeed-0.8.3/deepspeed/ops/sparse_attention/bert_sparse_self_attention.py
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)    36731 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/ops/sparse_attention/matmul.py
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)    12381 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/ops/sparse_attention/softmax.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    12537 2022-08-05 18:43:48.000000 deepspeed-0.8.3/deepspeed/ops/sparse_attention/sparse_attention_utils.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     6990 2022-08-05 18:43:48.000000 deepspeed-0.8.3/deepspeed/ops/sparse_attention/sparse_self_attention.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    42911 2022-08-05 18:43:48.000000 deepspeed-0.8.3/deepspeed/ops/sparse_attention/sparsity_config.py
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.593852 deepspeed-0.8.3/deepspeed/ops/sparse_attention/trsrc/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      982 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/ops/sparse_attention/trsrc/__init__.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     6530 2021-06-23 17:42:12.000000 deepspeed-0.8.3/deepspeed/ops/sparse_attention/trsrc/matmul.tr
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1825 2021-06-23 17:42:12.000000 deepspeed-0.8.3/deepspeed/ops/sparse_attention/trsrc/softmax_bwd.tr
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     3949 2021-06-23 17:42:12.000000 deepspeed-0.8.3/deepspeed/ops/sparse_attention/trsrc/softmax_fwd.tr
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.603019 deepspeed-0.8.3/deepspeed/ops/transformer/
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)      363 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/ops/transformer/__init__.py
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.621353 deepspeed-0.8.3/deepspeed/ops/transformer/inference/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      265 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/ops/transformer/inference/__init__.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1007 2023-02-28 00:54:52.000000 deepspeed-0.8.3/deepspeed/ops/transformer/inference/bias_add.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     5353 2023-01-05 00:22:07.000000 deepspeed-0.8.3/deepspeed/ops/transformer/inference/config.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      192 2022-11-14 17:49:34.000000 deepspeed-0.8.3/deepspeed/ops/transformer/inference/diffusers_2d_transformer.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    11127 2023-02-28 00:54:52.000000 deepspeed-0.8.3/deepspeed/ops/transformer/inference/diffusers_attention.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     5938 2023-02-28 00:54:52.000000 deepspeed-0.8.3/deepspeed/ops/transformer/inference/diffusers_transformer_block.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    12628 2023-03-02 00:27:13.000000 deepspeed-0.8.3/deepspeed/ops/transformer/inference/ds_attention.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4539 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/ops/transformer/inference/ds_mlp.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    21296 2023-02-28 00:54:52.000000 deepspeed-0.8.3/deepspeed/ops/transformer/inference/moe_inference.py
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.639686 deepspeed-0.8.3/deepspeed/ops/transformer/inference/op_binding/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      332 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/ops/transformer/inference/op_binding/__init__.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      501 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/ops/transformer/inference/op_binding/base.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1237 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/ops/transformer/inference/op_binding/gelu_gemm.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1067 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/ops/transformer/inference/op_binding/linear.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1662 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/ops/transformer/inference/op_binding/mlp_gemm.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1656 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/ops/transformer/inference/op_binding/qkv_gemm.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1477 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/ops/transformer/inference/op_binding/residual_add.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1451 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/ops/transformer/inference/op_binding/softmax.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2004 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/ops/transformer/inference/op_binding/softmax_context.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      747 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/ops/transformer/inference/op_binding/vector_matmul.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4462 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/ops/transformer/inference/triton_ops.py
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)    25317 2023-02-28 00:54:52.000000 deepspeed-0.8.3/deepspeed/ops/transformer/transformer.py
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.639686 deepspeed-0.8.3/deepspeed/pipe/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      114 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/pipe/__init__.py
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.648852 deepspeed-0.8.3/deepspeed/profiling/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)       45 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/profiling/__init__.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2073 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/profiling/config.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1124 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/profiling/constants.py
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.648852 deepspeed-0.8.3/deepspeed/profiling/flops_profiler/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)       70 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/profiling/flops_profiler/__init__.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    48141 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/profiling/flops_profiler/profiler.py
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.722186 deepspeed-0.8.3/deepspeed/runtime/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      142 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/runtime/__init__.py
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.722186 deepspeed-0.8.3/deepspeed/runtime/activation_checkpointing/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)       45 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/runtime/activation_checkpointing/__init__.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    33758 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/runtime/activation_checkpointing/checkpointing.py
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)     4004 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/runtime/activation_checkpointing/config.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    19274 2023-03-02 00:27:13.000000 deepspeed-0.8.3/deepspeed/runtime/bf16_optimizer.py
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.731352 deepspeed-0.8.3/deepspeed/runtime/checkpoint_engine/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)       45 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/runtime/checkpoint_engine/__init__.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      604 2023-03-20 17:58:16.000000 deepspeed-0.8.3/deepspeed/runtime/checkpoint_engine/checkpoint_engine.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     5142 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/runtime/checkpoint_engine/nebula_checkpoint_engine.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1009 2023-03-20 17:58:16.000000 deepspeed-0.8.3/deepspeed/runtime/checkpoint_engine/torch_checkpoint_engine.py
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.768019 deepspeed-0.8.3/deepspeed/runtime/comm/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)       45 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/runtime/comm/__init__.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4119 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/runtime/comm/coalesced_collectives.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    11838 2021-06-23 17:42:12.000000 deepspeed-0.8.3/deepspeed/runtime/comm/mpi.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     8376 2023-02-28 00:54:52.000000 deepspeed-0.8.3/deepspeed/runtime/comm/nccl.py
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.777186 deepspeed-0.8.3/deepspeed/runtime/compression/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)       45 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/runtime/compression/__init__.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      657 2021-06-23 17:42:12.000000 deepspeed-0.8.3/deepspeed/runtime/compression/cupy.py
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)    40817 2023-03-20 17:58:16.000000 deepspeed-0.8.3/deepspeed/runtime/config.py
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)     8190 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/runtime/config_utils.py
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)    12846 2023-03-20 17:58:16.000000 deepspeed-0.8.3/deepspeed/runtime/constants.py
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.777186 deepspeed-0.8.3/deepspeed/runtime/data_pipeline/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)       45 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/runtime/data_pipeline/__init__.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     6508 2022-12-20 18:47:22.000000 deepspeed-0.8.3/deepspeed/runtime/data_pipeline/config.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4738 2022-12-20 18:47:22.000000 deepspeed-0.8.3/deepspeed/runtime/data_pipeline/constants.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    10598 2022-12-20 18:47:22.000000 deepspeed-0.8.3/deepspeed/runtime/data_pipeline/curriculum_scheduler.py
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.786352 deepspeed-0.8.3/deepspeed/runtime/data_pipeline/data_routing/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)       45 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/runtime/data_pipeline/data_routing/__init__.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     5528 2022-12-20 18:47:22.000000 deepspeed-0.8.3/deepspeed/runtime/data_pipeline/data_routing/basic_layer.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1239 2022-12-20 18:47:22.000000 deepspeed-0.8.3/deepspeed/runtime/data_pipeline/data_routing/helper.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4696 2022-12-20 18:47:22.000000 deepspeed-0.8.3/deepspeed/runtime/data_pipeline/data_routing/scheduler.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      943 2022-12-20 18:47:22.000000 deepspeed-0.8.3/deepspeed/runtime/data_pipeline/data_routing/utils.py
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.804686 deepspeed-0.8.3/deepspeed/runtime/data_pipeline/data_sampling/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)       45 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/runtime/data_pipeline/data_sampling/__init__.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    28984 2023-02-28 00:54:52.000000 deepspeed-0.8.3/deepspeed/runtime/data_pipeline/data_sampling/data_analyzer.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    20741 2023-02-28 00:54:52.000000 deepspeed-0.8.3/deepspeed/runtime/data_pipeline/data_sampling/data_sampler.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    21329 2022-12-20 18:47:22.000000 deepspeed-0.8.3/deepspeed/runtime/data_pipeline/data_sampling/indexed_dataset.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1792 2022-12-20 18:47:22.000000 deepspeed-0.8.3/deepspeed/runtime/data_pipeline/data_sampling/utils.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     6692 2023-02-28 00:54:52.000000 deepspeed-0.8.3/deepspeed/runtime/dataloader.py
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)     5806 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/runtime/eigenvalue.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)   154686 2023-03-20 17:58:16.000000 deepspeed-0.8.3/deepspeed/runtime/engine.py
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.804686 deepspeed-0.8.3/deepspeed/runtime/fp16/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)       45 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/runtime/fp16/__init__.py
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)    20925 2023-02-28 00:54:52.000000 deepspeed-0.8.3/deepspeed/runtime/fp16/fused_optimizer.py
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)    11016 2023-03-20 17:58:16.000000 deepspeed-0.8.3/deepspeed/runtime/fp16/loss_scaler.py
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.804686 deepspeed-0.8.3/deepspeed/runtime/fp16/onebit/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      136 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/runtime/fp16/onebit/__init__.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    15520 2023-02-28 00:54:52.000000 deepspeed-0.8.3/deepspeed/runtime/fp16/onebit/adam.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    23899 2023-02-28 00:54:52.000000 deepspeed-0.8.3/deepspeed/runtime/fp16/onebit/lamb.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    19626 2023-02-28 00:54:52.000000 deepspeed-0.8.3/deepspeed/runtime/fp16/onebit/zoadam.py
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)    18859 2023-02-28 00:54:52.000000 deepspeed-0.8.3/deepspeed/runtime/fp16/unfused_optimizer.py
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)    35480 2022-08-05 18:43:48.000000 deepspeed-0.8.3/deepspeed/runtime/lr_schedules.py
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.859686 deepspeed-0.8.3/deepspeed/runtime/pipe/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      145 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/runtime/pipe/__init__.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    57914 2023-02-28 00:54:52.000000 deepspeed-0.8.3/deepspeed/runtime/pipe/engine.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    28069 2023-03-20 17:58:16.000000 deepspeed-0.8.3/deepspeed/runtime/pipe/module.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     5498 2023-02-28 00:54:52.000000 deepspeed-0.8.3/deepspeed/runtime/pipe/p2p.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    15590 2023-03-20 17:58:16.000000 deepspeed-0.8.3/deepspeed/runtime/pipe/schedule.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    17234 2022-08-05 18:43:48.000000 deepspeed-0.8.3/deepspeed/runtime/pipe/topology.py
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)     1301 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/runtime/progressive_layer_drop.py
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)     7687 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/runtime/quantize.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2480 2022-07-06 20:47:37.000000 deepspeed-0.8.3/deepspeed/runtime/sparse_tensor.py
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)    19721 2022-12-20 18:47:22.000000 deepspeed-0.8.3/deepspeed/runtime/state_dict_factory.py
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.887186 deepspeed-0.8.3/deepspeed/runtime/swap_tensor/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)       85 2021-06-23 17:42:12.000000 deepspeed-0.8.3/deepspeed/runtime/swap_tensor/__init__.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1512 2021-06-23 17:42:12.000000 deepspeed-0.8.3/deepspeed/runtime/swap_tensor/aio_config.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     6294 2022-08-05 18:43:48.000000 deepspeed-0.8.3/deepspeed/runtime/swap_tensor/async_swapper.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      665 2021-06-23 17:42:12.000000 deepspeed-0.8.3/deepspeed/runtime/swap_tensor/constants.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    19951 2022-08-05 18:43:48.000000 deepspeed-0.8.3/deepspeed/runtime/swap_tensor/optimizer_utils.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    10494 2023-02-28 00:54:52.000000 deepspeed-0.8.3/deepspeed/runtime/swap_tensor/partitioned_optimizer_swapper.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    18285 2023-02-28 00:54:52.000000 deepspeed-0.8.3/deepspeed/runtime/swap_tensor/partitioned_param_swapper.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    11409 2023-02-28 00:54:52.000000 deepspeed-0.8.3/deepspeed/runtime/swap_tensor/pipelined_optimizer_swapper.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     7930 2023-02-28 00:54:52.000000 deepspeed-0.8.3/deepspeed/runtime/swap_tensor/utils.py
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)    36627 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/runtime/utils.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     7528 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/runtime/weight_quantizer.py
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.942186 deepspeed-0.8.3/deepspeed/runtime/zero/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      414 2021-06-23 17:42:12.000000 deepspeed-0.8.3/deepspeed/runtime/zero/__init__.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    10191 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/runtime/zero/config.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    10979 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/runtime/zero/contiguous_memory_allocator.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     8027 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/runtime/zero/linear.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2956 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/runtime/zero/offload_config.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    21793 2023-02-28 00:54:52.000000 deepspeed-0.8.3/deepspeed/runtime/zero/parameter_offload.py
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)    70816 2023-02-28 00:54:52.000000 deepspeed-0.8.3/deepspeed/runtime/zero/partition_parameters.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    22672 2023-02-28 00:54:52.000000 deepspeed-0.8.3/deepspeed/runtime/zero/partitioned_param_coordinator.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)   115248 2023-03-20 17:58:16.000000 deepspeed-0.8.3/deepspeed/runtime/zero/stage3.py
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)   113383 2023-03-20 17:58:16.000000 deepspeed-0.8.3/deepspeed/runtime/zero/stage_1_and_2.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2677 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/runtime/zero/test.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    11931 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/runtime/zero/tiling.py
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)     2953 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/runtime/zero/utils.py
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.978852 deepspeed-0.8.3/deepspeed/utils/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      667 2023-03-02 00:27:13.000000 deepspeed-0.8.3/deepspeed/utils/__init__.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     6053 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/utils/comms_logging.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4385 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/utils/debug.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      101 2022-07-06 20:47:37.000000 deepspeed-0.8.3/deepspeed/utils/exceptions.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    15668 2022-08-05 18:43:48.000000 deepspeed-0.8.3/deepspeed/utils/groups.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     3056 2022-08-05 18:43:48.000000 deepspeed-0.8.3/deepspeed/utils/init_on_device.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4311 2023-03-20 17:58:16.000000 deepspeed-0.8.3/deepspeed/utils/logging.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2699 2023-03-02 00:27:13.000000 deepspeed-0.8.3/deepspeed/utils/mixed_precision_linkage.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      448 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/utils/nvtx.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     8812 2023-03-02 00:27:13.000000 deepspeed-0.8.3/deepspeed/utils/tensor_fragment.py
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)     8912 2023-02-28 00:54:52.000000 deepspeed-0.8.3/deepspeed/utils/timer.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      150 2023-02-28 00:58:40.000000 deepspeed-0.8.3/deepspeed/utils/types.py
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)    18902 2023-03-02 00:27:13.000000 deepspeed-0.8.3/deepspeed/utils/zero_to_fp32.py
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:47.007186 deepspeed-0.8.3/deepspeed.egg-info/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    24840 2023-03-20 17:59:45.000000 deepspeed-0.8.3/deepspeed.egg-info/PKG-INFO
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    20510 2023-03-20 17:59:45.000000 deepspeed-0.8.3/deepspeed.egg-info/SOURCES.txt
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)        1 2023-03-20 17:59:45.000000 deepspeed-0.8.3/deepspeed.egg-info/dependency_links.txt
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)       85 2023-03-20 17:59:45.000000 deepspeed-0.8.3/deepspeed.egg-info/entry_points.txt
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1094 2023-03-20 17:59:45.000000 deepspeed-0.8.3/deepspeed.egg-info/requires.txt
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)       10 2023-03-20 17:59:45.000000 deepspeed-0.8.3/deepspeed.egg-info/top_level.txt
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:48.070519 deepspeed-0.8.3/op_builder/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1980 2023-03-20 17:58:16.000000 deepspeed-0.8.3/op_builder/__init__.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1136 2023-02-28 00:54:52.000000 deepspeed-0.8.3/op_builder/all_ops.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     3628 2023-02-28 00:54:52.000000 deepspeed-0.8.3/op_builder/async_io.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)    28817 2023-02-28 00:54:52.000000 deepspeed-0.8.3/op_builder/builder.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1521 2023-02-28 00:54:52.000000 deepspeed-0.8.3/op_builder/cpu_adagrad.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1498 2023-02-28 00:54:52.000000 deepspeed-0.8.3/op_builder/cpu_adam.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1029 2022-11-14 17:49:34.000000 deepspeed-0.8.3/op_builder/fused_adam.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1247 2022-11-14 17:49:34.000000 deepspeed-0.8.3/op_builder/fused_lamb.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      755 2023-02-28 00:58:40.000000 deepspeed-0.8.3/op_builder/quantizer.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1105 2022-12-20 18:47:22.000000 deepspeed-0.8.3/op_builder/random_ltd.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     3029 2022-08-05 18:43:49.000000 deepspeed-0.8.3/op_builder/sparse_attn.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1532 2022-11-14 17:49:34.000000 deepspeed-0.8.3/op_builder/spatial_inference.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      521 2022-04-26 17:01:33.000000 deepspeed-0.8.3/op_builder/stochastic_transformer.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1344 2022-08-05 18:43:49.000000 deepspeed-0.8.3/op_builder/transformer.py
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)     2652 2023-02-28 00:58:40.000000 deepspeed-0.8.3/op_builder/transformer_inference.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      387 2021-06-23 17:42:12.000000 deepspeed-0.8.3/op_builder/utils.py
-drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-03-20 17:59:48.079686 deepspeed-0.8.3/requirements/
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)        7 2022-07-06 20:47:37.000000 deepspeed-0.8.3/requirements/requirements-1bit-mpi.txt
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)       23 2022-07-06 20:47:37.000000 deepspeed-0.8.3/requirements/requirements-autotuning-ml.txt
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        9 2022-07-06 20:47:37.000000 deepspeed-0.8.3/requirements/requirements-autotuning.txt
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      232 2022-11-14 17:49:34.000000 deepspeed-0.8.3/requirements/requirements-dev.txt
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)       72 2023-02-28 00:54:52.000000 deepspeed-0.8.3/requirements/requirements-inf.txt
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)       85 2022-11-23 21:50:53.000000 deepspeed-0.8.3/requirements/requirements-readthedocs.txt
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)       36 2022-11-14 17:49:34.000000 deepspeed-0.8.3/requirements/requirements-sd.txt
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)       14 2022-07-06 20:47:37.000000 deepspeed-0.8.3/requirements/requirements-sparse_attn.txt
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)       72 2023-02-28 00:54:52.000000 deepspeed-0.8.3/requirements/requirements.txt
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)      148 2023-03-20 17:59:48.088853 deepspeed-0.8.3/setup.cfg
--rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)    11186 2023-03-20 17:58:16.000000 deepspeed-0.8.3/setup.py
--rw-r--r--   0 jerasley  (1000) jerasley  (1000)        6 2023-03-07 18:16:05.000000 deepspeed-0.8.3/version.txt
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.747389 deepspeed-0.9.0/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    11357 2023-04-11 17:18:49.000000 deepspeed-0.9.0/LICENSE
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      290 2023-02-28 00:54:52.000000 deepspeed-0.9.0/MANIFEST.in
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    27085 2023-04-13 15:30:33.747389 deepspeed-0.9.0/PKG-INFO
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)    26202 2023-04-13 15:29:56.000000 deepspeed-0.9.0/README.md
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.577389 deepspeed-0.9.0/accelerator/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      214 2023-04-11 17:18:49.000000 deepspeed-0.9.0/accelerator/__init__.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4532 2023-04-11 17:18:49.000000 deepspeed-0.9.0/accelerator/abstract_accelerator.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     8029 2023-04-13 15:29:56.000000 deepspeed-0.9.0/accelerator/cuda_accelerator.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     3668 2023-04-11 17:18:49.000000 deepspeed-0.9.0/accelerator/real_accelerator.py
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.577389 deepspeed-0.9.0/bin/
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)      106 2022-07-06 20:47:37.000000 deepspeed-0.9.0/bin/deepspeed
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)      106 2022-07-06 20:47:37.000000 deepspeed-0.9.0/bin/deepspeed.pt
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)      106 2022-07-06 20:47:37.000000 deepspeed-0.9.0/bin/ds
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)      757 2022-08-05 18:43:48.000000 deepspeed-0.9.0/bin/ds_bench
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)     1869 2023-04-11 17:18:49.000000 deepspeed-0.9.0/bin/ds_elastic
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)      109 2022-07-06 20:47:37.000000 deepspeed-0.9.0/bin/ds_report
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)      680 2022-07-06 20:47:37.000000 deepspeed-0.9.0/bin/ds_ssh
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)      109 2022-07-06 20:47:37.000000 deepspeed-0.9.0/bin/dsr
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:32.000000 deepspeed-0.9.0/build.txt
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.547389 deepspeed-0.9.0/csrc/
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.577389 deepspeed-0.9.0/csrc/adagrad/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     8438 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/adagrad/cpu_adagrad.cpp
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.577389 deepspeed-0.9.0/csrc/adam/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    10428 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/adam/cpu_adam.cpp
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      880 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/adam/fused_adam_frontend.cpp
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     6573 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/adam/multi_tensor_adam.cu
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     5595 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/adam/multi_tensor_apply.cuh
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.537389 deepspeed-0.9.0/csrc/aio/
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.577389 deepspeed-0.9.0/csrc/aio/common/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    13040 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/aio/common/deepspeed_aio_common.cpp
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1364 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/aio/common/deepspeed_aio_common.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2033 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/aio/common/deepspeed_aio_types.cpp
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1402 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/aio/common/deepspeed_aio_types.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4330 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/aio/common/deepspeed_aio_utils.cpp
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2086 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/aio/common/deepspeed_aio_utils.h
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.587389 deepspeed-0.9.0/csrc/aio/py_lib/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2707 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/aio/py_lib/deepspeed_aio_thread.cpp
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1427 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/aio/py_lib/deepspeed_aio_thread.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1240 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/aio/py_lib/deepspeed_pin_tensor.cpp
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      722 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/aio/py_lib/deepspeed_pin_tensor.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4326 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/aio/py_lib/deepspeed_py_aio.cpp
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1052 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/aio/py_lib/deepspeed_py_aio.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    10113 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/aio/py_lib/deepspeed_py_aio_handle.cpp
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2459 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/aio/py_lib/deepspeed_py_aio_handle.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4422 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/aio/py_lib/deepspeed_py_copy.cpp
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1246 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/aio/py_lib/deepspeed_py_copy.h
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)     1805 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/aio/py_lib/py_ds_aio.cpp
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.587389 deepspeed-0.9.0/csrc/common/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1278 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/common/custom_cuda_kernel.cu
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.587389 deepspeed-0.9.0/csrc/includes/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1981 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/includes/StopWatch.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1180 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/includes/Timer.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      336 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/includes/compat.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     6957 2023-04-13 15:29:56.000000 deepspeed-0.9.0/csrc/includes/context.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    12108 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/includes/conversion_utils.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     5615 2023-04-13 15:29:56.000000 deepspeed-0.9.0/csrc/includes/cpu_adagrad.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     8137 2023-04-13 15:29:56.000000 deepspeed-0.9.0/csrc/includes/cpu_adam.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     3277 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/includes/cublas_wrappers.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    13156 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/includes/custom_cuda_layers.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     7301 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/includes/dequantization_utils.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2195 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/includes/dropout.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1178 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/includes/ds_kernel_utils.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     6164 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/includes/ds_transformer_cuda.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     3189 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/includes/feed_forward.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1018 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/includes/gelu.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    10326 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/includes/gemm_test.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1507 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/includes/general_kernels.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    33966 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/includes/memory_access_utils.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     7089 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/includes/normalize_layer.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2293 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/includes/quantization.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    17384 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/includes/quantization_utils.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      264 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/includes/quantizer.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    18641 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/includes/reduction_utils.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4526 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/includes/simd.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1642 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/includes/softmax.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     6807 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/includes/strided_batch_gemm.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     6388 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/includes/type_shim.h
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.587389 deepspeed-0.9.0/csrc/lamb/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4002 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/lamb/fused_lamb_cuda.cpp
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    15291 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/lamb/fused_lamb_cuda_kernel.cu
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.597389 deepspeed-0.9.0/csrc/quantization/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     3302 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/quantization/dequantize.cu
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    37581 2023-04-13 15:29:56.000000 deepspeed-0.9.0/csrc/quantization/fake_quantizer.cu
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     5784 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/quantization/pt_binding.cpp
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     6988 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/quantization/quantize.cu
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.597389 deepspeed-0.9.0/csrc/random_ltd/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     8408 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/random_ltd/gather_scatter.cu
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     9590 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/random_ltd/pt_binding.cpp
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     5076 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/random_ltd/slice_attn_masks.cu
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     7027 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/random_ltd/token_sort.cu
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.597389 deepspeed-0.9.0/csrc/sparse_attention/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4523 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/sparse_attention/utils.cpp
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.537389 deepspeed-0.9.0/csrc/spatial/
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.597389 deepspeed-0.9.0/csrc/spatial/csrc/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     6273 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/spatial/csrc/opt_bias_add.cu
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     3863 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/spatial/csrc/pt_binding.cpp
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.597389 deepspeed-0.9.0/csrc/spatial/includes/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      833 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/spatial/includes/spatial_cuda_layers.h
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.597389 deepspeed-0.9.0/csrc/transformer/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    17572 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/transformer/cublas_wrappers.cu
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    29835 2023-04-13 15:29:56.000000 deepspeed-0.9.0/csrc/transformer/dropout_kernels.cu
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    47589 2023-04-13 15:29:56.000000 deepspeed-0.9.0/csrc/transformer/ds_transformer_cuda.cpp
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    12191 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/transformer/gelu_kernels.cu
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    14532 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/transformer/general_kernels.cu
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.537389 deepspeed-0.9.0/csrc/transformer/inference/
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.597389 deepspeed-0.9.0/csrc/transformer/inference/csrc/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    16338 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/transformer/inference/csrc/apply_rotary_pos_emb.cu
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     6908 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/transformer/inference/csrc/dequantize.cu
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    29045 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/transformer/inference/csrc/gelu.cu
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    22685 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/transformer/inference/csrc/layer_norm.cu
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    77643 2023-04-13 15:29:56.000000 deepspeed-0.9.0/csrc/transformer/inference/csrc/pt_binding.cpp
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2195 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/transformer/inference/csrc/relu.cu
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    23789 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/transformer/inference/csrc/softmax.cu
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    29531 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/transformer/inference/csrc/transform.cu
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.597389 deepspeed-0.9.0/csrc/transformer/inference/includes/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    10279 2023-04-13 15:29:56.000000 deepspeed-0.9.0/csrc/transformer/inference/includes/inference_context.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    17739 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/transformer/inference/includes/inference_cublas_wrappers.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     8265 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/transformer/inference/includes/inference_cuda_layers.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    74900 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/transformer/normalize_kernels.cu
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    26758 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/transformer/softmax_kernels.cu
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    22709 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/transformer/transform_kernels.cu
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.597389 deepspeed-0.9.0/csrc/utils/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      788 2023-04-11 17:18:49.000000 deepspeed-0.9.0/csrc/utils/flatten_unflatten.cpp
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.597389 deepspeed-0.9.0/deepspeed/
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)    13947 2023-04-13 15:29:56.000000 deepspeed-0.9.0/deepspeed/__init__.py
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.607389 deepspeed-0.9.0/deepspeed/accelerator/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      214 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/accelerator/__init__.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4532 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/accelerator/abstract_accelerator.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     8029 2023-04-13 15:29:56.000000 deepspeed-0.9.0/deepspeed/accelerator/cuda_accelerator.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     3668 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/accelerator/real_accelerator.py
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.607389 deepspeed-0.9.0/deepspeed/autotuning/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      129 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/autotuning/__init__.py
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)    54204 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/autotuning/autotuner.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4633 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/autotuning/config.py
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.607389 deepspeed-0.9.0/deepspeed/autotuning/config_templates/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)       48 2022-07-06 20:47:37.000000 deepspeed-0.9.0/deepspeed/autotuning/config_templates/template_zero0.json
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      113 2022-07-06 20:47:37.000000 deepspeed-0.9.0/deepspeed/autotuning/config_templates/template_zero1.json
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      237 2022-07-06 20:47:37.000000 deepspeed-0.9.0/deepspeed/autotuning/config_templates/template_zero2.json
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      485 2022-07-06 20:47:37.000000 deepspeed-0.9.0/deepspeed/autotuning/config_templates/template_zero3.json
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     5945 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/autotuning/constants.py
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)    15644 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/autotuning/scheduler.py
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.607389 deepspeed-0.9.0/deepspeed/autotuning/tuner/
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)      235 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/autotuning/tuner/__init__.py
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)     2756 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/autotuning/tuner/base_tuner.py
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)     1820 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/autotuning/tuner/cost_model.py
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)     1158 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/autotuning/tuner/index_based_tuner.py
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)     5614 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/autotuning/tuner/model_based_tuner.py
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)     2329 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/autotuning/tuner/utils.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    15054 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/autotuning/utils.py
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.617389 deepspeed-0.9.0/deepspeed/checkpoint/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      561 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/checkpoint/__init__.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2335 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/checkpoint/constants.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    12009 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/checkpoint/deepspeed_checkpoint.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4674 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/checkpoint/reshape_3d_utils.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     7885 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/checkpoint/reshape_meg_2d.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2888 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/checkpoint/reshape_utils.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4888 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/checkpoint/universal_checkpoint.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      936 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/checkpoint/utils.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     5316 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/checkpoint/zero_checkpoint.py
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.617389 deepspeed-0.9.0/deepspeed/comm/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2124 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/comm/__init__.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1416 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/comm/backend.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    26108 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/comm/comm.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      855 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/comm/config.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1256 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/comm/constants.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    10310 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/comm/torch.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4184 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/comm/utils.py
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.617389 deepspeed-0.9.0/deepspeed/compression/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      243 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/compression/__init__.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    36034 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/compression/basic_layer.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    10519 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/compression/compress.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    23240 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/compression/config.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4959 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/compression/constants.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    12303 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/compression/helper.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     8039 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/compression/scheduler.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     7814 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/compression/utils.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      733 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/constants.py
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.617389 deepspeed-0.9.0/deepspeed/elasticity/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      383 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/elasticity/__init__.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4701 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/elasticity/config.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2452 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/elasticity/constants.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     7762 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/elasticity/elastic_agent.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    17374 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/elasticity/elasticity.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      459 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/elasticity/utils.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4793 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/env_report.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      756 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/git_version_info.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      790 2023-04-13 15:30:32.000000 deepspeed-0.9.0/deepspeed/git_version_info_installed.py
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.617389 deepspeed-0.9.0/deepspeed/inference/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      132 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/inference/__init__.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     9506 2023-04-13 15:29:56.000000 deepspeed-0.9.0/deepspeed/inference/config.py
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)    28924 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/inference/engine.py
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.617389 deepspeed-0.9.0/deepspeed/launcher/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      140 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/launcher/__init__.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      352 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/launcher/constants.py
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)    19078 2023-04-13 15:29:56.000000 deepspeed-0.9.0/deepspeed/launcher/launch.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    11952 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/launcher/multinode_runner.py
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)    23296 2023-04-13 15:29:56.000000 deepspeed-0.9.0/deepspeed/launcher/runner.py
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.617389 deepspeed-0.9.0/deepspeed/model_implementations/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      220 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/model_implementations/__init__.py
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.617389 deepspeed-0.9.0/deepspeed/model_implementations/diffusers/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      140 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/model_implementations/diffusers/__init__.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2792 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/model_implementations/diffusers/unet.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     6025 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/model_implementations/diffusers/vae.py
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.627389 deepspeed-0.9.0/deepspeed/model_implementations/features/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      140 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/model_implementations/features/__init__.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      563 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/model_implementations/features/cuda_graph.py
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.627389 deepspeed-0.9.0/deepspeed/model_implementations/transformers/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      140 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/model_implementations/transformers/__init__.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     3045 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/model_implementations/transformers/clip_encoder.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      388 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/model_implementations/transformers/ds_base.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      667 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/model_implementations/transformers/ds_bert.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      669 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/model_implementations/transformers/ds_bloom.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      665 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/model_implementations/transformers/ds_gpt.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      682 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/model_implementations/transformers/ds_megatron_gpt.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      665 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/model_implementations/transformers/ds_opt.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     7828 2023-04-13 15:29:56.000000 deepspeed-0.9.0/deepspeed/model_implementations/transformers/ds_transformer.py
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.627389 deepspeed-0.9.0/deepspeed/module_inject/
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)      444 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/module_inject/__init__.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4815 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/module_inject/auto_tp.py
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.637389 deepspeed-0.9.0/deepspeed/module_inject/containers/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      838 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/module_inject/containers/__init__.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    23292 2023-04-13 15:29:56.000000 deepspeed-0.9.0/deepspeed/module_inject/containers/base.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     5756 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/module_inject/containers/base_moe.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     3718 2023-04-13 15:29:56.000000 deepspeed-0.9.0/deepspeed/module_inject/containers/bert.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4762 2023-04-13 15:29:56.000000 deepspeed-0.9.0/deepspeed/module_inject/containers/bloom.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2807 2023-04-13 15:29:56.000000 deepspeed-0.9.0/deepspeed/module_inject/containers/clip.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     3137 2023-04-13 15:29:56.000000 deepspeed-0.9.0/deepspeed/module_inject/containers/distil_bert.py
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.637389 deepspeed-0.9.0/deepspeed/module_inject/containers/features/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      181 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/module_inject/containers/features/__init__.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1206 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/module_inject/containers/features/megatron.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2486 2023-04-13 15:29:56.000000 deepspeed-0.9.0/deepspeed/module_inject/containers/features/meta_tensor.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2249 2023-04-13 15:29:56.000000 deepspeed-0.9.0/deepspeed/module_inject/containers/gpt2.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     3833 2023-04-13 15:29:56.000000 deepspeed-0.9.0/deepspeed/module_inject/containers/gptj.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4107 2023-04-13 15:29:56.000000 deepspeed-0.9.0/deepspeed/module_inject/containers/gptneo.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     5005 2023-04-13 15:29:56.000000 deepspeed-0.9.0/deepspeed/module_inject/containers/gptneox.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     5045 2023-04-13 15:29:56.000000 deepspeed-0.9.0/deepspeed/module_inject/containers/megatron_gpt.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     3913 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/module_inject/containers/megatron_gpt_moe.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     5660 2023-04-13 15:29:56.000000 deepspeed-0.9.0/deepspeed/module_inject/containers/opt.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1732 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/module_inject/containers/unet.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1163 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/module_inject/containers/vae.py
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)     4719 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/module_inject/inject.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     3825 2023-04-13 15:29:56.000000 deepspeed-0.9.0/deepspeed/module_inject/layers.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    14122 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/module_inject/load_checkpoint.py
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)     3107 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/module_inject/module_quantize.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     7723 2023-04-13 15:29:56.000000 deepspeed-0.9.0/deepspeed/module_inject/policy.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    35947 2023-04-13 15:29:56.000000 deepspeed-0.9.0/deepspeed/module_inject/replace_module.py
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)      930 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/module_inject/replace_policy.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1653 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/module_inject/utils.py
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.637389 deepspeed-0.9.0/deepspeed/moe/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      140 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/moe/__init__.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1223 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/moe/experts.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     6082 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/moe/layer.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     3529 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/moe/mappings.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    20621 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/moe/sharded_moe.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     5214 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/moe/utils.py
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.637389 deepspeed-0.9.0/deepspeed/monitor/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      140 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/monitor/__init__.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2529 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/monitor/config.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2907 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/monitor/csv_monitor.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1604 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/monitor/monitor.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2227 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/monitor/tensorboard.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      754 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/monitor/utils.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1150 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/monitor/wandb.py
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.637389 deepspeed-0.9.0/deepspeed/nebula/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      140 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/nebula/__init__.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1764 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/nebula/config.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2786 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/nebula/constants.py
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.637389 deepspeed-0.9.0/deepspeed/ops/
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)      477 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/__init__.py
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.637389 deepspeed-0.9.0/deepspeed/ops/adagrad/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      141 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/adagrad/__init__.py
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)     5089 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/adagrad/cpu_adagrad.py
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.637389 deepspeed-0.9.0/deepspeed/ops/adam/
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)      169 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/adam/__init__.py
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)     8546 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/adam/cpu_adam.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     6878 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/adam/fused_adam.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      429 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/adam/multi_tensor_apply.py
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.637389 deepspeed-0.9.0/deepspeed/ops/aio/
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)      136 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/aio/__init__.py
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.557389 deepspeed-0.9.0/deepspeed/ops/csrc/
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.647389 deepspeed-0.9.0/deepspeed/ops/csrc/adagrad/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     8438 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/adagrad/cpu_adagrad.cpp
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.647389 deepspeed-0.9.0/deepspeed/ops/csrc/adam/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    10428 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/adam/cpu_adam.cpp
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      880 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/adam/fused_adam_frontend.cpp
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     6573 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/adam/multi_tensor_adam.cu
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     5595 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/adam/multi_tensor_apply.cuh
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.557389 deepspeed-0.9.0/deepspeed/ops/csrc/aio/
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.647389 deepspeed-0.9.0/deepspeed/ops/csrc/aio/common/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    13040 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/aio/common/deepspeed_aio_common.cpp
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1364 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/aio/common/deepspeed_aio_common.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2033 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/aio/common/deepspeed_aio_types.cpp
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1402 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/aio/common/deepspeed_aio_types.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4330 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/aio/common/deepspeed_aio_utils.cpp
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2086 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/aio/common/deepspeed_aio_utils.h
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.657389 deepspeed-0.9.0/deepspeed/ops/csrc/aio/py_lib/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2707 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/aio/py_lib/deepspeed_aio_thread.cpp
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1427 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/aio/py_lib/deepspeed_aio_thread.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1240 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/aio/py_lib/deepspeed_pin_tensor.cpp
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      722 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/aio/py_lib/deepspeed_pin_tensor.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4326 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/aio/py_lib/deepspeed_py_aio.cpp
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1052 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/aio/py_lib/deepspeed_py_aio.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    10113 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/aio/py_lib/deepspeed_py_aio_handle.cpp
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2459 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/aio/py_lib/deepspeed_py_aio_handle.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4422 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/aio/py_lib/deepspeed_py_copy.cpp
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1246 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/aio/py_lib/deepspeed_py_copy.h
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)     1805 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/aio/py_lib/py_ds_aio.cpp
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.657389 deepspeed-0.9.0/deepspeed/ops/csrc/aio/py_test/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      359 2021-08-17 18:54:40.000000 deepspeed-0.9.0/deepspeed/ops/csrc/aio/py_test/single_process_config.json
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.657389 deepspeed-0.9.0/deepspeed/ops/csrc/common/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1278 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/common/custom_cuda_kernel.cu
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.667389 deepspeed-0.9.0/deepspeed/ops/csrc/includes/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1981 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/includes/StopWatch.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1180 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/includes/Timer.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      336 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/includes/compat.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     6957 2023-04-13 15:29:56.000000 deepspeed-0.9.0/deepspeed/ops/csrc/includes/context.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    12108 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/includes/conversion_utils.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     5615 2023-04-13 15:29:56.000000 deepspeed-0.9.0/deepspeed/ops/csrc/includes/cpu_adagrad.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     8137 2023-04-13 15:29:56.000000 deepspeed-0.9.0/deepspeed/ops/csrc/includes/cpu_adam.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     3277 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/includes/cublas_wrappers.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    13156 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/includes/custom_cuda_layers.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     7301 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/includes/dequantization_utils.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2195 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/includes/dropout.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1178 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/includes/ds_kernel_utils.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     6164 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/includes/ds_transformer_cuda.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     3189 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/includes/feed_forward.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1018 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/includes/gelu.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    10326 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/includes/gemm_test.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1507 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/includes/general_kernels.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    33966 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/includes/memory_access_utils.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     7089 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/includes/normalize_layer.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2293 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/includes/quantization.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    17384 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/includes/quantization_utils.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      264 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/includes/quantizer.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    18641 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/includes/reduction_utils.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4526 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/includes/simd.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1642 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/includes/softmax.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     6807 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/includes/strided_batch_gemm.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     6388 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/includes/type_shim.h
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.667389 deepspeed-0.9.0/deepspeed/ops/csrc/lamb/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4002 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/lamb/fused_lamb_cuda.cpp
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    15291 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/lamb/fused_lamb_cuda_kernel.cu
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.667389 deepspeed-0.9.0/deepspeed/ops/csrc/quantization/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     3302 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/quantization/dequantize.cu
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    37581 2023-04-13 15:29:56.000000 deepspeed-0.9.0/deepspeed/ops/csrc/quantization/fake_quantizer.cu
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     5784 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/quantization/pt_binding.cpp
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     6988 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/quantization/quantize.cu
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.667389 deepspeed-0.9.0/deepspeed/ops/csrc/random_ltd/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     8408 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/random_ltd/gather_scatter.cu
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     9590 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/random_ltd/pt_binding.cpp
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     5076 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/random_ltd/slice_attn_masks.cu
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     7027 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/random_ltd/token_sort.cu
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.677389 deepspeed-0.9.0/deepspeed/ops/csrc/sparse_attention/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4523 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/sparse_attention/utils.cpp
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.557389 deepspeed-0.9.0/deepspeed/ops/csrc/spatial/
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.677389 deepspeed-0.9.0/deepspeed/ops/csrc/spatial/csrc/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     6273 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/spatial/csrc/opt_bias_add.cu
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     3863 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/spatial/csrc/pt_binding.cpp
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.677389 deepspeed-0.9.0/deepspeed/ops/csrc/spatial/includes/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      833 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/spatial/includes/spatial_cuda_layers.h
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.677389 deepspeed-0.9.0/deepspeed/ops/csrc/transformer/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    17572 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/transformer/cublas_wrappers.cu
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    29835 2023-04-13 15:29:56.000000 deepspeed-0.9.0/deepspeed/ops/csrc/transformer/dropout_kernels.cu
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    47589 2023-04-13 15:29:56.000000 deepspeed-0.9.0/deepspeed/ops/csrc/transformer/ds_transformer_cuda.cpp
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    12191 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/transformer/gelu_kernels.cu
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    14532 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/transformer/general_kernels.cu
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.557389 deepspeed-0.9.0/deepspeed/ops/csrc/transformer/inference/
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.677389 deepspeed-0.9.0/deepspeed/ops/csrc/transformer/inference/csrc/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    16338 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/transformer/inference/csrc/apply_rotary_pos_emb.cu
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     6908 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/transformer/inference/csrc/dequantize.cu
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    29045 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/transformer/inference/csrc/gelu.cu
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    22685 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/transformer/inference/csrc/layer_norm.cu
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    77643 2023-04-13 15:29:56.000000 deepspeed-0.9.0/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2195 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/transformer/inference/csrc/relu.cu
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    23789 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/transformer/inference/csrc/softmax.cu
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    29531 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.687389 deepspeed-0.9.0/deepspeed/ops/csrc/transformer/inference/includes/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    10279 2023-04-13 15:29:56.000000 deepspeed-0.9.0/deepspeed/ops/csrc/transformer/inference/includes/inference_context.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    17739 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/transformer/inference/includes/inference_cublas_wrappers.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     8265 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/transformer/inference/includes/inference_cuda_layers.h
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    74900 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/transformer/normalize_kernels.cu
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    26758 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/transformer/softmax_kernels.cu
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    22709 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/transformer/transform_kernels.cu
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.687389 deepspeed-0.9.0/deepspeed/ops/csrc/utils/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      788 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/csrc/utils/flatten_unflatten.cpp
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.687389 deepspeed-0.9.0/deepspeed/ops/lamb/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      130 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/lamb/__init__.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     7815 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/lamb/fused_lamb.py
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.697389 deepspeed-0.9.0/deepspeed/ops/op_builder/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1990 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/op_builder/__init__.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1180 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/op_builder/all_ops.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     3333 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/op_builder/async_io.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    28488 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/op_builder/builder.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1420 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/op_builder/cpu_adagrad.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1397 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/op_builder/cpu_adam.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1044 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/op_builder/fused_adam.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1216 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/op_builder/fused_lamb.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      805 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/op_builder/quantizer.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1079 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/op_builder/random_ltd.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2965 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/op_builder/sparse_attn.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1534 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/op_builder/spatial_inference.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      565 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/op_builder/stochastic_transformer.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1294 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/op_builder/transformer.py
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)     2622 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/op_builder/transformer_inference.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      431 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/op_builder/utils.py
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.697389 deepspeed-0.9.0/deepspeed/ops/quantizer/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      132 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/quantizer/__init__.py
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)     1193 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/quantizer/quantizer.py
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.697389 deepspeed-0.9.0/deepspeed/ops/random_ltd/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      191 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/random_ltd/__init__.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4901 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/random_ltd/dropping_utils.py
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.697389 deepspeed-0.9.0/deepspeed/ops/sparse_attention/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      467 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/sparse_attention/__init__.py
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)     3463 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/sparse_attention/bert_sparse_self_attention.py
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)    32948 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/sparse_attention/matmul.py
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)    11322 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/sparse_attention/softmax.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    12300 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/sparse_attention/sparse_attention_utils.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     6746 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/sparse_attention/sparse_self_attention.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    42462 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/sparse_attention/sparsity_config.py
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.697389 deepspeed-0.9.0/deepspeed/ops/sparse_attention/trsrc/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1032 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/sparse_attention/trsrc/__init__.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     6628 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/sparse_attention/trsrc/matmul.tr
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1923 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/sparse_attention/trsrc/softmax_bwd.tr
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4047 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/sparse_attention/trsrc/softmax_fwd.tr
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.697389 deepspeed-0.9.0/deepspeed/ops/transformer/
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)      413 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/transformer/__init__.py
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.697389 deepspeed-0.9.0/deepspeed/ops/transformer/inference/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      315 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/transformer/inference/__init__.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      876 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/transformer/inference/bias_add.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     5612 2023-04-13 15:29:56.000000 deepspeed-0.9.0/deepspeed/ops/transformer/inference/config.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      236 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/transformer/inference/diffusers_2d_transformer.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     9944 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/transformer/inference/diffusers_attention.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4763 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/transformer/inference/diffusers_transformer_block.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    13928 2023-04-13 15:29:56.000000 deepspeed-0.9.0/deepspeed/ops/transformer/inference/ds_attention.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4585 2023-04-13 15:29:56.000000 deepspeed-0.9.0/deepspeed/ops/transformer/inference/ds_mlp.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    18473 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/transformer/inference/moe_inference.py
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.707389 deepspeed-0.9.0/deepspeed/ops/transformer/inference/op_binding/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      382 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/transformer/inference/op_binding/__init__.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      551 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/transformer/inference/op_binding/base.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1051 2023-04-13 15:29:56.000000 deepspeed-0.9.0/deepspeed/ops/transformer/inference/op_binding/gelu_gemm.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1007 2023-04-13 15:29:56.000000 deepspeed-0.9.0/deepspeed/ops/transformer/inference/op_binding/linear.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1273 2023-04-13 15:29:56.000000 deepspeed-0.9.0/deepspeed/ops/transformer/inference/op_binding/mlp_gemm.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1424 2023-04-13 15:29:56.000000 deepspeed-0.9.0/deepspeed/ops/transformer/inference/op_binding/qkv_gemm.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1215 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/transformer/inference/op_binding/residual_add.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1059 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/transformer/inference/op_binding/softmax.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1513 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/transformer/inference/op_binding/softmax_context.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      875 2023-04-13 15:29:56.000000 deepspeed-0.9.0/deepspeed/ops/transformer/inference/op_binding/vector_matmul.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4434 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/transformer/inference/triton_ops.py
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)    20600 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/ops/transformer/transformer.py
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.707389 deepspeed-0.9.0/deepspeed/pipe/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      164 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/pipe/__init__.py
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.707389 deepspeed-0.9.0/deepspeed/profiling/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      140 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/profiling/__init__.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1708 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/profiling/config.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1098 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/profiling/constants.py
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.707389 deepspeed-0.9.0/deepspeed/profiling/flops_profiler/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      120 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/profiling/flops_profiler/__init__.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    47039 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/profiling/flops_profiler/profiler.py
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.707389 deepspeed-0.9.0/deepspeed/runtime/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      192 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/__init__.py
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.717389 deepspeed-0.9.0/deepspeed/runtime/activation_checkpointing/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      140 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/activation_checkpointing/__init__.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    32764 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/activation_checkpointing/checkpointing.py
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)     3987 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/activation_checkpointing/config.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    18459 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/bf16_optimizer.py
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.717389 deepspeed-0.9.0/deepspeed/runtime/checkpoint_engine/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      140 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/checkpoint_engine/__init__.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      654 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/checkpoint_engine/checkpoint_engine.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4982 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/checkpoint_engine/nebula_checkpoint_engine.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1060 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/checkpoint_engine/torch_checkpoint_engine.py
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.717389 deepspeed-0.9.0/deepspeed/runtime/comm/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      140 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/comm/__init__.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     3698 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/comm/coalesced_collectives.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    10062 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/comm/mpi.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     7712 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/comm/nccl.py
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.717389 deepspeed-0.9.0/deepspeed/runtime/compression/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      140 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/compression/__init__.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      701 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/compression/cupy.py
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)    39382 2023-04-13 15:29:56.000000 deepspeed-0.9.0/deepspeed/runtime/config.py
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)     8199 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/config_utils.py
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)    12806 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/constants.py
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.717389 deepspeed-0.9.0/deepspeed/runtime/data_pipeline/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      140 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/data_pipeline/__init__.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     6081 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/data_pipeline/config.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4701 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/data_pipeline/constants.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    10025 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/data_pipeline/curriculum_scheduler.py
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.717389 deepspeed-0.9.0/deepspeed/runtime/data_pipeline/data_routing/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      140 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/data_pipeline/data_routing/__init__.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     5638 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/data_pipeline/data_routing/basic_layer.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1282 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/data_pipeline/data_routing/helper.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4638 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/data_pipeline/data_routing/scheduler.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      955 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/data_pipeline/data_routing/utils.py
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.717389 deepspeed-0.9.0/deepspeed/runtime/data_pipeline/data_sampling/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      140 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/data_pipeline/data_sampling/__init__.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    25015 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/data_pipeline/data_sampling/data_analyzer.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    19160 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/data_pipeline/data_sampling/data_sampler.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    20614 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/data_pipeline/data_sampling/indexed_dataset.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1756 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/data_pipeline/data_sampling/utils.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     6977 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/dataloader.py
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)     5625 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/eigenvalue.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)   148945 2023-04-13 15:29:56.000000 deepspeed-0.9.0/deepspeed/runtime/engine.py
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.717389 deepspeed-0.9.0/deepspeed/runtime/fp16/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      140 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/fp16/__init__.py
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)    20052 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/fp16/fused_optimizer.py
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)    11093 2023-04-13 15:29:56.000000 deepspeed-0.9.0/deepspeed/runtime/fp16/loss_scaler.py
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.727389 deepspeed-0.9.0/deepspeed/runtime/fp16/onebit/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      186 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/fp16/onebit/__init__.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    15258 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/fp16/onebit/adam.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    23085 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/fp16/onebit/lamb.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    19112 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/fp16/onebit/zoadam.py
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)    18076 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/fp16/unfused_optimizer.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    19007 2023-04-13 15:29:56.000000 deepspeed-0.9.0/deepspeed/runtime/hybrid_engine.py
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)    33557 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/lr_schedules.py
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.727389 deepspeed-0.9.0/deepspeed/runtime/pipe/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      195 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/pipe/__init__.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    56517 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/pipe/engine.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    27186 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/pipe/module.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     5477 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/pipe/p2p.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    15546 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/pipe/schedule.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    17167 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/pipe/topology.py
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)     1353 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/progressive_layer_drop.py
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)     7699 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/quantize.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2416 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/sparse_tensor.py
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)    18177 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/state_dict_factory.py
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.727389 deepspeed-0.9.0/deepspeed/runtime/swap_tensor/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)       95 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/swap_tensor/__init__.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1172 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/swap_tensor/aio_config.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     6282 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/swap_tensor/async_swapper.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      596 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/swap_tensor/constants.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    18967 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/swap_tensor/optimizer_utils.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     9654 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/swap_tensor/partitioned_optimizer_swapper.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    17684 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/swap_tensor/partitioned_param_swapper.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    10793 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/swap_tensor/pipelined_optimizer_swapper.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     7734 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/swap_tensor/utils.py
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)    35806 2023-04-13 15:29:56.000000 deepspeed-0.9.0/deepspeed/runtime/utils.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     7027 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/weight_quantizer.py
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.727389 deepspeed-0.9.0/deepspeed/runtime/zero/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      423 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/zero/__init__.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     9996 2023-04-13 15:29:56.000000 deepspeed-0.9.0/deepspeed/runtime/zero/config.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    10926 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/zero/contiguous_memory_allocator.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     7784 2023-04-13 15:29:56.000000 deepspeed-0.9.0/deepspeed/runtime/zero/linear.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2931 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/zero/offload_config.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    21889 2023-04-13 15:29:56.000000 deepspeed-0.9.0/deepspeed/runtime/zero/parameter_offload.py
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)    69726 2023-04-13 15:29:56.000000 deepspeed-0.9.0/deepspeed/runtime/zero/partition_parameters.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    22345 2023-04-13 15:29:56.000000 deepspeed-0.9.0/deepspeed/runtime/zero/partitioned_param_coordinator.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)   112178 2023-04-13 15:29:56.000000 deepspeed-0.9.0/deepspeed/runtime/zero/stage3.py
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)   110183 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/zero/stage_1_and_2.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2727 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/zero/test.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    11727 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/zero/tiling.py
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)     2931 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/runtime/zero/utils.py
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.737389 deepspeed-0.9.0/deepspeed/utils/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      717 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/utils/__init__.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     6096 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/utils/comms_logging.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4370 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/utils/debug.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      144 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/utils/exceptions.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    15606 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/utils/groups.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     3004 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/utils/init_on_device.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     4375 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/utils/logging.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2041 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/utils/mixed_precision_linkage.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      499 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/utils/nvtx.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     7997 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/utils/tensor_fragment.py
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)     8765 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/utils/timer.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      200 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/utils/types.py
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)    18737 2023-04-11 17:18:49.000000 deepspeed-0.9.0/deepspeed/utils/zero_to_fp32.py
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.607389 deepspeed-0.9.0/deepspeed.egg-info/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    27085 2023-04-13 15:30:32.000000 deepspeed-0.9.0/deepspeed.egg-info/PKG-INFO
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    20079 2023-04-13 15:30:33.000000 deepspeed-0.9.0/deepspeed.egg-info/SOURCES.txt
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)        1 2023-04-13 15:30:32.000000 deepspeed-0.9.0/deepspeed.egg-info/dependency_links.txt
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)       85 2023-04-13 15:30:32.000000 deepspeed-0.9.0/deepspeed.egg-info/entry_points.txt
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1094 2023-04-13 15:30:32.000000 deepspeed-0.9.0/deepspeed.egg-info/requires.txt
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)       10 2023-04-13 15:30:32.000000 deepspeed-0.9.0/deepspeed.egg-info/top_level.txt
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.737389 deepspeed-0.9.0/op_builder/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1990 2023-04-11 17:18:49.000000 deepspeed-0.9.0/op_builder/__init__.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1180 2023-04-11 17:18:49.000000 deepspeed-0.9.0/op_builder/all_ops.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     3333 2023-04-11 17:18:49.000000 deepspeed-0.9.0/op_builder/async_io.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)    28488 2023-04-11 17:18:49.000000 deepspeed-0.9.0/op_builder/builder.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1420 2023-04-11 17:18:49.000000 deepspeed-0.9.0/op_builder/cpu_adagrad.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1397 2023-04-11 17:18:49.000000 deepspeed-0.9.0/op_builder/cpu_adam.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1044 2023-04-11 17:18:49.000000 deepspeed-0.9.0/op_builder/fused_adam.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1216 2023-04-11 17:18:49.000000 deepspeed-0.9.0/op_builder/fused_lamb.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      805 2023-04-11 17:18:49.000000 deepspeed-0.9.0/op_builder/quantizer.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1079 2023-04-11 17:18:49.000000 deepspeed-0.9.0/op_builder/random_ltd.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     2965 2023-04-11 17:18:49.000000 deepspeed-0.9.0/op_builder/sparse_attn.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1534 2023-04-11 17:18:49.000000 deepspeed-0.9.0/op_builder/spatial_inference.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      565 2023-04-11 17:18:49.000000 deepspeed-0.9.0/op_builder/stochastic_transformer.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)     1294 2023-04-11 17:18:49.000000 deepspeed-0.9.0/op_builder/transformer.py
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)     2622 2023-04-11 17:18:49.000000 deepspeed-0.9.0/op_builder/transformer_inference.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      431 2023-04-11 17:18:49.000000 deepspeed-0.9.0/op_builder/utils.py
+drwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        0 2023-04-13 15:30:33.747389 deepspeed-0.9.0/requirements/
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)        7 2022-07-06 20:47:37.000000 deepspeed-0.9.0/requirements/requirements-1bit-mpi.txt
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)       23 2022-07-06 20:47:37.000000 deepspeed-0.9.0/requirements/requirements-autotuning-ml.txt
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)        9 2022-07-06 20:47:37.000000 deepspeed-0.9.0/requirements/requirements-autotuning.txt
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      232 2022-11-14 17:49:34.000000 deepspeed-0.9.0/requirements/requirements-dev.txt
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)       72 2023-02-28 00:54:52.000000 deepspeed-0.9.0/requirements/requirements-inf.txt
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)       85 2022-11-23 21:50:53.000000 deepspeed-0.9.0/requirements/requirements-readthedocs.txt
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)       36 2023-04-11 17:18:49.000000 deepspeed-0.9.0/requirements/requirements-sd.txt
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)       14 2022-07-06 20:47:37.000000 deepspeed-0.9.0/requirements/requirements-sparse_attn.txt
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)       72 2023-02-28 00:54:52.000000 deepspeed-0.9.0/requirements/requirements.txt
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)      148 2023-04-13 15:30:33.747389 deepspeed-0.9.0/setup.cfg
+-rwxr-xr-x   0 jerasley  (1000) jerasley  (1000)    11109 2023-04-11 17:18:49.000000 deepspeed-0.9.0/setup.py
+-rw-r--r--   0 jerasley  (1000) jerasley  (1000)        6 2023-04-13 15:29:56.000000 deepspeed-0.9.0/version.txt
```

### Comparing `deepspeed-0.8.3/PKG-INFO` & `deepspeed-0.9.0/README.md`

 * *Files 4% similar despite different names*

```diff
@@ -1,62 +1,35 @@
-Metadata-Version: 2.1
-Name: deepspeed
-Version: 0.8.3
-Summary: DeepSpeed library
-Home-page: http://deepspeed.ai
-Author: DeepSpeed Team
-Author-email: deepspeed-info@microsoft.com
-License: MIT
-Project-URL: Documentation, https://deepspeed.readthedocs.io
-Project-URL: Source, https://github.com/microsoft/DeepSpeed
-Platform: UNKNOWN
-Classifier: Programming Language :: Python :: 3.6
-Classifier: Programming Language :: Python :: 3.7
-Classifier: Programming Language :: Python :: 3.8
-Classifier: Programming Language :: Python :: 3.9
-Classifier: Programming Language :: Python :: 3.10
-Description-Content-Type: text/markdown
-Provides-Extra: 1bit
-Provides-Extra: 1bit_mpi
-Provides-Extra: readthedocs
-Provides-Extra: dev
-Provides-Extra: autotuning
-Provides-Extra: autotuning_ml
-Provides-Extra: sparse_attn
-Provides-Extra: inf
-Provides-Extra: sd
-Provides-Extra: all
-License-File: LICENSE
-
-[![License MIT](https://badgen.net/badge/license/MIT/blue)](https://github.com/Microsoft/DeepSpeed/blob/master/LICENSE)
+[![License Apache 2.0](https://badgen.net/badge/license/apache2.0/blue)](https://github.com/Microsoft/DeepSpeed/blob/master/LICENSE)
 [![PyPI version](https://badge.fury.io/py/deepspeed.svg)](https://pypi.org/project/deepspeed/)
 [![Downloads](https://pepy.tech/badge/deepspeed)](https://pepy.tech/project/deepspeed)
 [![Build](https://badgen.net/badge/build/check-status/blue)](#build-pipeline-status)
 
 
 <div align="center">
  <img src="docs/assets/images/DeepSpeed_light.svg#gh-light-mode-only" width="400px">
  <img src="docs/assets/images/DeepSpeed_dark_transparent.svg#gh-dark-mode-only" width="400px">
 </div>
 
 ## Latest News
-<b> DeepSpeed trained the world's most powerful language models ([MT-530B](https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/), [BLOOM](https://huggingface.co/blog/bloom-megatron-deepspeed)); [learn how](https://www.deepspeed.ai/tutorials/large-models-w-deepspeed/).</b>
+<b> <span style="color:orange" > DeepSpeed empowers ChatGPT-like model training with a single click, offering 15x speedup over SOTA RLHF systems with unprecedented cost reduction at all scales; [learn how](https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-chat)</span>.</b>
 
-* [2023/02] [Automatic Tensor Parallelism: Enables tensor parallelism by default without providing an injection policy](https://www.deepspeed.ai/tutorials/automatic-tensor-parallelism/)
+* ***[2023/04] 🚀 [DeepSpeed Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales](https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-chat)*** [[English](https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-chat/README.md)] [[中文](https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-chat/chinese/README.md)] [[日本語](https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-chat/japanese/README.md)]🚀
+* [2023/03] [Scaling Large-Scale Generative Mixture-of-Expert Multimodal Model With VL-MoE](https://www.deepspeed.ai/2023/03/30/multi-modal.html)
+* [2023/02] [Automatic Tensor Parallelism: Enables tensor parallelism by default without an injection policy](https://www.deepspeed.ai/tutorials/automatic-tensor-parallelism/)
 * [2022/12] [DeepSpeed Data Efficiency: A composable library that makes better use of data, increases training efficiency, and improves model quality](https://www.deepspeed.ai/2022/12/11/data-efficiency.html)
 * [2022/11] [Stable Diffusion Image Generation under 1 second w. DeepSpeed MII](https://github.com/microsoft/DeepSpeed-MII/tree/main/examples/benchmark/txt2img)
 * [2022/10] [DeepSpeed-MII: instant speedup on 24,000+ open-source DL models with up to 40x cheaper inference](https://www.deepspeed.ai/2022/10/10/mii.html)
 * [2022/09] [ZeRO-Inference: Democratizing massive model inference](https://www.deepspeed.ai/2022/09/09/zero-inference.html)
 * [2022/07] [Azure and DeepSpeed empower easy-to-use and high-performance model training](https://azure.microsoft.com/en-us/blog/azure-empowers-easytouse-highperformance-and-hyperscale-model-training-using-deepspeed/)
 
 ---
 
 # Extreme Speed and Scale for DL Training and Inference
 
-[DeepSpeed](https://www.deepspeed.ai/) is an easy-to-use deep learning optimization software suite that enables unprecedented scale and speed for Deep Learning Training and Inference. With DeepSpeed you can:
+***[DeepSpeed](https://www.deepspeed.ai/) enables world's most powerful language models like [MT-530B](https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/) and [BLOOM](https://huggingface.co/blog/bloom-megatron-deepspeed)***. It is an easy-to-use deep learning optimization software suite that powers unprecedented scale and speed for both training and inference. With DeepSpeed you can:
 
 * Train/Inference dense or sparse models with billions or trillions of parameters
 * Achieve excellent system throughput and efficiently scale to thousands of GPUs
 * Train/Inference on resource constrained GPU systems
 * Achieve unprecedented low latency and high throughput for inference
 * Achieve extreme compression for an unparalleled inference latency and model size reduction with low costs
 
@@ -129,33 +102,34 @@
 
 ---
 
 # Build Pipeline Status
 
 | Description | Status |
 | ----------- | ------ |
-| NVIDIA | [![nv-torch12-p40](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch12-p40.yml/badge.svg)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch12-p40.yml) [![nv-torch18-v100](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch18-v100.yml/badge.svg)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch18-v100.yml) [![nv-torch-latest-v100](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch-latest-v100.yml/badge.svg)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch-latest-v100.yml) [![nv-inference](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-inference.yml/badge.svg)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-inference.yml) [![nv-nightly](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-nightly.yml/badge.svg)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-nightly.yml) |
-| AMD | [![amd](https://github.com/microsoft/DeepSpeed/actions/workflows/amd.yml/badge.svg)](https://github.com/microsoft/DeepSpeed/actions/workflows/amd.yml) |
-| PyTorch Nightly | [![nv-torch-nightly-v100](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch-nightly-v100.yml/badge.svg)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch-nightly-v100.yml) |
-| Integrations | [![nv-transformers-v100](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-transformers-v100.yml/badge.svg)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-transformers-v100.yml) [![nv-lightning-v100](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-lightning-v100.yml/badge.svg)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-lightning-v100.yml) [![nv-accelerate-v100](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-accelerate-v100.yml/badge.svg)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-accelerate-v100.yml) |
-| Misc | [![Formatting](https://github.com/microsoft/DeepSpeed/actions/workflows/formatting.yml/badge.svg)](https://github.com/microsoft/DeepSpeed/actions/workflows/formatting.yml) [![pages-build-deployment](https://github.com/microsoft/DeepSpeed/actions/workflows/pages/pages-build-deployment/badge.svg)](https://github.com/microsoft/DeepSpeed/actions/workflows/pages/pages-build-deployment) [![Documentation Status](https://readthedocs.org/projects/deepspeed/badge/?version=latest)](https://deepspeed.readthedocs.io/en/latest/?badge=latest)|
+| NVIDIA | [![nv-torch19-p40](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch19-p40.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch19-p40.yml) [![nv-torch19-v100](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch19-v100.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch19-v100.yml) [![nv-torch-latest-v100](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch-latest-v100.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch-latest-v100.yml) [![nv-inference](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-inference.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-inference.yml) [![nv-nightly](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-nightly.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-nightly.yml) |
+| AMD | [![amd-mi100](https://github.com/microsoft/DeepSpeed/actions/workflows/amd-mi100.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/amd-mi100.yml) [![amd-mi200](https://github.com/microsoft/DeepSpeed/actions/workflows/amd-mi200.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/amd-mi200.yml) |
+| CPU | [![nv-torch-latest-cpu](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch-latest-cpu.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch-latest-cpu.yml) |
+| PyTorch Nightly | [![nv-torch-nightly-v100](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch-nightly-v100.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch-nightly-v100.yml) |
+| Integrations | [![nv-transformers-v100](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-transformers-v100.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-transformers-v100.yml) [![nv-lightning-v100](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-lightning-v100.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-lightning-v100.yml) [![nv-accelerate-v100](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-accelerate-v100.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-accelerate-v100.yml)[![nv-megatron](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-megatron.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-megatron.yml)[![nv-mii](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-mii.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-mii.yml) |
+| Misc | [![Formatting](https://github.com/microsoft/DeepSpeed/actions/workflows/formatting.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/formatting.yml) [![pages-build-deployment](https://github.com/microsoft/DeepSpeed/actions/workflows/pages/pages-build-deployment/badge.svg)](https://github.com/microsoft/DeepSpeed/actions/workflows/pages/pages-build-deployment) [![Documentation Status](https://readthedocs.org/projects/deepspeed/badge/?version=latest)](https://deepspeed.readthedocs.io/en/latest/?badge=latest)[![python](https://github.com/microsoft/DeepSpeed/actions/workflows/python.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/python.yml) |
 
 # Installation
 
 The quickest way to get started with DeepSpeed is via pip, this will install
 the latest release of DeepSpeed which is not tied to specific PyTorch or CUDA
 versions. DeepSpeed includes several C++/CUDA extensions that we commonly refer
 to as our 'ops'.  By default, all of these extensions/ops will be built
 just-in-time (JIT) using [torch's JIT C++ extension loader that relies on
 ninja](https://pytorch.org/docs/stable/cpp_extension.html) to build and
 dynamically link them at runtime.
 
 ## Requirements
 * [PyTorch](https://pytorch.org/) must be installed _before_ installing DeepSpeed.
-* For full feature support we recommend a version of PyTorch that is >= 1.8 and ideally the latest PyTorch stable release.
+* For full feature support we recommend a version of PyTorch that is >= 1.9 and ideally the latest PyTorch stable release.
 * A CUDA or ROCm compiler such as [nvcc](https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/#introduction) or [hipcc](https://github.com/ROCm-Developer-Tools/HIPCC) used to compile C++/CUDA/HIP extensions.
 * Specific GPUs we develop and test against are listed below, this doesn't mean your GPU will not work if it doesn't fall into this category it's just DeepSpeed is most well tested on the following:
   * NVIDIA: Pascal, Volta, Ampere, and Hopper architectures
   * AMD: MI100 and MI200
 
 ## PyPI
 We regularly push releases to [PyPI](https://pypi.org/project/deepspeed/) and encourage users to install from there in most cases.
@@ -238,14 +212,15 @@
 13. Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, Yuxiong He. (2022) ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers. [arXiv:2206.01861](https://arxiv.org/abs/2206.01861) and [NeurIPS 2022](https://openreview.net/forum?id=f-fVCElZ-G1).
 14. Reza Yazdani Aminabadi, Samyam Rajbhandari, Minjia Zhang, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Jeff Rasley, Shaden Smith, Olatunji Ruwase, Yuxiong He. (2022) DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale. [arXiv:2207.00032](https://arxiv.org/abs/2207.00032) and [SC 2022](https://dl.acm.org/doi/abs/10.5555/3571885.3571946).
 15. Zhewei Yao, Xiaoxia Wu, Conglong Li, Connor Holmes, Minjia Zhang, Cheng Li, Yuxiong He. (2022) Random-LTD: Random and Layerwise Token Dropping Brings Efficient Training for Large-scale Transformers. [arXiv:2211.11586](https://arxiv.org/abs/2211.11586).
 16. Conglong Li, Zhewei Yao, Xiaoxia Wu, Minjia Zhang, Yuxiong He. (2022) DeepSpeed Data Efficiency: Improving Deep Learning Model Quality and Training Efficiency via Efficient Data Sampling and Routing. [arXiv:2212.03597](https://arxiv.org/abs/2212.03597).
 17. Xiaoxia Wu, Cheng Li, Reza Yazdani Aminabadi, Zhewei Yao, Yuxiong He. (2023) Understanding INT4 Quantization for Transformer Models: Latency Speedup, Composability, and Failure Cases. [arXiv:2301.12017](https://arxiv.org/abs/2301.12017).
 18. Syed Zawad, Cheng Li, Zhewei Yao, Elton Zheng, Yuxiong He, Feng Yan. (2023) DySR: Adaptive Super-Resolution via Algorithm and System Co-design. [ICLR:2023](https://openreview.net/forum?id=Pgtn4l6eKjv).
 19. Sheng Shen, Zhewei Yao, Chunyuan Li, Trevor Darrell, Kurt Keutzer, Yuxiong He. (2023) Scaling Vision-Language Models with Sparse Mixture of Experts. [arXiv:2303.07226](https://arxiv.org/abs/2303.07226).
+20. Quentin Anthony, Ammar Ahmad Awan, Jeff Rasley, Yuxiong He, Aamir Shafi, Mustafa Abduljabbar, Hari Subramoni, Dhabaleswar Panda. (2023) MCR-DL: Mix-and-Match Communication Runtime for Deep Learning [arXiv:2303.08374](https://arxiv.org/abs/2303.08374) and will appear at IPDPS 2023.
 
 
 # Videos
 1. DeepSpeed KDD 2020 Tutorial
     1. [Overview](https://www.youtube.com/watch?v=CaseqC45DNc&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=29)
     2. [ZeRO + large model training](https://www.youtube.com/watch?v=y4_bCiAsIAk&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=28)
     3. [17B T-NLG demo](https://www.youtube.com/watch?v=9V-ZbP92drg&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=27)
@@ -256,9 +231,7 @@
     * Registration is free and all videos are available on-demand.
     * [ZeRO & Fastest BERT: Increasing the scale and speed of deep learning training in DeepSpeed](https://note.microsoft.com/MSR-Webinar-DeepSpeed-Registration-On-Demand.html).
 3. [DeepSpeed on AzureML](https://youtu.be/yBVXR8G8Bg8)
 4. Community Tutorials
     * [DeepSpeed: All the tricks to scale to gigantic models (Mark Saroufim)](https://www.youtube.com/watch?v=pDGI668pNg0)
     * [Turing-NLG, DeepSpeed and the ZeRO optimizer (Yannic Kilcher)](https://www.youtube.com/watch?v=tC01FRB0M7w)
     * [Ultimate Guide To Scaling ML Models (The AI Epiphany)](https://www.youtube.com/watch?v=hc0u4avAkuM)
-
-
```

### Comparing `deepspeed-0.8.3/README.md` & `deepspeed-0.9.0/PKG-INFO`

 * *Files 10% similar despite different names*

```diff
@@ -1,33 +1,64 @@
-[![License MIT](https://badgen.net/badge/license/MIT/blue)](https://github.com/Microsoft/DeepSpeed/blob/master/LICENSE)
+Metadata-Version: 2.1
+Name: deepspeed
+Version: 0.9.0
+Summary: DeepSpeed library
+Home-page: http://deepspeed.ai
+Author: DeepSpeed Team
+Author-email: deepspeed-info@microsoft.com
+License: MIT
+Project-URL: Documentation, https://deepspeed.readthedocs.io
+Project-URL: Source, https://github.com/microsoft/DeepSpeed
+Platform: UNKNOWN
+Classifier: Programming Language :: Python :: 3.6
+Classifier: Programming Language :: Python :: 3.7
+Classifier: Programming Language :: Python :: 3.8
+Classifier: Programming Language :: Python :: 3.9
+Classifier: Programming Language :: Python :: 3.10
+Description-Content-Type: text/markdown
+Provides-Extra: 1bit
+Provides-Extra: 1bit_mpi
+Provides-Extra: readthedocs
+Provides-Extra: dev
+Provides-Extra: autotuning
+Provides-Extra: autotuning_ml
+Provides-Extra: sparse_attn
+Provides-Extra: inf
+Provides-Extra: sd
+Provides-Extra: all
+License-File: LICENSE
+
+[![License Apache 2.0](https://badgen.net/badge/license/apache2.0/blue)](https://github.com/Microsoft/DeepSpeed/blob/master/LICENSE)
 [![PyPI version](https://badge.fury.io/py/deepspeed.svg)](https://pypi.org/project/deepspeed/)
 [![Downloads](https://pepy.tech/badge/deepspeed)](https://pepy.tech/project/deepspeed)
 [![Build](https://badgen.net/badge/build/check-status/blue)](#build-pipeline-status)
 
 
 <div align="center">
  <img src="docs/assets/images/DeepSpeed_light.svg#gh-light-mode-only" width="400px">
  <img src="docs/assets/images/DeepSpeed_dark_transparent.svg#gh-dark-mode-only" width="400px">
 </div>
 
 ## Latest News
-<b> DeepSpeed trained the world's most powerful language models ([MT-530B](https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/), [BLOOM](https://huggingface.co/blog/bloom-megatron-deepspeed)); [learn how](https://www.deepspeed.ai/tutorials/large-models-w-deepspeed/).</b>
+<b> <span style="color:orange" > DeepSpeed empowers ChatGPT-like model training with a single click, offering 15x speedup over SOTA RLHF systems with unprecedented cost reduction at all scales; [learn how](https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-chat)</span>.</b>
 
-* [2023/02] [Automatic Tensor Parallelism: Enables tensor parallelism by default without providing an injection policy](https://www.deepspeed.ai/tutorials/automatic-tensor-parallelism/)
+* ***[2023/04] 🚀 [DeepSpeed Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales](https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-chat)*** [[English](https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-chat/README.md)] [[中文](https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-chat/chinese/README.md)] [[日本語](https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-chat/japanese/README.md)]🚀
+* [2023/03] [Scaling Large-Scale Generative Mixture-of-Expert Multimodal Model With VL-MoE](https://www.deepspeed.ai/2023/03/30/multi-modal.html)
+* [2023/02] [Automatic Tensor Parallelism: Enables tensor parallelism by default without an injection policy](https://www.deepspeed.ai/tutorials/automatic-tensor-parallelism/)
 * [2022/12] [DeepSpeed Data Efficiency: A composable library that makes better use of data, increases training efficiency, and improves model quality](https://www.deepspeed.ai/2022/12/11/data-efficiency.html)
 * [2022/11] [Stable Diffusion Image Generation under 1 second w. DeepSpeed MII](https://github.com/microsoft/DeepSpeed-MII/tree/main/examples/benchmark/txt2img)
 * [2022/10] [DeepSpeed-MII: instant speedup on 24,000+ open-source DL models with up to 40x cheaper inference](https://www.deepspeed.ai/2022/10/10/mii.html)
 * [2022/09] [ZeRO-Inference: Democratizing massive model inference](https://www.deepspeed.ai/2022/09/09/zero-inference.html)
 * [2022/07] [Azure and DeepSpeed empower easy-to-use and high-performance model training](https://azure.microsoft.com/en-us/blog/azure-empowers-easytouse-highperformance-and-hyperscale-model-training-using-deepspeed/)
 
 ---
 
 # Extreme Speed and Scale for DL Training and Inference
 
-[DeepSpeed](https://www.deepspeed.ai/) is an easy-to-use deep learning optimization software suite that enables unprecedented scale and speed for Deep Learning Training and Inference. With DeepSpeed you can:
+***[DeepSpeed](https://www.deepspeed.ai/) enables world's most powerful language models like [MT-530B](https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/) and [BLOOM](https://huggingface.co/blog/bloom-megatron-deepspeed)***. It is an easy-to-use deep learning optimization software suite that powers unprecedented scale and speed for both training and inference. With DeepSpeed you can:
 
 * Train/Inference dense or sparse models with billions or trillions of parameters
 * Achieve excellent system throughput and efficiently scale to thousands of GPUs
 * Train/Inference on resource constrained GPU systems
 * Achieve unprecedented low latency and high throughput for inference
 * Achieve extreme compression for an unparalleled inference latency and model size reduction with low costs
 
@@ -100,33 +131,34 @@
 
 ---
 
 # Build Pipeline Status
 
 | Description | Status |
 | ----------- | ------ |
-| NVIDIA | [![nv-torch12-p40](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch12-p40.yml/badge.svg)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch12-p40.yml) [![nv-torch18-v100](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch18-v100.yml/badge.svg)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch18-v100.yml) [![nv-torch-latest-v100](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch-latest-v100.yml/badge.svg)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch-latest-v100.yml) [![nv-inference](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-inference.yml/badge.svg)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-inference.yml) [![nv-nightly](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-nightly.yml/badge.svg)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-nightly.yml) |
-| AMD | [![amd](https://github.com/microsoft/DeepSpeed/actions/workflows/amd.yml/badge.svg)](https://github.com/microsoft/DeepSpeed/actions/workflows/amd.yml) |
-| PyTorch Nightly | [![nv-torch-nightly-v100](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch-nightly-v100.yml/badge.svg)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch-nightly-v100.yml) |
-| Integrations | [![nv-transformers-v100](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-transformers-v100.yml/badge.svg)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-transformers-v100.yml) [![nv-lightning-v100](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-lightning-v100.yml/badge.svg)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-lightning-v100.yml) [![nv-accelerate-v100](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-accelerate-v100.yml/badge.svg)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-accelerate-v100.yml) |
-| Misc | [![Formatting](https://github.com/microsoft/DeepSpeed/actions/workflows/formatting.yml/badge.svg)](https://github.com/microsoft/DeepSpeed/actions/workflows/formatting.yml) [![pages-build-deployment](https://github.com/microsoft/DeepSpeed/actions/workflows/pages/pages-build-deployment/badge.svg)](https://github.com/microsoft/DeepSpeed/actions/workflows/pages/pages-build-deployment) [![Documentation Status](https://readthedocs.org/projects/deepspeed/badge/?version=latest)](https://deepspeed.readthedocs.io/en/latest/?badge=latest)|
+| NVIDIA | [![nv-torch19-p40](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch19-p40.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch19-p40.yml) [![nv-torch19-v100](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch19-v100.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch19-v100.yml) [![nv-torch-latest-v100](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch-latest-v100.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch-latest-v100.yml) [![nv-inference](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-inference.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-inference.yml) [![nv-nightly](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-nightly.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-nightly.yml) |
+| AMD | [![amd-mi100](https://github.com/microsoft/DeepSpeed/actions/workflows/amd-mi100.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/amd-mi100.yml) [![amd-mi200](https://github.com/microsoft/DeepSpeed/actions/workflows/amd-mi200.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/amd-mi200.yml) |
+| CPU | [![nv-torch-latest-cpu](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch-latest-cpu.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch-latest-cpu.yml) |
+| PyTorch Nightly | [![nv-torch-nightly-v100](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch-nightly-v100.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch-nightly-v100.yml) |
+| Integrations | [![nv-transformers-v100](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-transformers-v100.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-transformers-v100.yml) [![nv-lightning-v100](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-lightning-v100.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-lightning-v100.yml) [![nv-accelerate-v100](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-accelerate-v100.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-accelerate-v100.yml)[![nv-megatron](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-megatron.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-megatron.yml)[![nv-mii](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-mii.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-mii.yml) |
+| Misc | [![Formatting](https://github.com/microsoft/DeepSpeed/actions/workflows/formatting.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/formatting.yml) [![pages-build-deployment](https://github.com/microsoft/DeepSpeed/actions/workflows/pages/pages-build-deployment/badge.svg)](https://github.com/microsoft/DeepSpeed/actions/workflows/pages/pages-build-deployment) [![Documentation Status](https://readthedocs.org/projects/deepspeed/badge/?version=latest)](https://deepspeed.readthedocs.io/en/latest/?badge=latest)[![python](https://github.com/microsoft/DeepSpeed/actions/workflows/python.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/python.yml) |
 
 # Installation
 
 The quickest way to get started with DeepSpeed is via pip, this will install
 the latest release of DeepSpeed which is not tied to specific PyTorch or CUDA
 versions. DeepSpeed includes several C++/CUDA extensions that we commonly refer
 to as our 'ops'.  By default, all of these extensions/ops will be built
 just-in-time (JIT) using [torch's JIT C++ extension loader that relies on
 ninja](https://pytorch.org/docs/stable/cpp_extension.html) to build and
 dynamically link them at runtime.
 
 ## Requirements
 * [PyTorch](https://pytorch.org/) must be installed _before_ installing DeepSpeed.
-* For full feature support we recommend a version of PyTorch that is >= 1.8 and ideally the latest PyTorch stable release.
+* For full feature support we recommend a version of PyTorch that is >= 1.9 and ideally the latest PyTorch stable release.
 * A CUDA or ROCm compiler such as [nvcc](https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/#introduction) or [hipcc](https://github.com/ROCm-Developer-Tools/HIPCC) used to compile C++/CUDA/HIP extensions.
 * Specific GPUs we develop and test against are listed below, this doesn't mean your GPU will not work if it doesn't fall into this category it's just DeepSpeed is most well tested on the following:
   * NVIDIA: Pascal, Volta, Ampere, and Hopper architectures
   * AMD: MI100 and MI200
 
 ## PyPI
 We regularly push releases to [PyPI](https://pypi.org/project/deepspeed/) and encourage users to install from there in most cases.
@@ -209,14 +241,15 @@
 13. Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, Yuxiong He. (2022) ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers. [arXiv:2206.01861](https://arxiv.org/abs/2206.01861) and [NeurIPS 2022](https://openreview.net/forum?id=f-fVCElZ-G1).
 14. Reza Yazdani Aminabadi, Samyam Rajbhandari, Minjia Zhang, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Jeff Rasley, Shaden Smith, Olatunji Ruwase, Yuxiong He. (2022) DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale. [arXiv:2207.00032](https://arxiv.org/abs/2207.00032) and [SC 2022](https://dl.acm.org/doi/abs/10.5555/3571885.3571946).
 15. Zhewei Yao, Xiaoxia Wu, Conglong Li, Connor Holmes, Minjia Zhang, Cheng Li, Yuxiong He. (2022) Random-LTD: Random and Layerwise Token Dropping Brings Efficient Training for Large-scale Transformers. [arXiv:2211.11586](https://arxiv.org/abs/2211.11586).
 16. Conglong Li, Zhewei Yao, Xiaoxia Wu, Minjia Zhang, Yuxiong He. (2022) DeepSpeed Data Efficiency: Improving Deep Learning Model Quality and Training Efficiency via Efficient Data Sampling and Routing. [arXiv:2212.03597](https://arxiv.org/abs/2212.03597).
 17. Xiaoxia Wu, Cheng Li, Reza Yazdani Aminabadi, Zhewei Yao, Yuxiong He. (2023) Understanding INT4 Quantization for Transformer Models: Latency Speedup, Composability, and Failure Cases. [arXiv:2301.12017](https://arxiv.org/abs/2301.12017).
 18. Syed Zawad, Cheng Li, Zhewei Yao, Elton Zheng, Yuxiong He, Feng Yan. (2023) DySR: Adaptive Super-Resolution via Algorithm and System Co-design. [ICLR:2023](https://openreview.net/forum?id=Pgtn4l6eKjv).
 19. Sheng Shen, Zhewei Yao, Chunyuan Li, Trevor Darrell, Kurt Keutzer, Yuxiong He. (2023) Scaling Vision-Language Models with Sparse Mixture of Experts. [arXiv:2303.07226](https://arxiv.org/abs/2303.07226).
+20. Quentin Anthony, Ammar Ahmad Awan, Jeff Rasley, Yuxiong He, Aamir Shafi, Mustafa Abduljabbar, Hari Subramoni, Dhabaleswar Panda. (2023) MCR-DL: Mix-and-Match Communication Runtime for Deep Learning [arXiv:2303.08374](https://arxiv.org/abs/2303.08374) and will appear at IPDPS 2023.
 
 
 # Videos
 1. DeepSpeed KDD 2020 Tutorial
     1. [Overview](https://www.youtube.com/watch?v=CaseqC45DNc&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=29)
     2. [ZeRO + large model training](https://www.youtube.com/watch?v=y4_bCiAsIAk&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=28)
     3. [17B T-NLG demo](https://www.youtube.com/watch?v=9V-ZbP92drg&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=27)
@@ -227,7 +260,9 @@
     * Registration is free and all videos are available on-demand.
     * [ZeRO & Fastest BERT: Increasing the scale and speed of deep learning training in DeepSpeed](https://note.microsoft.com/MSR-Webinar-DeepSpeed-Registration-On-Demand.html).
 3. [DeepSpeed on AzureML](https://youtu.be/yBVXR8G8Bg8)
 4. Community Tutorials
     * [DeepSpeed: All the tricks to scale to gigantic models (Mark Saroufim)](https://www.youtube.com/watch?v=pDGI668pNg0)
     * [Turing-NLG, DeepSpeed and the ZeRO optimizer (Yannic Kilcher)](https://www.youtube.com/watch?v=tC01FRB0M7w)
     * [Ultimate Guide To Scaling ML Models (The AI Epiphany)](https://www.youtube.com/watch?v=hc0u4avAkuM)
+
+
```

### Comparing `deepspeed-0.8.3/accelerator/abstract_accelerator.py` & `deepspeed-0.9.0/accelerator/abstract_accelerator.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,14 +1,18 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import abc
 from abc import ABC
 
 
 class DeepSpeedAccelerator(ABC):
+
     def __init__(self):
         self._name = None
         self._communication_backend_name = None
 
     # Device APIs
     @abc.abstractmethod
     def device_name(self, device_index):
```

### Comparing `deepspeed-0.8.3/accelerator/cuda_accelerator.py` & `deepspeed-0.9.0/accelerator/cuda_accelerator.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,11 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import os
 import pkgutil
 import importlib
 
 from .abstract_accelerator import DeepSpeedAccelerator
 # During setup stage torch may not be installed, pass on no torch will
@@ -10,29 +13,27 @@
 try:
     import torch.cuda
 except ImportError:
     pass
 
 
 class CUDA_Accelerator(DeepSpeedAccelerator):
+
     def __init__(self):
         self._name = 'cuda'
         self._communication_backend_name = 'nccl'
 
         # begin initialize for create_op_builder()
         # put all valid class name <--> class type mapping into class_dict
         op_builder_dir = self.op_builder_dir()
         op_builder_module = importlib.import_module(op_builder_dir)
-
         for _, module_name, _ in pkgutil.iter_modules([os.path.dirname(op_builder_module.__file__)]):
             # avoid self references
             if module_name != 'all_ops' and module_name != 'builder':
-                module = importlib.import_module("{}.{}".format(
-                    op_builder_dir,
-                    module_name))
+                module = importlib.import_module("{}.{}".format(op_builder_dir, module_name))
                 for member_name in module.__dir__():
                     if member_name.endswith(
                             'Builder'
                     ) and member_name != "OpBuilder" and member_name != "CUDAOpBuilder" and member_name != "TorchCPUOpBuilder":  # avoid abstract classes
                         if not member_name in self.class_dict:
                             self.class_dict[member_name] = getattr(module, member_name)
         # end initialize for create_op_builder()
```

### Comparing `deepspeed-0.8.3/accelerator/real_accelerator.py` & `deepspeed-0.9.0/accelerator/real_accelerator.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,8 +1,11 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 try:
     from accelerator.abstract_accelerator import DeepSpeedAccelerator as dsa1
 except ImportError as e:
     dsa1 = None
 try:
     from deepspeed.accelerator.abstract_accelerator import DeepSpeedAccelerator as dsa2
@@ -19,21 +22,16 @@
     # and extension would import the
     # run time abstract_accelerator/DeepSpeedAccelerator as its base
     # class, so we need to compare accel_obj with both base class.
     # if accel_obj is instance of DeepSpeedAccelerator in one of
     # accelerator.abstractor_accelerator
     # or deepspeed.accelerator.abstract_accelerator, consider accel_obj
     # is a conforming object
-    if not ((dsa1 != None and isinstance(accel_obj,
-                                         dsa1)) or
-            (dsa2 != None and isinstance(accel_obj,
-                                         dsa2))):
-        raise AssertionError(
-            f'{accel_obj.__class__.__name__} accelerator is not subclass of DeepSpeedAccelerator'
-        )
+    if not ((dsa1 != None and isinstance(accel_obj, dsa1)) or (dsa2 != None and isinstance(accel_obj, dsa2))):
+        raise AssertionError(f'{accel_obj.__class__.__name__} accelerator is not subclass of DeepSpeedAccelerator')
 
     # TODO: turn off is_available test since this breaks tests
     #assert accel_obj.is_available(), \
     #    f'{accel_obj.__class__.__name__} accelerator fails is_available() test'
 
 
 def get_accelerator():
```

### Comparing `deepspeed-0.8.3/bin/ds_bench` & `deepspeed-0.9.0/bin/ds_bench`

 * *Files identical despite different names*

### Comparing `deepspeed-0.8.3/bin/ds_elastic` & `deepspeed-0.9.0/bin/ds_elastic`

 * *Files 5% similar despite different names*

```diff
@@ -5,32 +5,30 @@
 
 import deepspeed
 from deepspeed.elasticity import compute_elastic_config
 
 if __name__ == '__main__':
     parser = argparse.ArgumentParser()
     parser.add_argument('-c', '--config', type=str, help="DeepSpeed config json")
-    parser.add_argument('-w',
-                        '--world-size',
-                        type=int,
-                        default=0,
-                        help="Intended/current world size")
+    parser.add_argument('-w', '--world-size', type=int, default=0, help="Intended/current world size")
     args = parser.parse_args()
     ds_config = json.load(open(args.config, 'r'))
 
     ds_version = deepspeed.__version__
 
     elastic_config = ds_config['elasticity']
     print('------------------------------------------')
     print("Elasticity config:")
     print('------------------------------------------')
     print(json.dumps(elastic_config, indent=4, sort_keys=True))
 
     if args.world_size > 0:
-        final_batch_size, valid_gpus, micro_batch_size = compute_elastic_config(ds_config=ds_config, target_deepspeed_version=ds_version, world_size=args.world_size)
+        final_batch_size, valid_gpus, micro_batch_size = compute_elastic_config(ds_config=ds_config,
+                                                                                target_deepspeed_version=ds_version,
+                                                                                world_size=args.world_size)
         print('------------------------------------------')
         print(f"Calculated results for world size {args.world_size}:")
         print('------------------------------------------')
         print(f'final_batch_size .... {final_batch_size}')
         print(f'valid_gpus .......... {valid_gpus}')
         print(f'micro_batch_size .... {micro_batch_size}')
     else:
```

### Comparing `deepspeed-0.8.3/bin/ds_ssh` & `deepspeed-0.9.0/bin/ds_ssh`

 * *Files identical despite different names*

### Comparing `deepspeed-0.8.3/csrc/adagrad/cpu_adagrad.cpp` & `deepspeed-0.9.0/csrc/adagrad/cpu_adagrad.cpp`

 * *Files 2% similar despite different names*

```diff
@@ -1,7 +1,12 @@
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
 #include "cpu_adagrad.h"
 #include <torch/extension.h>
 #include <iostream>
 #include <memory>
 #include <type_traits>
 #include <unordered_map>
 #if defined(__ENABLE_CUDA__)
@@ -169,15 +174,15 @@
     float* grads_ptr = (float*)grads_c.data_ptr();
     float* exp_avg_sq_ptr = (float*)exp_avg_sq_c.data_ptr();
 
     std::shared_ptr<Adagrad_Optimizer> opt =
         std::static_pointer_cast<Adagrad_Optimizer>(s_optimizers[optimizer_id]);
     opt->IncrementStep(step);
     opt->update_state(lr, epsilon, weight_decay);
-    opt->Step_8(params_ptr, grads_ptr, exp_avg_sq_ptr, params_c.size(0));
+    opt->Step_8(params_ptr, grads_ptr, exp_avg_sq_ptr, params_c.numel());
 
 #if defined(__ENABLE_CUDA__)
     opt->SynchronizeStreams();
 #endif
     return 0;
 }
 
@@ -205,15 +210,15 @@
     std::shared_ptr<Adagrad_Optimizer> opt =
         std::static_pointer_cast<Adagrad_Optimizer>(s_optimizers[optimizer_id]);
     opt->IncrementStep(step);
     opt->update_state(lr, epsilon, weight_decay);
     opt->Step_8(params_ptr,
                 grads_ptr,
                 exp_avg_sq_ptr,
-                params_c.size(0),
+                params_c.numel(),
                 gpu_params_ptr,
                 (params.options().dtype() == at::kHalf));
 
     opt->SynchronizeStreams();
 #else
     assert(false);
 #endif
```

### Comparing `deepspeed-0.8.3/csrc/adam/cpu_adam.cpp` & `deepspeed-0.9.0/csrc/adam/cpu_adam.cpp`

 * *Files 1% similar despite different names*

```diff
@@ -1,7 +1,12 @@
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
 #include "cpu_adam.h"
 #include <torch/extension.h>
 #include <cassert>
 #include <iostream>
 #include <memory>
 #include <type_traits>
 #include <unordered_map>
@@ -226,15 +231,15 @@
     opt->IncrementStep(step, beta1, beta2);
     opt->update_state(lr, epsilon, weight_decay, bias_correction);
 
     opt->Step_8(params_ptr,
                 grads_ptr,
                 exp_avg_ptr,
                 exp_avg_sq_ptr,
-                params_c.size(0),
+                params_c.numel(),
                 nullptr,
                 (params.options().dtype() == at::kHalf));
 
 #if defined(__ENABLE_CUDA__)
     opt->SynchronizeStreams();
 #endif
     return 0;
@@ -271,15 +276,15 @@
         std::static_pointer_cast<Adam_Optimizer>(s_optimizers[optimizer_id]);
     opt->IncrementStep(step, beta1, beta2);
     opt->update_state(lr, epsilon, weight_decay, bias_correction);
     opt->Step_8(params_ptr,
                 grads_ptr,
                 exp_avg_ptr,
                 exp_avg_sq_ptr,
-                params_c.size(0),
+                params_c.numel(),
                 gpu_params_ptr,
                 (params.options().dtype() == at::kHalf));
 
     opt->SynchronizeStreams();
 #else
     assert(false);
 #endif
```

### Comparing `deepspeed-0.8.3/csrc/adam/multi_tensor_adam.cu` & `deepspeed-0.9.0/csrc/adam/multi_tensor_adam.cu`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,15 @@
-/* Copyright 2020 The Microsoft DeepSpeed Team
-   Copyright NVIDIA/apex
-   This file is adapted from fused adam in NVIDIA/apex, commit a109f85
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
+/*
+Copyright NVIDIA/apex
+This file is adapted from fused adam in NVIDIA/apex, commit a109f85
 */
 
 #include <ATen/ATen.h>
 #include <ATen/AccumulateType.h>
 #include <ATen/cuda/CUDAContext.h>
 #include <ATen/cuda/Exceptions.h>
 // Another possibility:
```

### Comparing `deepspeed-0.8.3/csrc/adam/multi_tensor_apply.cuh` & `deepspeed-0.9.0/csrc/adam/multi_tensor_apply.cuh`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,15 @@
-/* Copyright 2020 The Microsoft DeepSpeed Team
-   Copyright NVIDIA/apex
-   This file is adapted from fused adam in NVIDIA/apex, commit a109f85
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
+/*
+Copyright NVIDIA/apex
+This file is adapted from fused adam in NVIDIA/apex, commit a109f85
 */
 
 #include <ATen/ATen.h>
 #include <ATen/AccumulateType.h>
 #include <ATen/cuda/CUDAContext.h>
 #include <ATen/cuda/Exceptions.h>
 #include <c10/cuda/CUDAGuard.h>
```

### Comparing `deepspeed-0.8.3/csrc/aio/common/deepspeed_aio_common.cpp` & `deepspeed-0.9.0/csrc/aio/common/deepspeed_aio_common.cpp`

 * *Files 2% similar despite different names*

```diff
@@ -1,11 +1,13 @@
-/*
-Copyright 2020 The Microsoft DeepSpeed Team
-Licensed under the MIT license.
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
+/*
 Functionality for swapping optimizer tensors to/from (NVMe) storage devices.
 */
 
 #include <assert.h>
 #include <stdlib.h>
 #include <string.h>
 
@@ -258,15 +260,15 @@
     std::string err_msg = file_op + std::string(" failed on ") + std::string(filename) +
                           " error = " + std::to_string(error_code);
     std::cerr << c_library_name << ":  " << err_msg << std::endl;
 }
 
 int open_file(const char* filename, const bool read_op)
 {
-    const int flags = read_op ? (O_RDONLY | __O_DIRECT) : (O_WRONLY | O_CREAT | __O_DIRECT);
+    const int flags = read_op ? (O_RDONLY | O_DIRECT) : (O_WRONLY | O_CREAT | O_DIRECT);
     const int mode = 0600;
     const auto fd = open(filename, flags, mode);
     if (fd == -1) {
         const auto error_code = errno;
         const auto error_msg = read_op ? " open for read " : " open for write ";
         report_file_error(filename, error_msg, error_code);
         return -1;
```

### Comparing `deepspeed-0.8.3/csrc/aio/common/deepspeed_aio_common.h` & `deepspeed-0.9.0/csrc/aio/common/deepspeed_aio_common.h`

 * *Files 10% similar despite different names*

```diff
@@ -1,11 +1,13 @@
-/*
-Copyright 2020 The Microsoft DeepSpeed Team
-Licensed under the MIT license.
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
+/*
 Functionality for swapping optimizer tensors to/from (NVMe) storage devices.
 */
 
 #include <deepspeed_aio_utils.h>
 #include <stdlib.h>
 #include <memory>
 #include <string>
```

### Comparing `deepspeed-0.8.3/csrc/aio/common/deepspeed_aio_types.cpp` & `deepspeed-0.9.0/csrc/aio/common/deepspeed_aio_types.cpp`

 * *Files 21% similar despite different names*

```diff
@@ -1,11 +1,13 @@
-/*
-Copyright 2020 The Microsoft DeepSpeed Team
-Licensed under the MIT license.
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
+/*
 Functionality for swapping optimizer tensors to/from (NVMe) storage devices.
 */
 
 #include <cmath>
 
 #include "deepspeed_aio_utils.h"
```

### Comparing `deepspeed-0.8.3/csrc/aio/common/deepspeed_aio_types.h` & `deepspeed-0.9.0/csrc/aio/common/deepspeed_aio_types.h`

 * *Files 13% similar despite different names*

```diff
@@ -1,11 +1,13 @@
-/*
-Copyright 2020 The Microsoft DeepSpeed Team
-Licensed under the MIT license.
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
+/*
 Functionality for swapping optimizer tensors to/from (NVMe) storage devices.
 */
 
 #include <libaio.h>
 #include <stdlib.h>
 
 #include <string>
```

### Comparing `deepspeed-0.8.3/csrc/aio/common/deepspeed_aio_utils.cpp` & `deepspeed-0.9.0/csrc/aio/common/deepspeed_aio_utils.cpp`

 * *Files 6% similar despite different names*

```diff
@@ -1,11 +1,13 @@
-/*
-Copyright 2020 The Microsoft DeepSpeed Team
-Licensed under the MIT license.
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
+/*
 Functionality for swapping optimizer tensors to/from (NVMe) storage devices.
 */
 
 #include <cmath>
 #include <iostream>
 
 #include "deepspeed_aio_utils.h"
```

### Comparing `deepspeed-0.8.3/csrc/aio/common/deepspeed_aio_utils.h` & `deepspeed-0.9.0/csrc/aio/common/deepspeed_aio_utils.h`

 * *Files 8% similar despite different names*

```diff
@@ -1,11 +1,13 @@
-/*
-Copyright 2020 The Microsoft DeepSpeed Team
-Licensed under the MIT license.
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
+/*
 Functionality for swapping optimizer tensors to/from (NVMe) storage devices.
 */
 
 #pragma once
 
 #include <assert.h>
 #include <stdlib.h>
```

### Comparing `deepspeed-0.8.3/csrc/aio/py_lib/deepspeed_aio_thread.cpp` & `deepspeed-0.9.0/csrc/aio/py_lib/deepspeed_aio_thread.cpp`

 * *Files 6% similar despite different names*

```diff
@@ -1,11 +1,13 @@
-/*
-Copyright 2020 The Microsoft DeepSpeed Team
-Licensed under the MIT license.
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
+/*
 Functionality for swapping optimizer tensors to/from (NVMe) storage devices.
 */
 
 #include "deepspeed_aio_thread.h"
 
 using namespace std;
```

### Comparing `deepspeed-0.8.3/csrc/aio/py_lib/deepspeed_aio_thread.h` & `deepspeed-0.9.0/csrc/aio/py_lib/deepspeed_aio_thread.h`

 * *Files 11% similar despite different names*

```diff
@@ -1,11 +1,13 @@
-/*
-Copyright 2020 The Microsoft DeepSpeed Team
-Licensed under the MIT license.
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
+/*
 Functionality for swapping optimizer tensors to/from (NVMe) storage devices.
 */
 
 #include <condition_variable>
 #include <memory>
 #include <queue>
 #include "deepspeed_py_aio.h"
```

### Comparing `deepspeed-0.8.3/csrc/aio/py_lib/deepspeed_pin_tensor.cpp` & `deepspeed-0.9.0/csrc/aio/py_lib/deepspeed_pin_tensor.cpp`

 * *Files 9% similar despite different names*

```diff
@@ -1,11 +1,13 @@
-/*
-Copyright 2023 The Microsoft DeepSpeed Team
-Licensed under the MIT license.
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
+/*
 Functionality for managing CPU tensors occupying page-locked memory.
 */
 
 #include "deepspeed_pin_tensor.h"
 
 using namespace std;
```

### Comparing `deepspeed-0.8.3/csrc/aio/py_lib/deepspeed_pin_tensor.h` & `deepspeed-0.9.0/csrc/aio/py_lib/deepspeed_pin_tensor.h`

 * *Files 20% similar despite different names*

```diff
@@ -1,15 +1,18 @@
-/*
-Copyright 2023 The Microsoft DeepSpeed Team
-Licensed under the MIT license.
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
+/*
 Functionality for managing CPU tensors occupying page-locked memory.
 TODO: Implement a full-featured manager that
- 1. Avoid page-locked memory leaks
- 2. Minimize page-locked memory usage by reducing internal fragmentation
+1. Avoid page-locked memory leaks
+2. Minimize page-locked memory usage by reducing internal fragmentation
+Functionality for managing CPU tensors occupying page-locked memory.
 */
 
 #include <map>
 #include "deepspeed_py_aio.h"
 
 struct deepspeed_pin_tensor_t {
     std::map<void*, size_t> _locked_tensors;
```

### Comparing `deepspeed-0.8.3/csrc/aio/py_lib/deepspeed_py_aio.cpp` & `deepspeed-0.9.0/csrc/aio/py_lib/deepspeed_py_aio.cpp`

 * *Files 2% similar despite different names*

```diff
@@ -1,7 +1,11 @@
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 /*
 Copyright 2020 The Microsoft DeepSpeed Team
 Licensed under the MIT license.
 
 Functionality for swapping optimizer tensors to/from (NVMe) storage devices.
 */
```

### Comparing `deepspeed-0.8.3/csrc/aio/py_lib/deepspeed_py_aio.h` & `deepspeed-0.9.0/csrc/aio/py_lib/deepspeed_py_aio.h`

 * *Files 20% similar despite different names*

```diff
@@ -1,7 +1,11 @@
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 /*
 Copyright 2020 The Microsoft DeepSpeed Team
 Licensed under the MIT license.
 
 Functionality for swapping optimizer tensors to/from (NVMe) storage devices.
 */
```

### Comparing `deepspeed-0.8.3/csrc/aio/py_lib/deepspeed_py_aio_handle.cpp` & `deepspeed-0.9.0/csrc/aio/py_lib/deepspeed_py_aio_handle.cpp`

 * *Files 1% similar despite different names*

```diff
@@ -1,7 +1,11 @@
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 /*
 Copyright 2020 The Microsoft DeepSpeed Team
 Licensed under the MIT license.
 
 Functionality for swapping optimizer tensors to/from (NVMe) storage devices.
 */
```

### Comparing `deepspeed-0.8.3/csrc/aio/py_lib/deepspeed_py_aio_handle.h` & `deepspeed-0.9.0/csrc/aio/py_lib/deepspeed_py_aio_handle.h`

 * *Files 3% similar despite different names*

```diff
@@ -1,11 +1,13 @@
-/*
-Copyright 2020 The Microsoft DeepSpeed Team
-Licensed under the MIT license.
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
+/*
 Functionality for swapping optimizer tensors to/from (NVMe) storage devices.
 */
 
 #include <condition_variable>
 #include <memory>
 #include "deepspeed_aio_thread.h"
 #include "deepspeed_pin_tensor.h"
```

### Comparing `deepspeed-0.8.3/csrc/aio/py_lib/deepspeed_py_copy.cpp` & `deepspeed-0.9.0/csrc/aio/py_lib/deepspeed_py_copy.cpp`

 * *Files 3% similar despite different names*

```diff
@@ -1,11 +1,13 @@
-/*
-Copyright 2020 The Microsoft DeepSpeed Team
-Licensed under the MIT license.
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
+/*
 Functionality for swapping optimizer tensors to/from (NVMe) storage devices.
 */
 
 #include "deepspeed_py_copy.h"
 #include <omp.h>
 
 #define ROUND_DOWN(size, step) ((size) & ~((step)-1))
```

### Comparing `deepspeed-0.8.3/csrc/aio/py_lib/py_ds_aio.cpp` & `deepspeed-0.9.0/csrc/aio/py_lib/py_ds_aio.cpp`

 * *Files 8% similar despite different names*

```diff
@@ -1,11 +1,13 @@
-/*
-Copyright 2020 The Microsoft DeepSpeed Team
-Licensed under the MIT license.
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
+/*
 Functionality for swapping optimizer tensors to/from (NVMe) storage devices.
 */
 
 #include <torch/extension.h>
 #include "deepspeed_py_aio_handle.h"
 #include "deepspeed_py_copy.h"
```

### Comparing `deepspeed-0.8.3/csrc/common/custom_cuda_kernel.cu` & `deepspeed-0.9.0/csrc/common/custom_cuda_kernel.cu`

 * *Files 23% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #include "custom_cuda_layers.h"
 
 __global__ void param_update_kernel(const float* input, __half* output, int size)
 {
     int id = blockIdx.x * blockDim.x + threadIdx.x;
```

### Comparing `deepspeed-0.8.3/csrc/includes/StopWatch.h` & `deepspeed-0.9.0/csrc/includes/StopWatch.h`

 * *Files 11% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #pragma once
 #ifdef _WIN32
 #include <windows.h>
 #else
 #include <time.h>
 #endif
```

### Comparing `deepspeed-0.8.3/csrc/includes/Timer.h` & `deepspeed-0.9.0/csrc/includes/Timer.h`

 * *Files 10% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #ifndef __TIMER_H__
 #define __TIMER_H__
 
 #include <cuda_runtime.h>
 #include <chrono>
 #include "cuda.h"
```

### Comparing `deepspeed-0.8.3/csrc/includes/context.h` & `deepspeed-0.9.0/csrc/includes/context.h`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #pragma once
 
 #include <ATen/cuda/CUDAContext.h>
 #include <cuda_runtime_api.h>
 #include <cassert>
 #include <iostream>
@@ -39,36 +40,36 @@
 {
     return (std::max)(
         (std::min)((N + DS_CUDA_NUM_THREADS - 1) / DS_CUDA_NUM_THREADS, DS_MAXIMUM_NUM_BLOCKS),
         // Use at least 1 block, since CUDA does not allow empty block
         1);
 }
 
-class Context {
+class TrainingContext {
 public:
-    Context() : _workspace(nullptr), _seed(42), _curr_offset(0)
+    TrainingContext() : _workspace(nullptr), _seed(42), _curr_offset(0)
     {
         curandCreateGenerator(&_gen, CURAND_RNG_PSEUDO_DEFAULT);
         curandSetPseudoRandomGeneratorSeed(_gen, 123);
         if (cublasCreate(&_cublasHandle) != CUBLAS_STATUS_SUCCESS) {
             auto message = std::string("Fail to create cublas handle.");
             std::cerr << message << std::endl;
             throw std::runtime_error(message);
         }
     }
 
-    virtual ~Context()
+    virtual ~TrainingContext()
     {
         cublasDestroy(_cublasHandle);
         cudaFree(_workspace);
     }
 
-    static Context& Instance()
+    static TrainingContext& Instance()
     {
-        static Context _ctx;
+        static TrainingContext _ctx;
         return _ctx;
     }
 
     void SetWorkSpace(void* workspace)
     {
         if (!workspace) { throw std::runtime_error("Workspace is null."); }
         _workspace = workspace;
```

### Comparing `deepspeed-0.8.3/csrc/includes/conversion_utils.h` & `deepspeed-0.9.0/csrc/includes/conversion_utils.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright 2022 The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #pragma once
 
 #include "ds_kernel_utils.h"
 
 #include <cuda_fp16.h>
 #include <stdint.h>
```

### Comparing `deepspeed-0.8.3/csrc/includes/cpu_adagrad.h` & `deepspeed-0.9.0/csrc/includes/cpu_adagrad.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #pragma once
 
 #define NOMINMAX  // Windows idiosyncrasy
                   // https://stackoverflow.com/questions/4913922/possible-problems-with-nominmax-on-visual-c
 
 #include <stdio.h>
@@ -34,16 +35,16 @@
     Adagrad_Optimizer(float alpha = 1e-2, float eps = 1e-8, float weight_decay = 0)
         : _alpha(alpha), _eps(eps), _weight_decay(weight_decay)
     {
 #if defined(__ENABLE_CUDA__)
         cudaMallocHost((void**)_doubled_buffer, TILE * sizeof(float));
         cudaMallocHost((void**)(_doubled_buffer + 1), TILE * sizeof(float));
 
-        _streams[0] = Context::Instance().GetCurrentStream();
-        _streams[1] = Context::Instance().GetNewStream();
+        _streams[0] = TrainingContext::Instance().GetCurrentStream();
+        _streams[1] = TrainingContext::Instance().GetNewStream();
         _buf_index = false;
 #endif
     }
     ~Adagrad_Optimizer()
     {
 #if defined(__ENABLE_CUDA__)
         cudaFreeHost(_doubled_buffer[0]);
```

### Comparing `deepspeed-0.8.3/csrc/includes/cpu_adam.h` & `deepspeed-0.9.0/csrc/includes/cpu_adam.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #pragma once
 
 #define NOMINMAX  // Windows idiosyncrasy
                   // https://stackoverflow.com/questions/4913922/possible-problems-with-nominmax-on-visual-c
 
 #include <stdio.h>
@@ -49,16 +50,16 @@
           _step(0),
           _adamw_mode(adamw_mode)
     {
 #if defined(__ENABLE_CUDA__)
         cudaMallocHost((void**)_doubled_buffer, TILE * sizeof(float));
         cudaMallocHost((void**)(_doubled_buffer + 1), TILE * sizeof(float));
 
-        _streams[0] = Context::Instance().GetCurrentStream();
-        _streams[1] = Context::Instance().GetNewStream();
+        _streams[0] = TrainingContext::Instance().GetCurrentStream();
+        _streams[1] = TrainingContext::Instance().GetNewStream();
         _buf_index = false;
 #endif
     }
     ~Adam_Optimizer()
     {
 #if defined(__ENABLE_CUDA__)
         cudaFreeHost(_doubled_buffer[0]);
```

### Comparing `deepspeed-0.8.3/csrc/includes/cublas_wrappers.h` & `deepspeed-0.9.0/csrc/includes/cublas_wrappers.h`

 * *Files 11% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #pragma once
 
 #include <assert.h>
 #include <cublas_v2.h>
 #include <cuda.h>
 #include <cuda_fp16.h>
```

### Comparing `deepspeed-0.8.3/csrc/includes/custom_cuda_layers.h` & `deepspeed-0.9.0/csrc/includes/custom_cuda_layers.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright 2022 The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #pragma once
 
 #include "ds_kernel_utils.h"
 
 #include <cuda.h>
 #include <cuda_fp16.h>
```

### Comparing `deepspeed-0.8.3/csrc/includes/dequantization_utils.h` & `deepspeed-0.9.0/csrc/includes/dequantization_utils.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright 2022 The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #include "conversion_utils.h"
 #include "ds_kernel_utils.h"
 #include "quantization.h"
 #include "quantization_utils.h"
 
 namespace cg = cooperative_groups;
```

### Comparing `deepspeed-0.8.3/csrc/includes/dropout.h` & `deepspeed-0.9.0/csrc/includes/dropout.h`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #pragma once
 
 #include <cuda.h>
 #include <cuda_fp16.h>
 #include <stdio.h>
```

### Comparing `deepspeed-0.8.3/csrc/includes/ds_kernel_utils.h` & `deepspeed-0.9.0/csrc/includes/ds_kernel_utils.h`

 * *Files 10% similar despite different names*

```diff
@@ -1,10 +1,13 @@
-/*
-Copyright 2022 The Microsoft DeepSpeed Team
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
+/*
 Centralized header file for preprocessor macros and constants
 used throughout the codebase.
 */
 
 #pragma once
 
 #include <cuda.h>
```

### Comparing `deepspeed-0.8.3/csrc/includes/ds_transformer_cuda.h` & `deepspeed-0.9.0/csrc/includes/ds_transformer_cuda.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #pragma once
 
 #include <cuda_runtime_api.h>
 #include <curand.h>
 #include <memory>
 #include <vector>
```

### Comparing `deepspeed-0.8.3/csrc/includes/feed_forward.h` & `deepspeed-0.9.0/csrc/includes/feed_forward.h`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #ifndef __FEEDFORWARD_H__
 #define __FEEDFORWARD_H__
 
 #include <cuda.h>
 #include <cuda_fp16.h>
 #include <stdio.h>
```

### Comparing `deepspeed-0.8.3/csrc/includes/gelu.h` & `deepspeed-0.9.0/csrc/includes/gelu.h`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #pragma once
 
 #include <cuda.h>
 #include <cuda_fp16.h>
 #include <stdio.h>
 #include "custom_cuda_layers.h"
```

### Comparing `deepspeed-0.8.3/csrc/includes/gemm_test.h` & `deepspeed-0.9.0/csrc/includes/gemm_test.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #pragma once
 
 #include <cuda_fp16.h>
 #ifndef __HIP_PLATFORM_HCC__
 #include <cuda_profiler_api.h>
 #endif
```

### Comparing `deepspeed-0.8.3/csrc/includes/general_kernels.h` & `deepspeed-0.9.0/csrc/includes/general_kernels.h`

 * *Files 20% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #include <cuda.h>
 #include <cuda_fp16.h>
 #include <stdio.h>
 #include <stdlib.h>
 
 #ifdef __HIP_PLATFORM_HCC__
```

### Comparing `deepspeed-0.8.3/csrc/includes/memory_access_utils.h` & `deepspeed-0.9.0/csrc/includes/memory_access_utils.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright 2022 The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #pragma once
 
 #include <cuda.h>
 #include "ds_kernel_utils.h"
 
 /////////////////////////////// Memory Access Utils ///////////////////////////////
```

### Comparing `deepspeed-0.8.3/csrc/includes/normalize_layer.h` & `deepspeed-0.9.0/csrc/includes/normalize_layer.h`

 * *Files 11% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #pragma once
 
 #include <cuda.h>
 #include <cuda_fp16.h>
 #include <stdio.h>
 #include <fstream>
```

### Comparing `deepspeed-0.8.3/csrc/includes/quantization.h` & `deepspeed-0.9.0/csrc/includes/quantization.h`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #pragma once
 
 #include <cuda_fp16.h>
 #include "ds_kernel_utils.h"
 
 namespace quantize {
```

### Comparing `deepspeed-0.8.3/csrc/includes/quantization_utils.h` & `deepspeed-0.9.0/csrc/includes/quantization_utils.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright 2022 The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #include <cassert>
 #include "conversion_utils.h"
 #include "ds_kernel_utils.h"
 #include "memory_access_utils.h"
 #include "quantization.h"
 #include "reduction_utils.h"
```

### Comparing `deepspeed-0.8.3/csrc/includes/reduction_utils.h` & `deepspeed-0.9.0/csrc/includes/reduction_utils.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright 2022 The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #pragma once
 
 #include "conversion_utils.h"
 #include "ds_kernel_utils.h"
 #include "memory_access_utils.h"
```

### Comparing `deepspeed-0.8.3/csrc/includes/simd.h` & `deepspeed-0.9.0/csrc/includes/simd.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #pragma once
 
 #if (__x86_64__ || __i386__)
 #include <cpuid.h>
 #include <x86intrin.h>
 #endif
```

### Comparing `deepspeed-0.8.3/csrc/includes/softmax.h` & `deepspeed-0.9.0/csrc/includes/softmax.h`

 * *Files 10% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #pragma once
 
 #include <cuda.h>
 #include <cuda_fp16.h>
 #include <stdio.h>
 #include "custom_cuda_layers.h"
```

### Comparing `deepspeed-0.8.3/csrc/includes/strided_batch_gemm.h` & `deepspeed-0.9.0/csrc/includes/strided_batch_gemm.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #pragma once
 
 #include <cuda.h>
 #include <cuda_fp16.h>
 #include <stdio.h>
 #include "context.h"
```

### Comparing `deepspeed-0.8.3/csrc/includes/type_shim.h` & `deepspeed-0.9.0/csrc/includes/type_shim.h`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 /* Taken from NVIDIA/apex commit 855808f3fc268e9715d613f3c2e56469d8c986d8 */
 #include <ATen/ATen.h>
 
 // Forward/backward compatibility hack around
 // https://github.com/pytorch/pytorch/commit/3aeb78079bcd68282fe9117088e138b77318e288
 // pending more future-proof guidance from upstream.
```

### Comparing `deepspeed-0.8.3/csrc/lamb/fused_lamb_cuda.cpp` & `deepspeed-0.9.0/csrc/lamb/fused_lamb_cuda.cpp`

 * *Files 6% similar despite different names*

```diff
@@ -1,8 +1,12 @@
-/* Copyright 2019 The Microsoft DeepSpeed Team */
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
 #include <torch/extension.h>
 
 // CUDA forward declaration
 void fused_lamb_cuda(at::Tensor& p,
                      at::Tensor& p_copy,
                      at::Tensor& m,
                      at::Tensor& v,
```

### Comparing `deepspeed-0.8.3/csrc/lamb/fused_lamb_cuda_kernel.cu` & `deepspeed-0.9.0/csrc/lamb/fused_lamb_cuda_kernel.cu`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,12 @@
-/* Copyright 2019 The Microsoft DeepSpeed Team */
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
 #include <cuda.h>
 #include <cuda_runtime.h>
 #include <stdio.h>
 #include <cmath>
 #include "ATen/ATen.h"
 #include "ATen/TensorUtils.h"
 #include "ATen/cuda/CUDAContext.h"
```

### Comparing `deepspeed-0.8.3/csrc/quantization/dequantize.cu` & `deepspeed-0.9.0/csrc/quantization/dequantize.cu`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright 2022 The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #include "dequantization_utils.h"
 #include "memory_access_utils.h"
 
 namespace cg = cooperative_groups;
 
 template <typename T, int numBits, dequantize::Type qType, int unroll, int threads>
```

### Comparing `deepspeed-0.8.3/csrc/quantization/fake_quantizer.cu` & `deepspeed-0.9.0/csrc/quantization/fake_quantizer.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #include <math.h>
 #include "custom_cuda_layers.h"
 #include "memory_access_utils.h"
 
 namespace cg = cooperative_groups;
 
@@ -452,15 +453,15 @@
                                     int num_bits,
                                     cudaStream_t stream)
 {
     dim3 block_dim(1024);
     dim3 grid_dim(group_num);
 
     uint64_t inc = total_count / grid_dim.x / block_dim.x;
-    std::pair<uint64_t, uint64_t> seed = Context::Instance().IncrementOffset(inc);
+    std::pair<uint64_t, uint64_t> seed = TrainingContext::Instance().IncrementOffset(inc);
 
     sr_fake_quantize_kernel<<<grid_dim, block_dim, 0, stream>>>(
         vals, (total_count / group_num) / 4, group_num, num_bits, seed);
 }
 template void launch_sr_fake_quantize_kernel(float* vals,
                                              int total_count,
                                              int group_num,
@@ -1006,15 +1007,15 @@
                                          int num_bits,
                                          cudaStream_t stream)
 {
     dim3 block_dim(1024);
     dim3 grid_dim(group_num);
 
     uint64_t inc = total_count / grid_dim.x / block_dim.x;
-    std::pair<uint64_t, uint64_t> seed = Context::Instance().IncrementOffset(inc);
+    std::pair<uint64_t, uint64_t> seed = TrainingContext::Instance().IncrementOffset(inc);
 
     sr_fake_quantize_kernel<<<grid_dim, block_dim, 0, stream>>>(
         vals, (total_count / group_num) / 4, group_num, num_bits, seed);
 }
 template void launch_sr_fake_quantize_kernel_asym(float* vals,
                                                   int total_count,
                                                   int group_num,
```

### Comparing `deepspeed-0.8.3/csrc/quantization/pt_binding.cpp` & `deepspeed-0.9.0/csrc/quantization/pt_binding.cpp`

 * *Files 7% similar despite different names*

```diff
@@ -1,7 +1,12 @@
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
 #include <ATen/cuda/CUDAContext.h>
 #include <torch/extension.h>
 #include <cassert>
 #include <vector>
 #include "quantization.h"
 
 template <typename T>
```

### Comparing `deepspeed-0.8.3/csrc/quantization/quantize.cu` & `deepspeed-0.9.0/csrc/quantization/quantize.cu`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright 2022 The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #include "ds_kernel_utils.h"
 #include "memory_access_utils.h"
 #include "quantization.h"
 #include "quantization_utils.h"
 #include "reduction_utils.h"
```

### Comparing `deepspeed-0.8.3/csrc/random_ltd/gather_scatter.cu` & `deepspeed-0.9.0/csrc/random_ltd/gather_scatter.cu`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright 2022 The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #include "custom_cuda_layers.h"
 #include "memory_access_utils.h"
 
 namespace cg = cooperative_groups;
 
 namespace td_data {
```

### Comparing `deepspeed-0.8.3/csrc/random_ltd/pt_binding.cpp` & `deepspeed-0.9.0/csrc/random_ltd/pt_binding.cpp`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright 2022 The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #include <torch/extension.h>
 #include <vector>
 #include "custom_cuda_layers.h"
 
 torch::Tensor token_sort_(torch::Tensor& unsorted_token_ids, int64_t original_tokens)
 {
```

### Comparing `deepspeed-0.8.3/csrc/random_ltd/slice_attn_masks.cu` & `deepspeed-0.9.0/csrc/random_ltd/slice_attn_masks.cu`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright 2022 The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #include "custom_cuda_layers.h"
 #include "memory_access_utils.h"
 
 namespace cg = cooperative_groups;
 
 template <typename T>
```

### Comparing `deepspeed-0.8.3/csrc/random_ltd/token_sort.cu` & `deepspeed-0.9.0/csrc/random_ltd/token_sort.cu`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright 2022 The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #include <cassert>
 #include "custom_cuda_layers.h"
 #include "memory_access_utils.h"
 
 namespace cg = cooperative_groups;
```

### Comparing `deepspeed-0.8.3/csrc/sparse_attention/utils.cpp` & `deepspeed-0.9.0/csrc/sparse_attention/utils.cpp`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,16 @@
-// DeepSpeed note, code taken & adapted from commit 9aa94789f13ada713af36cfd8cca2fc9a7f6b79a
-// https://github.com/ptillet/torch-blocksparse/blob/master/csrc/utils.cpp
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
+/*
+DeepSpeed note, code taken & adapted from commit 9aa94789f13ada713af36cfd8cca2fc9a7f6b79a
+ https:github.com/ptillet/torch-blocksparse/blob/master/csrc/utils.cpp
+*/
 
 #include <torch/extension.h>
 #include <string>
 #include <tuple>
 #include <vector>
 #ifdef _OPENMP
 #include <omp.h>
```

### Comparing `deepspeed-0.8.3/csrc/spatial/csrc/opt_bias_add.cu` & `deepspeed-0.9.0/csrc/spatial/csrc/opt_bias_add.cu`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright 2022 The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #include <cassert>
 #include "memory_access_utils.h"
 #include "spatial_cuda_layers.h"
 
 /*
 Fused bias add variants
```

### Comparing `deepspeed-0.8.3/csrc/spatial/csrc/pt_binding.cpp` & `deepspeed-0.9.0/csrc/spatial/csrc/pt_binding.cpp`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright 2022 The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #include <c10/cuda/CUDAStream.h>
 #include <torch/extension.h>
 #include <cstdio>
 #include <vector>
 #include "spatial_cuda_layers.h"
```

### Comparing `deepspeed-0.8.3/csrc/spatial/includes/spatial_cuda_layers.h` & `deepspeed-0.9.0/csrc/spatial/includes/spatial_cuda_layers.h`

 * *Files 14% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright 2022 The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #pragma once
 
 #if __CUDA_ARCH__ >= 530
 #define HALF_PRECISION_AVAILABLE = 1
 #endif
```

### Comparing `deepspeed-0.8.3/csrc/transformer/cublas_wrappers.cu` & `deepspeed-0.9.0/csrc/transformer/cublas_wrappers.cu`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #include "cublas_wrappers.h"
 
 #ifdef __HIP_PLATFORM_HCC__
 int cublas_gemm_ex(rocblas_handle handle,
                    rocblas_operation transa,
                    rocblas_operation transb,
```

### Comparing `deepspeed-0.8.3/csrc/transformer/dropout_kernels.cu` & `deepspeed-0.9.0/csrc/transformer/dropout_kernels.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #include "custom_cuda_layers.h"
 
 const int unroll_factor = 4;
 
 __global__ void dropout_kernel(const int N,
                                const float ratio,
@@ -273,15 +274,15 @@
     dim3 block_dim = DS_CUDA_NUM_THREADS;
 
     if (dim > 512) {
         block_dim.x >>= 1;
         grid_dim.x <<= 1;
     }
     uint64_t inc = total_count / grid_dim.x / block_dim.x;
-    std::pair<uint64_t, uint64_t> seed = Context::Instance().IncrementOffset(inc);
+    std::pair<uint64_t, uint64_t> seed = TrainingContext::Instance().IncrementOffset(inc);
     if (bwd)
         dropout_kernel_bwd<<<grid_dim, block_dim, 0, stream>>>(
             total_count, ratio, vals, out, mask, seed);
     else
         dropout_kernel<<<grid_dim, block_dim, 0, stream>>>(
             total_count, ratio, out, vals, mask, seed);
 }
@@ -620,15 +621,15 @@
 
     int total_count = batch * dim / unroll_factor;
 
     dim3 grid_dim = DS_GET_BLOCKS(total_count);
     dim3 block_dim = DS_CUDA_NUM_THREADS;
 
     uint64_t inc = (batch * dim) / grid_dim.x / block_dim.x;
-    std::pair<uint64_t, uint64_t> seed = Context::Instance().IncrementOffset(inc);
+    std::pair<uint64_t, uint64_t> seed = TrainingContext::Instance().IncrementOffset(inc);
 
     dropout_kernel<<<grid_dim, block_dim, 0, stream>>>(
         total_count, dim, ratio, bias, out, mask, seed);
 }
 
 template void launch_dropout(float*,
                              const float* bias,
@@ -842,15 +843,15 @@
     assert(unroll_factor == 4);
 
     int total_count = batch * dim / unroll_factor;
     dim3 grid_dim = DS_GET_BLOCKS(total_count);
     dim3 block_dim = DS_CUDA_NUM_THREADS;
 
     uint64_t inc = (batch * dim) / grid_dim.x / block_dim.x;
-    std::pair<uint64_t, uint64_t> seed = Context::Instance().IncrementOffset(inc);
+    std::pair<uint64_t, uint64_t> seed = TrainingContext::Instance().IncrementOffset(inc);
 
     dropout_kernel<<<grid_dim, block_dim, 0, stream>>>(
         total_count, dim, ratio, input, residual, bias, out, mask, seed);
 }
 
 template void launch_dropout(float*,
                              const float*,
```

### Comparing `deepspeed-0.8.3/csrc/transformer/ds_transformer_cuda.cpp` & `deepspeed-0.9.0/csrc/transformer/ds_transformer_cuda.cpp`

 * *Files 1% similar despite different names*

```diff
@@ -1,7 +1,12 @@
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
 #include <torch/extension.h>
 
 #include <cublas_v2.h>
 #include <cuda_fp16.h>
 #include <cuda_runtime.h>
 #include <type_traits>
 #include <unordered_map>
@@ -69,16 +74,16 @@
       _seq_length(seq_length),
       _training(true),
       _pre_or_postLayerNorm(pre_or_postLayerNorm),
       _attn_dropout_checkpoint(attn_dropout_checkpoint),
       _normalize_invertible(normalize_invertible),
       _gelu_checkpoint(gelu_checkpoint),
       _stochastic_mode(stochastic_mode),
-      _stream(Context::Instance().GetCurrentStream()),
-      _cublasHandle(Context::Instance().GetCublasHandle()),
+      _stream(TrainingContext::Instance().GetCurrentStream()),
+      _cublasHandle(TrainingContext::Instance().GetCublasHandle()),
       _qkv_linear(typename FeedForward<T>::Config(batch_size * seq_length,
                                                   3 * hidden_size,
                                                   hidden_size,
                                                   gemm_algos[0])),
       _attn_out_linear(typename FeedForward<T>::Config(batch_size * seq_length,
                                                        hidden_size,
                                                        hidden_size,
@@ -174,15 +179,15 @@
                                       T* gelu_inp_ptr,
                                       T* ff2_inp_ptr)
 {
     cublasSetStream(_cublasHandle, _stream);
 
     if (!_stochastic_mode) cudaStreamSynchronize(_stream);
 
-    T* workspace = static_cast<T*>(Context::Instance().GetWorkSpace());
+    T* workspace = static_cast<T*>(TrainingContext::Instance().GetWorkSpace());
     size_t small_buf_size = bsz * _seq_length * _hidden_size;
     T* buf_0 = workspace;
     T* buf_1 = buf_0 + small_buf_size;
     T* buf_2 = buf_1;
 
     if (_normalize_invertible) {
         add_res_ptr = buf_1 + 3 * small_buf_size;
@@ -334,15 +339,15 @@
                                        T* grad_norm_w_ptr,
                                        T* grad_norm_b_ptr)
 {
     cublasSetStream(_cublasHandle, _stream);
 
     if (!_stochastic_mode) cudaStreamSynchronize(_stream);
 
-    T* workspace = static_cast<T*>(Context::Instance().GetWorkSpace());
+    T* workspace = static_cast<T*>(TrainingContext::Instance().GetWorkSpace());
     size_t small_buf_size = bsz * _seq_length * _hidden_size;
     T* buf_0 = workspace;
     T* buf_1 = buf_0 + small_buf_size;
     T* buf_2 = buf_1 + small_buf_size;
     T* buf_3 = buf_2 + small_buf_size;
 
     T* ff2_buf = (_gelu_checkpoint ? buf_3 + (bsz * _seq_length * _intermediate_size)
@@ -600,33 +605,34 @@
                              bool pre_or_postLayerNorm,
                              bool test_gemm,
                              bool attn_dropout_checkpoint,
                              bool normalize_invertible,
                              bool gelu_checkpoint,
                              bool stochastic_mode)
 {
-    Context::Instance().SetSeed(seed);
-    Context::Instance().TestGemmFP16(
+    TrainingContext::Instance().SetSeed(seed);
+    TrainingContext::Instance().TestGemmFP16(
         test_gemm, batch_size, init_seq_length, num_heads, hidden_dim / num_heads);
 
-    auto layer = std::make_shared<BertTransformerLayer<T>>(layer_id,
-                                                           batch_size,
-                                                           hidden_dim,
-                                                           num_heads,
-                                                           intermediate_size,
-                                                           init_seq_length,
-                                                           attn_dropout_ratio,
-                                                           hidden_dropout_ratio,
-                                                           layer_norm_eps,
-                                                           pre_or_postLayerNorm,
-                                                           Context::Instance().GetGemmAlgos(),
-                                                           attn_dropout_checkpoint,
-                                                           normalize_invertible,
-                                                           gelu_checkpoint,
-                                                           stochastic_mode);
+    auto layer =
+        std::make_shared<BertTransformerLayer<T>>(layer_id,
+                                                  batch_size,
+                                                  hidden_dim,
+                                                  num_heads,
+                                                  intermediate_size,
+                                                  init_seq_length,
+                                                  attn_dropout_ratio,
+                                                  hidden_dropout_ratio,
+                                                  layer_norm_eps,
+                                                  pre_or_postLayerNorm,
+                                                  TrainingContext::Instance().GetGemmAlgos(),
+                                                  attn_dropout_checkpoint,
+                                                  normalize_invertible,
+                                                  gelu_checkpoint,
+                                                  stochastic_mode);
 
     s_transformer_layers[layer_id] = layer;
 
     std::string dtype = (std::is_same<T, __half>::value) ? "half" : "float";
 
     std::cout << "layer #" << layer_id << " is created with date type [" << dtype << "]."
               << std::endl;
@@ -716,15 +722,15 @@
                                                          seq_len,
                                                          layer->GetHiddenSize(),
                                                          layer->GetIntermediateSize(),
                                                          layer->GetNumHeads(),
                                                          layer->IsTrainingMode(),
                                                          layer->GeluCheckpoint())},
                                   options);
-    Context::Instance().SetWorkSpace((T*)workspace.data_ptr());
+    TrainingContext::Instance().SetWorkSpace((T*)workspace.data_ptr());
 
     auto inp_norm = ((prelayernorm || !normalize_invertible) ? torch::empty_like(input) : output);
     auto add_res = (normalize_invertible ? inp_norm : torch::empty_like(input));
     auto attn_o_inp = torch::empty_like(input);
     auto qkv_tf = torch::empty({(bsz * seq_len), output_w.size(0) * 3}, options);
 
     auto attn_prob_dropout_mask =
@@ -900,15 +906,15 @@
                                                          seq_len,
                                                          layer->GetHiddenSize(),
                                                          layer->GetIntermediateSize(),
                                                          layer->GetNumHeads(),
                                                          layer->IsTrainingMode(),
                                                          layer->GeluCheckpoint())},
                                   options);
-    Context::Instance().SetWorkSpace((T*)workspace.data_ptr());
+    TrainingContext::Instance().SetWorkSpace((T*)workspace.data_ptr());
 
     auto grad_input = torch::empty_like(input);
     auto grad_attn_qkvw = torch::empty_like(attn_qkvw);
     auto grad_attn_qkvb = torch::empty_like(attn_qkvb);
     auto grad_attn_ow = torch::empty_like(attn_ow);
     auto grad_attn_ob = torch::empty_like(attn_ob);
     auto grad_attn_nw = torch::empty_like(attn_nw);
```

### Comparing `deepspeed-0.8.3/csrc/transformer/gelu_kernels.cu` & `deepspeed-0.9.0/csrc/transformer/gelu_kernels.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #include "custom_cuda_layers.h"
 
 inline __device__ float gelu(const float x)
 {
     const float sqrt_param = 0.79788456080286535587989211986876f;
     const float mul_param = 0.044715;
```

### Comparing `deepspeed-0.8.3/csrc/transformer/general_kernels.cu` & `deepspeed-0.9.0/csrc/transformer/general_kernels.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #include "general_kernels.h"
 
 namespace cg = cooperative_groups;
 
 template <typename T>
 __global__ void column_sum_reduce(const T* __restrict__ inp,
@@ -157,15 +158,15 @@
                               int seq_length,
                               int hidden_dim,
                               cudaStream_t& stream)
 {
     int total_count = batch_size * seq_length * hidden_dim / 4;
     dim3 grid_dim = DS_GET_BLOCKS(total_count);  //(batch_size * seq_length);
 
-    dim3 block_dim = DS_CUDA_NUM_THREADS;  //(hidden_dim / 4);
+    dim3 block_dim = DS_CUDA_NUM_THREADS;        //(hidden_dim / 4);
 
     fused_add2_kernel<<<grid_dim, block_dim, 0, stream>>>(total_count, out, inp1, inp2);
 }
 
 template <>
 void launch_fused_add2<__half>(__half* out,
                                const __half* inp1,
@@ -174,15 +175,15 @@
                                int seq_length,
                                int hidden_dim,
                                cudaStream_t& stream)
 {
     int total_count = batch_size * seq_length * hidden_dim / 4;
     dim3 grid_dim = DS_GET_BLOCKS(total_count);  //(batch_size * seq_length);
 
-    dim3 block_dim = DS_CUDA_NUM_THREADS;  //(hidden_dim / 4);
+    dim3 block_dim = DS_CUDA_NUM_THREADS;        //(hidden_dim / 4);
 
     fused_add2_kernel<<<grid_dim, block_dim, 0, stream>>>(total_count, out, inp1, inp2);
 }
 
 __global__ void fused_add3_kernel(float* out,
                                   const float* inp1,
                                   const float* inp2,
```

### Comparing `deepspeed-0.8.3/csrc/transformer/inference/csrc/apply_rotary_pos_emb.cu` & `deepspeed-0.9.0/csrc/transformer/inference/csrc/apply_rotary_pos_emb.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright 2022 The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #include "inference_cuda_layers.h"
 
 #ifndef __HIP_PLATFORM_HCC__
 #include <cuda_profiler_api.h>
 #endif
```

### Comparing `deepspeed-0.8.3/csrc/transformer/inference/csrc/dequantize.cu` & `deepspeed-0.9.0/csrc/transformer/inference/csrc/dequantize.cu`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright 2022 The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #include "inference_cuda_layers.h"
 
 #define MAX_QUANTIZE_GROUPING 1024
 
 #define loop_unroll 1
 #define loop_unroll_bits 1
```

### Comparing `deepspeed-0.8.3/csrc/transformer/inference/csrc/gelu.cu` & `deepspeed-0.9.0/csrc/transformer/inference/csrc/gelu.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright 2022 The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #include "conversion_utils.h"
 #include "inference_cuda_layers.h"
 #include "memory_access_utils.h"
 
 namespace cg = cooperative_groups;
 #define MAX_CAP 4
```

### Comparing `deepspeed-0.8.3/csrc/transformer/inference/csrc/layer_norm.cu` & `deepspeed-0.9.0/csrc/transformer/inference/csrc/layer_norm.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright 2022 The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #include "conversion_utils.h"
 #include "ds_kernel_utils.h"
 #include "inference_cuda_layers.h"
 #include "memory_access_utils.h"
 #include "reduction_utils.h"
```

### Comparing `deepspeed-0.8.3/csrc/transformer/inference/csrc/pt_binding.cpp` & `deepspeed-0.9.0/csrc/transformer/inference/csrc/pt_binding.cpp`

 * *Files 12% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright 2022 The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #include <c10/cuda/CUDAStream.h>
 #include <torch/extension.h>
 #include <stdexcept>
 #include <vector>
 #include "inference_context.h"
 #include "inference_cublas_wrappers.h"
@@ -91,67 +92,69 @@
                            bsz,
                            heads,
                            seq_len,
                            soft_len,
                            head_offset,
                            mask_stride,
                            mp_size,
-                           Context::Instance().GetCurrentStream(async_op));
+                           InferenceContext::Instance().GetCurrentStream(async_op));
 
     return attn_scores_c;
 }
 
 template <typename T>
 void allocate_workspace(unsigned hidden_dim,
                         unsigned num_heads,
                         unsigned prompt_length,
                         unsigned batch_size,
                         unsigned num_layers,
                         unsigned mp_size = 1,
                         bool external_cache = false,
                         unsigned rank = 0,
-                        unsigned max_out_tokens = 1024)
+                        unsigned max_out_tokens = 1024,
+                        unsigned min_out_tokens = 1)
 {
-    Context::Instance().GenWorkSpace(num_layers,
-                                     num_heads,
-                                     batch_size,
-                                     prompt_length,
-                                     hidden_dim,
-                                     mp_size,
-                                     external_cache,
-                                     sizeof(T),
-                                     rank,
-                                     max_out_tokens);
+    InferenceContext::Instance().GenWorkSpace(num_layers,
+                                              num_heads,
+                                              batch_size,
+                                              prompt_length,
+                                              hidden_dim,
+                                              mp_size,
+                                              external_cache,
+                                              sizeof(T),
+                                              rank,
+                                              max_out_tokens,
+                                              min_out_tokens);
 }
 
 template <typename T>
 at::Tensor einsum_sec_sm_ecm(at::Tensor& Q, at::Tensor& W)
 {
     auto options = at::TensorOptions()
                        .dtype(Q.options().dtype())
                        .layout(at::kStrided)
                        .device(at::kCUDA)
                        .requires_grad(false);
-    T* workspace = (T*)Context::Instance().GetWorkSpace();
+    T* workspace = (T*)InferenceContext::Instance().GetWorkSpace();
     float alpha = 1;
     float gemm_beta = 0.0;
 
     /*
     // Reallocate memory if we received a new prompt
     if (!workspace || input.size(1) != 1) {
-        allocate_workspace<T>(W.size(1), Context::Instance().GetMaxTokenLenght(), Q.size(0), 1,
-    head_size); workspace = (T*)Context::Instance().GetWorkSpace();
+        allocate_workspace<T>(W.size(1), InferenceContext::Instance().GetMaxTokenLenght(),
+    Q.size(0), 1, head_size); workspace = (T*)InferenceContext::Instance().GetWorkSpace();
     }
     */
 
     auto O = at::from_blob(workspace, {Q.size(1), Q.size(2), W.size(1)}, options);
     unsigned m = W.size(1);
     unsigned n = Q.size(1) * Q.size(2);
     unsigned k = Q.size(0);
-    cublas_gemm_ex(Context::Instance().GetCublasHandle(),
+    cublas_gemm_ex(InferenceContext::Instance().GetCublasHandle(),
                    CUBLAS_OP_N,
                    CUBLAS_OP_T,
                    m,
                    n,
                    k,
                    &alpha,
                    &gemm_beta,
@@ -190,16 +193,17 @@
     float alpha = norm_factor;
     float gemm_beta = 0.0;
     auto attn_score = at::empty({bsz, heads, seq_len, soft_len}, options);
     int k = prev_value_cont.size(2) / heads;
 
     auto mask_stride = get_attn_mask_stride(attn_mask);
 
-    cublasSetStream(Context::Instance().GetCublasHandle(), Context::Instance().GetCurrentStream());
-    cublas_strided_batched_gemm(Context::Instance().GetCublasHandle(),
+    cublasSetStream(InferenceContext::Instance().GetCublasHandle(),
+                    InferenceContext::Instance().GetCurrentStream());
+    cublas_strided_batched_gemm(InferenceContext::Instance().GetCublasHandle(),
                                 soft_len,
                                 seq_len,
                                 k,
                                 &alpha,
                                 &gemm_beta,
                                 (T*)prev_key_cont.data_ptr(),
                                 (T*)query_cont.data_ptr(),
@@ -226,17 +230,17 @@
                            bsz,
                            heads,
                            seq_len,
                            soft_len,
                            0,
                            mask_stride,
                            1,
-                           Context::Instance().GetCurrentStream(false));
+                           InferenceContext::Instance().GetCurrentStream(false));
     alpha = 1.0;
-    cublas_strided_batched_gemm(Context::Instance().GetCublasHandle(),
+    cublas_strided_batched_gemm(InferenceContext::Instance().GetCublasHandle(),
                                 k,
                                 seq_len,
                                 soft_len,
                                 &alpha,
                                 &gemm_beta,
                                 (T*)prev_value_cont.data_ptr(),
                                 (T*)attn_score.data_ptr(),
@@ -359,29 +363,30 @@
                        int window_size,
                        at::Tensor& alibi,
                        int layer_id)
 {
     float layer_scale = alibi.sizes().size() > 1 ? std::max(1, layer_id) : 1.0;
     float alpha = norm_factor * norm_factor / layer_scale;
     float gemm_beta = 0.0;
-    T* workspace = (T*)Context::Instance().GetAttentionUnfusedWorkspace();
+    T* workspace = (T*)InferenceContext::Instance().GetAttentionUnfusedWorkspace();
 
-    cublasSetStream(Context::Instance().GetCublasHandle(), Context::Instance().GetCurrentStream());
-    cublas_strided_batched_gemm(Context::Instance().GetCublasHandle(),
+    cublasSetStream(InferenceContext::Instance().GetCublasHandle(),
+                    InferenceContext::Instance().GetCurrentStream());
+    cublas_strided_batched_gemm(InferenceContext::Instance().GetCublasHandle(),
                                 soft_len,
                                 seq_len,
                                 k,
                                 &alpha,
                                 &gemm_beta,
                                 (T*)prev_key_cont,
                                 (T*)query_cont,
                                 workspace,
                                 CUBLAS_OP_T,
                                 CUBLAS_OP_N,
-                                Context::Instance().GetMaxTokenLenght() * k,
+                                InferenceContext::Instance().GetMaxTokenLenght() * k,
                                 seq_len * k,
                                 seq_len * soft_len,
                                 bsz * heads,
 #ifdef __HIP_PLATFORM_HCC__
                                 rocblas_gemm_algo_standard);
 #else
                                 CUBLAS_GEMM_DEFAULT_TENSOR_OP);
@@ -395,37 +400,37 @@
                            local_attention,
                            window_size,
                            bsz,
                            seq_len,
                            soft_len,
                            heads);
     alpha = 1.0;
-    cublas_strided_batched_gemm(Context::Instance().GetCublasHandle(),
+    cublas_strided_batched_gemm(InferenceContext::Instance().GetCublasHandle(),
                                 k,
                                 seq_len,
                                 soft_len,
                                 &alpha,
                                 &gemm_beta,
                                 (T*)prev_value_cont,
                                 workspace,
                                 (T*)output,
                                 CUBLAS_OP_N,
                                 CUBLAS_OP_N,
-                                Context::Instance().GetMaxTokenLenght() * k,
+                                InferenceContext::Instance().GetMaxTokenLenght() * k,
                                 seq_len * soft_len,
                                 seq_len * k,
                                 bsz * heads,
 #ifdef __HIP_PLATFORM_HCC__
                                 rocblas_gemm_algo_standard);
 #else
                                 CUBLAS_GEMM_DEFAULT_TENSOR_OP);
 #endif
 }
 
-void reset_cache() { Context::Instance().reset_tokens(); }
+void reset_cache() { InferenceContext::Instance().reset_tokens(); }
 
 template <typename T>
 std::vector<at::Tensor> ds_softmax_context(at::Tensor& query_key_value,
                                            at::Tensor& attn_mask,
                                            int rotary_dim,
                                            bool rotate_half,
                                            bool rotate_every_two,
@@ -441,34 +446,35 @@
 {
     unsigned bsz = query_key_value.size(0);
     unsigned seq_len = query_key_value.size(1);
     unsigned hidden_dim = query_key_value.size(2) / 3;
 
     bool is_prompt = (seq_len > 1);
 
-    if (is_prompt) Context::Instance().reset_tokens(seq_len);
-    unsigned soft_len = Context::Instance().current_tokens();
+    if (is_prompt) InferenceContext::Instance().reset_tokens(seq_len);
+    unsigned soft_len = InferenceContext::Instance().current_tokens();
 
     int k = hidden_dim / heads;
     auto options = at::TensorOptions()
                        .dtype(query_key_value.options().dtype())
                        .layout(at::kStrided)
                        .device(at::kCUDA)
                        .requires_grad(false);
 
-    T* workspace = (T*)Context::Instance().GetWorkSpace();
+    T* workspace = (T*)InferenceContext::Instance().GetWorkSpace();
     size_t buf_size = bsz * seq_len * hidden_dim;
-    auto output = torch::from_blob(workspace + 4 * buf_size, {bsz, seq_len, hidden_dim}, options);
+    auto output = torch::from_blob(workspace + 3 * buf_size, {bsz, seq_len, hidden_dim}, options);
 
-    auto query_cont = workspace + 8 * buf_size;
-    size_t offset = 16 * (hidden_dim * bsz * Context::Instance().GetMaxTokenLenght()) +
-                    layer_id * 2 * bsz * Context::Instance().GetMaxTokenLenght() * hidden_dim;
+    auto query_cont = workspace + 4 * buf_size;
+    size_t offset =
+        10 * (hidden_dim * bsz * InferenceContext::Instance().GetMaxTokenLenght()) +
+        layer_id * 2 * bsz * InferenceContext::Instance().GetMaxTokenLenght() * hidden_dim;
     unsigned all_tokens = soft_len;
     auto kv_cache = workspace + offset + (hidden_dim / heads) * (is_prompt ? 0 : soft_len - 1);
-    size_t value_offset = bsz * Context::Instance().GetMaxTokenLenght() * hidden_dim;
+    size_t value_offset = bsz * InferenceContext::Instance().GetMaxTokenLenght() * hidden_dim;
 
     T* temp_buf = (T*)output.data_ptr() + at::numel(output);
     launch_bias_add_transform_0213<T>((T*)query_cont,
                                       kv_cache,
                                       kv_cache + value_offset,
                                       (T*)query_key_value.data_ptr(),
                                       nullptr,
@@ -477,30 +483,30 @@
                                       (is_prompt ? 0 : soft_len - 1),
                                       soft_len,
                                       hidden_dim,
                                       heads,
                                       rotary_dim,
                                       rotate_half,
                                       rotate_every_two,
-                                      Context::Instance().GetCurrentStream(),
+                                      InferenceContext::Instance().GetCurrentStream(),
                                       3,
-                                      Context::Instance().GetMaxTokenLenght());
+                                      InferenceContext::Instance().GetMaxTokenLenght());
     if (rotary_dim > 0 && rotate_half)
         launch_apply_rotary_pos_emb(query_cont,
                                     kv_cache,
                                     k,
                                     seq_len,
                                     rotary_dim,
                                     (is_prompt ? 0 : soft_len - 1),
                                     heads,
                                     bsz,
                                     rotate_half,
                                     rotate_every_two,
-                                    Context::Instance().GetCurrentStream(),
-                                    Context::Instance().GetMaxTokenLenght());
+                                    InferenceContext::Instance().GetCurrentStream(),
+                                    InferenceContext::Instance().GetMaxTokenLenght());
 
     attention_unfused<T>(workspace + offset,
                          (T*)query_cont,
                          attn_mask,
                          workspace + offset + value_offset,
                          temp_buf,
                          bsz,
@@ -517,21 +523,35 @@
                          layer_id);
     launch_transform4d_0213<T>((T*)output.data_ptr(),
                                temp_buf,
                                bsz,
                                heads,
                                seq_len,
                                output.size(2),
-                               Context::Instance().GetCurrentStream(false),
+                               InferenceContext::Instance().GetCurrentStream(false),
                                1);
 
-    if (layer_id == num_layers - 1) Context::Instance().advance_tokens();
-    auto prev_key = torch::from_blob(workspace + offset, {bsz, heads, all_tokens, k}, options);
+    if (layer_id == num_layers - 1) InferenceContext::Instance().advance_tokens();
+    auto prev_key = torch::from_blob(workspace + offset,
+                                     {bsz, heads, all_tokens, k},
+                                     {hidden_dim * InferenceContext::Instance().GetMaxTokenLenght(),
+                                      k * InferenceContext::Instance().GetMaxTokenLenght(),
+                                      k,
+                                      1},
+                                     options);
+
     auto prev_value =
-        torch::from_blob(workspace + offset + value_offset, {bsz, heads, all_tokens, k}, options);
+        torch::from_blob(workspace + offset + value_offset,
+                         {bsz, heads, all_tokens, k},
+                         {hidden_dim * InferenceContext::Instance().GetMaxTokenLenght(),
+                          k * InferenceContext::Instance().GetMaxTokenLenght(),
+                          k,
+                          1},
+                         options);
+
     return {output, prev_key, prev_value};
 }
 
 template <typename T>
 at::Tensor ds_bias_gelu(at::Tensor& input, at::Tensor& bias)
 {
     auto input_cont = input.contiguous();
@@ -539,15 +559,15 @@
     int bsz = input_cont.size(0) * input_cont.size(1);
     int intermediate_size = input_cont.size(2);
 
     launch_bias_gelu((T*)input_cont.data_ptr(),
                      (T*)bias.data_ptr(),
                      intermediate_size,
                      bsz,
-                     Context::Instance().GetCurrentStream());
+                     InferenceContext::Instance().GetCurrentStream());
     return input_cont;
 }
 
 at::Tensor ds_bias_geglu(at::Tensor& activation, at::Tensor& bias)
 {
     /*
     Used in FF of Stable diffusion
@@ -565,22 +585,22 @@
 
     if (activation.options().dtype() == torch::kFloat32) {
         launch_fused_bias_geglu((float*)output.data_ptr(),
                                 (const float*)activation.data_ptr(),
                                 (const float*)bias.data_ptr(),
                                 rows,
                                 channels,
-                                Context::Instance().GetCurrentStream());
+                                InferenceContext::Instance().GetCurrentStream());
     } else {
         launch_fused_bias_geglu((__half*)output.data_ptr(),
                                 (const __half*)activation.data_ptr(),
                                 (const __half*)bias.data_ptr(),
                                 rows,
                                 channels,
-                                Context::Instance().GetCurrentStream());
+                                InferenceContext::Instance().GetCurrentStream());
     }
 
     return output;
 }
 
 template <typename T>
 at::Tensor ds_bias_relu(at::Tensor& input, at::Tensor& bias)
@@ -590,15 +610,15 @@
     int bsz = input_cont.size(0) * input_cont.size(1);
     int intermediate_size = input_cont.size(2);
 
     launch_bias_relu((T*)input_cont.data_ptr(),
                      (T*)bias.data_ptr(),
                      intermediate_size,
                      bsz,
-                     Context::Instance().GetCurrentStream());
+                     InferenceContext::Instance().GetCurrentStream());
     return input_cont;
 }
 
 template <typename T>
 at::Tensor ds_bias_add(at::Tensor& input, at::Tensor& bias)
 {
     auto input_cont = input.contiguous();
@@ -606,15 +626,15 @@
     int bsz = input_cont.size(0) * input_cont.size(1);
     int hidden_size = input_cont.size(2);
 
     launch_bias_add((T*)input_cont.data_ptr(),
                     (T*)bias.data_ptr(),
                     hidden_size,
                     bsz,
-                    Context::Instance().GetCurrentStream());
+                    InferenceContext::Instance().GetCurrentStream());
     return input_cont;
 }
 
 template <typename T>
 at::Tensor ds_bias_residual(at::Tensor& input, at::Tensor& residual, at::Tensor& bias)
 {
     auto input_cont = input.contiguous();
@@ -623,15 +643,15 @@
     int bsz = input_cont.size(0) * input_cont.size(1);
     // launch_bias_residual((T*)input_cont.data_ptr(),
     //                      (T*)residual_cont.data_ptr(),
     //                      (T*)bias.data_ptr(),
     //                      bsz,
     //                      input_cont.size(2),
     //                      (bias.size(0) > 1),
-    //                      Context::Instance().GetCurrentStream());
+    //                      InferenceContext::Instance().GetCurrentStream());
     return input_cont;
 }
 
 at::Tensor ds_layer_norm(at::Tensor& input, at::Tensor& gamma, at::Tensor& beta, float epsilon)
 {
     const int rows = input.size(0) * input.size(1);
     const int elems_per_row = input.size(2);
@@ -641,24 +661,24 @@
         launch_fused_ln((__half*)output.data_ptr(),
                         (const __half*)input.data_ptr(),
                         (const __half*)gamma.data_ptr(),
                         (const __half*)beta.data_ptr(),
                         epsilon,
                         rows,
                         elems_per_row,
-                        Context::Instance().GetCurrentStream());
+                        InferenceContext::Instance().GetCurrentStream());
     } else {
         launch_fused_ln((float*)output.data_ptr(),
                         (const float*)input.data_ptr(),
                         (const float*)gamma.data_ptr(),
                         (const float*)beta.data_ptr(),
                         epsilon,
                         rows,
                         elems_per_row,
-                        Context::Instance().GetCurrentStream());
+                        InferenceContext::Instance().GetCurrentStream());
     }
 
     return output;
 }
 
 template <typename T>
 void ds_layer_norm_internal(T* workspace,
@@ -671,15 +691,15 @@
     launch_fused_ln(workspace,
                     (const T*)input.data_ptr(),
                     (const T*)gamma.data_ptr(),
                     (const T*)beta.data_ptr(),
                     epsilon,
                     bsz,
                     input.size(2),
-                    Context::Instance().GetCurrentStream());
+                    InferenceContext::Instance().GetCurrentStream());
 }
 
 /* Currently only used in unit testing */
 at::Tensor ds_layer_norm_residual(at::Tensor& input,
                                   at::Tensor& bias,
                                   at::Tensor& residual,
                                   at::Tensor& gamma,
@@ -696,26 +716,26 @@
                                  (const __half*)residual.data_ptr(),
                                  (const __half*)bias.data_ptr(),
                                  (const __half*)gamma.data_ptr(),
                                  (const __half*)beta.data_ptr(),
                                  epsilon,
                                  rows,
                                  elems_per_row,
-                                 Context::Instance().GetCurrentStream());
+                                 InferenceContext::Instance().GetCurrentStream());
     } else {
         launch_fused_residual_ln((float*)output.data_ptr(),
                                  (const float*)input.data_ptr(),
                                  (const float*)residual.data_ptr(),
                                  (const float*)bias.data_ptr(),
                                  (const float*)gamma.data_ptr(),
                                  (const float*)beta.data_ptr(),
                                  epsilon,
                                  rows,
                                  elems_per_row,
-                                 Context::Instance().GetCurrentStream());
+                                 InferenceContext::Instance().GetCurrentStream());
     }
 
     return output;
 }
 
 /* Currently only used in unit testing */
 std::vector<at::Tensor> ds_layer_norm_residual_store_pre_ln_res(at::Tensor& input,
@@ -737,61 +757,61 @@
                                                   (const __half*)residual.data_ptr(),
                                                   (const __half*)bias.data_ptr(),
                                                   (const __half*)gamma.data_ptr(),
                                                   (const __half*)beta.data_ptr(),
                                                   epsilon,
                                                   rows,
                                                   elems_per_row,
-                                                  Context::Instance().GetCurrentStream());
+                                                  InferenceContext::Instance().GetCurrentStream());
     } else {
         launch_fused_residual_ln_store_pre_ln_res((float*)norm_output.data_ptr(),
                                                   (float*)res_output.data_ptr(),
                                                   (const float*)input.data_ptr(),
                                                   (const float*)residual.data_ptr(),
                                                   (const float*)bias.data_ptr(),
                                                   (const float*)gamma.data_ptr(),
                                                   (const float*)beta.data_ptr(),
                                                   epsilon,
                                                   rows,
                                                   elems_per_row,
-                                                  Context::Instance().GetCurrentStream());
+                                                  InferenceContext::Instance().GetCurrentStream());
     }
 
     return {norm_output, res_output};
 }
 
 template <typename T>
 void quantized_gemm(void* output,
                     T* input,
                     at::Tensor& weight,
                     at::Tensor& qscale,
                     int groups,
                     int bsz,
                     int hidden_size)
 {
-    // T* weight16 = (T*)Context::Instance().GetWorkSpace() + 12 * hidden_size * bsz;
+    // T* weight16 = (T*)InferenceContext::Instance().GetWorkSpace() + 12 * hidden_size * bsz;
 
     auto options = at::TensorOptions()
                        .dtype(at::kHalf)
                        .layout(at::kStrided)
                        .device(at::kCUDA)
                        .requires_grad(false);
     auto tmp = torch::empty(weight.sizes(), options);
     T* weight16 = (T*)tmp.data_ptr();
     launch_dequantize(weight16,
                       (int8_t*)weight.data_ptr(),
                       (float*)qscale.data_ptr(),
                       weight.size(0),
                       weight.size(1),
                       groups,
-                      Context::Instance().GetCurrentStream());
+                      InferenceContext::Instance().GetCurrentStream());
 
     float alpha = (T)1.0;
     float gemm_beta = (T)0.0;
-    cublas_gemm_ex(Context::Instance().GetCublasHandle(),
+    cublas_gemm_ex(InferenceContext::Instance().GetCublasHandle(),
                    CUBLAS_OP_T,
                    CUBLAS_OP_N,
                    weight.size(0),
                    bsz,
                    weight.size(1),
                    &alpha,
                    &gemm_beta,
@@ -811,34 +831,35 @@
                               at::Tensor& weight,
                               at::Tensor& q_scale,
                               at::Tensor& bias,
                               at::Tensor& gamma,
                               at::Tensor& beta,
                               const float epsilon,
                               bool add_bias,
-                              bool q_int8)
+                              bool q_int8,
+                              bool transposed_mode)
 {
     int bsz = input.size(0) * input.size(1);
-    T* workspace = (T*)Context::Instance().GetWorkSpace();
+    T* workspace = (T*)InferenceContext::Instance().GetWorkSpace();
     workspace += (3 * bsz * input.size(2));
     ds_layer_norm_internal<T>(workspace, input, gamma, beta, epsilon);
 
     if (q_int8) {
         quantized_gemm<T>(
             output.data_ptr(), workspace, weight, q_scale, q_scale.size(0), bsz, input.size(2));
     } else {
         float alpha = (T)1.0;
         float gemm_beta = (T)0.0;
 
-        cublasSetStream(Context::Instance().GetCublasHandle(),
-                        Context::Instance().GetCurrentStream());
-        cublas_gemm_ex(Context::Instance().GetCublasHandle(),
-                       CUBLAS_OP_N,
+        cublasSetStream(InferenceContext::Instance().GetCublasHandle(),
+                        InferenceContext::Instance().GetCurrentStream());
+        cublas_gemm_ex(InferenceContext::Instance().GetCublasHandle(),
+                       (transposed_mode ? CUBLAS_OP_T : CUBLAS_OP_N),
                        CUBLAS_OP_N,
-                       weight.size(1),
+                       weight.size(transposed_mode ? 0 : 1),
                        bsz,
                        input.size(2),
                        &alpha,
                        &gemm_beta,
                        (T*)weight.data_ptr(),
                        workspace,
                        (T*)output.data_ptr(),
@@ -847,17 +868,17 @@
 #else
                        CUBLAS_GEMM_DEFAULT_TENSOR_OP);
 #endif
     }
     if (add_bias)
         launch_bias_add((T*)output.data_ptr(),
                         (T*)bias.data_ptr(),
-                        q_int8 ? weight.size(0) : weight.size(1),
+                        (transposed_mode || q_int8) ? weight.size(0) : weight.size(1),
                         bsz,
-                        Context::Instance().GetCurrentStream());
+                        InferenceContext::Instance().GetCurrentStream());
     return torch::from_blob(workspace, input.sizes(), input.options());
 }
 
 template <typename T>
 std::vector<at::Tensor> ds_qkv_gemm(at::Tensor& input,
                                     at::Tensor& weight,
                                     at::Tensor& q_scale,
@@ -866,29 +887,39 @@
                                     at::Tensor& beta,
                                     const float epsilon,
                                     bool add_bias,
                                     unsigned num_layers,
                                     bool external_cache,
                                     unsigned mp_size,
                                     unsigned rank,
-                                    bool q_int8)
+                                    bool q_int8,
+                                    bool transposed_mode)
 {
     int bsz = input.size(0) * input.size(1);
-    T* workspace = (T*)Context::Instance().GetWorkSpace();
-    int out_size = q_int8 ? weight.size(0) : weight.size(1);
+    T* workspace = (T*)InferenceContext::Instance().GetWorkSpace();
+    int out_size = (transposed_mode || q_int8) ? weight.size(0) : weight.size(1);
 
     auto options = at::TensorOptions()
                        .dtype(input.options().dtype())
                        .layout(at::kStrided)
                        .device(at::kCUDA)
                        .requires_grad(false);
 
     auto output = at::from_blob(workspace, {input.size(0), input.size(1), out_size}, options);
-    auto inp_norm = qkv_unfused_cublas<T>(
-        output, input, weight, q_scale, bias, gamma, beta, epsilon, add_bias, q_int8);
+    auto inp_norm = qkv_unfused_cublas<T>(output,
+                                          input,
+                                          weight,
+                                          q_scale,
+                                          bias,
+                                          gamma,
+                                          beta,
+                                          epsilon,
+                                          add_bias,
+                                          q_int8,
+                                          transposed_mode);
 
     return {output, inp_norm};
 }
 
 template <typename T>
 void quantized_gemm(at::Tensor& output,
                     at::Tensor& input,
@@ -908,19 +939,19 @@
     launch_dequantize((T*)weight16.data_ptr(),
                       (int8_t*)weight.data_ptr(),
                       (float*)qscale.data_ptr(),
                       weight.size(0),
                       weight.size(1),
                       groups,
                       merge_count,
-                      Context::Instance().GetCurrentStream());
+                      InferenceContext::Instance().GetCurrentStream());
 
     float alpha = (T)1.0;
     float gemm_beta = (T)0.0;
-    cublas_gemm_ex(Context::Instance().GetCublasHandle(),
+    cublas_gemm_ex(InferenceContext::Instance().GetCublasHandle(),
                    CUBLAS_OP_T,
                    CUBLAS_OP_N,
                    weight.size(0),
                    bsz,
                    input.size(2),
                    &alpha,
                    &gemm_beta,
@@ -959,47 +990,49 @@
 
     quantized_gemm<T>(output, inp_norm, weight, q_scale, groups, 0);
     if (add_bias)
         launch_bias_add((T*)output.data_ptr(),
                         (T*)bias.data_ptr(),
                         weight.size(1),
                         bsz,
-                        Context::Instance().GetCurrentStream());
+                        InferenceContext::Instance().GetCurrentStream());
 
     return output;
 }
 
 template <typename T>
 at::Tensor ds_linear_layer(at::Tensor& input,
                            at::Tensor& weight,
                            at::Tensor& bias,
                            bool add_bias,
                            bool do_flash_attn,
-                           int num_heads)
+                           int num_heads,
+                           bool transposed_mode)
 {
     auto input_cont = input.contiguous();
     auto options = at::TensorOptions()
                        .dtype(input_cont.options().dtype())
                        .layout(at::kStrided)
                        .device(at::kCUDA)
                        .requires_grad(false);
 
     int head_size = input_cont.size(2) / num_heads;
     int bsz = input.size(0) * input.size(1);
-    T* workspace = (T*)Context::Instance().GetWorkSpace();
+    T* workspace = (T*)InferenceContext::Instance().GetWorkSpace();
     auto output = at::from_blob(workspace, {input.size(0), input.size(1), weight.size(1)}, options);
 
     float alpha = (T)1.0;
     float gemm_beta = (T)0.0;
-    cublasSetStream(Context::Instance().GetCublasHandle(), Context::Instance().GetCurrentStream());
+    cublasSetStream(InferenceContext::Instance().GetCublasHandle(),
+                    InferenceContext::Instance().GetCurrentStream());
 
-    cublas_gemm_ex(Context::Instance().GetCublasHandle(),
+    cublas_gemm_ex(InferenceContext::Instance().GetCublasHandle(),
+                   (transposed_mode ? CUBLAS_OP_T : CUBLAS_OP_N),
                    CUBLAS_OP_N,
-                   CUBLAS_OP_N,
-                   weight.size(1),
+                   weight.size(transposed_mode ? 0 : 1),
                    bsz,
                    input_cont.size(2),
                    &alpha,
                    &gemm_beta,
                    (T*)weight.data_ptr(),
                    (T*)input_cont.data_ptr(),
                    (T*)output.data_ptr(),
@@ -1007,30 +1040,30 @@
                    rocblas_gemm_algo_standard);
 #else
                    CUBLAS_GEMM_DEFAULT_TENSOR_OP);
 #endif
     if (add_bias)
         launch_bias_add((T*)output.data_ptr(),
                         (T*)bias.data_ptr(),
-                        weight.size(1),
+                        weight.size(transposed_mode ? 0 : 1),
                         bsz,
-                        Context::Instance().GetCurrentStream());
+                        InferenceContext::Instance().GetCurrentStream());
     bool add_padding = (head_size % 32 != 0 && head_size < 64) || (head_size % 64 != 0);
     if (do_flash_attn) {
         if (add_padding) {
             int padded_head_size = head_size < 32 ? 32 : (head_size < 64 ? 64 : 128);
             auto padded_output = workspace + output.numel();
             auto final_output =
                 padded_output + (input.size(0) * input.size(1) * 3 * num_heads * padded_head_size);
             pad_data(padded_output,
                      workspace,
                      3 * bsz * num_heads,
                      head_size,
                      padded_head_size,
-                     Context::Instance().GetCurrentStream());
+                     InferenceContext::Instance().GetCurrentStream());
 
             launch_bias_add_transform_0213<T>(
                 final_output,
                 final_output + (input.size(0) * input.size(1) * num_heads * padded_head_size),
                 final_output + (input.size(0) * input.size(1) * 2 * num_heads * padded_head_size),
                 padded_output,
                 nullptr,
@@ -1039,15 +1072,15 @@
                 0,
                 input.size(1),
                 (num_heads * padded_head_size),
                 num_heads,
                 -1,
                 false,
                 false,
-                Context::Instance().GetCurrentStream(),
+                InferenceContext::Instance().GetCurrentStream(),
                 3,
                 input.size(1));
             return at::from_blob(final_output,
                                  {3, input.size(0), num_heads, input.size(1), padded_head_size},
                                  options);
             // return at::from_blob(padded_output, {input.size(0) * input.size(1), 3, num_heads,
             // padded_head_size}, options);
@@ -1064,15 +1097,15 @@
                 0,
                 input.size(1),
                 input_cont.size(2),
                 num_heads,
                 -1,
                 false,
                 false,
-                Context::Instance().GetCurrentStream(),
+                InferenceContext::Instance().GetCurrentStream(),
                 3,
                 input.size(1));
             return at::from_blob(
                 final_output, {3, input.size(0), num_heads, input.size(1), head_size}, options);
             // return at::from_blob(workspace, {input.size(0) * input.size(1), 3, num_heads,
             // head_size}, options);
         }
@@ -1082,41 +1115,41 @@
 }
 
 template <typename T>
 std::vector<at::Tensor> add_padding(at::Tensor& query, at::Tensor& key, at::Tensor& value)
 {
     int head_size = query.size(3);
     int padded_head_size = head_size < 32 ? 32 : (head_size < 64 ? 64 : 128);
-    T* workspace = (T*)Context::Instance().GetWorkSpace();
+    T* workspace = (T*)InferenceContext::Instance().GetWorkSpace();
     T* key_pad_ptr = workspace + padded_head_size * query.size(0) * query.size(1) * query.size(2);
     T* value_pad_ptr = key_pad_ptr + padded_head_size * query.size(0) * query.size(1) * 128;
     pad_head_seq(workspace,
                  (T*)query.data_ptr(),
                  query.size(0) * query.size(1),
                  query.size(2),
                  query.size(2),
                  head_size,
                  padded_head_size,
-                 Context::Instance().GetCurrentStream());
+                 InferenceContext::Instance().GetCurrentStream());
     pad_head_seq(key_pad_ptr,
                  (T*)key.data_ptr(),
                  query.size(0) * query.size(1),
                  key.size(2),
                  128,
                  head_size,
                  padded_head_size,
-                 Context::Instance().GetCurrentStream());
+                 InferenceContext::Instance().GetCurrentStream());
     pad_head_seq(value_pad_ptr,
                  (T*)value.data_ptr(),
                  query.size(0) * query.size(1),
                  key.size(2),
                  128,
                  head_size,
                  padded_head_size,
-                 Context::Instance().GetCurrentStream());
+                 InferenceContext::Instance().GetCurrentStream());
     return {
         at::from_blob(workspace,
                       {query.size(0), query.size(1), query.size(2), padded_head_size},
                       query.options()),
         at::from_blob(
             key_pad_ptr, {query.size(0), query.size(1), 128, padded_head_size}, query.options()),
         at::from_blob(
@@ -1130,44 +1163,44 @@
                                            int heads,
                                            bool add_padding)
 {
     int head_size = query.size(2) / heads;
     int key_value_length = add_padding ? 128 : key.size(1);
     int padded_head_size = add_padding ? (head_size < 32 ? 32 : (head_size < 64 ? 64 : 128))
                                        : head_size;
-    T* workspace = (T*)Context::Instance().GetWorkSpace();
+    T* workspace = (T*)InferenceContext::Instance().GetWorkSpace();
     T* key_pad_ptr = workspace + padded_head_size * query.size(0) * heads * query.size(1);
     T* value_pad_ptr = key_pad_ptr + padded_head_size * query.size(0) * heads * key_value_length;
     launch_pad_add_transform_0213(workspace,
                                   (T*)query.data_ptr(),
                                   query.size(0),
                                   query.size(2),
                                   query.size(1),
                                   query.size(1),
                                   heads,
                                   padded_head_size,
-                                  Context::Instance().GetCurrentStream());
+                                  InferenceContext::Instance().GetCurrentStream());
     launch_pad_add_transform_0213(key_pad_ptr,
                                   (T*)key.data_ptr(),
                                   key.size(0),
                                   key.size(2),
                                   key.size(1),
                                   key_value_length,
                                   heads,
                                   padded_head_size,
-                                  Context::Instance().GetCurrentStream());
+                                  InferenceContext::Instance().GetCurrentStream());
     launch_pad_add_transform_0213(value_pad_ptr,
                                   (T*)value.data_ptr(),
                                   value.size(0),
                                   value.size(2),
                                   value.size(1),
                                   key_value_length,
                                   heads,
                                   padded_head_size,
-                                  Context::Instance().GetCurrentStream());
+                                  InferenceContext::Instance().GetCurrentStream());
     return {
         at::from_blob(
             workspace, {query.size(0), heads, query.size(1), padded_head_size}, query.options()),
         at::from_blob(key_pad_ptr,
                       {query.size(0), heads, key_value_length, padded_head_size},
                       query.options()),
         at::from_blob(value_pad_ptr,
@@ -1192,52 +1225,53 @@
     auto output = at::empty({input_cont.size(0), input_cont.size(1), weight.size(1)}, options);
 
     quantized_gemm<T>(output, input_cont, weight, q_scale, groups, 0);
     launch_bias_add((T*)output.data_ptr(),
                     (T*)bias.data_ptr(),
                     weight.size(1),
                     bsz,
-                    Context::Instance().GetCurrentStream());
+                    InferenceContext::Instance().GetCurrentStream());
     return output;
 }
 
 template <typename T>
 at::Tensor ds_vector_matmul(at::Tensor& input,
                             at::Tensor& weight,
                             bool async_op,
                             at::Tensor& q_scale,
-                            bool q_int8)
+                            bool q_int8,
+                            bool transposed_mode)
 {
     auto options = at::TensorOptions()
                        .dtype(input.options().dtype())
                        .layout(at::kStrided)
                        .device(at::kCUDA)
                        .requires_grad(false);
     int out_size = q_int8 ? weight.size(0) : weight.size(1);
     int bsz = input.size(0) * input.size(1);
 
-    T* workspace = (T*)Context::Instance().GetWorkSpace();
+    T* workspace = (T*)InferenceContext::Instance().GetWorkSpace();
     auto output = at::from_blob(workspace, {input.size(0), input.size(1), out_size}, options);
     if (q_int8) {
         quantized_gemm<T>(output.data_ptr(),
                           (T*)input.data_ptr(),
                           weight,
                           q_scale,
                           q_scale.size(0),
                           bsz,
                           input.size(2));
     } else {
         float alpha = (T)1.0;
         float gemm_beta = (T)0.0;
-        cublasSetStream(Context::Instance().GetCublasHandle(),
-                        Context::Instance().GetCurrentStream(async_op));
-        cublas_gemm_ex(Context::Instance().GetCublasHandle(),
-                       CUBLAS_OP_N,
+        cublasSetStream(InferenceContext::Instance().GetCublasHandle(),
+                        InferenceContext::Instance().GetCurrentStream(async_op));
+        cublas_gemm_ex(InferenceContext::Instance().GetCublasHandle(),
+                       (transposed_mode ? CUBLAS_OP_T : CUBLAS_OP_N),
                        CUBLAS_OP_N,
-                       weight.size(1),
+                       weight.size(transposed_mode ? 0 : 1),
                        bsz,
                        input.size(2),
                        &alpha,
                        &gemm_beta,
                        (T*)weight.data_ptr(),
                        (T*)input.data_ptr(),
                        (T*)output.data_ptr(),
@@ -1282,47 +1316,48 @@
                               at::Tensor& beta,
                               const float epsilon,
                               bool preLayerNorm,
                               bool mlp_after_attn,
                               at::Tensor& q_scale,
                               at::Tensor& q_scale1,
                               bool q_int8,
-                              ActivationFuncType act_func_type)
+                              ActivationFuncType act_func_type,
+                              bool transposed_mode)
 {
     int bsz = input.size(0) * input.size(1);
-    T* inp_norm =
-        (T*)Context::Instance().GetWorkSpace() + torch::numel(input) + torch::numel(output);
+    T* inp_norm = (T*)InferenceContext::Instance().GetWorkSpace() + torch::numel(input) +
+                  torch::numel(output);
     T* intermediate = inp_norm + torch::numel(input);
 
     if (mlp_after_attn) {
         launch_fused_residual_ln((T*)inp_norm,
                                  (const T*)input.data_ptr(),
                                  (const T*)residual.data_ptr(),
                                  (const T*)input_bias.data_ptr(),
                                  (const T*)gamma.data_ptr(),
                                  (const T*)beta.data_ptr(),
                                  epsilon,
                                  bsz,
                                  input.size(2),
-                                 Context::Instance().GetCurrentStream());
+                                 InferenceContext::Instance().GetCurrentStream());
     } else {
         ds_layer_norm_internal(inp_norm, input, gamma, beta, epsilon);
     }
     if (q_int8) {
         quantized_gemm<T>(
             intermediate, inp_norm, weight, q_scale, q_scale.size(0), bsz, input.size(2));
     } else {
         float alpha = (T)1.0;
         float gemm_beta = (T)0.0;
-        cublasSetStream(Context::Instance().GetCublasHandle(),
-                        Context::Instance().GetCurrentStream());
-        cublas_gemm_ex(Context::Instance().GetCublasHandle(),
-                       CUBLAS_OP_N,
+        cublasSetStream(InferenceContext::Instance().GetCublasHandle(),
+                        InferenceContext::Instance().GetCurrentStream());
+        cublas_gemm_ex(InferenceContext::Instance().GetCublasHandle(),
+                       (transposed_mode ? CUBLAS_OP_T : CUBLAS_OP_N),
                        CUBLAS_OP_N,
-                       weight.size(1),
+                       weight.size(transposed_mode ? 0 : 1),
                        bsz,
                        input.size(2),
                        &alpha,
                        &gemm_beta,
                        (T*)weight.data_ptr(),
                        inp_norm,
                        intermediate,
@@ -1331,44 +1366,44 @@
 #else
                        CUBLAS_GEMM_DEFAULT_TENSOR_OP);
 #endif
     }
     if (act_func_type == ActivationFuncType::GELU) {
         launch_bias_gelu(intermediate,
                          (T*)bias.data_ptr(),
-                         q_int8 ? weight.size(0) : weight.size(1),
+                         (transposed_mode || q_int8) ? weight.size(0) : weight.size(1),
                          bsz,
-                         Context::Instance().GetCurrentStream());
+                         InferenceContext::Instance().GetCurrentStream());
     } else if (act_func_type == ActivationFuncType::ReLU) {
         launch_bias_relu(intermediate,
                          (T*)bias.data_ptr(),
-                         q_int8 ? weight.size(0) : weight.size(1),
+                         (transposed_mode || q_int8) ? weight.size(0) : weight.size(1),
                          bsz,
-                         Context::Instance().GetCurrentStream());
+                         InferenceContext::Instance().GetCurrentStream());
     }
 
     if (q_int8) {
         quantized_gemm<T>(output.data_ptr(),
                           intermediate,
                           weight1,
                           q_scale1,
                           q_scale1.size(0),
                           bsz,
                           input.size(2));
     } else {
         float alpha = (T)1.0;
         float gemm_beta = (T)0.0;
-        cublasSetStream(Context::Instance().GetCublasHandle(),
-                        Context::Instance().GetCurrentStream());
-        cublas_gemm_ex(Context::Instance().GetCublasHandle(),
+        cublasSetStream(InferenceContext::Instance().GetCublasHandle(),
+                        InferenceContext::Instance().GetCurrentStream());
+        cublas_gemm_ex(InferenceContext::Instance().GetCublasHandle(),
+                       (transposed_mode ? CUBLAS_OP_T : CUBLAS_OP_N),
                        CUBLAS_OP_N,
-                       CUBLAS_OP_N,
-                       weight1.size(1),
+                       weight1.size(transposed_mode ? 0 : 1),
                        bsz,
-                       weight1.size(0),
+                       weight1.size(transposed_mode ? 1 : 0),
                        &alpha,
                        &gemm_beta,
                        (T*)weight1.data_ptr(),
                        intermediate,
                        (T*)output.data_ptr(),
 #ifdef __HIP_PLATFORM_HCC__
                        rocblas_gemm_algo_standard);
@@ -1391,26 +1426,28 @@
                                     at::Tensor& beta,
                                     const float epsilon,
                                     bool preLayerNorm,
                                     bool mlp_after_attn,
                                     at::Tensor& q_scale,
                                     at::Tensor& q_scale1,
                                     bool q_int8,
-                                    int activation_type)
+                                    int activation_type,
+                                    bool transposed_mode)
 {
     auto options = at::TensorOptions()
                        .dtype(input.options().dtype())
                        .layout(at::kStrided)
                        .device(at::kCUDA)
                        .requires_grad(false);
 
-    int out_size = q_int8 ? weight_out.size(0) : weight_out.size(1);
-    auto output = at::from_blob((T*)Context::Instance().GetWorkSpace() + torch::numel(input),
-                                {input.size(0), input.size(1), out_size},
-                                options);
+    int out_size = (q_int8 || transposed_mode) ? weight_out.size(0) : weight_out.size(1);
+    auto output =
+        at::from_blob((T*)InferenceContext::Instance().GetWorkSpace() + torch::numel(input),
+                      {input.size(0), input.size(1), out_size},
+                      options);
     int bsz = input.size(0) * input.size(1);
 
     auto act_func_type = static_cast<ActivationFuncType>(activation_type);
     auto res_add = mlp_unfused_cublas<T>(output,
                                          mlp_after_attn ? input : residual,
                                          residual,
                                          input_bias,
@@ -1421,15 +1458,16 @@
                                          beta,
                                          epsilon,
                                          preLayerNorm,
                                          mlp_after_attn,
                                          q_scale,
                                          q_scale1,
                                          q_int8,
-                                         act_func_type);
+                                         act_func_type,
+                                         transposed_mode);
 
     return {output, res_add};
 }
 
 template <typename T>
 std::vector<at::Tensor> ds_mlp_gemm_int8(at::Tensor& input,
                                          at::Tensor& residual,
@@ -1457,40 +1495,42 @@
 
     auto residual_add = (preLayerNorm ? at::empty_like(input_cont) : inp_norm);
     quantized_gemm<T>(output, inp_norm, weight, q_scale, groups, 0);
     launch_bias_gelu((T*)output.data_ptr(),
                      (T*)bias.data_ptr(),
                      weight.size(1),
                      bsz,
-                     Context::Instance().GetCurrentStream());
+                     InferenceContext::Instance().GetCurrentStream());
 
     return {output, residual_add};
 }
 
 template <typename T>
 at::Tensor fused_gemm_gelu(at::Tensor& input,
                            at::Tensor& weight,
                            at::Tensor& weight_scale,
                            at::Tensor& bias,
                            at::Tensor& weight_out,
                            at::Tensor& weight_out_scale,
                            const float epsilon,
                            bool preLayerNorm,
                            bool q_int8,
-                           bool async_op)
+                           bool async_op,
+                           bool transposed_mode)
 {
     auto options = at::TensorOptions()
                        .dtype(input.options().dtype())
                        .layout(at::kStrided)
                        .device(at::kCUDA)
                        .requires_grad(false);
 
-    int intm_dim = q_int8 ? weight.size(0) : weight.size(1);
+    int intm_dim = (transposed_mode || q_int8) ? weight.size(0) : weight.size(1);
 
-    // auto output = at::from_blob((T*)Context::Instance().GetWorkSpace() + torch::numel(input),
+    // auto output = at::from_blob((T*)InferenceContext::Instance().GetWorkSpace() +
+    // torch::numel(input),
     //                            {input.size(0), input.size(1), out_size},
     //                            options);
     // T* intermediate = (T*)input.data_ptr() + torch::numel(input);
     auto intermediate = at::empty({input.size(0), input.size(1), intm_dim}, options);
 
     int bsz = input.size(0) * input.size(1);
 
@@ -1501,18 +1541,18 @@
                           (T*)input.data_ptr(),
                           weight,
                           weight_scale,
                           weight_scale.size(0),
                           bsz,
                           input.size(2));
     } else {
-        cublasSetStream(Context::Instance().GetCublasHandle(),
-                        Context::Instance().GetCurrentStream());
-        cublas_gemm_ex(Context::Instance().GetCublasHandle(),
-                       CUBLAS_OP_N,
+        cublasSetStream(InferenceContext::Instance().GetCublasHandle(),
+                        InferenceContext::Instance().GetCurrentStream());
+        cublas_gemm_ex(InferenceContext::Instance().GetCublasHandle(),
+                       (transposed_mode ? CUBLAS_OP_T : CUBLAS_OP_N),
                        CUBLAS_OP_N,
                        intm_dim,
                        bsz,
                        input.size(2),
                        &alpha,
                        &gemm_beta,
                        (T*)weight.data_ptr(),
@@ -1524,29 +1564,29 @@
                        CUBLAS_GEMM_DEFAULT_TENSOR_OP);
 #endif
     }
     launch_bias_gelu((T*)intermediate.data_ptr(),
                      (T*)bias.data_ptr(),
                      intm_dim,
                      bsz,
-                     Context::Instance().GetCurrentStream());
+                     InferenceContext::Instance().GetCurrentStream());
 
-    int out_size = q_int8 ? weight_out.size(0) : weight_out.size(1);
+    int out_size = (transposed_mode || q_int8) ? weight_out.size(0) : weight_out.size(1);
     auto output = at::empty({input.size(0), input.size(1), out_size}, options);
     if (q_int8) {
         quantized_gemm<T>(output.data_ptr(),
                           (T*)intermediate.data_ptr(),
                           weight_out,
                           weight_out_scale,
                           weight_out_scale.size(0),
                           bsz,
                           input.size(2));
     } else {
-        cublas_gemm_ex(Context::Instance().GetCublasHandle(),
-                       CUBLAS_OP_N,
+        cublas_gemm_ex(InferenceContext::Instance().GetCublasHandle(),
+                       (transposed_mode ? CUBLAS_OP_T : CUBLAS_OP_N),
                        CUBLAS_OP_N,
                        out_size,
                        bsz,
                        intm_dim,
                        &alpha,
                        &gemm_beta,
                        (T*)weight_out.data_ptr(),
@@ -1554,16 +1594,16 @@
                        (T*)output.data_ptr(),
 #ifdef __HIP_PLATFORM_HCC__
                        rocblas_gemm_algo_standard);
 #else
                        CUBLAS_GEMM_DEFAULT_TENSOR_OP);
 #endif
     }
-    // cudaEventRecord(Context::Instance().GetCompEvent(2),
-    //                Context::Instance().GetCurrentStream(true));
+    // cudaEventRecord(InferenceContext::Instance().GetCompEvent(2),
+    //                InferenceContext::Instance().GetCurrentStream(true));
     return output;
 }
 
 template <typename T>
 at::Tensor& residual_add_bias(at::Tensor& hidden_state,
                               at::Tensor& residual,
                               const at::Tensor& attention_output,
@@ -1582,26 +1622,26 @@
                              static_cast<T*>(attention_output.data_ptr()),
                              static_cast<T*>(final_bias.data_ptr()),
                              static_cast<T*>(attention_bias.data_ptr()),
                              bsz,
                              hidden_size,
                              mp_size,
                              preln,
-                             Context::Instance().GetCurrentStream());
+                             InferenceContext::Instance().GetCurrentStream());
     else
         launch_gptj_residual_add<T>(
             static_cast<T*>(residual.data_ptr()),
             static_cast<T*>(hidden_state.data_ptr()),
             static_cast<T*>(attention_output.data_ptr()),
             static_cast<T*>(final_bias.data_ptr()),
             static_cast<T*>((add_bias ? attention_bias.data_ptr() : nullptr)),
             hidden_size,
             bsz,
             mp_size,
-            Context::Instance().GetCurrentStream());
+            InferenceContext::Instance().GetCurrentStream());
     return residual;
 }
 
 std::vector<at::Tensor> apply_rotary_pos_emb(at::Tensor& mixed_query,
                                              at::Tensor& key_layer,
                                              unsigned rotary_dim,
                                              unsigned offset,
@@ -1623,29 +1663,29 @@
                                            seq_len,
                                            rotary_dim,
                                            offset,
                                            num_heads,
                                            bsz,
                                            rotate_half,
                                            rotate_every_two,
-                                           Context::Instance().GetCurrentStream(),
-                                           Context::Instance().GetMaxTokenLenght());
+                                           InferenceContext::Instance().GetCurrentStream(),
+                                           InferenceContext::Instance().GetMaxTokenLenght());
     else
         launch_apply_rotary_pos_emb<__half>((__half*)query_cont.data_ptr(),
                                             (__half*)key_cont.data_ptr(),
                                             head_size,
                                             seq_len,
                                             rotary_dim,
                                             offset,
                                             num_heads,
                                             bsz,
                                             rotate_half,
                                             rotate_every_two,
-                                            Context::Instance().GetCurrentStream(),
-                                            Context::Instance().GetMaxTokenLenght());
+                                            InferenceContext::Instance().GetCurrentStream(),
+                                            InferenceContext::Instance().GetMaxTokenLenght());
     return {query_cont, key_cont};
 }
 
 template <typename T>
 at::Tensor fused_gemm_gelu_int8(at::Tensor& input,
                                 at::Tensor& weight,
                                 at::Tensor& bias,
@@ -1666,24 +1706,24 @@
     int bsz = input_cont.size(0) * input_cont.size(1);
 
     quantized_gemm<T>(output, input_cont, weight, q_scale, groups, 0);
     launch_bias_gelu((T*)output.data_ptr(),
                      (T*)bias.data_ptr(),
                      weight.size(1),
                      bsz,
-                     Context::Instance().GetCurrentStream());
+                     InferenceContext::Instance().GetCurrentStream());
 
     return output;
 }
 
 at::Tensor moe_res_matmul(at::Tensor& moe_res, at::Tensor& coef, at::Tensor& output)
 {
     int M = moe_res.size(0) * moe_res.size(1);
     int N = moe_res.size(2);
-    Context::Instance().SynchComm();
+    InferenceContext::Instance().SynchComm();
     if (moe_res.scalar_type() == at::kFloat) {
         launch_moe_res_matmul<float>((float*)moe_res.data_ptr(),
                                      (float*)coef.data_ptr(),
                                      (float*)output.data_ptr(),
                                      M,
                                      N,
                                      at::cuda::getCurrentCUDAStream());
@@ -1694,14 +1734,18 @@
                                       M,
                                       N,
                                       at::cuda::getCurrentCUDAStream());
     }
     return output;
 }
 
+void ds_release_workspace() { InferenceContext::Instance().release_workspace(); }
+
+bool ds_retake_workspace() { return InferenceContext::Instance().retake_workspace(); }
+
 PYBIND11_MODULE(TORCH_EXTENSION_NAME, m)
 {
     m.def("softmax_fp32", &ds_softmax<float>, "DeepSpeed SoftMax with fp32 (CUDA)");
     m.def("softmax_fp16", &ds_softmax<__half>, "DeepSpeed SoftMax with fp16 (CUDA)");
     m.def(
         "softmax_context_fp32", &ds_softmax_context<float>, "DeepSpeed attention with fp32 (CUDA)");
     m.def("softmax_context_fp16",
@@ -1773,8 +1817,10 @@
     m.def("allocate_workspace_fp32",
           &allocate_workspace<float>,
           "DeepSpeed memory allocation for GPT inference with fp32 (CUDA)");
     m.def("allocate_workspace_fp16",
           &allocate_workspace<__half>,
           "DeepSpeed memory allocation for GPT inference with fp16 (CUDA)");
     m.def("reset_cache", &reset_cache, "Reset Cache for generation tasks");
+    m.def("release_workspace", &ds_release_workspace, "DeepSpeed Release Workspace");
+    m.def("retake_workspace", &ds_retake_workspace, "DeepSpeed Retake Workspace");
 }
```

### Comparing `deepspeed-0.8.3/csrc/transformer/inference/csrc/relu.cu` & `deepspeed-0.9.0/csrc/transformer/inference/csrc/relu.cu`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright 2022 The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #include "conversion_utils.h"
 #include "inference_cuda_layers.h"
 #include "memory_access_utils.h"
 
 namespace cg = cooperative_groups;
 #define MAX_CAP 4
```

### Comparing `deepspeed-0.8.3/csrc/transformer/inference/csrc/transform.cu` & `deepspeed-0.9.0/csrc/transformer/inference/csrc/transform.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright 2022 The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #ifndef __HIP_PLATFORM_HCC__
 #include <cuda_profiler_api.h>
 #endif
 #include "inference_cuda_layers.h"
 namespace cg = cooperative_groups;
```

### Comparing `deepspeed-0.8.3/csrc/transformer/inference/includes/inference_context.h` & `deepspeed-0.9.0/csrc/transformer/inference/includes/inference_context.h`

 * *Files 10% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright 2022 The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #pragma once
 
 #include <c10/cuda/CUDAStream.h>
 #include <cuda_runtime_api.h>
 #include <cassert>
 #include <iostream>
@@ -41,78 +42,82 @@
 {
     return std::max(
         std::min((N + DS_CUDA_NUM_THREADS - 1) / DS_CUDA_NUM_THREADS, DS_MAXIMUM_NUM_BLOCKS),
         // Use at least 1 block, since CUDA does not allow empty block
         1);
 }
 
-class Context {
+class InferenceContext {
 public:
-    Context()
+    InferenceContext()
         : _workspace(nullptr),
           _seed(42),
           _curr_offset(0),
           _stream(0),
           _free_memory_size(0),
           _num_tokens(1),
-          _attention_unfused_workspace_offset(0)
+          _attention_unfused_workspace_offset(0),
+          _workSpaceSize(0)
     {
+        _workSpaceSize = 0;
+        _workspace = 0;
         if (cublasCreate(&_cublasHandle) != CUBLAS_STATUS_SUCCESS) {
             auto message = std::string("Fail to create cublas handle.");
             std::cerr << message << std::endl;
             throw std::runtime_error(message);
         }
 #ifndef __HIP_PLATFORM_HCC__
         cublasSetMathMode(_cublasHandle, CUBLAS_TENSOR_OP_MATH);
 #endif
         cudaEventCreate(&_comp1_event);
         cudaEventCreate(&_comp2_event);
         cudaEventCreate(&_comp_event);
         cudaEventCreate(&_comm_event);
     }
 
-    virtual ~Context()
+    virtual ~InferenceContext()
     {
         cublasDestroy(_cublasHandle);
         cudaFree(_workspace);
         cudaEventDestroy(_comp1_event);
         cudaEventDestroy(_comp2_event);
         cudaEventDestroy(_comp_event);
         cudaEventDestroy(_comm_event);
     }
 
-    static Context& Instance()
+    static InferenceContext& Instance()
     {
-        static Context _ctx;
+        static InferenceContext _ctx;
         return _ctx;
     }
 
     void GenWorkSpace(const unsigned& num_layers,
                       const unsigned& num_heads,
                       const size_t& batch_size,
                       const size_t& prompt_len,
                       const size_t& hidden_dim,
                       const unsigned& mp_size,
                       const bool& external_cache,
                       const size_t& elem_size,
                       const unsigned& rank,
-                      unsigned max_out_tokens)
+                      unsigned max_out_tokens,
+                      unsigned min_out_tokens)
     {
         size_t total_size;
         if (!_free_memory_size) { cudaMemGetInfo(&_free_memory_size, &total_size); }
 
         // Flash attention requires padded heads and we'll conservatively allocate
         // for that here. Flash attention is only enabled for head size <= 128 right now
         const int head_size = hidden_dim / num_heads;
         const int padded_head_size = head_size <= 32 ? 32 : (head_size <= 64 ? 64 : 128);
         const int effective_head_size = (head_size > 128) ? head_size : padded_head_size;
 
-        size_t activation_size = 16 * (num_heads * effective_head_size) * batch_size;
+        size_t activation_size = 10 * (num_heads * effective_head_size) * batch_size;
         // Other sequence length dimension is added when the final workSpaceSize is calculated
-        size_t temp_size = batch_size * num_heads * max_out_tokens * 2;
+        size_t temp_size = batch_size * (num_heads / mp_size) * max_out_tokens;
         size_t cache_size =
             num_layers * batch_size * ((num_heads * effective_head_size) / mp_size) * 2;
         size_t minimal_requirements =
             temp_size + (_free_memory_size > GIGABYTE ? 500 : 100) * MEGABYTE;
         if (_free_memory_size < minimal_requirements) {
             printf("Requested:\t%lu\nFree:\t%lu\nTotal:\t%lu\n",
                    minimal_requirements,
@@ -124,33 +129,45 @@
         _max_seq_len = ((_free_memory_size - minimal_requirements) / elem_size) /
                        (activation_size + temp_size + cache_size);
         _max_seq_len = std::min((size_t)max_out_tokens, _max_seq_len);
         size_t workSpaceSize = ((external_cache ? (activation_size + temp_size)
                                                 : (activation_size + temp_size + cache_size))) *
                                _max_seq_len * elem_size;
         temp_size *= _max_seq_len * elem_size;
-        if (rank == 0 && !_workspace)
+
+        if (_max_seq_len < min_out_tokens) {
+            printf(
+                "Allocatable workspace available (%d tokens) is less than minimum requested "
+                "workspace (%d tokens)\n",
+                _max_seq_len,
+                min_out_tokens);
+            throw std::runtime_error("Workspace can't be allocated, not enough memory");
+        }
+
+        if (!_workspace) {
+            assert(_workspace == nullptr);
+            cudaMalloc(&_workspace, workSpaceSize);
+        } else if (_workSpaceSize < workSpaceSize) {
+            cudaFree(_workspace);
+            cudaMalloc(&_workspace, workSpaceSize);
+        }
+        if (rank == 0 && (!_workspace || _workSpaceSize < workSpaceSize))
             printf(
                 "------------------------------------------------------\n"
                 "Free memory : %f (GigaBytes)  \n"
                 "Total memory: %f (GigaBytes)  \n"
                 "Requested memory: %f (GigaBytes) \n"
                 "Setting maximum total tokens (input + output) to %lu \n"
+                "WorkSpace: %p \n"
                 "------------------------------------------------------\n",
                 (float)_free_memory_size / GIGABYTE,
                 (float)total_size / GIGABYTE,
                 (float)workSpaceSize / GIGABYTE,
-                _max_seq_len);
-        if (!_workspace) {
-            assert(_workspace == nullptr);
-            cudaMalloc(&_workspace, workSpaceSize);
-        } else if (_workSpaceSize < workSpaceSize) {
-            cudaFree(_workspace);
-            cudaMalloc(&_workspace, workSpaceSize);
-        }
+                _max_seq_len,
+                _workspace);
 
         if (!_workspace) {
             printf("Requested:\t%lu\nFree:\t%lu\nTotal:\t%lu\n",
                    workSpaceSize,
                    _free_memory_size,
                    total_size);
             throw std::runtime_error("Workspace is null.");
@@ -198,14 +215,25 @@
             if (!_stream) _stream = at::cuda::getStreamFromPool(true);
             return _stream;
         }
         cudaStream_t stream = at::cuda::getCurrentCUDAStream();
         return stream;
     }
 
+    void release_workspace()
+    {
+        cudaFree(_workspace);
+        _workspace = nullptr;
+    }
+    bool retake_workspace()
+    {
+        if (_workspace != nullptr || _workSpaceSize == 0) return true;
+        cudaMalloc(&_workspace, _workSpaceSize);
+        return _workspace != nullptr;
+    }
     cublasHandle_t GetCublasHandle() { return _cublasHandle; }
 
     std::pair<uint64_t, uint64_t> IncrementOffset(uint64_t offset_inc)
     {
         uint64_t offset = _curr_offset;
         _curr_offset += offset_inc;
         return std::pair<uint64_t, uint64_t>(_seed, offset);
```

### Comparing `deepspeed-0.8.3/csrc/transformer/inference/includes/inference_cublas_wrappers.h` & `deepspeed-0.9.0/csrc/transformer/inference/includes/inference_cublas_wrappers.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright 2022 The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #pragma once
 
 #include <assert.h>
 #include <cublas_v2.h>
 #include <cuda.h>
 #include <cuda_fp16.h>
```

### Comparing `deepspeed-0.8.3/csrc/transformer/inference/includes/inference_cuda_layers.h` & `deepspeed-0.9.0/csrc/transformer/inference/includes/inference_cuda_layers.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright 2022 The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #pragma once
 
 #include "ds_kernel_utils.h"
 
 #include <cuda.h>
 #include <cuda_fp16.h>
```

### Comparing `deepspeed-0.8.3/csrc/transformer/normalize_kernels.cu` & `deepspeed-0.9.0/csrc/transformer/normalize_kernels.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #include "custom_cuda_layers.h"
 
 namespace cg = cooperative_groups;
 
 /*
 Fused bias add, residual (elementwise) add, and normalization layer.
```

### Comparing `deepspeed-0.8.3/csrc/transformer/softmax_kernels.cu` & `deepspeed-0.9.0/csrc/transformer/softmax_kernels.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #include <math.h>
 #include "custom_cuda_layers.h"
 #include "general_kernels.h"
 
 namespace cg = cooperative_groups;
```

### Comparing `deepspeed-0.8.3/csrc/transformer/transform_kernels.cu` & `deepspeed-0.9.0/csrc/transformer/transform_kernels.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #include "custom_cuda_layers.h"
 
 #define rows_trans 16
 #define cols_trans 16
 
 template <typename T>
```

### Comparing `deepspeed-0.8.3/csrc/utils/flatten_unflatten.cpp` & `deepspeed-0.9.0/csrc/utils/flatten_unflatten.cpp`

 * *Files 8% similar despite different names*

```diff
@@ -1,11 +1,15 @@
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
 /*
-   Copyright 2020 The Microsoft DeepSpeed Team
-   Copyright NVIDIA/apex
-   This file is adapted from fused adam in NVIDIA/apex, commit a109f85
+Copyright NVIDIA/apex
+This file is adapted from fused adam in NVIDIA/apex, commit a109f85
 */
 
 #include <torch/csrc/utils/tensor_flatten.h>
 #include <torch/extension.h>
 // https://github.com/pytorch/pytorch/blob/master/torch/csrc/utils/tensor_flatten.h
 
 at::Tensor flatten(std::vector<at::Tensor> tensors)
```

### Comparing `deepspeed-0.8.3/deepspeed/__init__.py` & `deepspeed-0.9.0/deepspeed/__init__.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-'''
-Copyright 2020 The Microsoft DeepSpeed Team
-'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import sys
 import types
 import json
 from typing import Optional, Union
 import torch
 from torch.optim import Optimizer
@@ -12,24 +13,25 @@
 from packaging import version as pkg_version
 
 from . import ops
 from . import module_inject
 
 from .runtime.engine import DeepSpeedEngine, DeepSpeedOptimizerCallable, DeepSpeedSchedulerCallable
 from .runtime.engine import ADAM_OPTIMIZER, LAMB_OPTIMIZER
+from .runtime.hybrid_engine import DeepSpeedHybridEngine
 from .runtime.pipe.engine import PipelineEngine
 from .inference.engine import InferenceEngine
 from .inference.config import DeepSpeedInferenceConfig
 from .runtime.lr_schedules import add_tuning_arguments
 from .runtime.config import DeepSpeedConfig, DeepSpeedConfigError
 from .runtime.activation_checkpointing import checkpointing
 from .ops.transformer import DeepSpeedTransformerLayer, DeepSpeedTransformerConfig
 from .module_inject import replace_transformer_layer, revert_transformer_layer
 
-from .utils import log_dist, OnDevice
+from .utils import log_dist, OnDevice, logger
 from .comm.comm import init_distributed
 
 from .runtime import zero
 from .runtime import DeepSpeedOptimizer, ZeROOptimizer
 
 from .pipe import PipelineModule
 
@@ -47,20 +49,18 @@
 __version_major__, __version_minor__, __version_patch__ = _parse_version(__version__)
 __git_hash__ = git_hash
 __git_branch__ = git_branch
 
 
 def initialize(args=None,
                model: torch.nn.Module = None,
-               optimizer: Optional[Union[Optimizer,
-                                         DeepSpeedOptimizerCallable]] = None,
+               optimizer: Optional[Union[Optimizer, DeepSpeedOptimizerCallable]] = None,
                model_parameters: Optional[torch.nn.Module] = None,
                training_data: Optional[torch.utils.data.Dataset] = None,
-               lr_scheduler: Optional[Union[_LRScheduler,
-                                            DeepSpeedSchedulerCallable]] = None,
+               lr_scheduler: Optional[Union[_LRScheduler, DeepSpeedSchedulerCallable]] = None,
                mpu=None,
                dist_init_required: Optional[bool] = None,
                collate_fn=None,
                config=None,
                config_params=None):
     """Initialize the DeepSpeed Engine.
 
@@ -106,57 +106,85 @@
 
         * ``training_dataloader``: DeepSpeed dataloader if ``training_data`` was supplied,
           otherwise ``None``.
 
         * ``lr_scheduler``: Wrapped lr scheduler if user ``lr_scheduler`` is passed, or
           if ``lr_scheduler`` specified in JSON configuration. Otherwise ``None``.
     """
-    log_dist("DeepSpeed info: version={}, git-hash={}, git-branch={}".format(
-        __version__,
-        __git_hash__,
-        __git_branch__),
+    log_dist("DeepSpeed info: version={}, git-hash={}, git-branch={}".format(__version__, __git_hash__,
+                                                                             __git_branch__),
              ranks=[0])
 
     # Disable zero.Init context if it's currently enabled
     zero.partition_parameters.shutdown_init_context()
 
     assert model is not None, "deepspeed.initialize requires a model"
 
+    # Set config using config_params for backwards compat
+    if config is None and config_params is not None:
+        config = config_params
+
+    # Check for deepscale_config for backwards compat
+    if hasattr(args, "deepscale_config") and args.deepscale_config is not None:
+        logger.warning("************ --deepscale_config is deprecated, please use --deepspeed_config ************")
+        if hasattr(args, "deepspeed_config"):
+            assert (args.deepspeed_config is
+                    None), "Not sure how to proceed, we were given both a deepscale_config and deepspeed_config"
+        args.deepspeed_config = args.deepscale_config
+        args.deepscale_config = None
+
+    # Check that we have only one config passed
+    if hasattr(args, "deepspeed_config") and args.deepspeed_config is not None:
+        assert config is None, "Not sure how to proceed, we were given deepspeed configs in the deepspeed arguments and deepspeed.initialize() function call"
+        config = args.deepspeed_config
+    assert config != None, "DeepSpeed requires --deepspeed_config to specify configuration file"
+
     if not isinstance(model, PipelineModule):
-        engine = DeepSpeedEngine(args=args,
-                                 model=model,
-                                 optimizer=optimizer,
-                                 model_parameters=model_parameters,
-                                 training_data=training_data,
-                                 lr_scheduler=lr_scheduler,
-                                 mpu=mpu,
-                                 dist_init_required=dist_init_required,
-                                 collate_fn=collate_fn,
-                                 config=config,
-                                 config_params=config_params)
+        config_class = DeepSpeedConfig(config, mpu)
+        if config_class.hybrid_engine.enabled:
+            engine = DeepSpeedHybridEngine(args=args,
+                                           model=model,
+                                           optimizer=optimizer,
+                                           model_parameters=model_parameters,
+                                           training_data=training_data,
+                                           lr_scheduler=lr_scheduler,
+                                           mpu=mpu,
+                                           dist_init_required=dist_init_required,
+                                           collate_fn=collate_fn,
+                                           config=config,
+                                           config_class=config_class)
+        else:
+            engine = DeepSpeedEngine(args=args,
+                                     model=model,
+                                     optimizer=optimizer,
+                                     model_parameters=model_parameters,
+                                     training_data=training_data,
+                                     lr_scheduler=lr_scheduler,
+                                     mpu=mpu,
+                                     dist_init_required=dist_init_required,
+                                     collate_fn=collate_fn,
+                                     config=config,
+                                     config_class=config_class)
     else:
         assert mpu is None, "mpu must be None with pipeline parallelism"
+        mpu = model.mpu()
+        config_class = DeepSpeedConfig(config, mpu)
         engine = PipelineEngine(args=args,
                                 model=model,
                                 optimizer=optimizer,
                                 model_parameters=model_parameters,
                                 training_data=training_data,
                                 lr_scheduler=lr_scheduler,
-                                mpu=model.mpu(),
+                                mpu=mpu,
                                 dist_init_required=dist_init_required,
                                 collate_fn=collate_fn,
                                 config=config,
-                                config_params=config_params)
+                                config_class=config_class)
 
-    return_items = [
-        engine,
-        engine.optimizer,
-        engine.training_dataloader,
-        engine.lr_scheduler
-    ]
+    return_items = [engine, engine.optimizer, engine.training_dataloader, engine.lr_scheduler]
     return tuple(return_items)
 
 
 def _add_core_arguments(parser):
     r"""Helper (internal) function to update an argument parser with an argument group of the core DeepSpeed arguments.
         The core set of DeepSpeed arguments include the following:
         1) --deepspeed: boolean flag to enable DeepSpeed
@@ -167,46 +195,36 @@
     Arguments:
         parser: argument parser
     Return:
         parser: Updated Parser
     """
     group = parser.add_argument_group('DeepSpeed', 'DeepSpeed configurations')
 
-    group.add_argument(
-        '--deepspeed',
-        default=False,
-        action='store_true',
-        help=
-        'Enable DeepSpeed (helper flag for user code, no impact on DeepSpeed backend)')
-
-    group.add_argument('--deepspeed_config',
-                       default=None,
-                       type=str,
-                       help='DeepSpeed json configuration file.')
-
-    group.add_argument(
-        '--deepscale',
-        default=False,
-        action='store_true',
-        help=
-        'Deprecated enable DeepSpeed (helper flag for user code, no impact on DeepSpeed backend)'
-    )
+    group.add_argument('--deepspeed',
+                       default=False,
+                       action='store_true',
+                       help='Enable DeepSpeed (helper flag for user code, no impact on DeepSpeed backend)')
+
+    group.add_argument('--deepspeed_config', default=None, type=str, help='DeepSpeed json configuration file.')
+
+    group.add_argument('--deepscale',
+                       default=False,
+                       action='store_true',
+                       help='Deprecated enable DeepSpeed (helper flag for user code, no impact on DeepSpeed backend)')
 
     group.add_argument('--deepscale_config',
                        default=None,
                        type=str,
                        help='Deprecated DeepSpeed json configuration file.')
 
-    group.add_argument(
-        '--deepspeed_mpi',
-        default=False,
-        action='store_true',
-        help=
-        "Run via MPI, this will attempt to discover the necessary variables to initialize torch "
-        "distributed from the MPI environment")
+    group.add_argument('--deepspeed_mpi',
+                       default=False,
+                       action='store_true',
+                       help="Run via MPI, this will attempt to discover the necessary variables to initialize torch "
+                       "distributed from the MPI environment")
 
     return parser
 
 
 def add_config_arguments(parser):
     r"""Update the argument parser to enabling parsing of DeepSpeed command line arguments.
         The set of DeepSpeed arguments include the following:
@@ -274,40 +292,35 @@
         model: Required: original nn.module object without any wrappers
 
         config: Optional: instead of arguments, you can pass in a DS inference config dict or path to JSON file
 
     Returns:
         A deepspeed.InferenceEngine wrapped model.
     """
-    log_dist("DeepSpeed info: version={}, git-hash={}, git-branch={}".format(
-        __version__,
-        __git_hash__,
-        __git_branch__),
+    log_dist("DeepSpeed info: version={}, git-hash={}, git-branch={}".format(__version__, __git_hash__,
+                                                                             __git_branch__),
              ranks=[0])
 
     # Load config_dict from config first
     if config is None:
         config = {}
     if isinstance(config, str):
         with open(config, "r") as f:
             config_dict = json.load(f)
     elif isinstance(config, dict):
         config_dict = config
     else:
-        raise ValueError(
-            f"'config' argument expected string or dictionary, got {type(config)}")
+        raise ValueError(f"'config' argument expected string or dictionary, got {type(config)}")
 
     # Update with values from kwargs, ensuring no conflicting overlap between config and kwargs
     overlap_keys = set(config_dict.keys()).intersection(kwargs.keys())
     # If there is overlap, error out if values are different
     for key in overlap_keys:
         if config_dict[key] != kwargs[key]:
-            raise ValueError(
-                f"Conflicting argument '{key}' in 'config':{config_dict[key]} and kwargs:{kwargs[key]}"
-            )
+            raise ValueError(f"Conflicting argument '{key}' in 'config':{config_dict[key]} and kwargs:{kwargs[key]}")
     config_dict.update(kwargs)
 
     ds_inference_config = DeepSpeedInferenceConfig(**config_dict)
 
     engine = InferenceEngine(model, config=ds_inference_config)
 
     return engine
```

### Comparing `deepspeed-0.8.3/deepspeed/accelerator/abstract_accelerator.py` & `deepspeed-0.9.0/deepspeed/accelerator/abstract_accelerator.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,14 +1,18 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import abc
 from abc import ABC
 
 
 class DeepSpeedAccelerator(ABC):
+
     def __init__(self):
         self._name = None
         self._communication_backend_name = None
 
     # Device APIs
     @abc.abstractmethod
     def device_name(self, device_index):
```

### Comparing `deepspeed-0.8.3/deepspeed/accelerator/cuda_accelerator.py` & `deepspeed-0.9.0/deepspeed/accelerator/cuda_accelerator.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,11 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import os
 import pkgutil
 import importlib
 
 from .abstract_accelerator import DeepSpeedAccelerator
 # During setup stage torch may not be installed, pass on no torch will
@@ -10,29 +13,27 @@
 try:
     import torch.cuda
 except ImportError:
     pass
 
 
 class CUDA_Accelerator(DeepSpeedAccelerator):
+
     def __init__(self):
         self._name = 'cuda'
         self._communication_backend_name = 'nccl'
 
         # begin initialize for create_op_builder()
         # put all valid class name <--> class type mapping into class_dict
         op_builder_dir = self.op_builder_dir()
         op_builder_module = importlib.import_module(op_builder_dir)
-
         for _, module_name, _ in pkgutil.iter_modules([os.path.dirname(op_builder_module.__file__)]):
             # avoid self references
             if module_name != 'all_ops' and module_name != 'builder':
-                module = importlib.import_module("{}.{}".format(
-                    op_builder_dir,
-                    module_name))
+                module = importlib.import_module("{}.{}".format(op_builder_dir, module_name))
                 for member_name in module.__dir__():
                     if member_name.endswith(
                             'Builder'
                     ) and member_name != "OpBuilder" and member_name != "CUDAOpBuilder" and member_name != "TorchCPUOpBuilder":  # avoid abstract classes
                         if not member_name in self.class_dict:
                             self.class_dict[member_name] = getattr(module, member_name)
         # end initialize for create_op_builder()
```

### Comparing `deepspeed-0.8.3/deepspeed/accelerator/real_accelerator.py` & `deepspeed-0.9.0/deepspeed/accelerator/real_accelerator.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,8 +1,11 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 try:
     from accelerator.abstract_accelerator import DeepSpeedAccelerator as dsa1
 except ImportError as e:
     dsa1 = None
 try:
     from deepspeed.accelerator.abstract_accelerator import DeepSpeedAccelerator as dsa2
@@ -19,21 +22,16 @@
     # and extension would import the
     # run time abstract_accelerator/DeepSpeedAccelerator as its base
     # class, so we need to compare accel_obj with both base class.
     # if accel_obj is instance of DeepSpeedAccelerator in one of
     # accelerator.abstractor_accelerator
     # or deepspeed.accelerator.abstract_accelerator, consider accel_obj
     # is a conforming object
-    if not ((dsa1 != None and isinstance(accel_obj,
-                                         dsa1)) or
-            (dsa2 != None and isinstance(accel_obj,
-                                         dsa2))):
-        raise AssertionError(
-            f'{accel_obj.__class__.__name__} accelerator is not subclass of DeepSpeedAccelerator'
-        )
+    if not ((dsa1 != None and isinstance(accel_obj, dsa1)) or (dsa2 != None and isinstance(accel_obj, dsa2))):
+        raise AssertionError(f'{accel_obj.__class__.__name__} accelerator is not subclass of DeepSpeedAccelerator')
 
     # TODO: turn off is_available test since this breaks tests
     #assert accel_obj.is_available(), \
     #    f'{accel_obj.__class__.__name__} accelerator fails is_available() test'
 
 
 def get_accelerator():
```

### Comparing `deepspeed-0.8.3/deepspeed/autotuning/autotuner.py` & `deepspeed-0.9.0/deepspeed/autotuning/autotuner.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,11 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import shutil
 import subprocess
 import time
 import datetime
 import math
 import hjson
@@ -36,14 +39,15 @@
 ZERO_OPTIMIZATION_STAGE_DEFAULT = ZeroStageEnum.disabled
 
 
 class Autotuner:
     """The DeepSpeed Autotuner automatically discovers the optimal DeepSpeed configuration that delivers good training speed. The Autotuner uses model information, system information, and heuristics to efficiently tune system knobs that affect compute and memory efficiencies, such as ZeRO optimization stages, micro-batch sizes, and many other ZeRO optimization configurations. It not only reduces the time and resources user spend on tuning, but also can discover configurations better than hand-tuned methods.
     Autotuning with DeepSpeed requires no code change from DeepSpeed users. Please refer to the README for usage details.
     """
+
     def __init__(self, args, active_resources):
         self.args = args
         self.selected_exp_dir = None
 
         assert tabulate is not None, "Missing required package `tabulate`, please install with `pip install deepspeed[autotuning]`."
 
         logger.debug(f"autotunning args={args}")
@@ -88,15 +92,16 @@
         self.rm = self._get_resource_manager(active_resources)
 
         # get resource requirement for each autotuning experiment
         self.exp_num_nodes, self.exp_num_gpus = self._get_exp_resources(args)
 
         assert self.exp_num_gpus <= self.rm.num_gpus_per_node, "num_gpus in the autotuning configuration must not be less than the --num_gpus value in the train script if any"
         assert self.exp_num_nodes <= len(
-            self.rm.nodes), "num_nodes in the autotuning configuration must not be less than the --num_nodes value in the train script if any"
+            self.rm.nodes
+        ), "num_nodes in the autotuning configuration must not be less than the --num_nodes value in the train script if any"
 
         self.records = {}
         self.optimal_cmd = None
         self.optmal_ds_config = None
 
         self.mlflow_parent_id = None
 
@@ -121,40 +126,30 @@
                 else:
                     num_exps = val[2]
                 row.append(num_exps)
                 row.append(val[1])
                 row.append(val[0]['name'])
                 tab.append(row)
             summary = tabulate(tab,
-                               headers=[
-                                   "tuning_space",
-                                   "num_experiments",
-                                   "best_metric_val",
-                                   "best_exp_name"
-                               ],
+                               headers=["tuning_space", "num_experiments", "best_metric_val", "best_exp_name"],
                                tablefmt="pipe")
             print(summary)
-            with open(os.path.join(self.results_dir,
-                                   'summary.txt'),
-                      'w',
-                      buffering=BUFSIZE) as fd:
+            with open(os.path.join(self.results_dir, 'summary.txt'), 'w', buffering=BUFSIZE) as fd:
                 fd.write(summary)
                 fd.flush()
                 os.fsync(fd)
 
         if GLOBAL_TUNING_SPACE in best_space_records:
             best_exp, best_metric_val, total_num_exps = best_space_records[GLOBAL_TUNING_SPACE]
             if best_exp:
                 logger.info(
                     f"{best_exp['name']} is the optimal setup after tuning. The exp result is at {best_exp['result_dir']}."
                 )
             else:
-                logger.info(
-                    f"No optimal setup is found. Please check that experiments were run successfully."
-                )
+                logger.info(f"No optimal setup is found. Please check that experiments were run successfully.")
             tuning_duration = datetime.timedelta(seconds=(time.time() - self.start_time))
 
             logger.info(f"Tuning completed in {tuning_duration}")
             with open(os.path.join(self.results_dir, 'summary.txt'), 'a') as f:
                 f.write(
                     f"\n\nTuning completed in {tuning_duration}. Total number of experiments: {self.rm.experiment_count - 1}."
                 )
@@ -168,34 +163,29 @@
 
         Returns:
             [dict]: DeepSpeed configuration dictionary
         """
         user_config_file = None
         if "--deepspeed_config" in user_args:
             idx = user_args.index("--deepspeed_config")
-            assert ".json" in user_args[idx +
-                                        1],  "DeepSpeed --deepspeed_config requires a json file to specify the configuration"
+            assert ".json" in user_args[
+                idx + 1], "DeepSpeed --deepspeed_config requires a json file to specify the configuration"
 
             user_config_file = user_args[idx + 1]
         elif "--deepspeed" in user_args:
             idx = user_args.index("--deepspeed")
             if ".json" in user_args[idx + 1]:
                 user_config_file = user_args[idx + 1]
 
         logger.debug(f"user_config_file = {user_config_file}")
         if user_config_file is not None:
-            assert os.path.isfile(
-                user_config_file
-            ), "DeepSpeed configuration file: {} is not an existing file".format(
-                user_config_file
-            )
+            assert os.path.isfile(user_config_file), "DeepSpeed configuration file: {} is not an existing file".format(
+                user_config_file)
             if os.path.exists(user_config_file):
-                return json.load(open(user_config_file,
-                                      "r"),
-                                 object_pairs_hook=dict_raise_error_on_duplicate_keys)
+                return json.load(open(user_config_file, "r"), object_pairs_hook=dict_raise_error_on_duplicate_keys)
 
         return None
 
     def _get_resource_manager(self, active_resources):
         """Initialize and return a resource manager
 
         Args:
@@ -254,21 +244,19 @@
     def max_train_batch_size(self):
         return self.autotuning_config.max_train_batch_size
 
     def mp_size(self):
         return self.autotuning_config.mp_size
 
     def max_train_micro_batch_size_per_gpu(self):
-        if self.max_train_batch_size() and self.max_train_batch_size(
-        ) > 0:  # if the user specifies a max_train_batch_size
-            max_train_micro_batch_size = self.max_train_batch_size() * self.mp_size(
-            ) // (self.exp_num_gpus * self.exp_num_nodes
-                  )  # gradient accumulation steps >=1
-            return min(self.autotuning_config.max_train_micro_batch_size_per_gpu,
-                       max_train_micro_batch_size)
+        if self.max_train_batch_size(
+        ) and self.max_train_batch_size() > 0:  # if the user specifies a max_train_batch_size
+            max_train_micro_batch_size = self.max_train_batch_size() * self.mp_size() // (
+                self.exp_num_gpus * self.exp_num_nodes)  # gradient accumulation steps >=1
+            return min(self.autotuning_config.max_train_micro_batch_size_per_gpu, max_train_micro_batch_size)
         else:
             return self.autotuning_config.max_train_micro_batch_size_per_gpu
 
     def min_train_micro_batch_size_per_gpu(self):
         return self.autotuning_config.min_train_micro_batch_size_per_gpu
 
     def num_tuning_micro_batch_sizes(self):
@@ -357,27 +345,22 @@
         elif stage == 3:
             template_path = DEFAULT_TEMPLATE_PATH_ZERO_3
             template_config = hjson.load(open(template_path, 'r'))
             model_info = self.model_info
             if model_info and "hidden_size" in model_info:
                 hs = model_info["hidden_size"]
                 template_config[ZERO_OPTIMIZATION]['reduce_bucket_size'] = hs * hs
-                template_config[ZERO_OPTIMIZATION][
-                    'stage3_prefetch_bucket_size'] = 0.9 * hs * hs
-                template_config[ZERO_OPTIMIZATION][
-                    'stage3_param_persistence_threshold'] = 10 * hs
+                template_config[ZERO_OPTIMIZATION]['stage3_prefetch_bucket_size'] = 0.9 * hs * hs
+                template_config[ZERO_OPTIMIZATION]['stage3_param_persistence_threshold'] = 10 * hs
             prefix = "z3_"
         else:
             return exps
 
         # replace the corresponding parameter values if the user specifies them in the DeepSpeed configuration file
-        replace_dict(tuning_space,
-                     self.user_config,
-                     [ZERO_OPTIMIZATION,
-                      TRAIN_MICRO_BATCH_SIZE_PER_GPU])
+        replace_dict(tuning_space, self.user_config, [ZERO_OPTIMIZATION, TRAIN_MICRO_BATCH_SIZE_PER_GPU])
 
         logger.debug(f"tuning_space = {json.dumps(tuning_space)}")
 
         all_configs = get_all_configs(tuning_space, ignore_keys=["optimizer"])
 
         tuning_keys = get_tuning_keys(tuning_space)
 
@@ -393,19 +376,17 @@
             exp_config = copy.deepcopy(template_config)
             # fill the template with the expr config
             replace_dict(exp_config, config)
 
             # if the config does not use offloading, remove the offloading section
             config_zero = config.get(ZERO_OPTIMIZATION, None)
             if config_zero:
-                if OFFLOAD_OPTIMIZER not in config_zero and OFFLOAD_OPTIMIZER in exp_config[
-                        ZERO_OPTIMIZATION]:
+                if OFFLOAD_OPTIMIZER not in config_zero and OFFLOAD_OPTIMIZER in exp_config[ZERO_OPTIMIZATION]:
                     del exp_config[ZERO_OPTIMIZATION][OFFLOAD_OPTIMIZER]
-                if OFFLOAD_PARAM not in config_zero and OFFLOAD_PARAM in exp_config[
-                        ZERO_OPTIMIZATION]:
+                if OFFLOAD_PARAM not in config_zero and OFFLOAD_PARAM in exp_config[ZERO_OPTIMIZATION]:
                     del exp_config[ZERO_OPTIMIZATION][OFFLOAD_PARAM]
             # set gradient accumulation steps according to max_train_batch_size_per_gpu
             mbs = exp_config[TRAIN_MICRO_BATCH_SIZE_PER_GPU]
             gas = max_train_batch_size_per_gpu // mbs
             exp_config[GRADIENT_ACCUMULATION_STEPS] = gas
             exp_config[TRAIN_BATCH_SIZE] = mbs * gas * \
                 self.exp_num_gpus * self.exp_num_nodes // self.mp_size()
@@ -434,48 +415,41 @@
         # model info profile run with DEFAULT_MIN_MEM_CONFIG
         model_info = self.model_info_profile_run()
         if model_info:
             self.model_info = model_info
         else:
             return
 
-        logger.info(
-            f"The model has {number_to_string(self.get_model_num_params())} parameters.")
+        logger.info(f"The model has {number_to_string(self.get_model_num_params())} parameters.")
 
         self.gpu_mem = self.get_gpu_memory_info()
-        logger.info(
-            f"Memory per GPU in the system is {memory_to_string(self.gpu_mem, postfix='B')}."
-        )
+        logger.info(f"Memory per GPU in the system is {memory_to_string(self.gpu_mem, postfix='B')}.")
 
         self.activation_mem = self.get_activation_memory_per_gpu()
         logger.info(
             f"The model requires at least {memory_to_string(self.activation_mem, postfix='B')} activation memory for micro batch size 1."
         )
 
         #TODO: FIX THIS
-        stage = self.user_config.get(ZERO_OPTIMIZATION,
-                                     {}).get(ZERO_OPTIMIZATION_STAGE,
-                                             "all")
+        stage = self.user_config.get(ZERO_OPTIMIZATION, {}).get(ZERO_OPTIMIZATION_STAGE, "all")
         stage = "all"
         user_zero_stages = [stage] if not isinstance(stage, list) else stage
         logger.info(f"User-defined zero stages are {stage}.")
 
         mbs = 0
         max_mbs = 0
         metric_val = 0
 
-        required_gpu_mem = self.get_instantiation_memory_required_per_gpu(
-            ZeroStageEnum.disabled) + self.activation_mem
+        required_gpu_mem = self.get_instantiation_memory_required_per_gpu(ZeroStageEnum.disabled) + self.activation_mem
         if self.gpu_mem > required_gpu_mem:
             if "all" in user_zero_stages or ZeroStageEnum.disabled in user_zero_stages:
                 logger.info(
                     f"The model might be runable with ZERO 0 (which requires at least {memory_to_string(required_gpu_mem, postfix='B')} memory with mbs = 1), adding DEFAULT_TUNING_SPACE_ZERO_0 to the global tuning space"
                 )
-                next_max_mbs, next_mbs, next_metric_val = self.tune_space(
-                    DEFAULT_TUNING_SPACE_ZERO_0)
+                next_max_mbs, next_mbs, next_metric_val = self.tune_space(DEFAULT_TUNING_SPACE_ZERO_0)
                 if next_mbs > mbs:
                     mbs = next_mbs
                     max_mbs = next_max_mbs
                     metric_val = next_metric_val
                 if has_mlflow:
                     mlflow.log_metric(f"z0{self.metric()}", next_metric_val)
         else:
@@ -486,16 +460,18 @@
         required_gpu_mem = self.get_instantiation_memory_required_per_gpu(
             ZeroStageEnum.optimizer_states) + self.activation_mem
         if self.gpu_mem > required_gpu_mem:
             if "all" in user_zero_stages or ZeroStageEnum.optimizer_states in user_zero_stages:
                 logger.info(
                     f"The model might be runable with ZERO 1 (which requires at least {memory_to_string(required_gpu_mem, postfix='B')} memory), adding DEFAULT_TUNING_SPACE_ZERO_1 to the global tuning space"
                 )
-                next_max_mbs, next_mbs, next_metric_val = self.tune_space(
-                    DEFAULT_TUNING_SPACE_ZERO_1, prev_max_mbs = max_mbs, prev_best_mbs=mbs, prev_best_metric_val=metric_val)
+                next_max_mbs, next_mbs, next_metric_val = self.tune_space(DEFAULT_TUNING_SPACE_ZERO_1,
+                                                                          prev_max_mbs=max_mbs,
+                                                                          prev_best_mbs=mbs,
+                                                                          prev_best_metric_val=metric_val)
                 if next_mbs > mbs:
                     mbs = next_mbs
                     max_mbs = next_max_mbs
                     metric_val = next_metric_val
                 if has_mlflow:
                     mlflow.log_metric(f"z1{self.metric()}", next_metric_val)
         else:
@@ -506,96 +482,87 @@
         required_gpu_mem = self.get_instantiation_memory_required_per_gpu(
             ZeroStageEnum.gradients) + self.activation_mem
         if self.gpu_mem > required_gpu_mem:
             if "all" in user_zero_stages or ZeroStageEnum.gradients in user_zero_stages:
                 logger.info(
                     f"The model might be runable with ZERO 2 (which requires at least {memory_to_string(required_gpu_mem, postfix='B')} memory), adding DEFAULT_TUNING_SPACE_ZERO_2 to the global tuning space"
                 )
-                next_max_mbs, next_mbs, next_metric_val = self.tune_space(
-                    DEFAULT_TUNING_SPACE_ZERO_2, prev_max_mbs = max_mbs, prev_best_mbs=mbs, prev_best_metric_val=metric_val)
+                next_max_mbs, next_mbs, next_metric_val = self.tune_space(DEFAULT_TUNING_SPACE_ZERO_2,
+                                                                          prev_max_mbs=max_mbs,
+                                                                          prev_best_mbs=mbs,
+                                                                          prev_best_metric_val=metric_val)
                 if next_mbs > mbs:
                     mbs = next_mbs
                     max_mbs = next_max_mbs
                     metric_val = next_metric_val
                 if has_mlflow:
                     mlflow.log_metric(f"z2{self.metric()}", next_metric_val)
         else:
             logger.info(
                 f"The model is not runable with ZERO stage {ZeroStageEnum.gradients} (which requires at least {memory_to_string(required_gpu_mem, postfix='B')} memory with mbs = 1)"
             )
 
-        required_gpu_mem = self.get_instantiation_memory_required_per_gpu(
-            ZeroStageEnum.weights) + self.activation_mem
+        required_gpu_mem = self.get_instantiation_memory_required_per_gpu(ZeroStageEnum.weights) + self.activation_mem
         if self.gpu_mem > required_gpu_mem:
             if "all" in user_zero_stages or ZeroStageEnum.weights in user_zero_stages:
                 logger.info(
                     f"The model might be runable with ZERO 3 (which requires at least {memory_to_string(required_gpu_mem, postfix='B')} memory), adding DEFAULT_TUNING_SPACE_ZERO_3 to the global tuning space"
                 )
-                _, _, next_metric_val = self.tune_space(
-                    DEFAULT_TUNING_SPACE_ZERO_3, prev_max_mbs = max_mbs, prev_best_mbs=mbs, prev_best_metric_val=metric_val)
+                _, _, next_metric_val = self.tune_space(DEFAULT_TUNING_SPACE_ZERO_3,
+                                                        prev_max_mbs=max_mbs,
+                                                        prev_best_mbs=mbs,
+                                                        prev_best_metric_val=metric_val)
                 if has_mlflow:
                     mlflow.log_metric(f"z3{self.metric()}", next_metric_val)
         else:
             logger.info(
                 f"The model has {self.get_model_num_params()} parameters and requires at least {memory_to_string(required_gpu_mem, postfix='B')} memory per GPU with DeepSpeed Zero stage {ZeroStageEnum.weights} optimization. Memory per GPU in system is {memory_to_string(self.gpu_mem)}. No tuning is performed."
             )
             return
         if has_mlflow:
             mlflow.end_run()
 
-    def tune_space(self,
-                   tuning_space,
-                   prev_max_mbs=0,
-                   prev_best_mbs=0,
-                   prev_best_metric_val=0):
+    def tune_space(self, tuning_space, prev_max_mbs=0, prev_best_mbs=0, prev_best_metric_val=0):
         config_zero = tuning_space.get(ZERO_OPTIMIZATION, {})
         stage = config_zero.get(ZERO_OPTIMIZATION_STAGE, None)
         tuning_space_name = TUNING_MICRO_BATCH_SIZE_PREFIX + str(stage)
         tuning_micro_batch_sizes = []
         max_train_batch_size_per_gpu = 0
         tuning_micro_batch_sizes_overwritten = False
 
         # calculate max micro batch size using gpu memory, model instantiation memory and activation memory
         # calculated_max_micro_batch_size = (memory_per_gpu - instantiation_memory) // activation_memory_micro_batch_size_1
         calculated_max_micro_batch_size = int(
-            self.gpu_mem -
-            self.get_instantiation_memory_required_per_gpu(stage)) // self.activation_mem
+            self.gpu_mem - self.get_instantiation_memory_required_per_gpu(stage)) // self.activation_mem
         logger.info(
             f"Start tuning for space {tuning_space_name}, calculated_max_micro_batch_size = {calculated_max_micro_batch_size}"
         )
 
         if calculated_max_micro_batch_size < prev_max_mbs:
-            logger.info(
-                f"No need to tune Zero stage {stage}. End tuning for space {tuning_space_name}"
-            )
+            logger.info(f"No need to tune Zero stage {stage}. End tuning for space {tuning_space_name}")
             return 0, 0, 0
 
         if TRAIN_MICRO_BATCH_SIZE_PER_GPU in self.user_config and isinstance(
-                self.user_config[TRAIN_MICRO_BATCH_SIZE_PER_GPU],
-                list):
+                self.user_config[TRAIN_MICRO_BATCH_SIZE_PER_GPU], list):
             # user-specified micro batch size per gpu is a list which overwrites the default tuning behavior
             tuning_micro_batch_sizes = [
-                s for s in self.user_config[TRAIN_MICRO_BATCH_SIZE_PER_GPU]
-                if isinstance(s,
-                              int)
+                s for s in self.user_config[TRAIN_MICRO_BATCH_SIZE_PER_GPU] if isinstance(s, int)
             ]
             gas = self.get_gas_from_user_config()
             min_micro_batch_size = min(tuning_micro_batch_sizes)
             max_micro_batch_size = max(tuning_micro_batch_sizes)
             max_train_batch_size_per_gpu = max_micro_batch_size * gas
             tuning_micro_batch_sizes_overwritten = True
         else:
             # auto-detects the list of micro batch sizes to tune
             min_micro_batch_size, max_micro_batch_size = self.get_min_max_micro_batch_size(
                 stage, prev_max_mbs, calculated_max_micro_batch_size)
 
             if max_micro_batch_size < prev_max_mbs:
-                logger.info(
-                    f"No need to tune Zero stage {stage}. End tuning for space {tuning_space_name}"
-                )
+                logger.info(f"No need to tune Zero stage {stage}. End tuning for space {tuning_space_name}")
                 return 0, 0, 0
 
             tuning_micro_batch_sizes, max_train_batch_size_per_gpu = self.get_tuning_micro_batch_size_list(
                 min_micro_batch_size,
                 max_micro_batch_size,
                 num_tuning_micro_batch_sizes=self.num_tuning_micro_batch_sizes())
 
@@ -605,38 +572,33 @@
 
         # return if the tuning_micro_batch_sizes list is empty
         if not tuning_micro_batch_sizes:
             logger.info(f"End tuning for space {tuning_space_name}")
             return 0, 0, 0
 
         # tune micro batch sizes and gradient accumulation steps given max_train_batch_size_per_gpu
-        tuning_micro_batch_sizes = self.run_tuning_micro_batch_sizes(
-            tuning_micro_batch_sizes,
-            max_train_batch_size_per_gpu,
-            min_micro_batch_size,
-            stage,
-            tuning_micro_batch_sizes_overwritten)
+        tuning_micro_batch_sizes = self.run_tuning_micro_batch_sizes(tuning_micro_batch_sizes,
+                                                                     max_train_batch_size_per_gpu,
+                                                                     min_micro_batch_size, stage,
+                                                                     tuning_micro_batch_sizes_overwritten)
 
         fast_best_record = self.get_best_space_record(tuning_space_name)
         fast_best_metric_val = fast_best_record[1] if fast_best_record else 0
-        fast_best_mbs = fast_best_record[0][DS_CONFIG][
-            TRAIN_MICRO_BATCH_SIZE_PER_GPU] if fast_best_record else 0
-        logger.info(
-            f"fast_best_mbs = {fast_best_mbs}, name = {fast_best_record[0]['name']}")
+        fast_best_mbs = fast_best_record[0][DS_CONFIG][TRAIN_MICRO_BATCH_SIZE_PER_GPU] if fast_best_record else 0
+        logger.info(f"fast_best_mbs = {fast_best_mbs}, name = {fast_best_record[0]['name']}")
 
         if self.fast_enabled() or stage == 0:
             logger.info(f"End tuning for space: {tuning_space_name}")
             return max_micro_batch_size, fast_best_mbs, fast_best_metric_val
 
         # if the best metric or the micro batch size for that best metric in the current Zero stage after tuning micro batch size is less than the corresponding value in the previous Zero stage, return, do not tune other Zero configuration parameters
         if stage > 0:
             if fast_best_mbs <= prev_best_mbs or fast_best_metric_val < prev_best_metric_val:
                 logger.info(
-                    f"End tuning for space: {tuning_space_name}. No need to tune other Zero configuration parameters."
-                )
+                    f"End tuning for space: {tuning_space_name}. No need to tune other Zero configuration parameters.")
                 return max_micro_batch_size, fast_best_mbs, fast_best_metric_val
 
         tuning_space[TRAIN_MICRO_BATCH_SIZE_PER_GPU] = tuning_micro_batch_sizes
         tuning_space_name = canonical_name(tuning_space,
                                            tuning_keys=get_tuning_keys(tuning_space),
                                            prefix="z" + str(stage) + "_",
                                            omit_val=True)
@@ -650,54 +612,49 @@
         if self.autotuning_config.tuner_type == AUTOTUNING_TUNER_MODELBASED:
             t = ModelBasedTuner(exps, self.rm, self.metric(), tuning_space)
         elif self.autotuning_config.tuner_type == AUTOTUNING_TUNER_RANDOM:
             t = RandomTuner(exps, self.rm, self.metric())
         else:
             t = GridSearchTuner(exps, self.rm, self.metric())
 
-        sample_size = len(self.rm.nodes) * self.rm.num_gpus_per_node // (
-            self.exp_num_gpus * self.exp_num_nodes)
+        sample_size = len(self.rm.nodes) * self.rm.num_gpus_per_node // (self.exp_num_gpus * self.exp_num_nodes)
         num_exps = t.tune(sample_size=sample_size,
                           n_trials=self.autotuning_config.tuner_num_trials,
                           early_stopping=self.autotuning_config.tuner_early_stopping)
         exp = t.best_exp
         metric_val = t.best_metric_val
         if exp:
             self.update_records(tuning_space_name, exp, metric_val, num_exps)
 
         full_best_record = self.get_best_space_record(tuning_space_name)
         full_best_metric_val = full_best_record[1] if full_best_record else -1
 
         if full_best_metric_val > fast_best_metric_val:
             best_metric_val = full_best_metric_val
-            best_mbs = full_best_record[0][DS_CONFIG][
-                TRAIN_MICRO_BATCH_SIZE_PER_GPU] if full_best_record else -1
+            best_mbs = full_best_record[0][DS_CONFIG][TRAIN_MICRO_BATCH_SIZE_PER_GPU] if full_best_record else -1
         else:
             best_metric_val = fast_best_metric_val
             best_mbs = fast_best_mbs
 
         logger.info(f"End tuning for space: {tuning_space_name}")
         return max_micro_batch_size, best_mbs, best_metric_val
 
     def get_plauteu_mbs(self, tuning_space_name):
         if tuning_space_name not in self.records:
             return 0
         space_records = self.records[tuning_space_name]
-        sorted_space_records = sorted(
-            space_records,
-            key=lambda x: x[0][DS_CONFIG][TRAIN_MICRO_BATCH_SIZE_PER_GPU])
+        sorted_space_records = sorted(space_records, key=lambda x: x[0][DS_CONFIG][TRAIN_MICRO_BATCH_SIZE_PER_GPU])
         prev_metric_val = None
         prev_micro_batch_size = 0
         for (exp, metric_val, _) in sorted_space_records:
             if prev_metric_val:
                 if metric_val < prev_metric_val:
                     break
                 if (metric_val >= prev_metric_val
-                        and (metric_val - prev_metric_val) / prev_metric_val <
-                        METRIC_PERCENT_DIFF_CONST):
+                        and (metric_val - prev_metric_val) / prev_metric_val < METRIC_PERCENT_DIFF_CONST):
                     break
             prev_metric_val = metric_val
             prev_micro_batch_size = exp[DS_CONFIG][TRAIN_MICRO_BATCH_SIZE_PER_GPU]
         plateau_mbs = prev_micro_batch_size
         return plateau_mbs
 
     def get_model_num_params(self):
@@ -714,24 +671,16 @@
         model_info = self.autotuning_config.model_info
         if model_info and MODEL_INFO_NUM_PARAMS in model_info:
             return model_info
 
         ds_config = copy.deepcopy(self.user_config)
         replace_dict(ds_config, DEFAULT_MIN_MEM_CONFIG)
 
-        model_info_path = os.path.join(self.results_dir,
-                                       "profile_model_info",
-                                       "model_info.json")
-        ds_config[AUTOTUNING] = {
-            "enabled": True,
-            "model_info_path": model_info_path,
-            "model_info": {
-                "profile": True
-            }
-        }
+        model_info_path = os.path.join(self.results_dir, "profile_model_info", "model_info.json")
+        ds_config[AUTOTUNING] = {"enabled": True, "model_info_path": model_info_path, "model_info": {"profile": True}}
 
         exp_config = {}
         exp_name = "profile_model_info"
         exp_config['name'] = exp_name
         exp_config[DS_CONFIG] = ds_config
         exp_config['num_gpus'] = self.exp_num_gpus
         exp_config['num_nodes'] = self.exp_num_nodes
@@ -744,16 +693,15 @@
 
         self.rm.schedule_experiments([exp_path])
         self.rm.run()
 
         for exp_id, (exp_json, err) in self.rm.finished_experiments.items():
             self.rm.clear()
             if err:
-                logger.error(
-                    f"The model is not runnable with DeepSpeed with error = {err}")
+                logger.error(f"The model is not runnable with DeepSpeed with error = {err}")
                 return None
 
         if os.path.exists(model_info_path):
             with open(model_info_path, 'r') as f:
                 model_info = hjson.load(f)
                 return model_info
 
@@ -786,20 +734,16 @@
                 best_space_records[space_name] = best_space_record
                 if not global_best_record or best_space_record[1] > global_best_record[1]:
                     global_best_record = best_space_record
         if global_best_record:
             best_space_records[GLOBAL_TUNING_SPACE] = global_best_record
         return best_space_records
 
-    def run_tuning_micro_batch_sizes(self,
-                                     tuning_micro_batch_sizes,
-                                     max_train_batch_size_per_gpu,
-                                     min_micro_batch_size,
-                                     stage,
-                                     tuning_micro_batch_sizes_overwritten):
+    def run_tuning_micro_batch_sizes(self, tuning_micro_batch_sizes, max_train_batch_size_per_gpu,
+                                     min_micro_batch_size, stage, tuning_micro_batch_sizes_overwritten):
         assert tuning_micro_batch_sizes, "the tuning micro batch size list is empty"
         tuning_micro_batch_sizes.sort()
         max_micro_batch_size = tuning_micro_batch_sizes[-1]
         max_micro_batch_size_metric_val = 0
 
         ds_config = get_first_config(self.user_config)
         ds_config[ZERO_OPTIMIZATION] = {ZERO_OPTIMIZATION_STAGE: stage}
@@ -834,16 +778,15 @@
                 metric_file = exp[DS_CONFIG][AUTOTUNING][AUTOTUNING_METRIC_PATH]
                 if os.path.exists(metric_file):
 
                     with open(metric_file, 'r') as f:
                         results = hjson.load(f)
                         metric_val = results[self.metric()]
                         self.update_records(tuning_space_name, exp, metric_val, 1)
-                        if max_micro_batch_size == exp[DS_CONFIG][
-                                TRAIN_MICRO_BATCH_SIZE_PER_GPU]:
+                        if max_micro_batch_size == exp[DS_CONFIG][TRAIN_MICRO_BATCH_SIZE_PER_GPU]:
                             max_micro_batch_size_metric_val = metric_val
                         if has_mlflow:
                             os.environ.pop('MLFLOW_RUN_ID')
                             mlflow.start_run(nested=True, run_name=exp['name'])
                             for metric in results:
                                 mlflow.log_metric(metric, results[metric])
                             mlflow.end_run()
@@ -858,28 +801,24 @@
 
         if tuning_micro_batch_sizes_overwritten:
             return tuning_micro_batch_sizes
 
         # in a auto-detected tuning_micro_batch_sizs list, max_micro_batch_size might not be performant as the memory consumption is close to max
         # try smaller values while gas stays the same
         # if finding a more performant mbs value, use it to replace max_micro_batch_size in the list
-        min_micro_batch_size_with_same_gas = (
-            tuning_micro_batch_sizes[-2] +
-            1) if len(tuning_micro_batch_sizes) > 1 else min_micro_batch_size
+        min_micro_batch_size_with_same_gas = (tuning_micro_batch_sizes[-2] +
+                                              1) if len(tuning_micro_batch_sizes) > 1 else min_micro_batch_size
 
         prev_best_metric_val = max_micro_batch_size_metric_val
         prev_best_mbs = max_micro_batch_size
 
         stride = (max_micro_batch_size - min_micro_batch_size_with_same_gas) // 3
         if stride == 0:
             stride = 1
-        for mbs in reversed(
-                range(min_micro_batch_size_with_same_gas,
-                      max_micro_batch_size,
-                      stride)):
+        for mbs in reversed(range(min_micro_batch_size_with_same_gas, max_micro_batch_size, stride)):
             ds_config[TRAIN_MICRO_BATCH_SIZE_PER_GPU] = mbs
             gas = max_train_batch_size_per_gpu // mbs
             ds_config[GRADIENT_ACCUMULATION_STEPS] = gas
             ds_config[TRAIN_BATCH_SIZE] = mbs * gas * \
                 self.exp_num_gpus * self.exp_num_nodes // self.mp_size()
             exp_name = tuning_space_name + "_gas" + str(gas) + "_tmbspg" + str(mbs)
             exp, metric_val = self.run_ds_config(ds_config, exp_name)
@@ -904,18 +843,15 @@
             else:
                 self.update_records(tuning_space_name, exp, 0, 1)
                 break
         if prev_best_mbs != max_micro_batch_size:
             tuning_micro_batch_sizes[-1] = prev_best_mbs
         return tuning_micro_batch_sizes
 
-    def get_min_max_micro_batch_size(self,
-                                     stage,
-                                     min_micro_batch_size,
-                                     calculated_max_micro_batch_size):
+    def get_min_max_micro_batch_size(self, stage, min_micro_batch_size, calculated_max_micro_batch_size):
         # get min and max micro batch size with gradient accumulation steps = 1
         if min_micro_batch_size > calculated_max_micro_batch_size:
             return -1, -1
 
         used_micro_batch_sizes = []
         tuning_space_name = TUNING_MICRO_BATCH_SIZE_PREFIX + str(stage)
 
@@ -923,16 +859,15 @@
         ds_config[ZERO_OPTIMIZATION] = {ZERO_OPTIMIZATION_STAGE: stage}
         gas = self.get_gas_from_user_config()
         ds_config[GRADIENT_ACCUMULATION_STEPS] = gas
 
         # search for the min micro batch size
         if min_micro_batch_size < 1:
             if TRAIN_MICRO_BATCH_SIZE_PER_GPU in self.user_config and isinstance(
-                    self.user_config[TRAIN_MICRO_BATCH_SIZE_PER_GPU],
-                    int):
+                    self.user_config[TRAIN_MICRO_BATCH_SIZE_PER_GPU], int):
                 # user specifies train_micro_batch_size_per_gpu as an int
                 mbs = int(self.user_config[TRAIN_MICRO_BATCH_SIZE_PER_GPU])
             else:
                 # user does not specify train_micro_batch_size_per_gpu or sets it to "auto" when using Hugging Face
                 val = self.get_val_from_user_args(TRAIN_MICRO_BATCH_SIZE_PER_GPU)
                 if val:
                     mbs = int(val)
@@ -947,56 +882,48 @@
             exp, metric_val = self.run_ds_config(ds_config, exp_name)
             if metric_val:
                 self.update_records(tuning_space_name, exp, metric_val, 1)
                 used_micro_batch_sizes.append(mbs)
                 min_micro_batch_size = mbs
             else:
                 self.update_records(tuning_space_name, exp, 0, 1)
-                logger.info(
-                    f"User-specified micro batch size per GPU {mbs} does not run")
+                logger.info(f"User-specified micro batch size per GPU {mbs} does not run")
                 if self.min_train_micro_batch_size_per_gpu() == mbs:
                     return -1, -1
                 mbs = self.min_train_micro_batch_size_per_gpu()
                 ds_config[TRAIN_MICRO_BATCH_SIZE_PER_GPU] = mbs
                 ds_config[GRADIENT_ACCUMULATION_STEPS] = gas
                 ds_config[TRAIN_BATCH_SIZE] = mbs * gas * \
                     self.exp_num_gpus * self.exp_num_nodes // self.mp_size()
                 exp_name = tuning_space_name + "_gas" + str(gas) + "_tmbspg" + str(mbs)
                 exp, metric_val = self.run_ds_config(ds_config, exp_name)
                 if not metric_val:
                     self.update_records(tuning_space_name, exp, 0, 1)
-                    logger.info(
-                        f"min_train_micro_batch_size_per_gpu {mbs} is not runnable.")
+                    logger.info(f"min_train_micro_batch_size_per_gpu {mbs} is not runnable.")
                     return -1, -1
                 self.update_records(tuning_space_name, exp, metric_val, 1)
                 min_micro_batch_size = mbs
                 used_micro_batch_sizes.append(mbs)
         else:
             ds_config[TRAIN_MICRO_BATCH_SIZE_PER_GPU] = min_micro_batch_size
             ds_config[GRADIENT_ACCUMULATION_STEPS] = gas
             ds_config[TRAIN_BATCH_SIZE] = min_micro_batch_size * gas * \
                 self.exp_num_gpus * self.exp_num_nodes // self.mp_size()
-            exp_name = tuning_space_name + "_gas" + str(gas) + "_tmbspg" + str(
-                min_micro_batch_size)
+            exp_name = tuning_space_name + "_gas" + str(gas) + "_tmbspg" + str(min_micro_batch_size)
             exp, metric_val = self.run_ds_config(ds_config, exp_name)
             if metric_val:
                 self.update_records(tuning_space_name, exp, metric_val, 1)
                 used_micro_batch_sizes.append(min_micro_batch_size)
             else:
                 self.update_records(tuning_space_name, exp, 0, 1)
                 return -1, -1
 
         # search for the max micro batch size
-        max_micro_batch_size = min(calculated_max_micro_batch_size,
-                                   self.max_train_micro_batch_size_per_gpu())
-        for mbs in [
-                math.ceil(1.05 * max_micro_batch_size),
-                max_micro_batch_size,
-                int(0.95 * max_micro_batch_size)
-        ]:
+        max_micro_batch_size = min(calculated_max_micro_batch_size, self.max_train_micro_batch_size_per_gpu())
+        for mbs in [math.ceil(1.05 * max_micro_batch_size), max_micro_batch_size, int(0.95 * max_micro_batch_size)]:
             if mbs > self.max_train_micro_batch_size_per_gpu():
                 continue
             if mbs in used_micro_batch_sizes:
                 return min_micro_batch_size, mbs
             ds_config[TRAIN_MICRO_BATCH_SIZE_PER_GPU] = mbs
             ds_config[TRAIN_BATCH_SIZE] = mbs * gas * \
                 self.exp_num_gpus * self.exp_num_nodes // self.mp_size()
@@ -1007,20 +934,19 @@
                 logger.info(f"mbs = {mbs} is found as max mbs")
                 self.update_records(tuning_space_name, exp, metric_val, 1)
                 used_micro_batch_sizes.append(mbs)
                 return min_micro_batch_size, mbs
             else:
                 self.update_records(tuning_space_name, exp, 0, 1)
 
-        space_records = self.records[
-            tuning_space_name] if tuning_space_name in self.records else []
+        space_records = self.records[tuning_space_name] if tuning_space_name in self.records else []
         if space_records:
             prev_idx = min(range(len(space_records)),
-                           key=lambda i: abs(space_records[i][0][DS_CONFIG][
-                               TRAIN_MICRO_BATCH_SIZE_PER_GPU] - min_micro_batch_size))
+                           key=lambda i: abs(space_records[i][0][DS_CONFIG][TRAIN_MICRO_BATCH_SIZE_PER_GPU] -
+                                             min_micro_batch_size))
             prev_metric_val = space_records[prev_idx][1]
         else:
             prev_metric_val = None
 
         low = min_micro_batch_size
         high = max_micro_batch_size
         # binary search until low is the smallest micro batch size that OOMs.
@@ -1033,29 +959,27 @@
                     self.exp_num_gpus * self.exp_num_nodes // self.mp_size()
                 exp_name = tuning_space_name + "_gas" + str(gas) + "_tmbspg" + str(mid)
                 exp, metric_val = self.run_ds_config(ds_config, exp_name)
                 if metric_val:
                     low = mid + 1
                     self.update_records(tuning_space_name, exp, metric_val, 1)
                     used_micro_batch_sizes.append(mid)
-                    if prev_metric_val and ((metric_val - prev_metric_val) /
-                                            prev_metric_val) < METRIC_PERCENT_DIFF_CONST:
+                    if prev_metric_val and (
+                        (metric_val - prev_metric_val) / prev_metric_val) < METRIC_PERCENT_DIFF_CONST:
                         logger.info(f"performance plateaus at mbs = {low}")
                         break
                     prev_metric_val = metric_val
                 else:
                     self.update_records(tuning_space_name, exp, 0, 1)
                     high = mid - 1
             else:
                 low = mid + 1
         max_micro_batch_size = low - 1
 
-        logger.info(
-            f"min_micro_batch_size = {min_micro_batch_size}, max_micro_batch_size = {max_micro_batch_size}."
-        )
+        logger.info(f"min_micro_batch_size = {min_micro_batch_size}, max_micro_batch_size = {max_micro_batch_size}.")
 
         return min_micro_batch_size, max_micro_batch_size
 
     def get_gas_from_user_config(self):
         gas = 1
         if GRADIENT_ACCUMULATION_STEPS in self.user_config:
             gas_in_config = self.user_config[GRADIENT_ACCUMULATION_STEPS]
@@ -1063,66 +987,61 @@
                 gas = gas_in_config
             elif gas_in_config == "auto":  # GRADIENT_ACCUMULATION_STEPS: "auto"
                 val = self.get_val_from_config(GRADIENT_ACCUMULATION_STEPS)
                 if val:
                     gas = int(val)
             elif isinstance(gas_in_config, list):
                 logger.info(
-                    f"Specifying a list of {GRADIENT_ACCUMULATION_STEPS} to tune is not supported. 1 would be used."
-                )
+                    f"Specifying a list of {GRADIENT_ACCUMULATION_STEPS} to tune is not supported. 1 would be used.")
         assert gas > 0, "Gradient accumulation steps must be positive."
         return gas
 
     def get_val_from_user_args(self, ds_name):
         arg_mappings = self.autotuning_config.arg_mappings
         user_args = self.args.user_args
         if arg_mappings and ds_name in arg_mappings:
             arg_name = arg_mappings[ds_name]
             if arg_name in user_args:
                 idx = user_args.index(arg_name)
                 if user_args[idx + 1].isnumeric():
                     return (user_args[idx + 1])
         return None
 
-    def get_tuning_micro_batch_size_list(self,
-                                         min_micro_batch_size,
-                                         max_micro_batch_size,
+    def get_tuning_micro_batch_size_list(self, min_micro_batch_size, max_micro_batch_size,
                                          num_tuning_micro_batch_sizes):
         """Get a list of micro batch sizes to tune based on min and max values, as well as the size of the list.
         Args:
             min_micro_batch_size ([int]): min micro batch size per GPU
             max_micro_batch_size ([int]): max micro batch size per GPU
             num_tuning_micro_batch_sizes (int): the number of items in the returned list
 
         Returns:
             [list]: a list of micro batch sizes to tune.
         """
         if min_micro_batch_size <= 0 or max_micro_batch_size <= 0:
             logger.info(
-                f"min_micro_batch_size = {min_micro_batch_size}, max_micro_batch_size = {max_micro_batch_size}"
-            )
+                f"min_micro_batch_size = {min_micro_batch_size}, max_micro_batch_size = {max_micro_batch_size}")
             return [], 0
 
         # NUM_GPUS=$(( ${NUM_WORKERS} * ${NUM_GPUS_PER_WORKER} ))
         # DP_SIZE=$(( ${NUM_GPUS} / (${PP_SIZE} * ${MP_SIZE}) ))
         # GRAD_ACC_STEPS=$(( ${TARGET_GLOBAL_BATCH_SIZE} / (${BATCH_SIZE} * ${DP_SIZE}) ))
-        if self.max_train_batch_size() and self.max_train_batch_size(
-        ) > 0:  # if the user specifies a max_train_batch_size
-            max_train_batch_size_per_gpu = self.max_train_batch_size() * self.mp_size(
-            ) // (self.exp_num_gpus * self.exp_num_nodes)
+        if self.max_train_batch_size(
+        ) and self.max_train_batch_size() > 0:  # if the user specifies a max_train_batch_size
+            max_train_batch_size_per_gpu = self.max_train_batch_size() * self.mp_size() // (self.exp_num_gpus *
+                                                                                            self.exp_num_nodes)
         else:
             gas = self.get_gas_from_user_config()
             max_train_batch_size_per_gpu = max_micro_batch_size * gas // self.mp_size()
         logger.info(f"max_train_batch_size_per_gpu = {max_train_batch_size_per_gpu}")
         if min_micro_batch_size < max_micro_batch_size // 2:
             min_micro_batch_size = max_micro_batch_size // 2
 
         # constant stride
-        stride = (max_micro_batch_size -
-                  min_micro_batch_size) // num_tuning_micro_batch_sizes
+        stride = (max_micro_batch_size - min_micro_batch_size) // num_tuning_micro_batch_sizes
         if stride == 0:
             stride = 1
         ls = []
         min_gas = max_train_batch_size_per_gpu // max_micro_batch_size
         # if gas is the same as min_gas, do not add mbs to the tuning list
         for mbs in range(min_micro_batch_size, max_micro_batch_size, stride):
             if max_train_batch_size_per_gpu // mbs != min_gas:
@@ -1183,12 +1102,10 @@
         """ Launches the training with the optimal DeepSpeed configuration found through the autotuning process.
             "ds_config_optimal.json" describing the optmimal DeepSpeed configuration as well the command used to launch training "cmd_optimal.txt" are saved to self.results_dir.
         """
         if self.optimal_cmd:
             result = subprocess.Popen(self.optimal_cmd)
             result.wait()
 
-            logger.info(
-                f"Done running with the optimal DeepSpeed configuration using {self.optimal_cmd}"
-            )
+            logger.info(f"Done running with the optimal DeepSpeed configuration using {self.optimal_cmd}")
         else:
             logger.info(f"No optimal DeepSpeed configuration found by autotuning.")
```

### Comparing `deepspeed-0.8.3/deepspeed/autotuning/constants.py` & `deepspeed-0.9.0/deepspeed/autotuning/constants.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,30 +1,25 @@
-'''Copyright The Microsoft DeepSpeed Team'''
-"""
-Copyright (c) Microsoft Corporation
-Licensed under the MIT license.
-"""
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 #########################################
 # autotunner implementation constants
 #########################################
 
 import os
 
-DEFAULT_TEMPLATE_PATH_ZERO_0 = os.path.join(os.path.dirname(os.path.realpath(__file__)),
-                                            "config_templates",
+DEFAULT_TEMPLATE_PATH_ZERO_0 = os.path.join(os.path.dirname(os.path.realpath(__file__)), "config_templates",
                                             "template_zero0.json")
-DEFAULT_TEMPLATE_PATH_ZERO_1 = os.path.join(os.path.dirname(os.path.realpath(__file__)),
-                                            "config_templates",
+DEFAULT_TEMPLATE_PATH_ZERO_1 = os.path.join(os.path.dirname(os.path.realpath(__file__)), "config_templates",
                                             "template_zero1.json")
-DEFAULT_TEMPLATE_PATH_ZERO_2 = os.path.join(os.path.dirname(os.path.realpath(__file__)),
-                                            "config_templates",
+DEFAULT_TEMPLATE_PATH_ZERO_2 = os.path.join(os.path.dirname(os.path.realpath(__file__)), "config_templates",
                                             "template_zero2.json")
-DEFAULT_TEMPLATE_PATH_ZERO_3 = os.path.join(os.path.dirname(os.path.realpath(__file__)),
-                                            "config_templates",
+DEFAULT_TEMPLATE_PATH_ZERO_3 = os.path.join(os.path.dirname(os.path.realpath(__file__)), "config_templates",
                                             "template_zero3.json")
 
 METRIC_PERCENT_DIFF_CONST = 0.05
 DS_CONFIG = "ds_config"
 BUFSIZE = 1  # line buffer size for writing files
 
 #########################################
@@ -153,57 +148,38 @@
 }
 
 DEFAULT_TUNING_SPACE_ZERO_0 = {"zero_optimization": {"stage": 0}}
 
 DEFAULT_TUNING_SPACE_ZERO_1 = {
     "zero_optimization": {
         "stage": 1,
-        "reduce_bucket_size": [5e7,
-                               5e8,
-                               1e9],
-        "allgather_bucket_size": [5e7,
-                                  5e8,
-                                  1e9],
+        "reduce_bucket_size": [5e7, 5e8, 1e9],
+        "allgather_bucket_size": [5e7, 5e8, 1e9],
     }
 }
 
 DEFAULT_TUNING_SPACE_ZERO_2 = {
     "zero_optimization": {
         "stage": 2,
-        "overlap_comm": [True,
-                         False],
-        "reduce_scatter": [False,
-                           True],
-        "reduce_bucket_size": [5e7,
-                               5e8,
-                               1e9],
-        "allgather_bucket_size": [5e7,
-                                  5e8,
-                                  1e9],
-        "contiguous_gradients": [False,
-                                 True]
+        "overlap_comm": [True, False],
+        "reduce_scatter": [False, True],
+        "reduce_bucket_size": [5e7, 5e8, 1e9],
+        "allgather_bucket_size": [5e7, 5e8, 1e9],
+        "contiguous_gradients": [False, True]
     },
 }
 
 DEFAULT_TUNING_SPACE_ZERO_3 = {
     "zero_optimization": {
         "stage": 3,
-        "overlap_comm": [True,
-                         False],
-        "reduce_scatter": [False,
-                           True],
-        "reduce_bucket_size": [5e7,
-                               5e8,
-                               1e9],
-        "allgather_partitions": [True,
-                                 False],
-        "allgather_bucket_size": [5e7,
-                                  5e8,
-                                  1e9],
-        "contiguous_gradients": [False,
-                                 True]
+        "overlap_comm": [True, False],
+        "reduce_scatter": [False, True],
+        "reduce_bucket_size": [5e7, 5e8, 1e9],
+        "allgather_partitions": [True, False],
+        "allgather_bucket_size": [5e7, 5e8, 1e9],
+        "contiguous_gradients": [False, True]
     },
 }
 
 GLOBAL_TUNING_SPACE = 'global'
 # TUNING_MICRO_BATCH_SIZE_PREFIX="tune_micro_batch_size_z"
 TUNING_MICRO_BATCH_SIZE_PREFIX = "z"
```

### Comparing `deepspeed-0.8.3/deepspeed/autotuning/scheduler.py` & `deepspeed-0.9.0/deepspeed/autotuning/scheduler.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,8 +1,11 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import copy
 
 from numpy import BUFSIZE
 import json
 import subprocess
 import sys
@@ -24,21 +27,16 @@
 
 from deepspeed import comm as dist
 
 TIMEOUT = 5
 
 
 class ResourceManager:
-    def __init__(self,
-                 args,
-                 hosts,
-                 num_gpus_per_node,
-                 results_dir,
-                 exps_dir,
-                 arg_mappings):
+
+    def __init__(self, args, hosts, num_gpus_per_node, results_dir, exps_dir, arg_mappings):
         self.results_dir = results_dir
         self.exps_dir = exps_dir
 
         self.nodes = []
         self.num_gpus_per_node = num_gpus_per_node
         for host in hosts:
             self.nodes.append(Node(host, num_gpus_per_node))
@@ -65,36 +63,30 @@
             else:
                 self.exp_paths.add(exp_path)
                 with open(exp_path, "r") as fd:
                     exp = hjson.load(fd)
                     exp["exp_id"] = self.experiment_count
                     self.experiment_count += 1
 
-                    result_dir = exp["result_dir"] = os.path.join(
-                        self.results_dir,
-                        exp['name'])
+                    result_dir = exp["result_dir"] = os.path.join(self.results_dir, exp['name'])
                     if AUTOTUNING in exp["ds_config"]:
                         metric_file = os.path.join(result_dir, "metrics.json")
-                        exp["ds_config"][AUTOTUNING][
-                            AUTOTUNING_METRIC_PATH] = metric_file
+                        exp["ds_config"][AUTOTUNING][AUTOTUNING_METRIC_PATH] = metric_file
                     stderr_file = os.path.join(result_dir, "stderr.log")
                     model_info_file = os.path.join(result_dir, "model_info.json")
                     metric_file = os.path.join(result_dir, "metrics.json")
 
                     # skip existing experiments (except for the ones that were interrupted)
                     if os.path.exists(result_dir) and os.path.exists(stderr_file):
                         if not was_interruptted(stderr_file):
                             err = search_error(stderr_file)
                             exp_id = exp["exp_id"]
                             self.finished_experiments[exp_id] = (exp, err)
-                            if err or os.path.exists(metric_file) or os.path.exists(
-                                    model_info_file):
-                                logger.info(
-                                    f"Skipping exp {exp['name']} whose result already exists"
-                                )
+                            if err or os.path.exists(metric_file) or os.path.exists(model_info_file):
+                                logger.info(f"Skipping exp {exp['name']} whose result already exists")
                                 continue
 
                     self.experiment_queue.append(exp)
 
     def run_job(self, exp: dict, reservations):
         exp_id = exp["exp_id"]
         exp["master_port"] = self.args.master_port + exp_id
@@ -109,19 +101,15 @@
                 if val in user_args:
                     idx = user_args.index(val)
                     user_args[idx + 1] = str(nval)
                 else:
                     user_args.append(val)
                     user_args.append(str(nval))
 
-        t = threading.Thread(target=run_experiment,
-                             args=(exp,
-                                   reservations,
-                                   user_script,
-                                   user_args))
+        t = threading.Thread(target=run_experiment, args=(exp, reservations, user_script, user_args))
         t.start()
         self.running_experiments[exp_id] = (t, exp, reservations, time.time())
 
     def experiment_check(self, pbar):
         finished_exps = []
         for exp_id, exp_data in self.running_experiments.items():
             thread, exp_json, reservations, start_time = exp_data
@@ -266,28 +254,30 @@
             clean_up(exp_json, reservations)
         self.running_experiments = {}
         self.finished_experiments = {}
         self.exp_paths = set()
 
 
 class Node:
+
     def __init__(self, host, max_slots):
         self.host = host
         self.max_slots = max_slots
         self.idle_slots = list(range(max_slots))
 
     def reserve_slots(self, slot_request: int) -> list:
         if len(self.idle_slots) >= slot_request:
             return [self.idle_slots.pop(0) for _ in range(slot_request)]
 
     def restore_slots(self, slots: list):
         self.idle_slots += slots
 
 
 class Reservation:
+
     def __init__(self, node, slots):
         self.node = node
         self.slots = slots
 
     def restore_slots(self):
         self.node.restore_slots(self.slots)
 
@@ -385,44 +375,40 @@
         fd.flush()
         os.fsync(fd)
 
     logger.info(
         f"Launching exp_id = {exp['exp_id']}, exp_name = {exp['name']}, with resource = {include_str}, and ds_config = {os.path.abspath(ds_config_path)}"
     )
 
-    with open(os.path.join(exp_dir, "stdout.log"), "wb") as out, open(
-        os.path.join(exp_dir, "stderr.log"), "wb"
-    ) as err:
+    with open(os.path.join(exp_dir, "stdout.log"), "wb") as out, open(os.path.join(exp_dir, "stderr.log"),
+                                                                      "wb") as err:
         result = subprocess.Popen(cmd, stdout=out, stderr=err)
         result.wait()
         out.flush()
         err.flush()
         os.fsync(out)
         os.fsync(err)
 
     clean_up(exp, reservations)
 
-    logger.info(
-        f"Done running exp_id = {exp['exp_id']}, exp_name = {exp['name']}, with resource = {include_str}"
-    )
+    logger.info(f"Done running exp_id = {exp['exp_id']}, exp_name = {exp['name']}, with resource = {include_str}")
 
 
 PDSH_MAX_FAN_OUT = 1024
 
 
 def clean_up(exp: dict, reservations):
     env = os.environ.copy()
     env['PDSH_RCMD_TYPE'] = 'ssh'
 
     nodes_str = ""
     for reservation in reservations:
         nodes_str += f"{reservation.node.host},"
     nodes_str = nodes_str[:-1]
-    logger.debug(
-        f"Cleaning up exp_id = {exp['exp_id']} on the following workers: {nodes_str}")
+    logger.debug(f"Cleaning up exp_id = {exp['exp_id']} on the following workers: {nodes_str}")
 
     # PDSH flags for max node fan out and specific hosts to launch on
     # See https://linux.die.net/man/1/pdsh for flag details
     pdsh_cmd = ['pdsh', '-f', str(PDSH_MAX_FAN_OUT), '-w', nodes_str]
 
     kill_cmd = [
         'pkill',
@@ -437,10 +423,8 @@
 
     # In case of failure must propagate the error-condition back to the caller (usually shell). The
     # actual error and traceback should have been printed in the subprocess, so in order to avoid
     # unnecessary noise we just quietly exit here with the same code as the subprocess
     if result.returncode > 0:
         sys.exit(result.returncode)
 
-    logger.info(
-        f"Done cleaning up exp_id = {exp['exp_id']} on the following workers: {nodes_str}"
-    )
+    logger.info(f"Done cleaning up exp_id = {exp['exp_id']} on the following workers: {nodes_str}")
```

### Comparing `deepspeed-0.8.3/deepspeed/autotuning/tuner/base_tuner.py` & `deepspeed-0.9.0/deepspeed/autotuning/tuner/base_tuner.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,17 +1,21 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import sys
 
 from deepspeed.autotuning.constants import *
 from deepspeed.autotuning.utils import write_experiments
 from deepspeed.utils import logger
 
 
 class BaseTuner:
+
     def __init__(self, exps, resource_manager, metric):
         self.all_exps = exps
         self.rm = resource_manager
         self.best_iter = 0
         self.best_exp = None
         self.best_metric_val = None
         self.metric = metric if metric else AUTOTUNING_METRIC_DEFAULT
@@ -38,16 +42,16 @@
                 # Select the next batch of configuratiosn for evaluation
                 sampled_exps = self.next_batch(sample_size)
                 # Generate experiments for measurement of performance
                 exp_paths = write_experiments(sampled_exps, self.rm.exps_dir)
                 self.rm.schedule_experiments(exp_paths)
                 self.rm.run()
                 exp, metric_val = self.rm.parse_results(self.metric)
-                if self.best_exp == None or self.best_metric_val == None or (
-                        metric_val and metric_val > self.best_metric_val):
+                if self.best_exp == None or self.best_metric_val == None or (metric_val
+                                                                             and metric_val > self.best_metric_val):
                     # logger.info(f"tuner finds better = {exp}")
                     self.best_exp = exp
                     self.best_metric_val = metric_val
                     self.best_iter = i
 
                 i += len(sampled_exps)
```

### Comparing `deepspeed-0.8.3/deepspeed/autotuning/tuner/cost_model.py` & `deepspeed-0.9.0/deepspeed/autotuning/tuner/cost_model.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,18 +1,22 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 from .utils import *
 
 try:
     import xgboost as xgb
 except ImportError:
     xgb = None
 
 
 class XGBoostCostModel():
+
     def __init__(self, loss_type, num_threads=None, log_interval=25, upper_model=None):
 
         assert xgb is not None, "missing requirements, please install deepspeed w. 'autotuning_ml' extra."
 
         self.loss_type = loss_type
 
         if loss_type == "reg":
```

### Comparing `deepspeed-0.8.3/deepspeed/autotuning/tuner/model_based_tuner.py` & `deepspeed-0.9.0/deepspeed/autotuning/tuner/model_based_tuner.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,11 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import hjson
 
 from ..constants import AUTOTUNING, AUTOTUNING_METRIC_PATH
 from .base_tuner import BaseTuner
 from .cost_model import XGBoostCostModel
 from .utils import *
@@ -11,26 +14,26 @@
 from ..constants import AUTOTUNING_METRIC_LATENCY
 
 INIT_NUM = 2
 
 
 class ModelBasedTuner(BaseTuner):
     """Exploring the search space with a cost model"""
+
     def __init__(self, exps: list, resource_manager, metric, tuning_sapce):
         super().__init__(exps, resource_manager, metric)
         self.tuning_space = tuning_sapce
         self.best_iter = 0
 
         self.all_configs = [e['ds_config'] for e in exps]
         self.num_all_configs = len(self.all_configs)
 
         self.dims = dict_to_dims(self.tuning_space)
 
-        logger.info(
-            f"Create config dim: {self.dims}, all configs: {self.num_all_configs}")
+        logger.info(f"Create config dim: {self.dims}, all configs: {self.num_all_configs}")
 
         self.visited = set([])
 
         self.trials = []
         self.trial_pt = 0
 
         init_num = min(INIT_NUM, self.num_all_configs)
@@ -67,17 +70,15 @@
         # print(configs)
         # TODO the current implementation requires that all configs have the same shape.
         configs = np.array(configs, dtype=np.float32)
         estimates = self.cost_model.predict(configs)
 
         n = len(estimates)
         top_idx = np.argsort(estimates)
-        top_idx_ret = top_idx if self.metric == AUTOTUNING_METRIC_LATENCY else top_idx[::
-                                                                                       -1][:
-                                                                                           n]
+        top_idx_ret = top_idx if self.metric == AUTOTUNING_METRIC_LATENCY else top_idx[::-1][:n]
 
         # top_configs = [self.all_configs[i] for i in top_idx]
 
         return top_idx_ret
 
     def next_batch(self, sample_size):
         sampled_batch = []
@@ -141,17 +142,15 @@
                 flattened_ds_config = flatten(ds_config)
                 for k, v in flattened_ds_config.items():
                     if isinstance(v, numbers.Number):
                         feature_val.append(v)
                 self.evaluated_configs.append(feature_val)
                 self.evaluated_perf.append(curr_iter)
 
-        logger.debug(
-            f"**Evaluated configs: {len(self.evaluated_configs)}, evaluated perf: {self.evaluated_perf}"
-        )
+        logger.debug(f"**Evaluated configs: {len(self.evaluated_configs)}, evaluated perf: {self.evaluated_perf}")
 
         self.cost_model.fit(self.evaluated_configs, self.evaluated_perf)
 
         estimated_top_configs = self.find_estimated_top_configs()
 
         self.trials = estimated_top_configs
         self.trial_pt = 0
```

### Comparing `deepspeed-0.8.3/deepspeed/autotuning/tuner/utils.py` & `deepspeed-0.9.0/deepspeed/autotuning/tuner/utils.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,8 +1,11 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import numpy as np
 import itertools
 from ..utils import *
 import collections.abc
 
 
@@ -40,17 +43,15 @@
 
 
 def gen_combinations(d: dict):
     keys, values = d.keys(), d.values()
     for v in values:
         if not isinstance(v, list):
             v = [v]
-    values_choices = (gen_combinations(v) if isinstance(v,
-                                                        dict) else get_list(v)
-                      for v in values)
+    values_choices = (gen_combinations(v) if isinstance(v, dict) else get_list(v) for v in values)
     for comb in itertools.product(*values_choices):
         yield dict(zip(keys, comb))
 
 
 def flatten(d, parent_key='', sep='_'):
     items = []
     for k, v in d.items():
```

### Comparing `deepspeed-0.8.3/deepspeed/autotuning/utils.py` & `deepspeed-0.9.0/deepspeed/autotuning/utils.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,11 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import re
 import collections.abc
 import os
 import json
 from deepspeed.runtime.constants import GRADIENT_ACCUMULATION_STEPS, TRAIN_MICRO_BATCH_SIZE_PER_GPU
 import itertools
@@ -172,30 +175,29 @@
                 raise ValueError("host {} is already defined".format(hostname))
             resource_pool[hostname] = slot_count
 
     return resource_pool
 
 
 def validate_ds_config(config: dict):
+
     def is_False(config: dict, key):
         if config is None:
             return False
         return bool(config.get(key))
 
     config_zero = config.get("zero_optimization", {})
     if not config_zero:
         return True
     stage = config_zero.get("stage")
     offload = False
     if stage == 1:
         return True
     elif stage == 2:
-        if is_False(config_zero,
-                    "cpu_offload") and is_False(config_zero,
-                                                "cpu_offload_params"):
+        if is_False(config_zero, "cpu_offload") and is_False(config_zero, "cpu_offload_params"):
             return False
     elif stage == 3:
         offload_devices = ["cpu", "nvme"]
         if config_zero.get("offload_optimizer", {}).get("device") in offload_devices:
             offload = True
         if config_zero.get("offload_param", {}).get("device") in offload_devices:
             offload = True
@@ -285,22 +287,21 @@
 
 def get_all_configs(tuning_space: dict, ignore_keys=None):
     """ Splits the tuning space dictionary to result in all combinations of values.
 
     Args:
         tuning_space (dict): the tuning space where tunable parameters are lists of values.
     """
+
     def gen_combinations(d: dict):
         keys, values = d.keys(), d.values()
         for v in values:
             if not isinstance(v, list):
                 v = [v]
-        values_choices = (gen_combinations(v) if isinstance(v,
-                                                            dict) else get_list(v)
-                          for v in values)
+        values_choices = (gen_combinations(v) if isinstance(v, dict) else get_list(v) for v in values)
         for comb in itertools.product(*values_choices):
             yield dict(zip(keys, comb))
 
     all_configs = []
     ignored_key_vals = {}
     for ik in ignore_keys:
         ignored_key_vals[ik] = tuning_space.get(ik, {})
```

### Comparing `deepspeed-0.8.3/deepspeed/checkpoint/__init__.py` & `deepspeed-0.9.0/deepspeed/checkpoint/__init__.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,16 +1,17 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 from .reshape_meg_2d import reshape_meg_2d_parallel
 
 from .deepspeed_checkpoint import DeepSpeedCheckpoint
 
-from .utils import (get_layer_ckpt_name_for_rank,
-                    get_model_ckpt_name_for_rank,
-                    get_zero_ckpt_name_for_rank)
+from .utils import (get_layer_ckpt_name_for_rank, get_model_ckpt_name_for_rank, get_zero_ckpt_name_for_rank)
 
 from .reshape_utils import (merge_state)
 
 from .reshape_3d_utils import (model_3d_desc, get_model_3d_descriptor)
 
 from .zero_checkpoint import ZeROCheckpoint
```

### Comparing `deepspeed-0.8.3/deepspeed/checkpoint/constants.py` & `deepspeed-0.9.0/deepspeed/checkpoint/constants.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,11 +1,14 @@
-'''Copyright The Microsoft DeepSpeed Team'''
-'''
-    Various symbolic constants used for model checkpointing
-'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+"""
+Various symbolic constants used for model checkpointing
+"""
 
 #########################################
 # Optimizer checkpoint keys
 #########################################
 OPTIMIZER_STATE_DICT = "optimizer_state_dict"
 FP32_GROUPS = "fp32_groups"
 FP32_FLAT_GROUPS = 'fp32_flat_groups'
```

### Comparing `deepspeed-0.8.3/deepspeed/checkpoint/deepspeed_checkpoint.py` & `deepspeed-0.9.0/deepspeed/checkpoint/deepspeed_checkpoint.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,95 +1,80 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import os
 from typing import Dict
 import torch
 
 from .reshape_3d_utils import model_3d_desc
-from .reshape_utils import (basic_folder_validation,
-                            merge_state,
-                            partition_data,
-                            get_files,
-                            get_files_with_prefix)
+from .reshape_utils import (basic_folder_validation, merge_state, partition_data, get_files, get_files_with_prefix)
 
 from .constants import (MODEL_FILE_PREFIX, LAYER_FILE_PREFIX)
 
 from .reshape_meg_2d import reshape_meg_2d_parallel, meg_2d_parallel_map
 from .zero_checkpoint import ZeROCheckpoint
 from .constants import *
 
 EMBEDDING_LAYER_INDEX = 0
 FINAL_LAYER_NORM_INDEX = -1
 ARGS_KEY = 'args'
 CHECKPOINT_INFO_KEY = 'checkpoint_info'
 ITERATION_KEY = 'iteration'
 
 SEQUENTIAL_LAYERS = [
-    'input_layernorm.weight',
-    'input_layernorm.bias',
-    'self_attention.dense.bias',
-    'post_attention_layernorm.weight',
-    'post_attention_layernorm.bias',
-    'mlp.dense_4h_to_h.bias',
-    'position_embeddings.weight'
+    'input_layernorm.weight', 'input_layernorm.bias', 'self_attention.dense.bias', 'post_attention_layernorm.weight',
+    'post_attention_layernorm.bias', 'mlp.dense_4h_to_h.bias', 'position_embeddings.weight'
 ]
 
 LAYER_CONCAT_DIM = {'self_attention.dense.weight': 1, 'mlp.dense_4h_to_h.weight': 1}
 
 
 class DeepSpeedCheckpoint(object):
+
     def __init__(self, dir, tp_degree=None, pp_degree=None, dp_degree=None):
         self.dir = dir
         self._validate_folder(dir)
 
         self.zero_checkpoint = ZeROCheckpoint(dir)
 
         self.file_list = get_files(dir)
         self.layer_files = get_files_with_prefix(self.file_list, LAYER_FILE_PREFIX)
         self.mp_rank_files = get_files_with_prefix(self.file_list, MODEL_FILE_PREFIX)
 
         self.layer_keys = self._get_layer_keys()
         self.layer_count = len(self.layer_keys)
 
-        self.tp_degree = self.zero_checkpoint.get_src_tp_degree(
-        ) if tp_degree is None else tp_degree
-        self.pp_degree = self.zero_checkpoint.get_src_pp_degree(
-        ) if pp_degree is None else pp_degree
-        self.dp_degree = self.zero_checkpoint.get_src_dp_degree(
-        ) if dp_degree is None else dp_degree
+        self.tp_degree = self.zero_checkpoint.get_src_tp_degree() if tp_degree is None else tp_degree
+        self.pp_degree = self.zero_checkpoint.get_src_pp_degree() if pp_degree is None else pp_degree
+        self.dp_degree = self.zero_checkpoint.get_src_dp_degree() if dp_degree is None else dp_degree
 
-        self.original_world_size = self.zero_checkpoint.get_src_tp_degree(
-        ) * self.zero_checkpoint.get_src_pp_degree(
+        self.original_world_size = self.zero_checkpoint.get_src_tp_degree() * self.zero_checkpoint.get_src_pp_degree(
         ) * self.zero_checkpoint.get_src_dp_degree()
         self.world_size = self.tp_degree * self.pp_degree * self.dp_degree
 
         self.old_2d_map = meg_2d_parallel_map(self.zero_checkpoint.get_src_pp_degree(),
                                               self.zero_checkpoint.get_src_tp_degree())
         self.old_2d_map.simple_init()
-        self.new_2d_map = reshape_meg_2d_parallel(
-            old_pp_degree=self.zero_checkpoint.get_src_pp_degree(),
-            old_tp_degree=self.zero_checkpoint.get_src_tp_degree(),
-            new_pp_degree=self.pp_degree,
-            new_tp_degree=self.tp_degree)
-
-        if self.is_change_pp_degree() or self.is_change_tp_degree(
-        ) or self.is_change_dp_degree():
-            self.zero_checkpoint.reshape(
-                model_3d_desc(self.pp_degree,
-                              self.tp_degree,
-                              self.dp_degree))
+        self.new_2d_map = reshape_meg_2d_parallel(old_pp_degree=self.zero_checkpoint.get_src_pp_degree(),
+                                                  old_tp_degree=self.zero_checkpoint.get_src_tp_degree(),
+                                                  new_pp_degree=self.pp_degree,
+                                                  new_tp_degree=self.tp_degree)
+
+        if self.is_change_pp_degree() or self.is_change_tp_degree() or self.is_change_dp_degree():
+            self.zero_checkpoint.reshape(model_3d_desc(self.pp_degree, self.tp_degree, self.dp_degree))
 
         self.global_state = {}
 
         self._sanity_check()
         self.pp_to_transformer_map = self._build_pp_transformer_map()
         self.transformer_file_map = self._build_transformer_file_map()
         self.tp_to_embedding_map = self._build_tp_other_layer_map(EMBEDDING_LAYER_INDEX)
-        self.tp_to_final_norm_map = self._build_tp_other_layer_map(
-            FINAL_LAYER_NORM_INDEX)
+        self.tp_to_final_norm_map = self._build_tp_other_layer_map(FINAL_LAYER_NORM_INDEX)
         self._build_global_state()
 
     def is_change_tp_degree(self):
         return self.tp_degree != self.zero_checkpoint.get_src_tp_degree()
 
     def is_change_pp_degree(self):
         return self.pp_degree != self.zero_checkpoint.get_src_pp_degree()
@@ -127,17 +112,15 @@
     def get_zero_checkpoint_state(self, pp_index, tp_index, dp_index) -> dict:
         return self.zero_checkpoint.get_state_for_rank(pp_index=pp_index,
                                                        tp_index=tp_index,
                                                        dp_index=dp_index,
                                                        keys_to_ignore=[PARAM_SHAPES])
 
     def get_zero_files(self, pp_index, tp_index, dp_index) -> list:
-        return self.zero_checkpoint.get_files_for_rank(pp_index=pp_index,
-                                                       tp_index=tp_index,
-                                                       dp_index=dp_index)
+        return self.zero_checkpoint.get_files_for_rank(pp_index=pp_index, tp_index=tp_index, dp_index=dp_index)
 
     def get_embedding_layer_id(self):
         return self.layer_keys[EMBEDDING_LAYER_INDEX]
 
     def get_final_norm_layer_id(self):
         return self.layer_keys[FINAL_LAYER_NORM_INDEX]
 
@@ -146,19 +129,15 @@
             sd = torch.load(self.mp_rank_files[0], map_location=torch.device('cpu'))
             self.global_state[ITERATION_KEY] = sd.get(ITERATION_KEY, 0)
 
         return self.global_state[ITERATION_KEY]
 
     def get_embedding_state(self, tp_index: int) -> Dict:
         assert tp_index in self.tp_to_embedding_map.keys()
-        sd_list = [
-            torch.load(fname,
-                       map_location=torch.device('cpu'))
-            for fname in self.tp_to_embedding_map[tp_index]
-        ]
+        sd_list = [torch.load(fname, map_location=torch.device('cpu')) for fname in self.tp_to_embedding_map[tp_index]]
         sd = self._merge_state_dicts(sd_list)
         return sd
 
     def get_embedding_files(self, tp_index: int) -> list:
         assert tp_index in self.tp_to_embedding_map.keys()
         return self.tp_to_embedding_map[tp_index]
 
@@ -175,18 +154,15 @@
     def get_checkpoint_info(self, info_key=CHECKPOINT_INFO_KEY):
         return self._get_checkpoint_value(info_key)
 
     def get_2d_parallel_state(self, tp_index: int, pp_index: int) -> dict:
         assert tp_index < self.tp_degree
         assert pp_index < self.pp_degree
         fname_list = self.get_2d_parallel_files(tp_index=tp_index, pp_index=pp_index)
-        sd_list = [
-            torch.load(fname,
-                       map_location=torch.device('cpu')) for fname in fname_list
-        ]
+        sd_list = [torch.load(fname, map_location=torch.device('cpu')) for fname in fname_list]
 
         merged_sd = None
         for sd in sd_list:
             if merged_sd is None:
                 merged_sd = sd
             else:
                 merged_sd = merge_state(merged_sd, sd)
@@ -194,59 +170,50 @@
         return merged_sd
 
     def get_transformer_state(self, tp_index: int, pp_index: int) -> list:
         assert tp_index < self.tp_degree
         assert pp_index < self.pp_degree
         t_list = []
         for fname_list in self.transformer_file_map[(tp_index, pp_index)]:
-            sd_list = [
-                torch.load(fname,
-                           map_location=torch.device('cpu')) for fname in fname_list
-            ]
+            sd_list = [torch.load(fname, map_location=torch.device('cpu')) for fname in fname_list]
             sd = self._merge_state_dicts(sd_list)
             t_list.append(sd)
         return t_list
 
     def get_pp_transformer_map(self, pp_index: int) -> list:
         assert pp_index < self.pp_degree
         return self.pp_to_transformer_map[pp_index]
 
     def get_final_norm_state(self, tp_index: int) -> Dict:
         assert tp_index in self.tp_to_final_norm_map.keys()
-        sd = torch.load(self.tp_to_final_norm_map[tp_index][0],
-                        map_location=torch.device('cpu'))
+        sd = torch.load(self.tp_to_final_norm_map[tp_index][0], map_location=torch.device('cpu'))
         return sd
 
     def get_final_norm_files(self, tp_index: int) -> list:
         assert tp_index in self.tp_to_final_norm_map.keys()
         return self.tp_to_final_norm_map[tp_index]
 
     def _build_tp_other_layer_map(self, layer_index: int):
         assert layer_index < len(self.layer_files)
-        layer_files = get_files_with_prefix(self.layer_files,
-                                            self.layer_keys[layer_index])
+        layer_files = get_files_with_prefix(self.layer_files, self.layer_keys[layer_index])
         layer_file_partitions = partition_data(layer_files, self.tp_degree)
         data_map = {i: flist for i, flist in enumerate(layer_file_partitions)}
         return data_map
 
     def get_2d_parallel_files(self, tp_index: int, pp_index: int) -> list:
         assert tp_index < self.tp_degree
         assert pp_index < self.pp_degree
         file_indices = self.new_2d_map.get_data(pp_index=pp_index, tp_index=tp_index)
         return [self.mp_rank_files[i] for i in file_indices]
 
     def _build_pp_transformer_map(self):
         data_map = {}
         transformer_layers = self.layer_keys[1:-1]
         layers_per_pp = len(transformer_layers) // self.pp_degree
-        data_map = {
-            i: transformer_layers[i * layers_per_pp:(i + 1) * layers_per_pp]
-            for i in range(0,
-                           self.pp_degree)
-        }
+        data_map = {i: transformer_layers[i * layers_per_pp:(i + 1) * layers_per_pp] for i in range(0, self.pp_degree)}
         return data_map
 
     def _dump_mapping(self, data_map, map_tag=None):
         if map_tag is not None:
             print(f'Dump mapping: {map_tag}')
         for k, v in data_map.items():
             print(f'{k} = {v}')
@@ -304,14 +271,12 @@
         return merged_sd
 
     def _validate_folder(self, dir):
         basic_folder_validation(dir)
 
         file_list = get_files(dir)
 
-        for file_prefix in [
-                MODEL_FILE_PREFIX,
-                LAYER_FILE_PREFIX,
-                f'{LAYER_FILE_PREFIX}01'
-        ]:
+        for file_prefix in [MODEL_FILE_PREFIX, LAYER_FILE_PREFIX, f'{LAYER_FILE_PREFIX}01']:
             ckpt_files = get_files_with_prefix(file_list, file_prefix)
-            assert len(ckpt_files) > 0, f'{dir} seems a bogus DeepSpeed checkpoint folder: Cannot find {file_prefix}* files in there.'
+            assert len(
+                ckpt_files
+            ) > 0, f'{dir} seems a bogus DeepSpeed checkpoint folder: Cannot find {file_prefix}* files in there.'
```

### Comparing `deepspeed-0.8.3/deepspeed/checkpoint/reshape_3d_utils.py` & `deepspeed-0.9.0/deepspeed/checkpoint/reshape_3d_utils.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,24 +1,25 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
 
-from .reshape_utils import (get_files,
-                            get_files_with_prefix,
-                            partition_data,
-                            get_zero_files)
+# DeepSpeed Team
+
+from .reshape_utils import (get_files, get_files_with_prefix, partition_data, get_zero_files)
 
 from .constants import (MODEL_FILE_PREFIX, LAYER_FILE_PREFIX)
 
 from .reshape_meg_2d import (reshape_meg_2d_parallel, meg_2d_parallel_map)
 
 PP_DIM = 'PP'
 TP_DIM = 'TP'
 DP_DIM = 'DP'
 
 
 class model_3d_desc(object):
+
     def __init__(self, pp_degree=1, tp_degree=1, dp_degree=1):
         self.pp_degree = pp_degree
         self.tp_degree = tp_degree
         self.dp_degree = dp_degree
 
     def reshape(self, target_3d_desc, verbose=False):
         valid_reshape, reshape_errors = self.can_reshape(target_3d_desc)
@@ -29,53 +30,46 @@
                                              new_tp_degree=target_3d_desc.tp_degree,
                                              verbose=verbose)
 
         flat_3d_map = flatten_dp_dimension(meg_2d_map=tgt_2d_map,
                                            src_2d_size=self.pp_degree * self.tp_degree,
                                            dp_degree=self.dp_degree)
 
-        return unflatten_dp_dimension(meg_2d_map=flat_3d_map,
-                                      dp_degree=target_3d_desc.dp_degree)
+        return unflatten_dp_dimension(meg_2d_map=flat_3d_map, dp_degree=target_3d_desc.dp_degree)
 
     def get_desc(self):
         return f'{PP_DIM},{TP_DIM},{DP_DIM} = ({self.pp_degree}, {self.tp_degree}, {self.dp_degree})'
 
     def world_size(self):
         return self.pp_degree * self.tp_degree * self.dp_degree
 
     def is_valid(self, pp_index, tp_index, dp_index):
         err_msg = []
         valid = True
-        for index, degree, dim_name in [
-            (pp_index, self.pp_degree, PP_DIM),
-            (tp_index, self.tp_degree, TP_DIM),
-            (dp_index, self.dp_degree, DP_DIM)]:
+        for index, degree, dim_name in [(pp_index, self.pp_degree, PP_DIM), (tp_index, self.tp_degree, TP_DIM),
+                                        (dp_index, self.dp_degree, DP_DIM)]:
             if index >= degree:
                 valid = False
-                err_msg.append(
-                    f'{dim_name} indexing error: index {index} >= degree {degree}')
+                err_msg.append(f'{dim_name} indexing error: index {index} >= degree {degree}')
 
         return valid, err_msg
 
     def can_reshape(self, target_3d_desc):
         err_msg = []
         if target_3d_desc.pp_degree > self.pp_degree:
             err_msg.append(
-                f'Expansion reshape not supported - {PP_DIM}: {self.pp_degree} ---> {target_3d_desc.pp_degree}'
-            )
+                f'Expansion reshape not supported - {PP_DIM}: {self.pp_degree} ---> {target_3d_desc.pp_degree}')
 
         if target_3d_desc.tp_degree > self.tp_degree:
             err_msg.append(
-                f'Expansion reshape not supported - {TP_DIM}: {self.tp_degree} ---> {target_3d_desc.tp_degree}'
-            )
+                f'Expansion reshape not supported - {TP_DIM}: {self.tp_degree} ---> {target_3d_desc.tp_degree}')
 
         if target_3d_desc.dp_degree > self.dp_degree:
             err_msg.append(
-                f'Expansion reshape not supported - {DP_DIM}: {self.dp_degree} ---> {target_3d_desc.dp_degree}'
-            )
+                f'Expansion reshape not supported - {DP_DIM}: {self.dp_degree} ---> {target_3d_desc.dp_degree}')
 
         return len(err_msg) == 0, err_msg
 
 
 def get_model_3d_descriptor(dir):
     file_list = get_files(dir)
     zero_file_list = get_zero_files(dir)
@@ -102,18 +96,15 @@
                 new_meg_2d_map.add_data(pp_index, tp_index, dpX_indices)
     return new_meg_2d_map
 
 
 def unflatten_dp_dimension(meg_2d_map, dp_degree):
     pp_degree = meg_2d_map.pp_degree
     tp_degree = meg_2d_map.tp_degree
-    meg_2d_map_list = [
-        meg_2d_parallel_map(pp_degree=pp_degree,
-                            tp_degree=tp_degree) for _ in range(dp_degree)
-    ]
+    meg_2d_map_list = [meg_2d_parallel_map(pp_degree=pp_degree, tp_degree=tp_degree) for _ in range(dp_degree)]
     for pp_index in range(pp_degree):
         for tp_index in range(tp_degree):
             flat_dp_indices = meg_2d_map.get_data(pp_index, tp_index)
             partitioned_dp_indices = partition_data(flat_dp_indices, dp_degree)
             for dp_indices, _2d_map in zip(partitioned_dp_indices, meg_2d_map_list):
                 _2d_map.add_data(pp_index, tp_index, dp_indices)
```

### Comparing `deepspeed-0.8.3/deepspeed/checkpoint/reshape_meg_2d.py` & `deepspeed-0.9.0/deepspeed/checkpoint/reshape_meg_2d.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,22 +1,25 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 from .reshape_utils import partition_data
 
 
 class meg_2d_parallel_map(object):
+
     def __init__(self, pp_degree, tp_degree):
         self.pp_degree = pp_degree
         self.tp_degree = tp_degree
         self.map = {}
 
     def simple_init(self):
         self.map = {
-            self._make_key(i // self.tp_degree,
-                           i % self.tp_degree): [i]
+            self._make_key(i // self.tp_degree, i % self.tp_degree): [i]
             for i in range(self.pp_degree * self.tp_degree)
         }
 
     def add_data(self, pp_index, tp_index, data):
         self._validate_indices(pp_index, tp_index)
         assert type(data) is list
 
@@ -70,19 +73,15 @@
         split_ranks = partition_data(ranks_for_tp_index, new_pp_degree)
         for j in range(new_pp_degree):
             new_2d_map.add_data(j, i, split_ranks[j])
 
     return new_2d_map
 
 
-def reshape_meg_2d_parallel(old_pp_degree,
-                            old_tp_degree,
-                            new_pp_degree,
-                            new_tp_degree,
-                            verbose=False):
+def reshape_meg_2d_parallel(old_pp_degree, old_tp_degree, new_pp_degree, new_tp_degree, verbose=False):
     assert new_pp_degree <= old_pp_degree
     assert new_tp_degree <= old_tp_degree
 
     old_2d_map = meg_2d_parallel_map(old_pp_degree, old_tp_degree)
     old_2d_map.simple_init()
     if verbose:
         old_2d_map.print_data(f'original_2d_map:')
@@ -133,16 +132,15 @@
 
     world_size = tp_size * pp_size * dp_size
 
     print(f"\n\n*** tp={tp_size}, pp={pp_size}, dp={dp_size}, world={world_size}")
 
     tensor_model_parallel_size = min(tp_size, world_size)
     pipeline_model_parallel_size = min(pp_size, world_size)
-    data_parallel_size = world_size // (tensor_model_parallel_size *
-                                        pipeline_model_parallel_size)
+    data_parallel_size = world_size // (tensor_model_parallel_size * pipeline_model_parallel_size)
 
     num_tensor_model_parallel_groups = world_size // tensor_model_parallel_size
     num_pipeline_model_parallel_groups = world_size // pipeline_model_parallel_size
     num_data_parallel_groups = world_size // data_parallel_size
 
     # Build the data-parallel groups.
     all_dp_group_ranks = []
@@ -154,27 +152,23 @@
             all_dp_group_ranks.append(list(ranks))
 
     print("DP", all_dp_group_ranks)
 
     # Build the model-parallel groups.
     all_pp_group_ranks = []
     for i in range(data_parallel_size):
-        ranks = [
-            data_parallel_group_ranks[i]
-            for data_parallel_group_ranks in all_dp_group_ranks
-        ]
+        ranks = [data_parallel_group_ranks[i] for data_parallel_group_ranks in all_dp_group_ranks]
         all_pp_group_ranks.append(list(ranks))
 
     print(f"PP", all_pp_group_ranks)
 
     # Build the tensor model-parallel groups.
     all_tp_group_ranks = []
     for i in range(num_tensor_model_parallel_groups):
-        ranks = range(i * tensor_model_parallel_size,
-                      (i + 1) * tensor_model_parallel_size)
+        ranks = range(i * tensor_model_parallel_size, (i + 1) * tensor_model_parallel_size)
         all_tp_group_ranks.append(list(ranks))
 
     print(f"TP", all_tp_group_ranks)
 
     return all_tp_group_ranks, all_pp_group_ranks, all_dp_group_ranks
 
     # # Build the pipeline model-parallel groups and embedding groups
```

### Comparing `deepspeed-0.8.3/deepspeed/checkpoint/reshape_utils.py` & `deepspeed-0.9.0/deepspeed/checkpoint/reshape_utils.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,8 +1,11 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import os
 import torch
 from collections import OrderedDict
 from .constants import (ZERO_FILE_PREFIX, FP16_ZERO_FILE_PREFIX, BF16_ZERO_FILE_PREFIX)
 
 
@@ -45,19 +48,15 @@
     return []
 
 
 def partition_data(data_list, num_partitions):
     num_elems = len(data_list)
     assert num_elems % num_partitions == 0
     partition_size = num_elems // num_partitions
-    partitions_list = [
-        data_list[i:i + partition_size] for i in range(0,
-                                                       num_elems,
-                                                       partition_size)
-    ]
+    partitions_list = [data_list[i:i + partition_size] for i in range(0, num_elems, partition_size)]
     return partitions_list
 
 
 def _key_list_to_string(key_list):
     return '.'.join(key_list)
 
 
@@ -72,27 +71,24 @@
 
     return merged_dict
 
 
 def merge_state_list(list_a, list_b, key_list):
     if len(list_a) != len(list_b):
         print(f'{_key_list_to_string(key_list)}')
-        raise ValueError(
-            f'Cannot merge lists of different lengths, a = {len(list_a)} b = {len(list_b)}'
-        )
+        raise ValueError(f'Cannot merge lists of different lengths, a = {len(list_a)} b = {len(list_b)}')
 
     return [merge_state(a, b, key_list) for a, b in zip(list_a, list_b)]
 
 
 def merge_state(state_a, state_b, key_list=[]):
     if type(state_a) != type(state_b):
         key_list_string = _key_list_to_string(key_list)
         print(f'key_list = {key_list_string}')
-        raise ValueError(
-            f'Cannot merge two states of types {type(state_a)} and type {type(state_b)}')
+        raise ValueError(f'Cannot merge two states of types {type(state_a)} and type {type(state_b)}')
 
     if type(state_a) in (dict, OrderedDict):
         return merge_state_dict(state_a, state_b, key_list)
     elif type(state_a) in (list, tuple):
         return type(state_a)(merge_state_list(state_a, state_b, key_list))
     elif torch.is_tensor(state_a):
         return torch.cat([state_a, state_b], 0)
```

### Comparing `deepspeed-0.8.3/deepspeed/checkpoint/universal_checkpoint.py` & `deepspeed-0.9.0/deepspeed/checkpoint/universal_checkpoint.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,17 +1,16 @@
-"""
-Copyright 2022 The Microsoft DeepSpeed Team
-"""
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+
 import os
 import torch
 import types
-from .constants import (FP32_WEIGHT_KEY,
-                        PARAM,
-                        VOCAB_DIVISIBILITY_PADDING_TENSOR,
-                        CAT_DIM)
+from .constants import (FP32_WEIGHT_KEY, PARAM, VOCAB_DIVISIBILITY_PADDING_TENSOR, CAT_DIM)
 
 
 def load_hp_checkpoint_state(self, folder, tp_rank, tp_world_size):
     hp_mapping = self._hp_mapping
     optim_state_keys = hp_mapping.get_optim_state_keys()
     hp_keys = [FP32_WEIGHT_KEY] + optim_state_keys
     checkpoint_files = {key: os.path.join(folder, f"{key}.pt") for key in hp_keys}
@@ -40,69 +39,57 @@
             tp_rank = 0
             tp_world_size = 1
 
         # special case for word_embeddings weights which get padded differently depending on TP degree.
         # the converter to universal currently strips the original padding completely so the saved
         # weight is padding-free and we just need to add new padding depending on the target TP
         # degree
-        vocab_divisibility_padding_tensor = ckpt_dict.get(
-            VOCAB_DIVISIBILITY_PADDING_TENSOR,
-            None)
+        vocab_divisibility_padding_tensor = ckpt_dict.get(VOCAB_DIVISIBILITY_PADDING_TENSOR, None)
         if vocab_divisibility_padding_tensor is not None:
             # In the absence of data passed from the user wrt new padded vocab specific to tp degree
             # we can again derive that data by reverse engineering the target shapes like so:
             padded_target_vocab_size = self.shape[0] * tp_world_size
             if padded_target_vocab_size > full_hp_param.shape[0]:
                 # Need to expand
                 padding_size = padded_target_vocab_size - full_hp_param.shape[0]
                 # Implement the following concat in efficient way using pad
                 #full_hp_param = torch.cat((full_hp_param, padding_tensor), 0)
-                full_hp_param = torch.nn.functional.pad(full_hp_param,
-                                                        (0,
-                                                         0,
-                                                         0,
-                                                         padding_size),
-                                                        "constant",
-                                                        0)
+                full_hp_param = torch.nn.functional.pad(full_hp_param, (0, 0, 0, padding_size), "constant", 0)
                 full_hp_param[:-padding_size, :] = vocab_divisibility_padding_tensor
             else:
                 # Need to shrink or keep the same
                 full_hp_param = full_hp_param[:padded_target_vocab_size, :]
 
         full_param_numel = full_hp_param.numel()
         tp_slice_numel = self.numel()
         #        if key == FP32_WEIGHT_KEY and 'word_embeddings.weight' in folder:
         #            print_rank_0(f'{full_hp_param[:10]=}', force=True)
 
 
         assert full_param_numel == tp_world_size * tp_slice_numel, \
             f'Loading {ckpt_file} full param numel {full_param_numel} != tensor slice numel {tp_slice_numel} * tp_world_size {tp_world_size}'
-        dst_tensor = hp_mapping.hp_fragment if key == FP32_WEIGHT_KEY else hp_mapping.get_optim_state_fragment(
-            key)
+        dst_tensor = hp_mapping.hp_fragment if key == FP32_WEIGHT_KEY else hp_mapping.get_optim_state_fragment(key)
 
         #        print(f"{full_hp_param.shape=} {full_param_numel=} {folder=}")
         #        print(f"{dst_tensor.shape=} {dst_tensor.numel()=}{folder=}")
 
         # since when we do many to 1 on tp we cat sometimes on dim=0 and other times on dim=1 we have to do exactly the same in reverse
         chunk_dim = ckpt_dict.get(CAT_DIM, 0)
 
         # this performs the opposite of cat when merging TP slices
         tp_hp_slice = full_hp_param.chunk(tp_world_size, chunk_dim)[tp_rank]
         tp_hp_slice = tp_hp_slice.flatten()
 
         lp_frag_address = hp_mapping.lp_fragment_address
-        tp_hp_fragment = tp_hp_slice.narrow(0,
-                                            lp_frag_address.start,
-                                            lp_frag_address.numel)
+        tp_hp_fragment = tp_hp_slice.narrow(0, lp_frag_address.start, lp_frag_address.numel)
         assert dst_tensor.numel() == lp_frag_address.numel, \
             f'Load checkpoint {key} dst_tensor numel {dst_tensor.numel()} != src numel {lp_frag_address.numel}'
 
         #        print(f"{key} SHAPE: {tp_hp_slice.shape=}")
         #        print(f"{key} SHAPE: {dst_tensor.shape=}")
         #        print(f"{key} SHAPE: {tp_hp_fragment.shape=}")
         dst_tensor.data.copy_(tp_hp_fragment.data)
 
 
 def enable_universal_checkpoint(param_list):
     for param in param_list:
-        param.load_hp_checkpoint_state = types.MethodType(load_hp_checkpoint_state,
-                                                          param)
+        param.load_hp_checkpoint_state = types.MethodType(load_hp_checkpoint_state, param)
```

### Comparing `deepspeed-0.8.3/deepspeed/checkpoint/utils.py` & `deepspeed-0.9.0/deepspeed/checkpoint/utils.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,14 +1,14 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import os
-from .constants import (MODEL_FILE_PREFIX,
-                        MODEL_FILE_SUFFIX,
-                        OPTIM_FILE_SUFFIX,
-                        ZERO_FILE_PREFIX)
+from .constants import (MODEL_FILE_PREFIX, MODEL_FILE_SUFFIX, OPTIM_FILE_SUFFIX, ZERO_FILE_PREFIX)
 
 
 def get_model_ckpt_name_for_rank(base_folder, mp_rank_str):
     ckpt_name = os.path.join(
         base_folder,
         MODEL_FILE_PREFIX + mp_rank_str + MODEL_FILE_SUFFIX,
     )
```

### Comparing `deepspeed-0.8.3/deepspeed/checkpoint/zero_checkpoint.py` & `deepspeed-0.9.0/deepspeed/checkpoint/zero_checkpoint.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,24 +1,25 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import torch
 
-from .constants import (BASE_OPTIMIZER_STATE,
-                        GROUP_PADDINGS,
-                        OPTIMIZER_STATE_DICT,
-                        PARTITION_COUNT)
+from .constants import (BASE_OPTIMIZER_STATE, GROUP_PADDINGS, OPTIMIZER_STATE_DICT, PARTITION_COUNT)
 
 from .reshape_utils import (basic_folder_validation, get_zero_files, merge_state)
 
 from .reshape_3d_utils import (model_3d_desc, get_model_3d_descriptor)
 
 GROUP_STATE_KEY = 'state'
 
 
 class ZeROCheckpoint(object):
+
     def __init__(self, dir):
         basic_folder_validation(dir)
         self.dir = dir
         self.file_list = get_zero_files(dir)
         self.num_files = len(self.file_list)
         assert self.num_files > 0, f'No ZeRO files found in {dir}'
 
@@ -45,20 +46,15 @@
         dp_2d_map = self._3d_file_map[dp_index]
         return dp_2d_map.get_data(pp_index, tp_index)
 
     def get_files_for_rank(self, pp_index, tp_index, dp_index):
         file_idx_list = self.get_file_indices_for_rank(pp_index, tp_index, dp_index)
         return [self.file_list[idx] for idx in file_idx_list]
 
-    def get_state_for_rank(self,
-                           pp_index,
-                           tp_index,
-                           dp_index,
-                           keys_to_ignore=[],
-                           strip_tensor_paddings=True):
+    def get_state_for_rank(self, pp_index, tp_index, dp_index, keys_to_ignore=[], strip_tensor_paddings=True):
         state_file_list = self.get_files_for_rank(pp_index, tp_index, dp_index)
         merged_sd = None
         for state_file in state_file_list:
             sd = torch.load(state_file, map_location=torch.device('cpu'))
             for key in keys_to_ignore:
                 sd.pop(key, None)
 
@@ -107,18 +103,15 @@
 
         for key, group_state in param_group_states.items():
             if group_paddings[key] == 0:
                 continue
             for state_name, state_value in group_state.items():
                 if torch.is_tensor(state_value):
                     raw_length = state_value.numel() - group_paddings[key]
-                    group_state[state_name] = torch.narrow(state_value,
-                                                           0,
-                                                           0,
-                                                           raw_length).clone()
+                    group_state[state_name] = torch.narrow(state_value, 0, 0, raw_length).clone()
 
     def _clear_group_paddings(self, sd):
         group_paddings = self._get_optimizer_state(sd, GROUP_PADDINGS)
         if group_paddings:
             num_groups = len(group_paddings)
             sd[OPTIMIZER_STATE_DICT][GROUP_PADDINGS] = [0] * num_groups
 
@@ -140,9 +133,8 @@
 
         return base_optimizer_state.get(GROUP_STATE_KEY, None)
 
     def _update_partition_count(self, sd):
         partition_counts = self._get_optimizer_state(sd, PARTITION_COUNT)
         if partition_counts:
             num_groups = len(partition_counts)
-            sd[OPTIMIZER_STATE_DICT][PARTITION_COUNT] = [self.target_3d.dp_degree
-                                                         ] * num_groups
+            sd[OPTIMIZER_STATE_DICT][PARTITION_COUNT] = [self.target_3d.dp_degree] * num_groups
```

### Comparing `deepspeed-0.8.3/deepspeed/comm/__init__.py` & `deepspeed-0.9.0/deepspeed/comm/__init__.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,8 +1,11 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import torch
 from .utils import *
 from deepspeed import utils
 
 supported_torch_version = False
 
@@ -35,18 +38,13 @@
 
     def reduce_scatter_fn(output_tensor, input_tensor, group=None, async_op=False):
         from torch.distributed import reduce_scatter, get_world_size
         from torch import chunk
         input_tensor_lst = list(chunk(input_tensor, get_world_size(group)))
         return reduce_scatter(output_tensor, input_tensor_lst, group=group)
 
-    def configure(deepspeed_config=None,
-                  enabled=None,
-                  prof_all=None,
-                  prof_ops=None,
-                  verbose=None):
-        utils.logger.warn(
-            "Communication logging is not supported in torch versions older than 1.8")
+    def configure(deepspeed_config=None, enabled=None, prof_all=None, prof_ops=None, verbose=None):
+        utils.logger.warn("Communication logging is not supported in torch versions older than 1.8")
 
 else:
     supported_torch_version = True
     from .comm import *
```

### Comparing `deepspeed-0.8.3/deepspeed/comm/backend.py` & `deepspeed-0.9.0/deepspeed/comm/backend.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,29 +1,33 @@
-'''Copyright The Microsoft DeepSpeed Team'''
-''' DeepSpeed Communication Backend.
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+"""
+DeepSpeed Communication Backend.
 
 In the future, directly use NCCL/MPI/Gloo/etc without requiring torch.distributed. Simply wrap torch.distributed for now.
 
 # Custom DS Backends -- Direct C/Ops
  - NCCL -- [EXPERIMENTAL]
  - MPI -- [EXPERIMENTAL]
  - RCCL -- [EXPERIMENTAL]
  - GLOO -- [EXPERIMENTAL]
-
 # DS backend wrapper for torch.distributed [DEFAULT]
  - T-NCCL -- [DEFAULT]
  - T-GLOO
  - T-MPI
-'''
-''' Backend is the base class
+
+# Backend is the base class
      -- NcclBackend, MpiBackend, and TorchBackend are the main subclasses. TorchBackend is the only officially supported backend for now.
-'''
+"""
 
 
 class Backend(object):
+
     def __init__(self, name='backend', rank=0, size=1):
         self.name = name
         # The world size and rank of the world process group
         self.world_group = None
         self.world_size = rank
         self.world_rank = size
         # Single process group (pg) implementation for now but keep a list for future
```

### Comparing `deepspeed-0.8.3/deepspeed/comm/comm.py` & `deepspeed-0.9.0/deepspeed/comm/comm.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,30 +1,26 @@
-"""
-    Copyright 2021 The Microsoft DeepSpeed Team
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
 
+# DeepSpeed Team
+"""
     DeepSpeed Communication Package: deepspeed.comm
-
     deepspeed.comm
         -- import and use deepspeeed.ops.comm
         -- use torch.distributed directly if both this package and torch.distributed use the same NCCL version
         -- use custom collectives
             -- can either use torch.dist or ds.ops.comm?
-
         Note: the old 1-bit compressed allreduce variants that resided in deepspeed.runtime.comm will be moved here as well.
-
     deepspeed.comm API
         -- must be kept fully compatible (same signatures) as torch.dist API to ensure backward/cross-framework compatibility.
         -- e.g. if a client code used
             from deepspeed import comm as dist
-
             instead of
             import torch.distributed as dist
-
             The code should work without breaking any of the public torch.distributed functionality
-
     Future:
         -- deepspeed groups API should be brought into ds.comm
 """
 
 from enum import Enum
 import torch
 import os
@@ -102,20 +98,21 @@
 
     if debug is not None:
         comms_logger.debug = debug
 
 
 # Logging wrapper for timing ops
 def timed_op(func):
+
     def log_wrapper(*args, **kwargs):
         # Add enabled flag so that overhead to each comm op is two if conditions at most
         if comms_logger.enabled:
-            if ('prof' in kwargs and kwargs['prof']) or comms_logger.prof_all or (
-                    'log_name' in kwargs
-                    and kwargs['log_name'] in comms_logger.prof_ops):
+            if ('prof' in kwargs
+                    and kwargs['prof']) or comms_logger.prof_all or ('log_name' in kwargs
+                                                                     and kwargs['log_name'] in comms_logger.prof_ops):
                 # Need func args for their defaults
                 func_args = get_default_args(func)
                 func_args.update(kwargs)
                 msg_size = get_msg_size_from_args(func, *args, **kwargs)
                 log_name = get_debug_log_name(func_args, comms_logger.debug)
                 timers(log_name).start()
         # Return the op, then stop the op's timer
@@ -125,16 +122,15 @@
             if comms_logger.enabled:
                 # Need to make op blocking for accurate logging
                 get_accelerator().synchronize()
                 # If we're using MPI, we can't simply sync the stream
                 if cdb.using_mpi:
                     cdb.barrier()
                 if ('prof' in kwargs and kwargs['prof']) or comms_logger.prof_all or (
-                        'log_name' in kwargs
-                        and kwargs['log_name'] in comms_logger.prof_ops):
+                        'log_name' in kwargs and kwargs['log_name'] in comms_logger.prof_ops):
                     log_name = get_debug_log_name(func_args, comms_logger.debug)
                     raw_name = func.__name__
                     timers(log_name).stop()
                     # need temp var since 'elapsed' resets events
                     time_elapsed = timers(log_name).elapsed(reset=False)
                     comms_logger.append(raw_name, log_name, time_elapsed, msg_size)
 
@@ -174,36 +170,35 @@
 def destroy_process_group(group=None):
     global cdb
     return cdb.destroy_process_group(group=group)
 
 
 def new_group(ranks):
     global cdb
-    assert cdb is not None and cdb.is_initialized(), 'DeepSpeed backend not set, please initialize it using init_process_group()'
+    assert cdb is not None and cdb.is_initialized(
+    ), 'DeepSpeed backend not set, please initialize it using init_process_group()'
     return cdb.new_group(ranks)
 
 
 def is_available() -> bool:
 
     # Returns ``True`` if the deepspeed comm package is available.
 
     # TODO: load other ops. Clients including deepspeed itself should use deepspeed.comm to import
     # any communication related primitives from this package.
     # use hasattr(deepspeed.csrc.ops, "_comm") or something
     return True
 
 
-def set_backend(backend):
+def set_backend(backend_name):
     if not use_ds_backend:
         utils.logger.error(
             "DeepSpeed communication backend is required. Please use deepspeed.comm.init_distributed(backend, use_deepspeed=True) to use this functionality"
         )
-        raise RuntimeError(
-            'Error: Custom DeepSpeed backend called without initializing DeepSpeed distributed.'
-        )
+        raise RuntimeError('Error: Custom DeepSpeed backend called without initializing DeepSpeed distributed.')
 
     global cdb
     global nccl_backend
     global mpi_backend
 
     try:
         if backend_name == NCCL_BACKEND:
@@ -213,143 +208,120 @@
             if mpi_backend is not None and mpi_backend.is_initialized():
                 cdb = mpi_backend
     except Exception as inst:
         print(inst)
 
 
 @timed_op
-def broadcast(tensor,
-              src,
-              group=None,
-              async_op=False,
-              prof=False,
-              log_name='broadcast',
-              debug=get_caller_func()):
+def broadcast(tensor, src, group=None, async_op=False, prof=False, log_name='broadcast', debug=get_caller_func()):
     global cdb
     return cdb.broadcast(tensor=tensor, src=src, group=group, async_op=async_op)
 
 
 @timed_op
 def all_gather(tensor_list,
                tensor,
                group=None,
                async_op=False,
                prof=False,
                log_name='all_gather',
                debug=get_caller_func()):
     global cdb
-    return cdb.all_gather(tensor_list=tensor_list,
-                          tensor=tensor,
-                          group=group,
-                          async_op=async_op)
+    return cdb.all_gather(tensor_list=tensor_list, tensor=tensor, group=group, async_op=async_op)
 
 
-def has_reduce_scatter_base():
+def has_reduce_scatter_tensor():
     global cdb
-    assert cdb is not None and cdb.is_initialized(), 'DeepSpeed backend not set, please initialize it using init_process_group()'
-    assert cdb.has_reduce_scatter_base is not None, 'has_reduce_scatter_base is not yet defined'
-    return cdb.has_reduce_scatter_base
+    assert cdb is not None and cdb.is_initialized(
+    ), 'DeepSpeed backend not set, please initialize it using init_process_group()'
+    return cdb.has_reduce_scatter_tensor()
 
 
 def reduce_scatter_fn(output_tensor,
                       tensor,
                       op=ReduceOp.SUM,
                       group=None,
                       async_op=False,
                       prof=False,
                       debug=get_caller_func()):
     global cdb
-    assert cdb is not None and cdb.is_initialized(), 'DeepSpeed backend not set, please initialize it using init_process_group()'
-    if cdb.has_reduce_scatter_base:
-        return reduce_scatter_base(output_tensor,
-                                   tensor,
-                                   op=op,
-                                   group=group,
-                                   async_op=async_op,
-                                   prof=prof,
-                                   debug=debug)
+    assert cdb is not None and cdb.is_initialized(
+    ), 'DeepSpeed backend not set, please initialize it using init_process_group()'
+    if cdb.has_reduce_scatter_tensor():
+        return reduce_scatter_tensor(output_tensor,
+                                     tensor,
+                                     op=op,
+                                     group=group,
+                                     async_op=async_op,
+                                     prof=prof,
+                                     debug=debug)
     else:
-        utils.logger.warning_once(
-            "unable to find torch.distributed._reduce_scatter_base. will fall back to "
-            "torch.distributed.all_gather which will result in suboptimal performance. "
-            "please consider upgrading your pytorch installation.")
+        if get_rank() == 0:
+            utils.logger.warning_once("unable to find torch.distributed.reduce_scatter_tensor. will fall back to "
+                                      "torch.distributed.all_gather which will result in suboptimal performance. "
+                                      "please consider upgrading your pytorch installation.")
         input_tensor_lst = list(torch.chunk(tensor, cdb.get_world_size(group)))
         return reduce_scatter(output_tensor,
                               input_tensor_lst,
                               op=op,
                               group=group,
                               async_op=async_op,
                               prof=prof,
                               debug=debug)
 
 
 @timed_op
-def reduce_scatter_base(output_tensor,
-                        tensor,
-                        op=ReduceOp.SUM,
-                        group=None,
-                        async_op=False,
-                        prof=False,
-                        log_name='reduce_scatter_base',
-                        debug=get_caller_func()):
-    global cdb
-    return cdb.reduce_scatter_base(output_tensor=output_tensor,
-                                   input_tensor=tensor,
-                                   op=op,
-                                   group=group,
-                                   async_op=async_op)
+def reduce_scatter_tensor(output_tensor,
+                          tensor,
+                          op=ReduceOp.SUM,
+                          group=None,
+                          async_op=False,
+                          prof=False,
+                          log_name='reduce_scatter_tensor',
+                          debug=get_caller_func()):
+    global cdb
+    return cdb.reduce_scatter_tensor(output_tensor=output_tensor,
+                                     input_tensor=tensor,
+                                     op=op,
+                                     group=group,
+                                     async_op=async_op)
 
 
 @timed_op
-def all_gather_base(output_tensor,
-                    tensor,
-                    group=None,
-                    async_op=False,
-                    prof=False,
-                    log_name='all_gather_base',
-                    debug=get_caller_func()):
-    global cdb
-    return cdb.all_gather_base(output_tensor=output_tensor,
-                               input_tensor=tensor,
-                               group=group,
-                               async_op=async_op)
-
-
-def has_allgather_base():
-    global cdb
-    assert cdb is not None and cdb.is_initialized(), 'DeepSpeed backend not set, please initialize it using init_process_group()'
-    assert cdb.has_allgather_base is not None, 'has_allgather_base is not yet defined'
-    return cdb.has_allgather_base
-
-
-def allgather_fn(output_tensor,
-                 input_tensor,
-                 group=None,
-                 async_op=False,
-                 debug=get_caller_func()):
-    global cdb
-    assert cdb is not None and cdb.is_initialized(), 'DeepSpeed backend not set, please initialize it using init_process_group()'
-    if cdb.has_allgather_base:
-        return all_gather_base(output_tensor,
-                               input_tensor,
-                               group=group,
-                               async_op=async_op,
-                               debug=debug)
+def all_gather_into_tensor(output_tensor,
+                           tensor,
+                           group=None,
+                           async_op=False,
+                           prof=False,
+                           log_name='all_gather_into_tensor',
+                           debug=get_caller_func()):
+    global cdb
+    return cdb.all_gather_into_tensor(output_tensor=output_tensor, input_tensor=tensor, group=group, async_op=async_op)
+
+
+def has_all_gather_into_tensor():
+    global cdb
+    assert cdb is not None and cdb.is_initialized(
+    ), 'DeepSpeed backend not set, please initialize it using init_process_group()'
+    return cdb.has_all_gather_into_tensor()
+
+
+def allgather_fn(output_tensor, input_tensor, group=None, async_op=False, debug=get_caller_func()):
+    global cdb
+    assert cdb is not None and cdb.is_initialized(
+    ), 'DeepSpeed backend not set, please initialize it using init_process_group()'
+    if cdb.has_all_gather_into_tensor():
+        return all_gather_into_tensor(output_tensor, input_tensor, group=group, async_op=async_op, debug=debug)
     else:
         if get_rank() == 0:
-            utils.logger.warning_once(
-                "unable to find torch.distributed._all_gather_base. will fall back to "
-                "torch.distributed.all_gather which will result in suboptimal performance. "
-                "please consider upgrading your pytorch installation.")
+            utils.logger.warning_once("unable to find torch.distributed.all_gather_into_tensor. will fall back to "
+                                      "torch.distributed.all_gather which will result in suboptimal performance. "
+                                      "please consider upgrading your pytorch installation.")
         output_tensors = list(torch.chunk(output_tensor, cdb.get_world_size(group)))
-        return all_gather(output_tensors,
-                          input_tensor,
-                          group=group,
-                          async_op=async_op,
-                          debug=debug)
+        return all_gather(output_tensors, input_tensor, group=group, async_op=async_op, debug=debug)
 
 
 @timed_op
 def all_to_all_single(output,
                       tensor,
                       output_split_sizes=None,
                       input_split_sizes=None,
@@ -364,102 +336,65 @@
                                  output_split_sizes=output_split_sizes,
                                  input_split_sizes=input_split_sizes,
                                  group=group,
                                  async_op=async_op)
 
 
 @timed_op
-def send(tensor,
-         dst,
-         group=None,
-         tag=0,
-         prof=False,
-         log_name='send',
-         debug=get_caller_func()):
+def send(tensor, dst, group=None, tag=0, prof=False, log_name='send', debug=get_caller_func()):
     global cdb
     return cdb.send(tensor=tensor, dst=dst, group=group, tag=tag)
 
 
 @timed_op
-def recv(tensor,
-         src=None,
-         group=None,
-         tag=0,
-         prof=False,
-         log_name='recv',
-         debug=get_caller_func()):
+def recv(tensor, src=None, group=None, tag=0, prof=False, log_name='recv', debug=get_caller_func()):
     global cdb
     return cdb.recv(tensor=tensor, src=src, group=group, tag=tag)
 
 
 @timed_op
-def isend(tensor,
-          dst,
-          group=None,
-          tag=0,
-          prof=False,
-          log_name='isend',
-          debug=get_caller_func()):
+def isend(tensor, dst, group=None, tag=0, prof=False, log_name='isend', debug=get_caller_func()):
     global cdb
     return cdb.send(tensor=tensor, dst=dst, group=group, tag=tag)
 
 
 @timed_op
-def irecv(tensor,
-          src=None,
-          group=None,
-          tag=0,
-          prof=False,
-          log_name='irecv',
-          debug=get_caller_func()):
+def irecv(tensor, src=None, group=None, tag=0, prof=False, log_name='irecv', debug=get_caller_func()):
     global cdb
     return cdb.recv(tensor=tensor, src=src, group=group, tag=tag)
 
 
 @timed_op
 def gather(tensor,
            gather_list=None,
            dst=0,
            group=None,
            async_op=False,
            prof=False,
            log_name='gather',
            debug=get_caller_func()):
     global cdb
-    return cdb.gather(tensor=tensor,
-                      gather_list=gather_list,
-                      dst=dst,
-                      group=group,
-                      async_op=async_op)
+    return cdb.gather(tensor=tensor, gather_list=gather_list, dst=dst, group=group, async_op=async_op)
 
 
 @timed_op
 def scatter(tensor,
             scatter_list=None,
             src=0,
             group=None,
             async_op=False,
             prof=False,
             log_name='scatter',
             debug=get_caller_func()):
     global cdb
-    return cdb.scatter(tensor=tensor,
-                       scatter_list=scatter_list,
-                       src=src,
-                       group=group,
-                       async_op=async_op)
+    return cdb.scatter(tensor=tensor, scatter_list=scatter_list, src=src, group=group, async_op=async_op)
 
 
 @timed_op
-def barrier(group=None,
-            async_op=False,
-            device_ids=None,
-            prof=False,
-            log_name='barrier',
-            debug=get_caller_func()):
+def barrier(group=None, async_op=False, device_ids=None, prof=False, log_name='barrier', debug=get_caller_func()):
     global cdb
     return cdb.barrier(group=group, async_op=async_op, device_ids=device_ids)
 
 
 @timed_op
 def monitored_barrier(group=None,
                       timeout=None,
@@ -498,19 +433,15 @@
                    op=ReduceOp.SUM,
                    group=None,
                    async_op=False,
                    prof=False,
                    log_name='reduce_scatter',
                    debug=get_caller_func()):
     global cdb
-    return cdb.reduce_scatter(output=output,
-                              input_list=input_list,
-                              op=op,
-                              group=group,
-                              async_op=async_op)
+    return cdb.reduce_scatter(output=output, input_list=input_list, op=op, group=group, async_op=async_op)
 
 
 @timed_op
 def all_reduce(tensor,
                op=ReduceOp.SUM,
                group=None,
                async_op=False,
@@ -524,15 +455,16 @@
     global cdb
     #print(f'op = {op}, cdb= {cdb.name}')
     return cdb.all_reduce(tensor, op, group, async_op)
 
 
 def get_world_group():
     global cdb
-    assert cdb is not None and cdb.is_initialized(), 'DeepSpeed backend not set, please initialize it using init_process_group()'
+    assert cdb is not None and cdb.is_initialized(
+    ), 'DeepSpeed backend not set, please initialize it using init_process_group()'
     return cdb.get_world_group()
 
 
 def get_world_size(group=None) -> int:
     """
     Returns the number of processes in the current process group
     Args:
@@ -540,15 +472,16 @@
             the default process group will be used.
     Returns:
         The world size of the process group
         -1, if not part of the group
     """
     global cdb
 
-    assert cdb is not None and cdb.is_initialized(), 'DeepSpeed backend not set, please initialize it using init_process_group()'
+    assert cdb is not None and cdb.is_initialized(
+    ), 'DeepSpeed backend not set, please initialize it using init_process_group()'
     return cdb.get_world_size(group)
 
 
 def get_rank(group=None):
     """
     Returns the rank of the current process in the provided ``group`` or the
     default group if none was provided.
@@ -559,34 +492,37 @@
         group (ProcessGroup, optional): The process group to work on. If None,
             the default process group will be used.
     Returns:
         The rank of the process group
         -1, if not part of the group
     """
     global cdb
-    assert cdb is not None and cdb.is_initialized(), 'DeepSpeed backend not set, please initialize it using init_process_group()'
+    assert cdb is not None and cdb.is_initialized(
+    ), 'DeepSpeed backend not set, please initialize it using init_process_group()'
     return cdb.get_rank(group)
 
 
 def get_local_rank():
     """
         Helper function to get local rank after a backend has been set and initialized
         Args:
             None
         Returns:
             local rank (= GPU device ID)
     """
     global cdb
-    assert cdb is not None and cdb.is_initialized(), 'DeepSpeed backend not set, please initialize it using init_process_group()'
+    assert cdb is not None and cdb.is_initialized(
+    ), 'DeepSpeed backend not set, please initialize it using init_process_group()'
     return get_local_rank_from_launcher()
 
 
 def get_global_rank(group=None, group_rank=0):
     global cdb
-    assert cdb is not None and cdb.is_initialized(), 'DeepSpeed backend not set, please initialize it using init_process_group()'
+    assert cdb is not None and cdb.is_initialized(
+    ), 'DeepSpeed backend not set, please initialize it using init_process_group()'
     return cdb.get_global_rank(group, group_rank)
 
 
 # Main DeepSpeed Comms. public API.
 def init_distributed(dist_backend=None,
                      auto_mpi_discovery=True,
                      distributed_port=TORCH_DISTRIBUTED_DEFAULT_PORT,
@@ -627,17 +563,15 @@
             cdb is not None and cdb.is_initialized() is True
         ), "Distributed backend is not initialized. Please set dist_init_required to True or initialize before calling deepspeed.initialize()"
     else:
         # Initialize torch distributed if needed
         required_env = ["RANK", "WORLD_SIZE", "MASTER_ADDR", "MASTER_PORT", "LOCAL_RANK"]
         if auto_mpi_discovery and not all(map(lambda v: v in os.environ, required_env)):
             if verbose:
-                utils.logger.info(
-                    "Not using the DeepSpeed or dist launchers, attempting to detect MPI environment..."
-                )
+                utils.logger.info("Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...")
             if in_aml() and not in_dlts():
                 patch_aml_env_for_torch_nccl_backend(verbose=verbose)
             elif in_aws_sm():
                 patch_aws_sm_env_for_torch_nccl_backend(verbose=verbose)
             else:
                 mpi_discovery(distributed_port=distributed_port, verbose=verbose)
 
@@ -645,17 +579,15 @@
             if int(os.getenv('RANK', '0')) == 0:
                 utils.logger.info('Distributed backend already initialized')
         else:
             assert isinstance(timeout, timedelta)
             if dist_backend == None:
                 dist_backend = get_accelerator().communication_backend_name()
             if int(os.getenv('RANK', '0')) == 0:
-                utils.logger.info(
-                    'Initializing TorchBackend in DeepSpeed with backend {}'.format(
-                        dist_backend))
+                utils.logger.info('Initializing TorchBackend in DeepSpeed with backend {}'.format(dist_backend))
             # Create a torch backend object, initialize torch distributed, and assign to cdb
             cdb = TorchBackend(dist_backend, timeout, init_method, rank, world_size)
 
 
 def mpi_discovery(distributed_port=TORCH_DISTRIBUTED_DEFAULT_PORT, verbose=True):
     '''
     Discovery MPI environment via mpi4py and map to relevant dist state
@@ -682,24 +614,20 @@
     os.environ['WORLD_SIZE'] = str(world_size)
     os.environ['LOCAL_RANK'] = str(local_rank)
     os.environ['MASTER_ADDR'] = master_addr
     os.environ['MASTER_PORT'] = str(distributed_port)
 
     if verbose:
         utils.logger.info(
-            "Discovered MPI settings of world_rank={}, local_rank={}, world_size={}, master_addr={}, master_port={}"
-            .format(os.environ['RANK'],
-                    os.environ['LOCAL_RANK'],
-                    os.environ['WORLD_SIZE'],
-                    os.environ['MASTER_ADDR'],
-                    os.environ['MASTER_PORT']))
+            "Discovered MPI settings of world_rank={}, local_rank={}, world_size={}, master_addr={}, master_port={}".
+            format(os.environ['RANK'], os.environ['LOCAL_RANK'], os.environ['WORLD_SIZE'], os.environ['MASTER_ADDR'],
+                   os.environ['MASTER_PORT']))
 
     if cdb is not None and cdb.is_initialized():
-        assert cdb.get_rank() == rank, "MPI rank {} does not match torch rank {}".format(
-            rank, cdb.get_rank())
+        assert cdb.get_rank() == rank, "MPI rank {} does not match torch rank {}".format(rank, cdb.get_rank())
         assert cdb.get_world_size() == world_size, "MPI world size {} does not match torch world size {}".format(
             world_size, cdb.get_world_size())
 
 
 def in_aml():
     # Are we running inside an Azure Machine Learning (AML) environment?
     return 'AZUREML_EXPERIMENT_ID' in os.environ
@@ -718,52 +646,44 @@
 def patch_aml_env_for_torch_nccl_backend(master_port=6105, verbose=True):
     """Helper routine to get and set environment variables.
     This is adapted from Azure ML's documentation available from:
     https://azure.github.io/azureml-web/docs/cheatsheet/distributed-training/#environment-variables-from-openmpi
     """
     os.environ["RANK"] = os.environ["OMPI_COMM_WORLD_RANK"]
     os.environ["WORLD_SIZE"] = os.environ["OMPI_COMM_WORLD_SIZE"]
-    single_node = int(os.environ["OMPI_COMM_WORLD_LOCAL_SIZE"]) == int(
-        os.environ["WORLD_SIZE"])
+    single_node = int(os.environ["OMPI_COMM_WORLD_LOCAL_SIZE"]) == int(os.environ["WORLD_SIZE"])
 
     if not single_node:
         master_node_params = os.environ["AZ_BATCH_MASTER_NODE"].split(":")
         os.environ["MASTER_ADDR"] = master_node_params[0]
         # Do not overwrite master port with that defined in AZ_BATCH_MASTER_NODE
         if "MASTER_PORT" not in os.environ:
             os.environ["MASTER_PORT"] = str(master_port)
     else:
         os.environ["MASTER_ADDR"] = os.environ["AZ_BATCHAI_MPI_MASTER_NODE"]
         os.environ["MASTER_PORT"] = DEFAULT_AML_MASTER_PORT
 
     if verbose:
-        utils.logger.info("NCCL_SOCKET_IFNAME original value = {}".format(
-            os.environ["NCCL_SOCKET_IFNAME"]))
+        utils.logger.info("NCCL_SOCKET_IFNAME original value = {}".format(os.environ["NCCL_SOCKET_IFNAME"]))
 
     os.environ["NCCL_SOCKET_IFNAME"] = DEFAULT_AML_NCCL_SOCKET_IFNAME
     os.environ['LOCAL_RANK'] = os.environ["OMPI_COMM_WORLD_LOCAL_RANK"]
 
     if verbose:
         utils.logger.info(
             "Discovered AzureML settings of world_rank={}, local_rank={}, world_size={}, master_addr={}, master_port={}"
-            .format(os.environ['RANK'],
-                    os.environ['LOCAL_RANK'],
-                    os.environ['WORLD_SIZE'],
-                    os.environ['MASTER_ADDR'],
+            .format(os.environ['RANK'], os.environ['LOCAL_RANK'], os.environ['WORLD_SIZE'], os.environ['MASTER_ADDR'],
                     os.environ['MASTER_PORT']))
 
 
 def patch_aws_sm_env_for_torch_nccl_backend(verbose=True):
     """Helper routine to get and set environment variables when running inside an AWS SageMaker environment.
     """
     os.environ["RANK"] = os.environ["OMPI_COMM_WORLD_RANK"]
     os.environ['LOCAL_RANK'] = os.environ["OMPI_COMM_WORLD_LOCAL_RANK"]
     os.environ["WORLD_SIZE"] = os.environ["OMPI_COMM_WORLD_SIZE"]
 
     if verbose:
         utils.logger.info(
             "Discovered AWS SageMaker settings of world_rank={}, local_rank={}, world_size={}, master_addr={}, master_port={}"
-            .format(os.environ['RANK'],
-                    os.environ['LOCAL_RANK'],
-                    os.environ['WORLD_SIZE'],
-                    os.environ['MASTER_ADDR'],
+            .format(os.environ['RANK'], os.environ['LOCAL_RANK'], os.environ['WORLD_SIZE'], os.environ['MASTER_ADDR'],
                     os.environ['MASTER_PORT']))
```

### Comparing `deepspeed-0.8.3/deepspeed/comm/config.py` & `deepspeed-0.9.0/deepspeed/comm/config.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,18 +1,18 @@
-'''Copyright The Microsoft DeepSpeed Team'''
-"""
-Copyright (c) Microsoft Corporation
-Licensed under the MIT license.
-"""
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 from pydantic import BaseModel
 from .constants import *
 
 
 class CommsConfig(BaseModel):
+
     class Config:
         validate_all = True
         validate_assignment = True
         use_enum_values = True
         extra = 'forbid'
 
 
@@ -21,12 +21,13 @@
     prof_all: bool = COMMS_LOGGER_PROF_ALL_DEFAULT
     prof_ops: list = COMMS_LOGGER_PROF_OPS_DEFAULT
     verbose: bool = COMMS_LOGGER_VERBOSE_DEFAULT
     debug: bool = COMMS_LOGGER_DEBUG_DEFAULT
 
 
 class DeepSpeedCommsConfig:
+
     def __init__(self, ds_config):
         self.comms_logger_enabled = 'comms_logger' in ds_config
 
         if self.comms_logger_enabled:
             self.comms_logger = CommsLoggerConfig(**ds_config['comms_logger'])
```

### Comparing `deepspeed-0.8.3/deepspeed/comm/constants.py` & `deepspeed-0.9.0/deepspeed/comm/constants.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,8 +1,11 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 NCCL_BACKEND = 'nccl'
 MPI_BACKEND = 'mpi'
 GLOO_BACKEND = 'gloo'
 SCCL_BACKEND = 'sccl'
 
 DEFAULT_AML_MASTER_PORT = "54965"
```

### Comparing `deepspeed-0.8.3/deepspeed/comm/torch.py` & `deepspeed-0.9.0/deepspeed/comm/torch.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,12 @@
-'''
-Copyright 2021 The Microsoft DeepSpeed Team
-'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+
 from deepspeed import utils
 
 from .utils import *
 from .backend import *
 from .comm import *
 
 
@@ -12,115 +14,102 @@
     """
         A light-weight wrapper class for torch.distributed API.
         Only a subset of functions are wrapped. Once the init_process_group
         is initialized, standard torch.distributed.* can be used directly
         so no need to wrap all the functions. We can keep adding wrappers as
         needed.
     """
-    def __init__(self,
-                 backend,
-                 timeout,
-                 init_method,
-                 rank=-1,
-                 world_size=-1,
-                 name='torch'):
+
+    def __init__(self, backend, timeout, init_method, rank=-1, world_size=-1, name='torch'):
         super(TorchBackend, self).__init__()
         self.torch_version_before_18 = older_torch()
-        self.has_allgather_base = has_allgather_base()
-        self.has_reduce_scatter_base = has_reduce_scatter_base()
+        self.all_gather_function = self.get_all_gather_function()
+        self.reduce_scatter_function = self.get_reduce_scatter_function()
         self.initialized = True
         self.name = name
         # Future functionality to support ds.initialize() on a single GPU
         # The idea is to fake that dist backend is initialized even when
         # it is not so we can run on a single GPU without doing any init_process_group
         self.single_gpu_mode = True
         self.init_process_group(backend, timeout, init_method, rank, world_size)
 
+    @classmethod
+    def get_all_gather_function(self):
+        if hasattr(torch.distributed, "all_gather_into_tensor"):
+            return torch.distributed.all_gather_into_tensor
+        elif hasattr(torch.distributed, "_all_gather_base"):
+            return torch.distributed._all_gather_base
+        return None
+
+    @classmethod
+    def get_reduce_scatter_function(self):
+        if hasattr(torch.distributed, "reduce_scatter_tensor"):
+            return torch.distributed.reduce_scatter_tensor
+        elif hasattr(torch.distributed, "_reduce_scatter_base"):
+            return torch.distributed._reduce_scatter_base
+        return None
+
+    def has_all_gather_into_tensor(self):
+        return self.all_gather_function is not None
+
+    def has_reduce_scatter_tensor(self):
+        return self.reduce_scatter_function is not None
+
     def init_process_group(self, backend, timeout, init_method, rank, world_size):
         if not torch.distributed.is_initialized():
             torch.distributed.init_process_group(backend,
                                                  timeout=timeout,
                                                  init_method=init_method,
                                                  rank=rank,
                                                  world_size=world_size)
         self.using_mpi = torch.distributed.get_backend() == 'mpi'
 
-    def all_reduce(self,
-                   tensor,
-                   op=torch.distributed.ReduceOp.SUM,
-                   group=None,
-                   async_op=False):
+    def all_reduce(self, tensor, op=torch.distributed.ReduceOp.SUM, group=None, async_op=False):
         op = self._reduce_op(op)
-        return torch.distributed.all_reduce(tensor=tensor,
-                                            op=op,
-                                            group=group,
-                                            async_op=async_op)
+        return torch.distributed.all_reduce(tensor=tensor, op=op, group=group, async_op=async_op)
 
     def reduce(self, tensor, dst, op=ReduceOp.SUM, group=None, async_op=False):
-        return torch.distributed.reduce(tensor=tensor,
-                                        dst=dst,
-                                        op=self._reduce_op(op),
-                                        group=group,
-                                        async_op=async_op)
+        return torch.distributed.reduce(tensor=tensor, dst=dst, op=self._reduce_op(op), group=group, async_op=async_op)
 
-    def reduce_scatter(self,
-                       output,
-                       input_list,
-                       op=ReduceOp.SUM,
-                       group=None,
-                       async_op=False):
+    def reduce_scatter(self, output, input_list, op=ReduceOp.SUM, group=None, async_op=False):
         return torch.distributed.reduce_scatter(output=output,
                                                 input_list=input_list,
                                                 op=self._reduce_op(op),
                                                 group=group,
                                                 async_op=async_op)
 
     def broadcast(self, tensor, src, group=None, async_op=False):
-        return torch.distributed.broadcast(tensor=tensor,
-                                           src=src,
-                                           group=group,
-                                           async_op=async_op)
+        return torch.distributed.broadcast(tensor=tensor, src=src, group=group, async_op=async_op)
 
     def all_gather(self, tensor_list, tensor, group=None, async_op=False):
-        return torch.distributed.all_gather(tensor_list=tensor_list,
-                                            tensor=tensor,
+        return torch.distributed.all_gather(tensor_list=tensor_list, tensor=tensor, group=group, async_op=async_op)
+
+    def all_gather_into_tensor(self, output_tensor, input_tensor, group=None, async_op=False):
+        if self.has_all_gather_into_tensor():
+            return self.all_gather_function(output_tensor=output_tensor,
+                                            input_tensor=input_tensor,
                                             group=group,
                                             async_op=async_op)
-
-    def all_gather_base(self, output_tensor, input_tensor, group=None, async_op=False):
-        if self.has_allgather_base:
-            return torch.distributed.distributed_c10d._all_gather_base(
-                output_tensor=output_tensor,
-                input_tensor=input_tensor,
-                group=group,
-                async_op=async_op)
         else:
-            utils.logger.warning(
-                "unable to find torch.distributed._all_gather_base. will fall back to "
-                "torch.distributed.reduce_scatter which will result in suboptimal performance. "
-                "please consider upgrading your pytorch installation.")
+            utils.logger.warning("unable to find torch.distributed.all_gather_into_tensor. will fall back to "
+                                 "torch.distributed.all_gather which will result in suboptimal performance. "
+                                 "please consider upgrading your pytorch installation.")
             pass
 
-    def reduce_scatter_base(self,
-                            output_tensor,
-                            input_tensor,
-                            op=ReduceOp.SUM,
-                            group=None,
-                            async_op=False):
-        if self.has_reduce_scatter_base:
-            return torch.distributed._reduce_scatter_base(output_tensor,
-                                                          input_tensor,
-                                                          op=self._reduce_op(op),
-                                                          group=group,
-                                                          async_op=async_op)
+    def reduce_scatter_tensor(self, output_tensor, input_tensor, op=ReduceOp.SUM, group=None, async_op=False):
+        if self.has_reduce_scatter_tensor():
+            return self.reduce_scatter_function(output_tensor,
+                                                input_tensor,
+                                                op=self._reduce_op(op),
+                                                group=group,
+                                                async_op=async_op)
         else:
-            utils.logger.warning(
-                "unable to find torch.distributed._reduce_scatter_base. will fall back to "
-                "torch.distributed.reduce_scatter which will result in suboptimal performance. "
-                "please consider upgrading your pytorch installation.")
+            utils.logger.warning("unable to find torch.distributed.reduce_scatter_tensor. will fall back to "
+                                 "torch.distributed.reduce_scatter which will result in suboptimal performance. "
+                                 "please consider upgrading your pytorch installation.")
             pass
 
     def all_to_all_single(self,
                           output,
                           input,
                           output_split_sizes=None,
                           input_split_sizes=None,
@@ -155,33 +144,23 @@
     def scatter(self, tensor, scatter_list=None, src=0, group=None, async_op=False):
         return torch.distributed.scatter(tensor=tensor,
                                          scatter_list=scatter_list,
                                          src=src,
                                          group=group,
                                          async_op=async_op)
 
-    def barrier(self,
-                group=torch.distributed.GroupMember.WORLD,
-                async_op=False,
-                device_ids=None):
+    def barrier(self, group=torch.distributed.GroupMember.WORLD, async_op=False, device_ids=None):
         if group is None:
             group = torch.distributed.GroupMember.WORLD
-        return torch.distributed.barrier(group=group,
-                                         async_op=async_op,
-                                         device_ids=device_ids)
-
-    def monitored_barrier(self,
-                          group=torch.distributed.GroupMember.WORLD,
-                          timeout=None,
-                          wait_all_ranks=False):
+        return torch.distributed.barrier(group=group, async_op=async_op, device_ids=device_ids)
+
+    def monitored_barrier(self, group=torch.distributed.GroupMember.WORLD, timeout=None, wait_all_ranks=False):
         if group is None:
             group = torch.distributed.GroupMember.WORLD
-        return torch.distributed.monitored_barrier(group=group,
-                                                   timeout=timeout,
-                                                   wait_all_ranks=wait_all_ranks)
+        return torch.distributed.monitored_barrier(group=group, timeout=timeout, wait_all_ranks=wait_all_ranks)
 
     def get_rank(self, group=None):
         return torch.distributed.get_rank(group=group)
 
     def get_world_size(self, group=None):
         return torch.distributed.get_world_size(group=group)
```

### Comparing `deepspeed-0.8.3/deepspeed/comm/utils.py` & `deepspeed-0.9.0/deepspeed/comm/utils.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,8 +1,11 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import os
 import torch
 import inspect
 from deepspeed.utils import get_caller_func
 
 
@@ -16,28 +19,14 @@
     TORCH_MINOR = int(torch.__version__.split('.')[1])
     if TORCH_MAJOR == 1 and TORCH_MINOR < 8:
         return True
     else:
         return False
 
 
-def has_allgather_base():
-    '''
-        Helper to check if torch.distributed has _all_gather_base
-    '''
-    return hasattr(torch.distributed, "_all_gather_base")
-
-
-def has_reduce_scatter_base():
-    '''
-        Helper to check if torch.distributed has _reduce_scatter_base
-    '''
-    return hasattr(torch.distributed, "_reduce_scatter_base")
-
-
 def get_local_rank_from_launcher():
 
     # DeepSpeed launcher will set it so get from there
     rank = os.environ.get('LOCAL_RANK')
 
     if rank is None:
         rank = os.environ.get('OMPI_COMM_WORLD_LOCAL_RANK')
@@ -80,19 +69,15 @@
         print(f"set world size to {size}")
 
     return int(size)
 
 
 def get_default_args(func):
     signature = inspect.signature(func)
-    return {
-        k: v.default
-        for k,
-        v in signature.parameters.items() if v.default is not inspect.Parameter.empty
-    }
+    return {k: v.default for k, v in signature.parameters.items() if v.default is not inspect.Parameter.empty}
 
 
 # We need this hacky function since torch doesn't consistently name or place the input tensor args
 def get_tensor_position(func):
     sig_params = inspect.signature(func).parameters
     arg = None
     # most colls
```

### Comparing `deepspeed-0.8.3/deepspeed/compression/basic_layer.py` & `deepspeed-0.9.0/deepspeed/compression/basic_layer.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,8 +1,11 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import torch
 import math
 from torch import nn
 from torch.nn import init
 import deepspeed.comm as dist
 from .utils import TopKBinarizer, SymQuantizer, AsymQuantizer, TernaryQuantizer, BinaryQuantizer
@@ -17,14 +20,15 @@
     tokens/images for inference. This generally will affect some accuracy but achieve better latency performance.
     Parameters:
     ----------
     act_range_momentum : float, default 0.95
         Momentum for updating the activation quantization range.
     quant_mode : str, default 'symmetric'
     """
+
     def __init__(self, act_range_momentum=0.95, quant_mode='symmetric'):
         super(QuantAct, self).__init__()
 
         self.act_range_momentum = act_range_momentum
         self.quant_mode = quant_mode
         if quant_mode == 'symmetric':
             self.act_function = SymQuantizer.apply
@@ -46,46 +50,38 @@
 
             # Initialization
             if self.x_min_max[0] == self.x_min_max[1]:
                 self.x_min_max[0] = x_min
                 self.x_min_max[1] = x_max
 
             # if do not need momentum, please set self.act_range_momentum = 0
-            self.x_min_max[0] = self.x_min_max[0] * self.act_range_momentum + x_min * (
-                1 - self.act_range_momentum)
-            self.x_min_max[1] = self.x_min_max[1] * self.act_range_momentum + x_max * (
-                1 - self.act_range_momentum)
+            self.x_min_max[0] = self.x_min_max[0] * self.act_range_momentum + x_min * (1 - self.act_range_momentum)
+            self.x_min_max[1] = self.x_min_max[1] * self.act_range_momentum + x_max * (1 - self.act_range_momentum)
 
         x_q = self.act_function(x, num_bits, self.x_min_max[0], self.x_min_max[1])
 
         return x_q
 
 
 class Embedding_Compress(nn.Embedding):
+
     def __init__(self, *kargs):
         super(Embedding_Compress, self).__init__(*kargs)
         self.weight.start_bits = None
         self.weight.target_bits = None
         self.weight.q_period = None
         self.weight_quantization_enabled_in_forward = False
         self.weight_quantization_enabled = False
 
     def extra_repr(self):
         return 'num_embeddings={}, embedding_dim={}, weight_quantization={}'.format(
-            self.num_embeddings,
-            self.embedding_dim,
-            self.weight.target_bits)
-
-    def enable_weight_quantization(self,
-                                   start_bits,
-                                   target_bits,
-                                   quantization_period,
-                                   weight_quantization_enabled_in_forward,
-                                   quantization_type,
-                                   num_groups):
+            self.num_embeddings, self.embedding_dim, self.weight.target_bits)
+
+    def enable_weight_quantization(self, start_bits, target_bits, quantization_period,
+                                   weight_quantization_enabled_in_forward, quantization_type, num_groups):
         self.weight.start_bits = start_bits
         self.weight.target_bits = target_bits
         self.weight.q_period = quantization_period
         self.weight_quantization_enabled_in_forward = weight_quantization_enabled_in_forward
         if self.weight_quantization_enabled_in_forward:
             logger.warning(
                 "************ A lot of MoQ features are not supported in quantize_weight_in_forward mode, please consider to use DS-FP16 optimizer************"
@@ -101,46 +97,36 @@
             elif self.weight.target_bits == 1:
                 assert quantization_type == 'symmetric', 'Only symmetric quantization is supported for binary weight quantization'
                 self.weight_quantizer = BinaryQuantizer.apply
             # for embedding, we always use token-wise quantization
             self.weight_quantize_num_groups = self.weight.size(0)
 
     def fix_weight_quantization(self):
-        self.weight.data = self.weight_quantizer(self.weight,
-                                                 self.weight.target_bits,
-                                                 None,
-                                                 None,
+        self.weight.data = self.weight_quantizer(self.weight, self.weight.target_bits, None, None,
                                                  self.weight_quantize_num_groups).data
         self.weight_quantization_enabled_in_forward = False
         return None
 
     def forward(self, input):
         if self.weight_quantization_enabled_in_forward and self.weight_quantization_enabled:
-            weight = self.weight_quantizer(self.weight,
-                                           self.weight.target_bits,
-                                           None,
-                                           None,
+            weight = self.weight_quantizer(self.weight, self.weight.target_bits, None, None,
                                            self.weight_quantize_num_groups)
         else:
             weight = self.weight
 
-        out = nn.functional.embedding(input,
-                                      weight,
-                                      self.padding_idx,
-                                      self.max_norm,
-                                      self.norm_type,
-                                      self.scale_grad_by_freq,
-                                      self.sparse)
+        out = nn.functional.embedding(input, weight, self.padding_idx, self.max_norm, self.norm_type,
+                                      self.scale_grad_by_freq, self.sparse)
         return out
 
 
 class LinearLayer_Compress(nn.Linear):
     """
     Linear layer with compression.
     """
+
     def __init__(self, *kargs, bias=True):
         super(LinearLayer_Compress, self).__init__(*kargs, bias=bias)
         self.sparse_pruning_method = None
         self.row_pruning_method = None
         self.head_pruning_method = None
         self.activation_quantization_method = None
         self.weight.start_bits = None
@@ -165,16 +151,15 @@
         if method == 'l1':
             weight_norm = torch.abs(self.weight.data)
             mask = TopKBinarizer.apply(weight_norm, self.sparse_pruning_ratio, False)
             mask = mask.view(self.weight.size())
             mask = mask.to(self.weight.device)
         elif method == 'topk':
             self.sparse_mask_scores = nn.Parameter(torch.Tensor(self.weight.size()))
-            self.sparse_mask_scores.data = self.sparse_mask_scores.data.to(
-                self.weight.device)
+            self.sparse_mask_scores.data = self.sparse_mask_scores.data.to(self.weight.device)
             init.kaiming_uniform_(self.sparse_mask_scores, a=math.sqrt(5))
             mask = None
         else:
             raise NotImplementedError
 
         self.register_buffer('sparse_pruning_mask', mask)
 
@@ -205,19 +190,17 @@
         self.head_pruning_ratio = ratio
         self.head_pruning_method = method
 
         if method not in ['topk']:
             raise NotImplementedError
         else:
             self.head_pruning_ratio = ratio
-            self.head_pruning_scores = nn.Parameter(torch.Tensor(
-                1,
-                self.num_heads))  # we apply the pruning to O matrix
-            self.head_pruning_scores.data = self.head_pruning_scores.data.to(
-                self.weight.device)
+            self.head_pruning_scores = nn.Parameter(torch.Tensor(1,
+                                                                 self.num_heads))  # we apply the pruning to O matrix
+            self.head_pruning_scores.data = self.head_pruning_scores.data.to(self.weight.device)
             init.kaiming_uniform_(self.head_pruning_scores, a=math.sqrt(5))
 
     def fix_sparse_pruning_helper(self):
         mask = self.get_mask(pruning_type='sparse')
         self.weight.data = self.weight.data * mask
         del self.sparse_pruning_mask
         if self.sparse_pruning_method == 'topk':
@@ -275,26 +258,25 @@
             if self.head_pruning_method == 'topk':
                 mask = self.get_mask(pruning_type='head').bool()
                 if dim_reduction:
                     shape = self.weight.size(0)
                     start_bits = self.weight.start_bits
                     target_bits = self.weight.target_bits
                     q_period = self.weight.q_period
-                    self.weight = nn.Parameter(self.weight.data.t().reshape(num_heads, -1)[mask.view(-1), :].reshape(-1, shape).t())
+                    self.weight = nn.Parameter(self.weight.data.t().reshape(num_heads,
+                                                                            -1)[mask.view(-1), :].reshape(-1,
+                                                                                                          shape).t())
                     self.weight.start_bits = start_bits
                     self.weight.target_bits = target_bits
                     self.weight.q_period = q_period
                 else:
 
                     shape = self.weight.size()
-                    self.weight.data = (self.weight.data.t().reshape(self.num_heads,
-                                                                     -1) *
-                                        mask.view(-1,
-                                                  1)).reshape(shape[1],
-                                                              shape[0]).t()
+                    self.weight.data = (self.weight.data.t().reshape(self.num_heads, -1) * mask.view(-1, 1)).reshape(
+                        shape[1], shape[0]).t()
 
                 if self.head_pruning_method == 'topk':
                     del self.head_pruning_scores
                 self.head_pruning_method = None
             else:
                 raise NotImplementedError
         else:
@@ -312,45 +294,34 @@
         return mask
 
     def get_mask(self, pruning_type='row'):
         if pruning_type == 'sparse':
             if self.sparse_pruning_method == 'l1':
                 return self.sparse_pruning_mask.to(self.weight.device)
             elif self.sparse_pruning_method == 'topk':
-                return TopKBinarizer.apply(self.sparse_mask_scores,
-                                           self.sparse_pruning_ratio,
-                                           False)
+                return TopKBinarizer.apply(self.sparse_mask_scores, self.sparse_pruning_ratio, False)
             else:
                 raise NotImplementedError
         if pruning_type == 'row':
             if self.row_pruning_method == 'l1':
                 return self.row_pruning_mask.to(self.weight.device)
             elif self.row_pruning_method == 'topk':
-                return TopKBinarizer.apply(self.row_mask_scores,
-                                           self.row_pruning_ratio,
-                                           False)
+                return TopKBinarizer.apply(self.row_mask_scores, self.row_pruning_ratio, False)
             else:
                 raise NotImplementedError
         elif pruning_type == 'head':
             if self.head_pruning_method == 'topk':
-                return TopKBinarizer.apply(self.head_pruning_scores,
-                                           self.head_pruning_ratio,
-                                           False)
+                return TopKBinarizer.apply(self.head_pruning_scores, self.head_pruning_ratio, False)
             else:
                 raise NotImplementedError
         else:
             raise NotImplementedError
 
-    def enable_weight_quantization(self,
-                                   start_bits,
-                                   target_bits,
-                                   quantization_period,
-                                   weight_quantization_enabled_in_forward,
-                                   quantization_type,
-                                   num_groups):
+    def enable_weight_quantization(self, start_bits, target_bits, quantization_period,
+                                   weight_quantization_enabled_in_forward, quantization_type, num_groups):
         self.weight.start_bits = start_bits
         self.weight.target_bits = target_bits
         self.weight.q_period = quantization_period
         self.weight_quantization_enabled_in_forward = weight_quantization_enabled_in_forward
         if self.weight_quantization_enabled_in_forward:
             logger.warning(
                 "************ A lot of MoQ features are not supported in quantize_weight_in_forward mode, please consider to use DS-FP16 optimizer************"
@@ -365,18 +336,15 @@
                 self.weight_quantizer = TernaryQuantizer.apply
             elif self.weight.target_bits == 1:
                 assert quantization_type == 'symmetric', 'Only symmetric quantization is supported for binary weight quantization'
                 self.weight_quantizer = BinaryQuantizer.apply
             self.weight_quantize_num_groups = num_groups
 
     def fix_weight_quantization(self):
-        self.weight.data = self.weight_quantizer(self.weight,
-                                                 self.weight.target_bits,
-                                                 None,
-                                                 None,
+        self.weight.data = self.weight_quantizer(self.weight, self.weight.target_bits, None, None,
                                                  self.weight_quantize_num_groups).data
         self.weight_quantization_enabled_in_forward = False
         return None
 
     def enable_activation_quantization(self, bits, quantization_type, range_calibration):
         assert bits in [4, 8], 'Only 4/8 bits activation quantization are supported for now'
         self.activation_quantization_bits = bits
@@ -387,26 +355,20 @@
             if quantization_type == 'symmetric':
                 self.activation_quantizer = SymQuantizer.apply
             else:
                 self.activation_quantizer = AsymQuantizer.apply
 
     def head_pruning_reshape(self, w, mask):
         shape = w.shape
-        return (w.t().reshape(self.num_heads,
-                              -1) * mask.view(-1,
-                                              1)).reshape(shape[1],
-                                                          shape[0]).t()
+        return (w.t().reshape(self.num_heads, -1) * mask.view(-1, 1)).reshape(shape[1], shape[0]).t()
 
     def forward(self, input, skip_bias_add=False):
 
         if self.weight_quantization_enabled_in_forward and self.weight_quantization_enabled:
-            weight = self.weight_quantizer(self.weight,
-                                           self.weight.target_bits,
-                                           None,
-                                           None,
+            weight = self.weight_quantizer(self.weight, self.weight.target_bits, None, None,
                                            self.weight_quantize_num_groups)
             bias = self.bias
         else:
             weight = self.weight
             bias = self.bias
 
         if self.sparse_pruning_enabled and self.sparse_pruning_method:
@@ -424,33 +386,30 @@
             weight = self.head_pruning_reshape(weight, mask)
 
         if self.activation_quantization_enabled:
             if 'dynamic' in self.activation_quantization_method:
                 num_groups = input.numel() // input.size(-1)
             else:
                 num_groups = 1
-            input = self.activation_quantizer(input,
-                                              self.activation_quantization_bits,
-                                              None,
-                                              None,
-                                              num_groups)
+            input = self.activation_quantizer(input, self.activation_quantization_bits, None, None, num_groups)
 
         if skip_bias_add:
             # used for mpu linear layers
             output = nn.functional.linear(input, weight, None)
             return output, bias
         else:
             output = nn.functional.linear(input, weight, bias)
             return output
 
 
 class Conv2dLayer_Compress(nn.Conv2d):
     """
     Conv2D layer with compression.
     """
+
     def __init__(self, *kargs):
         super(Conv2dLayer_Compress, self).__init__(*kargs)
         self.sparse_pruning_method = None
         self.channel_pruning_method = None
         self.activation_quantization_method = None
         self.weight.start_bits = None
         self.weight.target_bits = None
@@ -474,31 +433,28 @@
         if self.bias is None:
             s += ', bias=False'
         if self.padding_mode != 'zeros':
             s += ', padding_mode={padding_mode}'
         output = s.format(**self.__dict__)
 
         return output + ' sparse pruning={}, channel pruning={}, activation quantization={}, weight_quantization={}'.format(
-            self.sparse_pruning_method is not None,
-            self.channel_pruning_method is not None,
-            self.activation_quantization_method is not None,
-            self.weight.target_bits)
+            self.sparse_pruning_method is not None, self.channel_pruning_method is not None,
+            self.activation_quantization_method is not None, self.weight.target_bits)
 
     def enable_sparse_pruning(self, ratio, method):
         self.sparse_pruning_ratio = ratio
         self.sparse_pruning_method = method
         if method == 'l1':
             weight_norm = torch.abs(self.weight.data)
             mask = TopKBinarizer.apply(weight_norm, self.sparse_pruning_ratio, False)
             mask = mask.view(self.weight.size())
             mask = mask.to(self.weight.device)
         elif method == 'topk':
             self.sparse_mask_scores = nn.Parameter(torch.Tensor(self.weight.size()))
-            self.sparse_mask_scores.data = self.sparse_mask_scores.data.to(
-                self.weight.device)
+            self.sparse_mask_scores.data = self.sparse_mask_scores.data.to(self.weight.device)
             init.kaiming_uniform_(self.sparse_mask_scores, a=math.sqrt(5))
             mask = None
         else:
             raise NotImplementedError
 
         self.register_buffer('sparse_pruning_mask', mask)
 
@@ -510,21 +466,16 @@
         if method == 'l1':
             # compute the l1 norm of each conv2d kernel (the last three dimension)
             weight_norm = torch.norm(self.weight.data, p=1, dim=[1, 2, 3])
             mask = TopKBinarizer.apply(weight_norm, self.channel_pruning_ratio, False)
             mask = mask.view(-1, 1, 1, 1)
             mask = mask.to(self.weight.device)
         elif method == 'topk':
-            self.channel_mask_scores = nn.Parameter(
-                torch.Tensor(self.weight.size(0),
-                             1,
-                             1,
-                             1))
-            self.channel_mask_scores.data = self.channel_mask_scores.data.to(
-                self.weight.device)
+            self.channel_mask_scores = nn.Parameter(torch.Tensor(self.weight.size(0), 1, 1, 1))
+            self.channel_mask_scores.data = self.channel_mask_scores.data.to(self.weight.device)
             init.kaiming_uniform_(self.channel_mask_scores, a=math.sqrt(5))
             mask = None
         else:
             raise NotImplementedError
 
         self.register_buffer('channel_pruning_mask', mask)
 
@@ -575,47 +526,35 @@
         return mask
 
     def get_mask(self, pruning_type='sparse'):
         if pruning_type == 'sparse':
             if self.sparse_pruning_method == 'l1':
                 return self.sparse_pruning_mask.to(self.weight.device)
             elif self.sparse_pruning_method == 'topk':
-                return TopKBinarizer.apply(self.sparse_mask_scores,
-                                           self.sparse_pruning_ratio,
-                                           False)
+                return TopKBinarizer.apply(self.sparse_mask_scores, self.sparse_pruning_ratio, False)
             else:
                 raise NotImplementedError
         elif pruning_type == 'channel':
             if self.channel_pruning_method == 'l1':
                 return self.channel_pruning_mask.to(self.weight.device)
             elif self.channel_pruning_method == 'topk':
-                return TopKBinarizer.apply(self.channel_mask_scores,
-                                           self.channel_pruning_ratio,
-                                           False)
+                return TopKBinarizer.apply(self.channel_mask_scores, self.channel_pruning_ratio, False)
             else:
                 raise NotImplementedError
         else:
             raise NotImplementedError
 
     def fix_weight_quantization(self):
-        self.weight.data = self.weight_quantizer(self.weight,
-                                                 self.weight.target_bits,
-                                                 None,
-                                                 None,
+        self.weight.data = self.weight_quantizer(self.weight, self.weight.target_bits, None, None,
                                                  self.weight_quantize_num_groups).data
         self.weight_quantization_enabled_in_forward = False
         return None
 
-    def enable_weight_quantization(self,
-                                   start_bits,
-                                   target_bits,
-                                   quantization_period,
-                                   weight_quantization_enabled_in_forward,
-                                   quantization_type,
-                                   num_groups):
+    def enable_weight_quantization(self, start_bits, target_bits, quantization_period,
+                                   weight_quantization_enabled_in_forward, quantization_type, num_groups):
         self.weight.start_bits = start_bits
         self.weight.target_bits = target_bits
         self.weight.q_period = quantization_period
         self.weight_quantization_enabled_in_forward = weight_quantization_enabled_in_forward
         if self.weight_quantization_enabled_in_forward:
             assert self.weight.target_bits >= 4, 'Only >=4 bits weight quantization are supported during forward pass for now'
             logger.warning(
@@ -638,18 +577,15 @@
                 self.activation_quantizer = SymQuantizer.apply
             else:
                 self.activation_quantizer = AsymQuantizer.apply
 
     def forward(self, input):
 
         if self.weight_quantization_enabled_in_forward and self.weight_quantization_enabled:
-            weight = self.weight_quantizer(self.weight,
-                                           self.weight.target_bits,
-                                           None,
-                                           None,
+            weight = self.weight_quantizer(self.weight, self.weight.target_bits, None, None,
                                            self.weight_quantize_num_groups)
             bias = self.bias
         else:
             weight = self.weight
             bias = self.bias
 
         if self.sparse_pruning_enabled and self.sparse_pruning_method:
@@ -663,30 +599,21 @@
                 bias = bias * mask.view(-1)
 
         if self.activation_quantization_enabled:
             if 'dynamic' in self.activation_quantization_method:
                 num_groups = input.numel() // input[0].numel()
             else:
                 num_groups = 1
-            input = self.activation_quantizer(input,
-                                              self.activation_quantization_bits,
-                                              None,
-                                              None,
-                                              num_groups)
-
-        return nn.functional.conv2d(input,
-                                    weight,
-                                    bias,
-                                    self.stride,
-                                    self.padding,
-                                    self.dilation,
-                                    self.groups)
+            input = self.activation_quantizer(input, self.activation_quantization_bits, None, None, num_groups)
+
+        return nn.functional.conv2d(input, weight, bias, self.stride, self.padding, self.dilation, self.groups)
 
 
 class BNLayer_Compress(nn.BatchNorm2d):
+
     def fix_channel_pruning_helper(self, mask, dim_reduction=True):
         self.weight = nn.Parameter(self.weight.data[mask.view(-1)])
         self.bias = nn.Parameter(self.bias.data[mask.view(-1)])
         self.running_mean = self.running_mean[mask.view(-1)]
         self.running_var = self.running_var[mask.view(-1)]
 
 
@@ -766,47 +693,51 @@
     output = torch.cat(tensor_list, dim=last_dim).contiguous()
 
     return output
 
 
 class _CopyToModelParallelRegion(torch.autograd.Function):
     """Pass the input to the model parallel region."""
+
     @staticmethod
     def forward(ctx, input_):
         return input_
 
     @staticmethod
     def backward(ctx, grad_output):
         return _reduce(grad_output)
 
 
 class _ReduceFromModelParallelRegion(torch.autograd.Function):
     """All-redcue the input from the model parallel region."""
+
     @staticmethod
     def forward(ctx, input_):
         return _reduce(input_)
 
     @staticmethod
     def backward(ctx, grad_output):
         return grad_output
 
 
 class _ScatterToModelParallelRegion(torch.autograd.Function):
     """Split the input and keep only the corresponding chuck to the rank."""
+
     @staticmethod
     def forward(ctx, input_):
         return _split(input_)
 
     @staticmethod
     def backward(ctx, grad_output):
         return _gather(grad_output)
 
 
 class _GatherFromModelParallelRegion(torch.autograd.Function):
     """Gather the input from model parallel region and concatinate."""
+
     @staticmethod
     def forward(ctx, input_):
         return _gather(input_)
 
     @staticmethod
     def backward(ctx, grad_output):
         return _split(grad_output)
@@ -830,38 +761,30 @@
 
 
 def gather_from_model_parallel_region(input_):
     return _GatherFromModelParallelRegion.apply(input_)
 
 
 class ColumnParallelLinear_Compress(LinearLayer_Compress):
-    def __init__(self,
-                 mpu,
-                 input_size,
-                 output_size,
-                 bias=True,
-                 gather_output=True,
-                 skip_bias_add=False):
+
+    def __init__(self, mpu, input_size, output_size, bias=True, gather_output=True, skip_bias_add=False):
         # Keep input parameters
         global g_mpu
         g_mpu = mpu
         self.input_size = input_size
         self.output_size = output_size
         self.gather_output = gather_output
         self.skip_bias_add = skip_bias_add
 
         # Divide the weight matrix along the last dimension.
         world_size = mpu.get_model_parallel_world_size()
         assert output_size % world_size == 0
         self.output_size_per_partition = output_size // world_size
 
-        super(ColumnParallelLinear_Compress,
-              self).__init__(self.input_size,
-                             self.output_size_per_partition,
-                             bias=bias)
+        super(ColumnParallelLinear_Compress, self).__init__(self.input_size, self.output_size_per_partition, bias=bias)
 
     def forward(self, input_):
         # Set up backprop all-reduce.
         input_parallel = copy_to_model_parallel_region(input_)
         # Matrix multiply.
         if self.skip_bias_add:
             output_parallel, bias = super().forward(input_parallel, True)
@@ -873,38 +796,30 @@
             output = gather_from_model_parallel_region(output_parallel)
         else:
             output = output_parallel
         return output, bias
 
 
 class RowParallelLinear_Compress(LinearLayer_Compress):
-    def __init__(self,
-                 mpu,
-                 input_size,
-                 output_size,
-                 bias=True,
-                 input_is_parallel=False,
-                 skip_bias_add=False):
+
+    def __init__(self, mpu, input_size, output_size, bias=True, input_is_parallel=False, skip_bias_add=False):
         # Keep input parameters
         global g_mpu
         g_mpu = mpu
         self.input_size = input_size
         self.output_size = output_size
         self.input_is_parallel = input_is_parallel
         self.skip_bias_add = skip_bias_add
 
         # Divide the weight matrix along the last dimension.
         world_size = mpu.get_model_parallel_world_size()
         assert input_size % world_size == 0
         self.input_size_per_partition = input_size // world_size
 
-        super(RowParallelLinear_Compress,
-              self).__init__(self.input_size_per_partition,
-                             self.output_size,
-                             bias=bias)
+        super(RowParallelLinear_Compress, self).__init__(self.input_size_per_partition, self.output_size, bias=bias)
 
     def forward(self, input_):
         # Set up backprop all-reduce.
         if self.input_is_parallel:
             input_parallel = input_
         else:
             input_parallel = scatter_to_model_parallel_region(input_)
```

### Comparing `deepspeed-0.8.3/deepspeed/compression/compress.py` & `deepspeed-0.9.0/deepspeed/compression/compress.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,101 +1,97 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import re
 from .helper import compression_preparation, fix_compression, recursive_getattr, is_module_compressible
 from .config import get_compression_config
 from ..runtime.config_utils import dict_raise_error_on_duplicate_keys
 from .constants import *
 import os
 import json
 
 
 def check_deepspeed_config(config):
     if isinstance(config, dict):
         return config
     elif os.path.exists(config):
-        return json.load(open(config,
-                              "r"),
-                         object_pairs_hook=dict_raise_error_on_duplicate_keys)
+        return json.load(open(config, "r"), object_pairs_hook=dict_raise_error_on_duplicate_keys)
     else:
         raise ValueError(
-            f"Expected a string path to an existing deepspeed config, or a dictionary. Received: {config}"
-        )
+            f"Expected a string path to an existing deepspeed config, or a dictionary. Received: {config}")
 
 
-def get_module_name(group_name,
-                    model,
-                    key_word,
-                    exist_module_name,
-                    mpu=None,
-                    verbose=True):
+def get_module_name(group_name, model, key_word, exist_module_name, mpu=None, verbose=True):
     '''
     get the associated module name from the model based on the key_word provided by users
     '''
     return_module_name = []
     for name, module in model.named_modules():
 
         module_check = is_module_compressible(module, mpu)
 
         if re.search(key_word, name) is not None and module_check:
             if name in exist_module_name and verbose:
                 # logger.warning
                 raise ValueError(
-                    f"{name} is already added to compression, please check your config file for {group_name}."
-                )
+                    f"{name} is already added to compression, please check your config file for {group_name}.")
             if name not in exist_module_name:
                 exist_module_name.add(name)
                 return_module_name.append(name)
     return return_module_name, exist_module_name
 
 
 def get_compress_methods(model, compress_methods, mpu=None):
     # extract the compression module for each method in compress_methods
     layer_added_compress_methods = []
     for method, method_content in compress_methods.items():
         if LAYER_REDUCTION in method:
             continue
         # for loop different methods, i.e., weight quantization, activation quantization etc
         exist_module_name = set()
-        shared_parameters = method_content[
-            SHARED_PARAMETERS]  # get all the shared parameters
+        shared_parameters = method_content[SHARED_PARAMETERS]  # get all the shared parameters
         for group_name, method_parameters in method_content[DIFFERENT_GROUPS].items():
             # for loop different groups, i.e., weight quantization group 1, weight quantization group 2 etc
             module_name_list = []
             related_module_name_list = []
             if method_parameters[DIFFERENT_GROUPS_RELATED_MODULE_SCOPE]:
                 # this is used for head/row/channel pruning, if users provide the related module scope, we can shrink the layer dim for them
                 # otherwise we just mask those as zeros
-                for key_word, related_key_words in zip(method_parameters[DIFFERENT_GROUPS_MODULE_SCOPE], method_parameters[DIFFERENT_GROUPS_RELATED_MODULE_SCOPE]):
-                    module_name, exist_module_name = get_module_name(group_name, model, key_word, exist_module_name, mpu=mpu)
+                for key_word, related_key_words in zip(method_parameters[DIFFERENT_GROUPS_MODULE_SCOPE],
+                                                       method_parameters[DIFFERENT_GROUPS_RELATED_MODULE_SCOPE]):
+                    module_name, exist_module_name = get_module_name(group_name,
+                                                                     model,
+                                                                     key_word,
+                                                                     exist_module_name,
+                                                                     mpu=mpu)
                     module_name_list.append(module_name)
                     tmp_related_module_name_list = []
                     for rkw in related_key_words:
                         # related key word can be a list, for instance the QKV for O matrix in Attention
                         module_name, _ = get_module_name(group_name, model, rkw, set(), mpu=mpu)
                         tmp_related_module_name_list.append(module_name)
                     related_module_name_list.append(tmp_related_module_name_list)
             else:
                 for key_word in method_parameters[DIFFERENT_GROUPS_MODULE_SCOPE]:
-                    module_name, exist_module_name = get_module_name(group_name, model, key_word, exist_module_name, mpu=mpu)
+                    module_name, exist_module_name = get_module_name(group_name,
+                                                                     model,
+                                                                     key_word,
+                                                                     exist_module_name,
+                                                                     mpu=mpu)
                     module_name_list.append(module_name)
 
             if module_name_list:
                 # combine shared parameters with each group
                 combined_method_parameters = {
                     **(method_parameters.copy().pop(DIFFERENT_GROUPS_PARAMETERS)),
                     **shared_parameters
                 }
-                compression_item = [
-                    module_name_list,
-                    related_module_name_list,
-                    {
-                        method: combined_method_parameters
-                    }
-                ]
+                compression_item = [module_name_list, related_module_name_list, {method: combined_method_parameters}]
                 layer_added_compress_methods.append(compression_item)
     return layer_added_compress_methods
 
 
 def init_compression(model, deepspeed_config, teacher_model=None, mpu=None):
     """
     Compress a model: replace linear/conv2d layer with deepspeed compression-aware modules
@@ -114,17 +110,15 @@
         c_model = model
 
     # For layer reduction
     if compress_methods[LAYER_REDUCTION][LAYER_REDUCTION_ENABLED]:
         assert teacher_model is not None, "Teacher model is required for layer reduction"
         student_initialization(c_model, teacher_model, deepspeed_config)
 
-    layer_added_compress_methods = get_compress_methods(c_model,
-                                                        compress_methods,
-                                                        mpu=mpu)
+    layer_added_compress_methods = get_compress_methods(c_model, compress_methods, mpu=mpu)
     compression_preparation(c_model, layer_added_compress_methods, mpu)
 
     return model
 
 
 def redundancy_clean(model, deepspeed_config, mpu=None):
     """
@@ -139,39 +133,28 @@
     """
     compress_methods = get_compression_config(check_deepspeed_config(deepspeed_config))
     if hasattr(model, 'module'):
         c_model = model.module
     else:
         c_model = model
 
-    layer_added_compress_methods_tmp = get_compress_methods(c_model,
-                                                            compress_methods,
-                                                            mpu=mpu)
+    layer_added_compress_methods_tmp = get_compress_methods(c_model, compress_methods, mpu=mpu)
     # sort methods
     order_list = [
-        WEIGHT_QUANTIZATION,
-        SPARSE_PRUNING,
-        ROW_PRUNING,
-        HEAD_PRUNING,
-        CHANNEL_PRUNING,
-        ACTIVATION_QUANTIZATION
+        WEIGHT_QUANTIZATION, SPARSE_PRUNING, ROW_PRUNING, HEAD_PRUNING, CHANNEL_PRUNING, ACTIVATION_QUANTIZATION
     ]
-    layer_added_compress_methods = sorted(
-        layer_added_compress_methods_tmp,
-        key=lambda x: order_list.index(list(x[2].keys())[0]))
+    layer_added_compress_methods = sorted(layer_added_compress_methods_tmp,
+                                          key=lambda x: order_list.index(list(x[2].keys())[0]))
 
     for module_name_lists, related_module_name_lists, compression_technique in layer_added_compress_methods:
         stored_mask = []
         need_mask = True if related_module_name_lists else False
         for i, mnl in enumerate(module_name_lists):
             for module_name in mnl:
-                mask = fix_compression(c_model,
-                                       module_name,
-                                       compression_technique,
-                                       dim_reduction=need_mask)
+                mask = fix_compression(c_model, module_name, compression_technique, dim_reduction=need_mask)
                 if need_mask:
                     stored_mask.append(mask)
             if need_mask:
                 for rmnl in related_module_name_lists[i]:
                     for j, module_name in enumerate(rmnl):
                         mask = fix_compression(c_model,
                                                module_name,
@@ -215,18 +198,16 @@
             The modules will be used for student's reinitializedion
             Example 1: ['bert.pooler', 'bert.embeddings', 'classifier'], means we want to apply the weight in teacher's embedding/pooler/classier module to the student
             Example 2: ['transformer.w', 'transformer.ln_f', 'lm_head'], means we want to apply the weight in teacher's embeddingn layers module to the student
     Note that teacher_layer should matches student layer
     '''
     assert len(student_layer) == len(teacher_layer)
     for s_name, t_name in zip(student_layer, teacher_layer):
-        s_module = recursive_getattr(student_model,
-                                     module_name_prefix + '.' + str(s_name))
-        t_module = recursive_getattr(teacher_model,
-                                     module_name_prefix + '.' + str(t_name))
+        s_module = recursive_getattr(student_model, module_name_prefix + '.' + str(s_name))
+        t_module = recursive_getattr(teacher_model, module_name_prefix + '.' + str(t_name))
         for s_param, t_param in zip(s_module.parameters(), t_module.parameters()):
             s_param.data.copy_(t_param.data)
     for name in other_module_name:
         s_module = recursive_getattr(student_model, name)
         t_module = recursive_getattr(teacher_model, name)
         print(name)
         for s_param, t_param in zip(s_module.parameters(), t_module.parameters()):
```

### Comparing `deepspeed-0.8.3/deepspeed/compression/config.py` & `deepspeed-0.9.0/deepspeed/compression/config.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,8 +1,11 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 from .constants import *
 import copy
 from ..runtime.config_utils import get_scalar_param
 
 
 def get_compression_config(param_dict):
@@ -32,17 +35,15 @@
         for key, val in get_layer_reduction_params(param_dict).items():
             output[key] = val
     return output
 
 
 def get_layer_reduction_enabled(param_dict):
     if LAYER_REDUCTION in param_dict.keys():
-        return get_scalar_param(param_dict[LAYER_REDUCTION],
-                                LAYER_REDUCTION_ENABLED,
-                                LAYER_REDUCTION_ENABLED_DEFAULT)
+        return get_scalar_param(param_dict[LAYER_REDUCTION], LAYER_REDUCTION_ENABLED, LAYER_REDUCTION_ENABLED_DEFAULT)
     else:
         return False
 
 
 def get_layer_reduction_params(param_dict):
     if LAYER_REDUCTION in param_dict.keys():
         layer_reduction_params = copy.copy(param_dict[LAYER_REDUCTION])
@@ -66,427 +67,368 @@
     if WEIGHT_QUANTIZATION not in param_dict.keys():
         param_dict[WEIGHT_QUANTIZATION] = {SHARED_PARAMETERS: {}, DIFFERENT_GROUPS: {}}
     sub_param_dict = param_dict[WEIGHT_QUANTIZATION]
     # shared parameters
     output[SHARED_PARAMETERS] = get_weight_quantization_shared_parameters(sub_param_dict)
     # each sub-groups
     if output[SHARED_PARAMETERS][WEIGHT_QUANTIZE_ENABLED]:
-        assert DIFFERENT_GROUPS in sub_param_dict.keys(), f"Weigh Quantization is enabled, {DIFFERENT_GROUPS} must be specified"
+        assert DIFFERENT_GROUPS in sub_param_dict.keys(
+        ), f"Weigh Quantization is enabled, {DIFFERENT_GROUPS} must be specified"
     output[DIFFERENT_GROUPS] = get_weight_quantization_different_groups(sub_param_dict)
     return output
 
 
 def get_weight_quantization_shared_parameters(param_dict):
     output = {}
     if SHARED_PARAMETERS in param_dict.keys():
         sub_param_dict = param_dict[SHARED_PARAMETERS]
-        output[WEIGHT_QUANTIZE_ENABLED] = get_scalar_param(
-            sub_param_dict,
-            WEIGHT_QUANTIZE_ENABLED,
-            WEIGHT_QUANTIZE_ENABLED_DEFAULT)
-        output[WEIGHT_QUANTIZE_KERNEL] = get_scalar_param(
-            sub_param_dict,
-            WEIGHT_QUANTIZE_KERNEL,
-            WEIGHT_QUANTIZE_KERNEL_DEFAULT)
-        output[WEIGHT_QUANTIZE_SCHEDULE_OFFSET] = get_scalar_param(
-            sub_param_dict,
-            WEIGHT_QUANTIZE_SCHEDULE_OFFSET,
-            WEIGHT_QUANTIZE_SCHEDULE_OFFSET_DEFAULT)
-        output[WEIGHT_QUANTIZE_GROUPS] = get_scalar_param(
-            sub_param_dict,
-            WEIGHT_QUANTIZE_GROUPS,
-            WEIGHT_QUANTIZE_GROUPS_DEFAULT)
-        output[WEIGHT_QUANTIZE_VERBOSE] = get_scalar_param(
-            sub_param_dict,
-            WEIGHT_QUANTIZE_VERBOSE,
-            WEIGHT_QUANTIZE_VERBOSE_DEFAULT)
-        output[WEIGHT_QUANTIZE_TYPE] = get_scalar_param(sub_param_dict,
-                                                        WEIGHT_QUANTIZE_TYPE,
+        output[WEIGHT_QUANTIZE_ENABLED] = get_scalar_param(sub_param_dict, WEIGHT_QUANTIZE_ENABLED,
+                                                           WEIGHT_QUANTIZE_ENABLED_DEFAULT)
+        output[WEIGHT_QUANTIZE_KERNEL] = get_scalar_param(sub_param_dict, WEIGHT_QUANTIZE_KERNEL,
+                                                          WEIGHT_QUANTIZE_KERNEL_DEFAULT)
+        output[WEIGHT_QUANTIZE_SCHEDULE_OFFSET] = get_scalar_param(sub_param_dict, WEIGHT_QUANTIZE_SCHEDULE_OFFSET,
+                                                                   WEIGHT_QUANTIZE_SCHEDULE_OFFSET_DEFAULT)
+        output[WEIGHT_QUANTIZE_GROUPS] = get_scalar_param(sub_param_dict, WEIGHT_QUANTIZE_GROUPS,
+                                                          WEIGHT_QUANTIZE_GROUPS_DEFAULT)
+        output[WEIGHT_QUANTIZE_VERBOSE] = get_scalar_param(sub_param_dict, WEIGHT_QUANTIZE_VERBOSE,
+                                                           WEIGHT_QUANTIZE_VERBOSE_DEFAULT)
+        output[WEIGHT_QUANTIZE_TYPE] = get_scalar_param(sub_param_dict, WEIGHT_QUANTIZE_TYPE,
                                                         WEIGHT_QUANTIZE_TYPE_DEFAULT)
-        output[WEIGHT_QUANTIZE_IN_FORWARD_ENABLED] = get_scalar_param(
-            sub_param_dict,
-            WEIGHT_QUANTIZE_IN_FORWARD_ENABLED,
-            WEIGHT_QUANTIZE_IN_FORWARD_ENABLED_DEFAULT)
-        assert output[WEIGHT_QUANTIZE_TYPE] in [WEIGHT_QUANTIZE_SYMMETRIC, WEIGHT_QUANTIZE_ASYMMETRIC], f"Invalid weight quantize type. Supported types: [{WEIGHT_QUANTIZE_SYMMETRIC}, {WEIGHT_QUANTIZE_ASYMMETRIC}]"
-        output[WEIGHT_QUANTIZE_ROUNDING] = get_scalar_param(
-            sub_param_dict,
-            WEIGHT_QUANTIZE_ROUNDING,
-            WEIGHT_QUANTIZE_ROUNDING_DEFAULT)
-        assert output[WEIGHT_QUANTIZE_ROUNDING] in [WEIGHT_QUANTIZE_NEAREST_ROUNDING, WEIGHT_QUANTIZE_STOCHASTIC_ROUNDING], f"Invalid weight quantize rounding. Supported types: [{WEIGHT_QUANTIZE_NEAREST_ROUNDING}, {WEIGHT_QUANTIZE_STOCHASTIC_ROUNDING}]"
+        output[WEIGHT_QUANTIZE_IN_FORWARD_ENABLED] = get_scalar_param(sub_param_dict,
+                                                                      WEIGHT_QUANTIZE_IN_FORWARD_ENABLED,
+                                                                      WEIGHT_QUANTIZE_IN_FORWARD_ENABLED_DEFAULT)
+        assert output[WEIGHT_QUANTIZE_TYPE] in [
+            WEIGHT_QUANTIZE_SYMMETRIC, WEIGHT_QUANTIZE_ASYMMETRIC
+        ], f"Invalid weight quantize type. Supported types: [{WEIGHT_QUANTIZE_SYMMETRIC}, {WEIGHT_QUANTIZE_ASYMMETRIC}]"
+        output[WEIGHT_QUANTIZE_ROUNDING] = get_scalar_param(sub_param_dict, WEIGHT_QUANTIZE_ROUNDING,
+                                                            WEIGHT_QUANTIZE_ROUNDING_DEFAULT)
+        assert output[WEIGHT_QUANTIZE_ROUNDING] in [
+            WEIGHT_QUANTIZE_NEAREST_ROUNDING, WEIGHT_QUANTIZE_STOCHASTIC_ROUNDING
+        ], f"Invalid weight quantize rounding. Supported types: [{WEIGHT_QUANTIZE_NEAREST_ROUNDING}, {WEIGHT_QUANTIZE_STOCHASTIC_ROUNDING}]"
         if WEIGHT_QUANTIZE_FP16_MIXED_QUANTIZE in sub_param_dict.keys():
             output[WEIGHT_QUANTIZE_FP16_MIXED_QUANTIZE] = get_scalar_param(
-                sub_param_dict[WEIGHT_QUANTIZE_FP16_MIXED_QUANTIZE],
-                WEIGHT_QUANTIZE_FP16_MIXED_QUANTIZE_ENABLED,
+                sub_param_dict[WEIGHT_QUANTIZE_FP16_MIXED_QUANTIZE], WEIGHT_QUANTIZE_FP16_MIXED_QUANTIZE_ENABLED,
                 WEIGHT_QUANTIZE_FP16_MIXED_QUANTIZE_ENABLED_DEFAULT)
             output[WEIGHT_QUANTIZE_CHANGE_RATIO] = get_scalar_param(
-                sub_param_dict[WEIGHT_QUANTIZE_FP16_MIXED_QUANTIZE],
-                WEIGHT_QUANTIZE_CHANGE_RATIO,
+                sub_param_dict[WEIGHT_QUANTIZE_FP16_MIXED_QUANTIZE], WEIGHT_QUANTIZE_CHANGE_RATIO,
                 WEIGHT_QUANTIZE_CHANGE_RATIO_DEFAULT)
         else:
-            output[
-                WEIGHT_QUANTIZE_FP16_MIXED_QUANTIZE] = WEIGHT_QUANTIZE_FP16_MIXED_QUANTIZE_ENABLED_DEFAULT
+            output[WEIGHT_QUANTIZE_FP16_MIXED_QUANTIZE] = WEIGHT_QUANTIZE_FP16_MIXED_QUANTIZE_ENABLED_DEFAULT
             output[WEIGHT_QUANTIZE_CHANGE_RATIO] = WEIGHT_QUANTIZE_CHANGE_RATIO_DEFAULT
     else:
         output[WEIGHT_QUANTIZE_ENABLED] = WEIGHT_QUANTIZE_ENABLED_DEFAULT
         output[WEIGHT_QUANTIZE_KERNEL] = WEIGHT_QUANTIZE_KERNEL_DEFAULT
         output[WEIGHT_QUANTIZE_SCHEDULE_OFFSET] = WEIGHT_QUANTIZE_SCHEDULE_OFFSET_DEFAULT
         output[WEIGHT_QUANTIZE_GROUPS] = WEIGHT_QUANTIZE_GROUPS_DEFAULT
         output[WEIGHT_QUANTIZE_VERBOSE] = WEIGHT_QUANTIZE_VERBOSE_DEFAULT
         output[WEIGHT_QUANTIZE_TYPE] = WEIGHT_QUANTIZE_TYPE_DEFAULT
         output[WEIGHT_QUANTIZE_ROUNDING] = WEIGHT_QUANTIZE_ROUNDING_DEFAULT
-        output[
-            WEIGHT_QUANTIZE_FP16_MIXED_QUANTIZE] = WEIGHT_QUANTIZE_FP16_MIXED_QUANTIZE_ENABLED_DEFAULT
+        output[WEIGHT_QUANTIZE_FP16_MIXED_QUANTIZE] = WEIGHT_QUANTIZE_FP16_MIXED_QUANTIZE_ENABLED_DEFAULT
         output[WEIGHT_QUANTIZE_CHANGE_RATIO] = WEIGHT_QUANTIZE_CHANGE_RATIO_DEFAULT
     return output
 
 
 def get_weight_quantization_different_groups(param_dict):
     output = {}
     sub_param_dict = param_dict[DIFFERENT_GROUPS]
 
     def get_params(name, group_dict):
-        assert WEIGHT_QUANTIZE_START_BITS in group_dict.keys(), f"{WEIGHT_QUANTIZE_START_BITS} must be specified for weight quantization group {name}"
-        assert WEIGHT_QUANTIZE_TARGET_BITS in group_dict.keys(), f"{WEIGHT_QUANTIZE_TARGET_BITS} must be specified for weight quantization group {name}"
-        group_dict[WEIGHT_QUANTIZATION_PERIOD] = get_scalar_param(
-            group_dict,
-            WEIGHT_QUANTIZATION_PERIOD,
-            WEIGHT_QUANTIZATION_PERIOD_DEFAULT)
+        assert WEIGHT_QUANTIZE_START_BITS in group_dict.keys(
+        ), f"{WEIGHT_QUANTIZE_START_BITS} must be specified for weight quantization group {name}"
+        assert WEIGHT_QUANTIZE_TARGET_BITS in group_dict.keys(
+        ), f"{WEIGHT_QUANTIZE_TARGET_BITS} must be specified for weight quantization group {name}"
+        group_dict[WEIGHT_QUANTIZATION_PERIOD] = get_scalar_param(group_dict, WEIGHT_QUANTIZATION_PERIOD,
+                                                                  WEIGHT_QUANTIZATION_PERIOD_DEFAULT)
         return group_dict
 
     for k, v in sub_param_dict.items():
         output[k] = {}
-        output[k][DIFFERENT_GROUPS_PARAMETERS] = get_params(
-            k,
-            sub_param_dict[k][DIFFERENT_GROUPS_PARAMETERS])
-        output[k][DIFFERENT_GROUPS_MODULE_SCOPE] = get_scalar_param(
-            sub_param_dict[k],
-            DIFFERENT_GROUPS_MODULE_SCOPE,
-            DIFFERENT_GROUPS_MODULE_SCOPE_DEFAULT)
+        output[k][DIFFERENT_GROUPS_PARAMETERS] = get_params(k, sub_param_dict[k][DIFFERENT_GROUPS_PARAMETERS])
+        output[k][DIFFERENT_GROUPS_MODULE_SCOPE] = get_scalar_param(sub_param_dict[k], DIFFERENT_GROUPS_MODULE_SCOPE,
+                                                                    DIFFERENT_GROUPS_MODULE_SCOPE_DEFAULT)
         output[k][DIFFERENT_GROUPS_RELATED_MODULE_SCOPE] = get_scalar_param(
-            sub_param_dict[k],
-            DIFFERENT_GROUPS_RELATED_MODULE_SCOPE,
-            DIFFERENT_GROUPS_RELATED_MODULE_SCOPE_DEFAULT)
+            sub_param_dict[k], DIFFERENT_GROUPS_RELATED_MODULE_SCOPE, DIFFERENT_GROUPS_RELATED_MODULE_SCOPE_DEFAULT)
 
     return output
 
 
 def get_activation_quantization(param_dict):
     output = {}
     if ACTIVATION_QUANTIZATION not in param_dict.keys():
-        param_dict[ACTIVATION_QUANTIZATION] = {
-            SHARED_PARAMETERS: {},
-            DIFFERENT_GROUPS: {}
-        }
+        param_dict[ACTIVATION_QUANTIZATION] = {SHARED_PARAMETERS: {}, DIFFERENT_GROUPS: {}}
     sub_param_dict = param_dict[ACTIVATION_QUANTIZATION]
     # shared parameters
-    output[SHARED_PARAMETERS] = get_activation_quantization_shared_parameters(
-        sub_param_dict)
+    output[SHARED_PARAMETERS] = get_activation_quantization_shared_parameters(sub_param_dict)
     # each sub-groups
     if output[SHARED_PARAMETERS][ACTIVATION_QUANTIZATION_ENABLED]:
-        assert DIFFERENT_GROUPS in sub_param_dict.keys(), f"Activation Quantization is enabled, {DIFFERENT_GROUPS} must be specified"
-    output[DIFFERENT_GROUPS] = get_activation_quantization_different_groups(
-        sub_param_dict)
+        assert DIFFERENT_GROUPS in sub_param_dict.keys(
+        ), f"Activation Quantization is enabled, {DIFFERENT_GROUPS} must be specified"
+    output[DIFFERENT_GROUPS] = get_activation_quantization_different_groups(sub_param_dict)
     return output
 
 
 def get_activation_quantization_shared_parameters(param_dict):
     output = {}
     if SHARED_PARAMETERS in param_dict.keys():
         sub_param_dict = param_dict[SHARED_PARAMETERS]
-        output[ACTIVATION_QUANTIZATION_ENABLED] = get_scalar_param(
-            sub_param_dict,
-            ACTIVATION_QUANTIZATION_ENABLED,
-            ACTIVATION_QUANTIZATION_ENABLED_DEFAULT)
-        output[ACTIVATION_QUANTIZE_TYPE] = get_scalar_param(
-            sub_param_dict,
-            ACTIVATION_QUANTIZE_TYPE,
-            ACTIVATION_QUANTIZE_TYPE_DEFAULT)
-        assert output[ACTIVATION_QUANTIZE_TYPE] in [ACTIVATION_QUANTIZE_SYMMETRIC, ACTIVATION_QUANTIZE_ASYMMETRIC], f"Invalid activation quantize type. Supported types: [{ACTIVATION_QUANTIZE_SYMMETRIC}, {ACTIVATION_QUANTIZE_ASYMMETRIC}]"
-        output[ACTIVATION_QUANTIZE_RANGE] = get_scalar_param(
-            sub_param_dict,
-            ACTIVATION_QUANTIZE_RANGE,
-            ACTIVATION_QUANTIZE_RANGE_DEFAULT)
-        assert output[ACTIVATION_QUANTIZE_RANGE] in [ACTIVATION_QUANTIZE_RANGE_DYNAMIC, ACTIVATION_QUANTIZE_RANGE_STATIC], f"Invalid activation quantize range calibration. Supported types: [{ACTIVATION_QUANTIZE_RANGE_DYNAMIC}, {ACTIVATION_QUANTIZE_RANGE_STATIC}]"
-        output[ACTIVATION_QUANTIZE_SCHEDULE_OFFSET] = get_scalar_param(
-            sub_param_dict,
-            ACTIVATION_QUANTIZE_SCHEDULE_OFFSET,
-            ACTIVATION_QUANTIZE_SCHEDULE_OFFSET_DEFAULT)
+        output[ACTIVATION_QUANTIZATION_ENABLED] = get_scalar_param(sub_param_dict, ACTIVATION_QUANTIZATION_ENABLED,
+                                                                   ACTIVATION_QUANTIZATION_ENABLED_DEFAULT)
+        output[ACTIVATION_QUANTIZE_TYPE] = get_scalar_param(sub_param_dict, ACTIVATION_QUANTIZE_TYPE,
+                                                            ACTIVATION_QUANTIZE_TYPE_DEFAULT)
+        assert output[ACTIVATION_QUANTIZE_TYPE] in [
+            ACTIVATION_QUANTIZE_SYMMETRIC, ACTIVATION_QUANTIZE_ASYMMETRIC
+        ], f"Invalid activation quantize type. Supported types: [{ACTIVATION_QUANTIZE_SYMMETRIC}, {ACTIVATION_QUANTIZE_ASYMMETRIC}]"
+        output[ACTIVATION_QUANTIZE_RANGE] = get_scalar_param(sub_param_dict, ACTIVATION_QUANTIZE_RANGE,
+                                                             ACTIVATION_QUANTIZE_RANGE_DEFAULT)
+        assert output[ACTIVATION_QUANTIZE_RANGE] in [
+            ACTIVATION_QUANTIZE_RANGE_DYNAMIC, ACTIVATION_QUANTIZE_RANGE_STATIC
+        ], f"Invalid activation quantize range calibration. Supported types: [{ACTIVATION_QUANTIZE_RANGE_DYNAMIC}, {ACTIVATION_QUANTIZE_RANGE_STATIC}]"
+        output[ACTIVATION_QUANTIZE_SCHEDULE_OFFSET] = get_scalar_param(sub_param_dict,
+                                                                       ACTIVATION_QUANTIZE_SCHEDULE_OFFSET,
+                                                                       ACTIVATION_QUANTIZE_SCHEDULE_OFFSET_DEFAULT)
     else:
         output[ACTIVATION_QUANTIZATION_ENABLED] = ACTIVATION_QUANTIZATION_ENABLED_DEFAULT
         output[ACTIVATION_QUANTIZE_TYPE] = ACTIVATION_QUANTIZE_TYPE_DEFAULT
         output[ACTIVATION_QUANTIZE_RANGE] = ACTIVATION_QUANTIZE_RANGE_DEFAULT
-        output[
-            ACTIVATION_QUANTIZE_SCHEDULE_OFFSET] = ACTIVATION_QUANTIZE_SCHEDULE_OFFSET_DEFAULT
+        output[ACTIVATION_QUANTIZE_SCHEDULE_OFFSET] = ACTIVATION_QUANTIZE_SCHEDULE_OFFSET_DEFAULT
     return output
 
 
 def get_activation_quantization_different_groups(param_dict):
     output = {}
     sub_param_dict = param_dict[DIFFERENT_GROUPS]
 
     def get_params(name, group_dict):
-        assert ACTIVATION_QUANTIZE_BITS in group_dict.keys(), f"{ACTIVATION_QUANTIZE_BITS} must be specified for activation quantization group {name}"
+        assert ACTIVATION_QUANTIZE_BITS in group_dict.keys(
+        ), f"{ACTIVATION_QUANTIZE_BITS} must be specified for activation quantization group {name}"
         return group_dict
 
     for k, v in sub_param_dict.items():
         output[k] = {}
-        output[k][DIFFERENT_GROUPS_PARAMETERS] = get_params(
-            k,
-            sub_param_dict[k][DIFFERENT_GROUPS_PARAMETERS])
-        output[k][DIFFERENT_GROUPS_MODULE_SCOPE] = get_scalar_param(
-            sub_param_dict[k],
-            DIFFERENT_GROUPS_MODULE_SCOPE,
-            DIFFERENT_GROUPS_MODULE_SCOPE_DEFAULT)
+        output[k][DIFFERENT_GROUPS_PARAMETERS] = get_params(k, sub_param_dict[k][DIFFERENT_GROUPS_PARAMETERS])
+        output[k][DIFFERENT_GROUPS_MODULE_SCOPE] = get_scalar_param(sub_param_dict[k], DIFFERENT_GROUPS_MODULE_SCOPE,
+                                                                    DIFFERENT_GROUPS_MODULE_SCOPE_DEFAULT)
         output[k][DIFFERENT_GROUPS_RELATED_MODULE_SCOPE] = get_scalar_param(
-            sub_param_dict[k],
-            DIFFERENT_GROUPS_RELATED_MODULE_SCOPE,
-            DIFFERENT_GROUPS_RELATED_MODULE_SCOPE_DEFAULT)
+            sub_param_dict[k], DIFFERENT_GROUPS_RELATED_MODULE_SCOPE, DIFFERENT_GROUPS_RELATED_MODULE_SCOPE_DEFAULT)
 
     return output
 
 
 def get_sparse_pruning(param_dict):
     output = {}
     if SPARSE_PRUNING not in param_dict.keys():
         param_dict[SPARSE_PRUNING] = {SHARED_PARAMETERS: {}, DIFFERENT_GROUPS: {}}
     sub_param_dict = param_dict[SPARSE_PRUNING]
     # shared parameters
     output[SHARED_PARAMETERS] = get_sparse_pruning_shared_parameters(sub_param_dict)
     # each sub-groups
     if output[SHARED_PARAMETERS][SPARSE_PRUNING_ENABLED]:
-        assert DIFFERENT_GROUPS in sub_param_dict.keys(), f"Sparse Pruning is enabled, {DIFFERENT_GROUPS} must be specified"
+        assert DIFFERENT_GROUPS in sub_param_dict.keys(
+        ), f"Sparse Pruning is enabled, {DIFFERENT_GROUPS} must be specified"
     output[DIFFERENT_GROUPS] = get_sparse_pruning_different_groups(sub_param_dict)
     return output
 
 
 def get_sparse_pruning_shared_parameters(param_dict):
     output = {}
     if SHARED_PARAMETERS in param_dict.keys():
         sub_param_dict = param_dict[SHARED_PARAMETERS]
-        output[SPARSE_PRUNING_ENABLED] = get_scalar_param(
-            sub_param_dict,
-            SPARSE_PRUNING_ENABLED,
-            SPARSE_PRUNING_ENABLED_DEFAULT)
-        output[SPARSE_PRUNING_METHOD] = get_scalar_param(sub_param_dict,
-                                                         SPARSE_PRUNING_METHOD,
+        output[SPARSE_PRUNING_ENABLED] = get_scalar_param(sub_param_dict, SPARSE_PRUNING_ENABLED,
+                                                          SPARSE_PRUNING_ENABLED_DEFAULT)
+        output[SPARSE_PRUNING_METHOD] = get_scalar_param(sub_param_dict, SPARSE_PRUNING_METHOD,
                                                          SPARSE_PRUNING_METHOD_DEFAULT)
-        assert output[SPARSE_PRUNING_METHOD] in [SPARSE_PRUNING_METHOD_L1, SPARSE_PRUNING_METHOD_TOPK], f"Invalid sparse pruning method. Supported types: [{SPARSE_PRUNING_METHOD_L1}, {SPARSE_PRUNING_METHOD_TOPK}]"
-        output[SPARSE_PRUNING_SCHEDULE_OFFSET] = get_scalar_param(
-            sub_param_dict,
-            SPARSE_PRUNING_SCHEDULE_OFFSET,
-            SPARSE_PRUNING_SCHEDULE_OFFSET_DEFAULT)
+        assert output[SPARSE_PRUNING_METHOD] in [
+            SPARSE_PRUNING_METHOD_L1, SPARSE_PRUNING_METHOD_TOPK
+        ], f"Invalid sparse pruning method. Supported types: [{SPARSE_PRUNING_METHOD_L1}, {SPARSE_PRUNING_METHOD_TOPK}]"
+        output[SPARSE_PRUNING_SCHEDULE_OFFSET] = get_scalar_param(sub_param_dict, SPARSE_PRUNING_SCHEDULE_OFFSET,
+                                                                  SPARSE_PRUNING_SCHEDULE_OFFSET_DEFAULT)
     else:
         output[SPARSE_PRUNING_ENABLED] = SPARSE_PRUNING_ENABLED_DEFAULT
         output[SPARSE_PRUNING_METHOD] = SPARSE_PRUNING_METHOD_DEFAULT
         output[SPARSE_PRUNING_SCHEDULE_OFFSET] = SPARSE_PRUNING_SCHEDULE_OFFSET_DEFAULT
     return output
 
 
 def get_sparse_pruning_different_groups(param_dict):
     output = {}
     sub_param_dict = param_dict[DIFFERENT_GROUPS]
 
     def get_params(name, group_dict):
-        assert SPARSE_PRUNING_DENSE_RATIO in group_dict.keys(), f"{SPARSE_PRUNING_DENSE_RATIO} must be specified for sparse pruning group {name}"
+        assert SPARSE_PRUNING_DENSE_RATIO in group_dict.keys(
+        ), f"{SPARSE_PRUNING_DENSE_RATIO} must be specified for sparse pruning group {name}"
         return group_dict
 
     for k, v in sub_param_dict.items():
         output[k] = {}
-        output[k][DIFFERENT_GROUPS_PARAMETERS] = get_params(
-            k,
-            sub_param_dict[k][DIFFERENT_GROUPS_PARAMETERS])
-        output[k][DIFFERENT_GROUPS_MODULE_SCOPE] = get_scalar_param(
-            sub_param_dict[k],
-            DIFFERENT_GROUPS_MODULE_SCOPE,
-            DIFFERENT_GROUPS_MODULE_SCOPE_DEFAULT)
+        output[k][DIFFERENT_GROUPS_PARAMETERS] = get_params(k, sub_param_dict[k][DIFFERENT_GROUPS_PARAMETERS])
+        output[k][DIFFERENT_GROUPS_MODULE_SCOPE] = get_scalar_param(sub_param_dict[k], DIFFERENT_GROUPS_MODULE_SCOPE,
+                                                                    DIFFERENT_GROUPS_MODULE_SCOPE_DEFAULT)
         output[k][DIFFERENT_GROUPS_RELATED_MODULE_SCOPE] = get_scalar_param(
-            sub_param_dict[k],
-            DIFFERENT_GROUPS_RELATED_MODULE_SCOPE,
-            DIFFERENT_GROUPS_RELATED_MODULE_SCOPE_DEFAULT)
+            sub_param_dict[k], DIFFERENT_GROUPS_RELATED_MODULE_SCOPE, DIFFERENT_GROUPS_RELATED_MODULE_SCOPE_DEFAULT)
 
     return output
 
 
 def get_row_pruning(param_dict):
     output = {}
     if ROW_PRUNING not in param_dict.keys():
         param_dict[ROW_PRUNING] = {SHARED_PARAMETERS: {}, DIFFERENT_GROUPS: {}}
     sub_param_dict = param_dict[ROW_PRUNING]
     # shared parameters
     output[SHARED_PARAMETERS] = get_row_pruning_shared_parameters(sub_param_dict)
     # each sub-groups
     if output[SHARED_PARAMETERS][ROW_PRUNING_ENABLED]:
-        assert DIFFERENT_GROUPS in sub_param_dict.keys(), f"Row Pruning is enabled, {DIFFERENT_GROUPS} must be specified"
+        assert DIFFERENT_GROUPS in sub_param_dict.keys(
+        ), f"Row Pruning is enabled, {DIFFERENT_GROUPS} must be specified"
     output[DIFFERENT_GROUPS] = get_row_pruning_different_groups(sub_param_dict)
     return output
 
 
 def get_row_pruning_shared_parameters(param_dict):
     output = {}
     if SHARED_PARAMETERS in param_dict.keys():
         sub_param_dict = param_dict[SHARED_PARAMETERS]
-        output[ROW_PRUNING_ENABLED] = get_scalar_param(sub_param_dict,
-                                                       ROW_PRUNING_ENABLED,
+        output[ROW_PRUNING_ENABLED] = get_scalar_param(sub_param_dict, ROW_PRUNING_ENABLED,
                                                        ROW_PRUNING_ENABLED_DEFAULT)
-        output[ROW_PRUNING_METHOD] = get_scalar_param(sub_param_dict,
-                                                      ROW_PRUNING_METHOD,
-                                                      ROW_PRUNING_METHOD_DEFAULT)
-        assert output[ROW_PRUNING_METHOD] in [ROW_PRUNING_METHOD_L1, ROW_PRUNING_METHOD_TOPK], f"Invalid row pruning method. Supported types: [{ROW_PRUNING_METHOD_L1}, {ROW_PRUNING_METHOD_TOPK}]"
-        output[ROW_PRUNING_SCHEDULE_OFFSET] = get_scalar_param(
-            sub_param_dict,
-            ROW_PRUNING_SCHEDULE_OFFSET,
-            ROW_PRUNING_SCHEDULE_OFFSET_DEFAULT)
+        output[ROW_PRUNING_METHOD] = get_scalar_param(sub_param_dict, ROW_PRUNING_METHOD, ROW_PRUNING_METHOD_DEFAULT)
+        assert output[ROW_PRUNING_METHOD] in [
+            ROW_PRUNING_METHOD_L1, ROW_PRUNING_METHOD_TOPK
+        ], f"Invalid row pruning method. Supported types: [{ROW_PRUNING_METHOD_L1}, {ROW_PRUNING_METHOD_TOPK}]"
+        output[ROW_PRUNING_SCHEDULE_OFFSET] = get_scalar_param(sub_param_dict, ROW_PRUNING_SCHEDULE_OFFSET,
+                                                               ROW_PRUNING_SCHEDULE_OFFSET_DEFAULT)
     else:
         output[ROW_PRUNING_ENABLED] = ROW_PRUNING_ENABLED_DEFAULT
         output[ROW_PRUNING_METHOD] = ROW_PRUNING_METHOD_DEFAULT
         output[ROW_PRUNING_SCHEDULE_OFFSET] = ROW_PRUNING_SCHEDULE_OFFSET_DEFAULT
     return output
 
 
 def get_row_pruning_different_groups(param_dict):
     output = {}
     sub_param_dict = param_dict[DIFFERENT_GROUPS]
 
     def get_params(name, group_dict):
-        assert ROW_PRUNING_DENSE_RATIO in group_dict.keys(), f"{ROW_PRUNING_DENSE_RATIO} must be specified for row pruning group {name}"
+        assert ROW_PRUNING_DENSE_RATIO in group_dict.keys(
+        ), f"{ROW_PRUNING_DENSE_RATIO} must be specified for row pruning group {name}"
         return group_dict
 
     for k, v in sub_param_dict.items():
         output[k] = {}
-        output[k][DIFFERENT_GROUPS_PARAMETERS] = get_params(
-            k,
-            sub_param_dict[k][DIFFERENT_GROUPS_PARAMETERS])
-        output[k][DIFFERENT_GROUPS_MODULE_SCOPE] = get_scalar_param(
-            sub_param_dict[k],
-            DIFFERENT_GROUPS_MODULE_SCOPE,
-            DIFFERENT_GROUPS_MODULE_SCOPE_DEFAULT)
+        output[k][DIFFERENT_GROUPS_PARAMETERS] = get_params(k, sub_param_dict[k][DIFFERENT_GROUPS_PARAMETERS])
+        output[k][DIFFERENT_GROUPS_MODULE_SCOPE] = get_scalar_param(sub_param_dict[k], DIFFERENT_GROUPS_MODULE_SCOPE,
+                                                                    DIFFERENT_GROUPS_MODULE_SCOPE_DEFAULT)
         output[k][DIFFERENT_GROUPS_RELATED_MODULE_SCOPE] = get_scalar_param(
-            sub_param_dict[k],
-            DIFFERENT_GROUPS_RELATED_MODULE_SCOPE,
-            DIFFERENT_GROUPS_RELATED_MODULE_SCOPE_DEFAULT)
+            sub_param_dict[k], DIFFERENT_GROUPS_RELATED_MODULE_SCOPE, DIFFERENT_GROUPS_RELATED_MODULE_SCOPE_DEFAULT)
     return output
 
 
 def get_head_pruning(param_dict):
     output = {}
     if HEAD_PRUNING not in param_dict.keys():
         param_dict[HEAD_PRUNING] = {SHARED_PARAMETERS: {}, DIFFERENT_GROUPS: {}}
     sub_param_dict = param_dict[HEAD_PRUNING]
     # shared parameters
     output[SHARED_PARAMETERS] = get_head_pruning_shared_parameters(sub_param_dict)
     # each sub-groups
     if output[SHARED_PARAMETERS][HEAD_PRUNING_ENABLED]:
-        assert DIFFERENT_GROUPS in sub_param_dict.keys(), f"Head Pruning is enabled, {DIFFERENT_GROUPS} must be specified"
+        assert DIFFERENT_GROUPS in sub_param_dict.keys(
+        ), f"Head Pruning is enabled, {DIFFERENT_GROUPS} must be specified"
     output[DIFFERENT_GROUPS] = get_head_pruning_different_groups(sub_param_dict)
     return output
 
 
 def get_head_pruning_shared_parameters(param_dict):
     output = {}
     if SHARED_PARAMETERS in param_dict.keys():
         sub_param_dict = param_dict[SHARED_PARAMETERS]
-        output[HEAD_PRUNING_ENABLED] = get_scalar_param(sub_param_dict,
-                                                        HEAD_PRUNING_ENABLED,
+        output[HEAD_PRUNING_ENABLED] = get_scalar_param(sub_param_dict, HEAD_PRUNING_ENABLED,
                                                         HEAD_PRUNING_ENABLED_DEFAULT)
-        output[HEAD_PRUNING_METHOD] = get_scalar_param(sub_param_dict,
-                                                       HEAD_PRUNING_METHOD,
+        output[HEAD_PRUNING_METHOD] = get_scalar_param(sub_param_dict, HEAD_PRUNING_METHOD,
                                                        HEAD_PRUNING_METHOD_DEFAULT)
-        assert output[HEAD_PRUNING_METHOD] in [HEAD_PRUNING_METHOD_L1, HEAD_PRUNING_METHOD_TOPK], f"Invalid head pruning method. Supported types: [{HEAD_PRUNING_METHOD_L1}, {HEAD_PRUNING_METHOD_TOPK}]"
-        output[HEAD_PRUNING_SCHEDULE_OFFSET] = get_scalar_param(
-            sub_param_dict,
-            HEAD_PRUNING_SCHEDULE_OFFSET,
-            HEAD_PRUNING_SCHEDULE_OFFSET_DEFAULT)
+        assert output[HEAD_PRUNING_METHOD] in [
+            HEAD_PRUNING_METHOD_L1, HEAD_PRUNING_METHOD_TOPK
+        ], f"Invalid head pruning method. Supported types: [{HEAD_PRUNING_METHOD_L1}, {HEAD_PRUNING_METHOD_TOPK}]"
+        output[HEAD_PRUNING_SCHEDULE_OFFSET] = get_scalar_param(sub_param_dict, HEAD_PRUNING_SCHEDULE_OFFSET,
+                                                                HEAD_PRUNING_SCHEDULE_OFFSET_DEFAULT)
         if output[HEAD_PRUNING_ENABLED]:
-            assert HEAD_PRUNING_NUM_HEADS in sub_param_dict.keys(), f"{HEAD_PRUNING_NUM_HEADS} must be specified for head pruning"
+            assert HEAD_PRUNING_NUM_HEADS in sub_param_dict.keys(
+            ), f"{HEAD_PRUNING_NUM_HEADS} must be specified for head pruning"
             output[HEAD_PRUNING_NUM_HEADS] = sub_param_dict[HEAD_PRUNING_NUM_HEADS]
     else:
         output[HEAD_PRUNING_ENABLED] = HEAD_PRUNING_ENABLED_DEFAULT
         output[HEAD_PRUNING_METHOD] = HEAD_PRUNING_METHOD_DEFAULT
         output[HEAD_PRUNING_SCHEDULE_OFFSET] = HEAD_PRUNING_SCHEDULE_OFFSET_DEFAULT
     return output
 
 
 def get_head_pruning_different_groups(param_dict):
     output = {}
     sub_param_dict = param_dict[DIFFERENT_GROUPS]
 
     def get_params(name, group_dict):
-        assert HEAD_PRUNING_DENSE_RATIO in group_dict.keys(), f"dense_ratio must be specified for head pruning group {name}"
+        assert HEAD_PRUNING_DENSE_RATIO in group_dict.keys(
+        ), f"dense_ratio must be specified for head pruning group {name}"
         return group_dict
 
     for k, v in sub_param_dict.items():
         output[k] = {}
-        output[k][DIFFERENT_GROUPS_PARAMETERS] = get_params(
-            k,
-            sub_param_dict[k][DIFFERENT_GROUPS_PARAMETERS])
-        output[k][DIFFERENT_GROUPS_MODULE_SCOPE] = get_scalar_param(
-            sub_param_dict[k],
-            DIFFERENT_GROUPS_MODULE_SCOPE,
-            DIFFERENT_GROUPS_MODULE_SCOPE_DEFAULT)
+        output[k][DIFFERENT_GROUPS_PARAMETERS] = get_params(k, sub_param_dict[k][DIFFERENT_GROUPS_PARAMETERS])
+        output[k][DIFFERENT_GROUPS_MODULE_SCOPE] = get_scalar_param(sub_param_dict[k], DIFFERENT_GROUPS_MODULE_SCOPE,
+                                                                    DIFFERENT_GROUPS_MODULE_SCOPE_DEFAULT)
         output[k][DIFFERENT_GROUPS_RELATED_MODULE_SCOPE] = get_scalar_param(
-            sub_param_dict[k],
-            DIFFERENT_GROUPS_RELATED_MODULE_SCOPE,
-            DIFFERENT_GROUPS_RELATED_MODULE_SCOPE_DEFAULT)
+            sub_param_dict[k], DIFFERENT_GROUPS_RELATED_MODULE_SCOPE, DIFFERENT_GROUPS_RELATED_MODULE_SCOPE_DEFAULT)
     return output
 
 
 def get_channel_pruning(param_dict):
     output = {}
     if CHANNEL_PRUNING not in param_dict.keys():
         param_dict[CHANNEL_PRUNING] = {SHARED_PARAMETERS: {}, DIFFERENT_GROUPS: {}}
     sub_param_dict = param_dict[CHANNEL_PRUNING]
     # shared parameters
     output[SHARED_PARAMETERS] = get_channel_pruning_shared_parameters(sub_param_dict)
     # each sub-groups
     if output[SHARED_PARAMETERS][CHANNEL_PRUNING_ENABLED]:
-        assert DIFFERENT_GROUPS in sub_param_dict.keys(), f"Sparse Pruning is enabled, {DIFFERENT_GROUPS} must be specified"
+        assert DIFFERENT_GROUPS in sub_param_dict.keys(
+        ), f"Sparse Pruning is enabled, {DIFFERENT_GROUPS} must be specified"
     output[DIFFERENT_GROUPS] = get_channel_pruning_different_groups(sub_param_dict)
     return output
 
 
 def get_channel_pruning_shared_parameters(param_dict):
     output = {}
     if SHARED_PARAMETERS in param_dict.keys():
         sub_param_dict = param_dict[SHARED_PARAMETERS]
-        output[CHANNEL_PRUNING_ENABLED] = get_scalar_param(
-            sub_param_dict,
-            CHANNEL_PRUNING_ENABLED,
-            CHANNEL_PRUNING_ENABLED_DEFAULT)
-        output[CHANNEL_PRUNING_METHOD] = get_scalar_param(
-            sub_param_dict,
-            CHANNEL_PRUNING_METHOD,
-            CHANNEL_PRUNING_METHOD_DEFAULT)
-        assert output[CHANNEL_PRUNING_METHOD] in [CHANNEL_PRUNING_METHOD_L1, CHANNEL_PRUNING_METHOD_TOPK], f"Invalid channel pruning method. Supported types: [{CHANNEL_PRUNING_METHOD_L1}, {CHANNEL_PRUNING_METHOD_TOPK}]"
-        output[CHANNEL_PRUNING_SCHEDULE_OFFSET] = get_scalar_param(
-            sub_param_dict,
-            CHANNEL_PRUNING_SCHEDULE_OFFSET,
-            CHANNEL_PRUNING_SCHEDULE_OFFSET_DEFAULT)
+        output[CHANNEL_PRUNING_ENABLED] = get_scalar_param(sub_param_dict, CHANNEL_PRUNING_ENABLED,
+                                                           CHANNEL_PRUNING_ENABLED_DEFAULT)
+        output[CHANNEL_PRUNING_METHOD] = get_scalar_param(sub_param_dict, CHANNEL_PRUNING_METHOD,
+                                                          CHANNEL_PRUNING_METHOD_DEFAULT)
+        assert output[CHANNEL_PRUNING_METHOD] in [
+            CHANNEL_PRUNING_METHOD_L1, CHANNEL_PRUNING_METHOD_TOPK
+        ], f"Invalid channel pruning method. Supported types: [{CHANNEL_PRUNING_METHOD_L1}, {CHANNEL_PRUNING_METHOD_TOPK}]"
+        output[CHANNEL_PRUNING_SCHEDULE_OFFSET] = get_scalar_param(sub_param_dict, CHANNEL_PRUNING_SCHEDULE_OFFSET,
+                                                                   CHANNEL_PRUNING_SCHEDULE_OFFSET_DEFAULT)
     else:
         output[CHANNEL_PRUNING_ENABLED] = CHANNEL_PRUNING_ENABLED_DEFAULT
         output[CHANNEL_PRUNING_METHOD] = CHANNEL_PRUNING_METHOD_DEFAULT
         output[CHANNEL_PRUNING_SCHEDULE_OFFSET] = CHANNEL_PRUNING_SCHEDULE_OFFSET_DEFAULT
     return output
 
 
 def get_channel_pruning_different_groups(param_dict):
     output = {}
     sub_param_dict = param_dict[DIFFERENT_GROUPS]
 
     def get_params(name, group_dict):
-        assert CHANNEL_PRUNING_DENSE_RATIO in group_dict.keys(), f"{CHANNEL_PRUNING_DENSE_RATIO} must be specified for channel pruning group {name}"
+        assert CHANNEL_PRUNING_DENSE_RATIO in group_dict.keys(
+        ), f"{CHANNEL_PRUNING_DENSE_RATIO} must be specified for channel pruning group {name}"
         return group_dict
 
     for k, v in sub_param_dict.items():
         output[k] = {}
-        output[k][DIFFERENT_GROUPS_PARAMETERS] = get_params(
-            k,
-            sub_param_dict[k][DIFFERENT_GROUPS_PARAMETERS])
-        output[k][DIFFERENT_GROUPS_MODULE_SCOPE] = get_scalar_param(
-            sub_param_dict[k],
-            DIFFERENT_GROUPS_MODULE_SCOPE,
-            DIFFERENT_GROUPS_MODULE_SCOPE_DEFAULT)
+        output[k][DIFFERENT_GROUPS_PARAMETERS] = get_params(k, sub_param_dict[k][DIFFERENT_GROUPS_PARAMETERS])
+        output[k][DIFFERENT_GROUPS_MODULE_SCOPE] = get_scalar_param(sub_param_dict[k], DIFFERENT_GROUPS_MODULE_SCOPE,
+                                                                    DIFFERENT_GROUPS_MODULE_SCOPE_DEFAULT)
         output[k][DIFFERENT_GROUPS_RELATED_MODULE_SCOPE] = get_scalar_param(
-            sub_param_dict[k],
-            DIFFERENT_GROUPS_RELATED_MODULE_SCOPE,
-            DIFFERENT_GROUPS_RELATED_MODULE_SCOPE_DEFAULT)
+            sub_param_dict[k], DIFFERENT_GROUPS_RELATED_MODULE_SCOPE, DIFFERENT_GROUPS_RELATED_MODULE_SCOPE_DEFAULT)
 
     return output
```

### Comparing `deepspeed-0.8.3/deepspeed/compression/constants.py` & `deepspeed-0.9.0/deepspeed/compression/constants.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,11 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 #########################################
 # Compression Methods
 # It has several sub-components
 # #########################################
 COMPRESSION_TRAINING = "compression_training"
 SHARED_PARAMETERS = "shared_parameters"
```

### Comparing `deepspeed-0.8.3/deepspeed/compression/helper.py` & `deepspeed-0.9.0/deepspeed/compression/helper.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,8 +1,11 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import torch
 from .basic_layer import Embedding_Compress, LinearLayer_Compress, Conv2dLayer_Compress, BNLayer_Compress, ColumnParallelLinear_Compress, RowParallelLinear_Compress
 from .constants import *
 
 
 def recursive_getattr(model, module_name):
@@ -55,154 +58,124 @@
     old_module = recursive_getattr(model, module_name)
 
     need_bias = False
     if hasattr(old_module, 'bias') and old_module.bias is not None:
         need_bias = True
 
     # Initialize the new module
-    if isinstance(old_module,
-                  LinearLayer_Compress) or isinstance(old_module,
-                                                      torch.nn.Linear):
+    if isinstance(old_module, LinearLayer_Compress) or isinstance(old_module, torch.nn.Linear):
         if isinstance(old_module, LinearLayer_Compress):
             new_module = old_module
         else:
-            new_module = LinearLayer_Compress(old_module.in_features,
-                                              old_module.out_features,
-                                              bias=need_bias).to(
-                                                  device=old_module.weight.device,
-                                                  dtype=old_module.weight.dtype)
+            new_module = LinearLayer_Compress(old_module.in_features, old_module.out_features,
+                                              bias=need_bias).to(device=old_module.weight.device,
+                                                                 dtype=old_module.weight.dtype)
             new_module.weight.data = old_module.weight.data
             if need_bias:
                 new_module.bias.data = old_module.bias.data
-    elif isinstance(old_module,
-                    Conv2dLayer_Compress) or isinstance(old_module,
-                                                        torch.nn.Conv2d):
+    elif isinstance(old_module, Conv2dLayer_Compress) or isinstance(old_module, torch.nn.Conv2d):
         if isinstance(old_module, Conv2dLayer_Compress):
             new_module = old_module
         else:
             new_module = Conv2dLayer_Compress(old_module.in_channels, old_module.out_channels, old_module.kernel_size, old_module.stride, old_module.padding, \
                                             old_module.dilation, old_module.groups, need_bias, \
                                             old_module.padding_mode).to(device=old_module.weight.device, dtype=old_module.weight.dtype)
             new_module.weight.data = old_module.weight.data
             if need_bias:
                 new_module.bias.data = old_module.bias.data
     elif isinstance(old_module, torch.nn.BatchNorm2d):
-        new_module = BNLayer_Compress(old_module.num_features,
-                                      old_module.eps,
-                                      old_module.momentum,
-                                      old_module.affine,
-                                      old_module.track_running_stats).to(
-                                          old_module.weight.device,
-                                          old_module.weight.dtype)
+        new_module = BNLayer_Compress(old_module.num_features, old_module.eps, old_module.momentum, old_module.affine,
+                                      old_module.track_running_stats).to(old_module.weight.device,
+                                                                         old_module.weight.dtype)
         new_module.weight.data = old_module.weight.data
         if need_bias:
             new_module.bias.data = old_module.bias.data
         new_module.running_mean.data = old_module.running_mean.data
         new_module.running_var.data = old_module.running_var.data
-    elif isinstance(old_module,
-                    Embedding_Compress) or isinstance(old_module,
-                                                      torch.nn.Embedding):
+    elif isinstance(old_module, Embedding_Compress) or isinstance(old_module, torch.nn.Embedding):
         if isinstance(old_module, Embedding_Compress):
             new_module = old_module
         else:
             new_module = Embedding_Compress(old_module.num_embeddings, old_module.embedding_dim, old_module.padding_idx, old_module.max_norm, old_module.norm_type, \
                                         old_module.scale_grad_by_freq, old_module.sparse).to(device=old_module.weight.device, dtype=old_module.weight.dtype)
             new_module.weight.data = old_module.weight.data
-    elif mpu is not None and (isinstance(old_module,
-                                         ColumnParallelLinear_Compress)
-                              or isinstance(old_module,
-                                            mpu.ColumnParallelLinear)):
+    elif mpu is not None and (isinstance(old_module, ColumnParallelLinear_Compress)
+                              or isinstance(old_module, mpu.ColumnParallelLinear)):
         if isinstance(old_module, ColumnParallelLinear_Compress):
             new_module = old_module
         else:
-            new_module = ColumnParallelLinear_Compress(
-                mpu,
-                old_module.input_size,
-                old_module.output_size,
-                gather_output=old_module.gather_output,
-                skip_bias_add=old_module.skip_bias_add,
-                bias=need_bias).to(device=old_module.weight.device,
-                                   dtype=old_module.weight.dtype)
+            new_module = ColumnParallelLinear_Compress(mpu,
+                                                       old_module.input_size,
+                                                       old_module.output_size,
+                                                       gather_output=old_module.gather_output,
+                                                       skip_bias_add=old_module.skip_bias_add,
+                                                       bias=need_bias).to(device=old_module.weight.device,
+                                                                          dtype=old_module.weight.dtype)
             new_module.weight.data = old_module.weight.data
             if need_bias:
                 new_module.bias.data = old_module.bias.data
-    elif mpu is not None and (isinstance(old_module,
-                                         RowParallelLinear_Compress)
-                              or isinstance(old_module,
-                                            mpu.RowParallelLinear)):
+    elif mpu is not None and (isinstance(old_module, RowParallelLinear_Compress)
+                              or isinstance(old_module, mpu.RowParallelLinear)):
         if isinstance(old_module, RowParallelLinear_Compress):
             new_module = old_module
         else:
-            new_module = RowParallelLinear_Compress(
-                mpu,
-                old_module.input_size,
-                old_module.output_size,
-                input_is_parallel=old_module.input_is_parallel,
-                skip_bias_add=old_module.skip_bias_add,
-                bias=need_bias).to(device=old_module.weight.device,
-                                   dtype=old_module.weight.dtype)
+            new_module = RowParallelLinear_Compress(mpu,
+                                                    old_module.input_size,
+                                                    old_module.output_size,
+                                                    input_is_parallel=old_module.input_is_parallel,
+                                                    skip_bias_add=old_module.skip_bias_add,
+                                                    bias=need_bias).to(device=old_module.weight.device,
+                                                                       dtype=old_module.weight.dtype)
             new_module.weight.data = old_module.weight.data
             if need_bias:
                 new_module.bias.data = old_module.bias.data
     else:
         new_module = None
 
     if compression_technique is not None:
         for k, v in compression_technique.items():
             if k == SPARSE_PRUNING:
                 if v[SPARSE_PRUNING_ENABLED]:
-                    new_module.enable_sparse_pruning(v[SPARSE_PRUNING_DENSE_RATIO],
-                                                     v[SPARSE_PRUNING_METHOD])
+                    new_module.enable_sparse_pruning(v[SPARSE_PRUNING_DENSE_RATIO], v[SPARSE_PRUNING_METHOD])
             elif k == ROW_PRUNING:
                 if v[ROW_PRUNING_ENABLED]:
-                    new_module.enable_row_pruning(v[ROW_PRUNING_DENSE_RATIO],
-                                                  v[ROW_PRUNING_METHOD])
+                    new_module.enable_row_pruning(v[ROW_PRUNING_DENSE_RATIO], v[ROW_PRUNING_METHOD])
             elif k == HEAD_PRUNING:
                 if v[HEAD_PRUNING_ENABLED]:
-                    new_module.enable_head_pruning(v[HEAD_PRUNING_DENSE_RATIO],
-                                                   v[HEAD_PRUNING_METHOD],
+                    new_module.enable_head_pruning(v[HEAD_PRUNING_DENSE_RATIO], v[HEAD_PRUNING_METHOD],
                                                    v[HEAD_PRUNING_NUM_HEADS])
             elif k == ACTIVATION_QUANTIZATION:
                 if v[ACTIVATION_QUANTIZATION_ENABLED]:
-                    new_module.enable_activation_quantization(
-                        v[ACTIVATION_QUANTIZE_BITS],
-                        v[ACTIVATION_QUANTIZE_TYPE],
-                        v[ACTIVATION_QUANTIZE_RANGE])
+                    new_module.enable_activation_quantization(v[ACTIVATION_QUANTIZE_BITS], v[ACTIVATION_QUANTIZE_TYPE],
+                                                              v[ACTIVATION_QUANTIZE_RANGE])
             elif k == WEIGHT_QUANTIZATION:
                 if v[WEIGHT_QUANTIZE_ENABLED]:
-                    new_module.enable_weight_quantization(
-                        v[WEIGHT_QUANTIZE_START_BITS],
-                        v[WEIGHT_QUANTIZE_TARGET_BITS],
-                        v[WEIGHT_QUANTIZATION_PERIOD],
-                        v[WEIGHT_QUANTIZE_IN_FORWARD_ENABLED],
-                        v[WEIGHT_QUANTIZE_TYPE],
-                        v[WEIGHT_QUANTIZE_GROUPS])
+                    new_module.enable_weight_quantization(v[WEIGHT_QUANTIZE_START_BITS],
+                                                          v[WEIGHT_QUANTIZE_TARGET_BITS],
+                                                          v[WEIGHT_QUANTIZATION_PERIOD],
+                                                          v[WEIGHT_QUANTIZE_IN_FORWARD_ENABLED],
+                                                          v[WEIGHT_QUANTIZE_TYPE], v[WEIGHT_QUANTIZE_GROUPS])
             elif k == CHANNEL_PRUNING:
                 if v[CHANNEL_PRUNING_ENABLED]:
-                    new_module.enable_channel_pruning(v[CHANNEL_PRUNING_DENSE_RATIO],
-                                                      v[CHANNEL_PRUNING_METHOD])
+                    new_module.enable_channel_pruning(v[CHANNEL_PRUNING_DENSE_RATIO], v[CHANNEL_PRUNING_METHOD])
             else:
-                raise NotImplementedError(
-                    'Compression technique {} is not implemented'.format(k))
+                raise NotImplementedError('Compression technique {} is not implemented'.format(k))
 
     # Replace the old module with the new one
     recursive_setattr(model, module_name, new_module)
 
 
 def is_module_compressible(module, mpu=None):
     ret = isinstance(module, torch.nn.Linear) or \
           isinstance(module, torch.nn.Conv2d) or \
           isinstance(module, torch.nn.Embedding) or \
           isinstance(module, torch.nn.BatchNorm2d)
 
     if mpu is not None:
-        ret = ret or isinstance(module,
-                                mpu.RowParallelLinear) or isinstance(
-                                    module,
-                                    mpu.ColumnParallelLinear)
+        ret = ret or isinstance(module, mpu.RowParallelLinear) or isinstance(module, mpu.ColumnParallelLinear)
 
     return ret
 
 
 def compression_preparation(model, compression_techinique_list, mpu):
     """
     Prepare the compression techniques of a model.
@@ -221,43 +194,36 @@
         for mnl in module_name_lists:
             for module_name in mnl:
                 module_replacement(model, module_name, compression_technique)
 
     return model
 
 
-def fix_compression(model,
-                    module_name,
-                    compression_technique,
-                    mask=None,
-                    dim_reduction=False):
+def fix_compression(model, module_name, compression_technique, mask=None, dim_reduction=False):
     """
     Fix the compression technique of a module.
     Args:
         model (`torch.nn.Module`)
             The model to fix the compression technique of.
         module_name (`str`)
             The name of the module to fix the compression technique of.
         compression_technique (`str`)
             The compression technique to fix the module to.
     """
     # Here we can make things much simpler by just replacing the module
     module = recursive_getattr(model, module_name)
     for k, v in compression_technique.items():
-        if k == WEIGHT_QUANTIZATION and v[WEIGHT_QUANTIZE_IN_FORWARD_ENABLED] and v[
-                WEIGHT_QUANTIZE_ENABLED]:
+        if k == WEIGHT_QUANTIZATION and v[WEIGHT_QUANTIZE_IN_FORWARD_ENABLED] and v[WEIGHT_QUANTIZE_ENABLED]:
             return module.fix_weight_quantization()
         elif k == SPARSE_PRUNING and v[SPARSE_PRUNING_ENABLED]:
             return module.fix_sparse_pruning_helper()
         elif k == ROW_PRUNING and (v[ROW_PRUNING_ENABLED] or mask is not None):
             return module.fix_row_col_pruning_helper(mask, dim_reduction=dim_reduction)
         elif k == HEAD_PRUNING and (v[HEAD_PRUNING_ENABLED] or mask is not None):
-            return module.fix_head_pruning_helper(mask,
-                                                  v[HEAD_PRUNING_NUM_HEADS],
-                                                  dim_reduction=dim_reduction)
+            return module.fix_head_pruning_helper(mask, v[HEAD_PRUNING_NUM_HEADS], dim_reduction=dim_reduction)
         elif k == CHANNEL_PRUNING and (v[CHANNEL_PRUNING_ENABLED] or mask is not None):
             return module.fix_channel_pruning_helper(mask, dim_reduction=dim_reduction)
 
 
 def convert_conv1d_to_linear(model, convert_type):
     '''
     This is a help function to convert conv1d to linear (e.g., convert GPT2 from HF)
@@ -266,18 +232,17 @@
         c_model = model.module
     else:
         c_model = model
 
     for name, module in c_model.named_modules():
         if isinstance(module, convert_type):
             old_module = recursive_getattr(c_model, name)
-            new_module = torch.nn.Linear(
-                old_module.weight.data.size(0),
-                old_module.weight.data.size(1),
-                bias=True if old_module.bias is not None else False)
+            new_module = torch.nn.Linear(old_module.weight.data.size(0),
+                                         old_module.weight.data.size(1),
+                                         bias=True if old_module.bias is not None else False)
             new_module.weight.data = old_module.weight.data.t().contiguous()
             if new_module.bias is not None:
                 new_module.bias.data = old_module.bias.data.view(-1)
 
             recursive_setattr(c_model, name, new_module)
 
     return model
```

### Comparing `deepspeed-0.8.3/deepspeed/compression/scheduler.py` & `deepspeed-0.9.0/deepspeed/compression/scheduler.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,19 +1,23 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 from .compress import get_module_name
 from .constants import *
 from .helper import recursive_getattr
 from deepspeed.utils import logger
 
 
 class compression_scheduler():
     '''
     Used to schedule different compression methods
     '''
+
     def __init__(self, model, compression_config):
         self.model = model
         self.compression_config = compression_config
         self.make_init()
         self.training_steps = 0
         self.weight_quantization_enabled = False
 
@@ -34,30 +38,30 @@
             self.different_compression_methods[method] = {
                 TECHNIQUE_ENABLED: False,
                 SHARED_PARAMETERS: None,
                 DIFFERENT_GROUPS: []
             }
             exist_module_name = set()
             shared_parameters = method_content[SHARED_PARAMETERS]
-            self.different_compression_methods[method][
-                TECHNIQUE_ENABLED] = shared_parameters[TECHNIQUE_ENABLED]
-            self.different_compression_methods[method][
-                SHARED_PARAMETERS] = shared_parameters
+            self.different_compression_methods[method][TECHNIQUE_ENABLED] = shared_parameters[TECHNIQUE_ENABLED]
+            self.different_compression_methods[method][SHARED_PARAMETERS] = shared_parameters
 
             for group_name, method_parameters in method_content[DIFFERENT_GROUPS].items():
                 module_name_list = []
                 for key_word in method_parameters[DIFFERENT_GROUPS_MODULE_SCOPE]:
-                    module_name, exist_module_name = get_module_name(group_name, self.model, key_word, exist_module_name, verbose=False)
+                    module_name, exist_module_name = get_module_name(group_name,
+                                                                     self.model,
+                                                                     key_word,
+                                                                     exist_module_name,
+                                                                     verbose=False)
                     module_name_list.extend(module_name)
                 if module_name_list:
-                    self.different_compression_methods[method][DIFFERENT_GROUPS].append([
-                        group_name,
-                        module_name_list,
-                        method_parameters.copy().pop('params')
-                    ])
+                    self.different_compression_methods[method][DIFFERENT_GROUPS].append(
+                        [group_name, module_name_list,
+                         method_parameters.copy().pop('params')])
 
     def check_weight_quantization(self):
         # check weight quantization
         wq = self.different_compression_methods[WEIGHT_QUANTIZATION]
         if not wq[TECHNIQUE_ENABLED]:
             return
         else:
@@ -65,16 +69,15 @@
             if self.training_steps >= shared_parameters[TECHNIQUE_SCHEDULE_OFFSET]:
                 for group_name, module_name_list, method_parameters in wq[DIFFERENT_GROUPS]:
                     for module_name in module_name_list:
                         module = recursive_getattr(self.model, module_name)
                         module.weight_quantization_enabled = True
 
                 if not self.verbose[WEIGHT_QUANTIZATION]:
-                    logger.info(
-                        f'Weight quantization is enabled at step {self.training_steps}')
+                    logger.info(f'Weight quantization is enabled at step {self.training_steps}')
                     self.weight_quantization_enabled = True
                     self.verbose[WEIGHT_QUANTIZATION] = True
 
     def check_activation_quantization(self):
         # check activation quantization
         aq = self.different_compression_methods[ACTIVATION_QUANTIZATION]
         if not aq[TECHNIQUE_ENABLED]:
@@ -83,17 +86,15 @@
             shared_parameters = aq[SHARED_PARAMETERS]
             if self.training_steps >= shared_parameters[TECHNIQUE_SCHEDULE_OFFSET]:
                 for group_name, module_name_list, method_parameters in aq[DIFFERENT_GROUPS]:
                     for module_name in module_name_list:
                         module = recursive_getattr(self.model, module_name)
                         module.activation_quantization_enabled = True
                 if not self.verbose[ACTIVATION_QUANTIZATION]:
-                    logger.info(
-                        f'Activation quantization is enabled at step {self.training_steps}'
-                    )
+                    logger.info(f'Activation quantization is enabled at step {self.training_steps}')
                     self.verbose[ACTIVATION_QUANTIZATION] = True
 
     def check_sparse_pruning(self):
         # check sparse pruning
         sp = self.different_compression_methods[SPARSE_PRUNING]
         if not sp[TECHNIQUE_ENABLED]:
             return
@@ -101,16 +102,15 @@
             shared_parameters = sp[SHARED_PARAMETERS]
             if self.training_steps >= shared_parameters[TECHNIQUE_SCHEDULE_OFFSET]:
                 for group_name, module_name_list, method_parameters in sp[DIFFERENT_GROUPS]:
                     for module_name in module_name_list:
                         module = recursive_getattr(self.model, module_name)
                         module.sparse_pruning_enabled = True
                 if not self.verbose[SPARSE_PRUNING]:
-                    logger.info(
-                        f'Sparse pruning is enabled at step {self.training_steps}')
+                    logger.info(f'Sparse pruning is enabled at step {self.training_steps}')
                     self.verbose[SPARSE_PRUNING] = True
 
     def check_head_pruning(self):
         # check head pruning
         hp = self.different_compression_methods[HEAD_PRUNING]
         if not hp[TECHNIQUE_ENABLED]:
             return
@@ -150,16 +150,15 @@
             shared_parameters = cp[SHARED_PARAMETERS]
             if self.training_steps >= shared_parameters[TECHNIQUE_SCHEDULE_OFFSET]:
                 for group_name, module_name_list, method_parameters in cp[DIFFERENT_GROUPS]:
                     for module_name in module_name_list:
                         module = recursive_getattr(self.model, module_name)
                         module.channel_pruning_enabled = True
                 if not self.verbose[CHANNEL_PRUNING]:
-                    logger.info(
-                        f'Channel pruning is enabled at step {self.training_steps}')
+                    logger.info(f'Channel pruning is enabled at step {self.training_steps}')
                     self.verbose[CHANNEL_PRUNING] = True
 
     def check_all_modules(self):
         # check all different compression methods we have
         self.check_weight_quantization()
         self.check_activation_quantization()
         self.check_sparse_pruning()
```

### Comparing `deepspeed-0.8.3/deepspeed/compression/utils.py` & `deepspeed-0.9.0/deepspeed/compression/utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,22 +1,26 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import torch
 from torch import autograd
 import math
 
 
 class TopKBinarizer(autograd.Function):
     """
     Top-k Binarizer.
     Computes a binary mask M from a real value matrix S such that `M_{i,j} = 1` if and only if `S_{i,j}`
     is among the k% highest values of S.
     Implementation is inspired from:
         https://github.com/yaozhewei/MLPruning
     """
+
     @staticmethod
     def forward(ctx, inputs: torch.tensor, threshold: float, sigmoid: bool):
         """
         Args:
             inputs (`torch.FloatTensor`)
                 The input matrix from which the binarizer computes the binary mask.
             threshold (`float`)
@@ -55,14 +59,15 @@
             return gradOutput.clone(), None, None
 
 
 class SymQuantizer(torch.autograd.Function):
     """
     Symmetric quantization
     """
+
     @staticmethod
     def forward(ctx, input, num_bits, min_value=None, max_value=None, num_groups=1):
         """
         Args:
             inputs (`torch.FloatTensor`)
                 The input which needs to be quantized
             num_bits (int, >=4)
@@ -71,17 +76,16 @@
                 Used for static activation quantization
             num_groups (int)
                 How many groups to partition the quantization into
         Returns:
             quantized_input (`torch.FloatTensor`)
                 Quantized input
         """
-        assert (min_value is None
-                and max_value is None) or (min_value is not None
-                                           and max_value is not None and num_groups == 1)
+        assert (min_value is None and max_value is None) or (min_value is not None and max_value is not None
+                                                             and num_groups == 1)
         q_range = 2**num_bits
         input_shape = input.shape
         if min_value is None:
             input = input.reshape(num_groups, -1)
             max_input = torch.amax(torch.abs(input), dim=-1).view(num_groups, -1)
         else:
             max_input = torch.max(min_value.abs(), max_value).view(-1)
@@ -97,14 +101,15 @@
         return grad_input, None, None, None, None
 
 
 class AsymQuantizer(torch.autograd.Function):
     """
     Asymmetric quantization
     """
+
     @staticmethod
     def forward(ctx, input, num_bits, min_value=None, max_value=None, num_groups=1):
         """
         Args:
             inputs (`torch.FloatTensor`)
                 The input which needs to be quantized
             num_bits (int, >=4)
@@ -114,43 +119,41 @@
             num_groups (int)
                 How many groups to partition the quantization into
         Returns:
             quantized_input (`torch.FloatTensor`)
                 Quantized input
         """
 
-        assert (min_value is None
-                and max_value is None) or (min_value is not None
-                                           and max_value is not None and num_groups == 1)
+        assert (min_value is None and max_value is None) or (min_value is not None and max_value is not None
+                                                             and num_groups == 1)
         q_range = 2**num_bits
         input_shape = input.shape
         if min_value is None:
             input = input.reshape(num_groups, -1)
             min_value = input.amin(dim=-1, keepdim=True)
             max_value = input.amax(dim=-1, keepdim=True)
 
         scale = (max_value - min_value) / q_range
         zero_point = (min_value / scale).round() * scale
 
-        output = (
-            (input - zero_point) / scale).round().clamp(0,
-                                                        q_range - 1) * scale + zero_point
+        output = ((input - zero_point) / scale).round().clamp(0, q_range - 1) * scale + zero_point
         output = output.reshape(input_shape).contiguous()
         return output
 
     @staticmethod
     def backward(ctx, grad_output):
         grad_input = grad_output.clone()
         return grad_input, None, None, None, None
 
 
 class TernaryQuantizer(torch.autograd.Function):
     """
     Ternary quantization
     """
+
     @staticmethod
     def forward(ctx, input, num_bits, min_value=None, max_value=None, num_groups=1):
         """
         Args:
             inputs (`torch.FloatTensor`)
                 The input which needs to be quantized
             num_bits (int)
@@ -183,14 +186,15 @@
         return grad_input, None, None, None, None
 
 
 class BinaryQuantizer(torch.autograd.Function):
     """
     Binary quantization
     """
+
     @staticmethod
     def forward(ctx, input, num_bits, min_value=None, max_value=None, num_groups=1):
         """
         Args:
             inputs (`torch.FloatTensor`)
                 The input which needs to be quantized
             num_bits (int)
```

### Comparing `deepspeed-0.8.3/deepspeed/constants.py` & `deepspeed-0.9.0/deepspeed/constants.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,12 @@
-'''
-Copyright 2020 The Microsoft DeepSpeed Team
-'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+
 from datetime import timedelta
 
 #############################################
 # Torch distributed constants
 #############################################
 TORCH_DISTRIBUTED_DEFAULT_PORT = 29500
```

### Comparing `deepspeed-0.8.3/deepspeed/elasticity/config.py` & `deepspeed-0.9.0/deepspeed/elasticity/config.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-"""
-Copyright 2020 The Microsoft DeepSpeed Team
-"""
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import json
 from .constants import *
 
 
 class ElasticityError(Exception):
     """
@@ -39,84 +40,71 @@
         "min_gpus": 1,
         "max_gpus" : 10000
         "min_time": 20
         "ignore_non_elastic_batch_info": false
         "version": 0.1
     }
     """
+
     def __init__(self, param_dict):
         self.enabled = param_dict.get(ENABLED, ENABLED_DEFAULT)
         if self.enabled:
             if MAX_ACCEPTABLE_BATCH_SIZE in param_dict:
                 self.max_acceptable_batch_size = param_dict[MAX_ACCEPTABLE_BATCH_SIZE]
             else:
-                raise ElasticityConfigError(
-                    f"Elasticity config missing {MAX_ACCEPTABLE_BATCH_SIZE}")
+                raise ElasticityConfigError(f"Elasticity config missing {MAX_ACCEPTABLE_BATCH_SIZE}")
             if MICRO_BATCHES in param_dict:
                 self.micro_batches = param_dict[MICRO_BATCHES]
             else:
                 raise ElasticityConfigError(f"Elasticity config missing {MICRO_BATCHES}")
         else:
-            self.max_acceptable_batch_size = param_dict.get(
-                MAX_ACCEPTABLE_BATCH_SIZE,
-                MAX_ACCEPTABLE_BATCH_SIZE_DEFAULT)
+            self.max_acceptable_batch_size = param_dict.get(MAX_ACCEPTABLE_BATCH_SIZE,
+                                                            MAX_ACCEPTABLE_BATCH_SIZE_DEFAULT)
             self.micro_batches = param_dict.get(MICRO_BATCHES, MICRO_BATCHES_DEFAULT)
 
         if not isinstance(self.micro_batches, list):
             raise ElasticityConfigError(
                 f"Elasticity expected value of {MICRO_BATCHES} to be a "
-                f"list of micro batches, instead is: {type(self.micro_batches)}, containing: {self.micro_batches}"
-            )
+                f"list of micro batches, instead is: {type(self.micro_batches)}, containing: {self.micro_batches}")
 
         if not all(map(lambda m: isinstance(m, int), self.micro_batches)):
-            raise ElasticityConfigError(
-                f"Elasticity expected {MICRO_BATCHES} to only contain a list of integers, "
-                f"instead contains: f{self.micro_batches}")
+            raise ElasticityConfigError(f"Elasticity expected {MICRO_BATCHES} to only contain a list of integers, "
+                                        f"instead contains: f{self.micro_batches}")
 
         if not all(map(lambda m: m > 0, self.micro_batches)):
-            raise ElasticityConfigError(
-                f"Elasticity expected {MICRO_BATCHES} to only contain positive integers, "
-                f"instead contains: f{self.micro_batches}")
+            raise ElasticityConfigError(f"Elasticity expected {MICRO_BATCHES} to only contain positive integers, "
+                                        f"instead contains: f{self.micro_batches}")
 
         self.min_gpus = param_dict.get(MIN_GPUS, MIN_GPUS_DEFAULT)
         self.max_gpus = param_dict.get(MAX_GPUS, MAX_GPUS_DEFAULT)
 
         if self.min_gpus < 1 or self.max_gpus < 1:
-            raise ElasticityConfigError(
-                "Elasticity min/max gpus must be > 0, "
-                f"given min_gpus: {self.min_gpus}, max_gpus: {self.max_gpus}")
+            raise ElasticityConfigError("Elasticity min/max gpus must be > 0, "
+                                        f"given min_gpus: {self.min_gpus}, max_gpus: {self.max_gpus}")
         if self.max_gpus < self.min_gpus:
-            raise ElasticityConfigError(
-                "Elasticity min_gpus cannot be greater than max_gpus, "
-                f"given min_gpus: {self.min_gpus}, max_gpus: {self.max_gpus}")
+            raise ElasticityConfigError("Elasticity min_gpus cannot be greater than max_gpus, "
+                                        f"given min_gpus: {self.min_gpus}, max_gpus: {self.max_gpus}")
 
-        self.model_parallel_size = param_dict.get(MODEL_PARLLEL_SIZE,
-                                                  MODEL_PARLLEL_SIZE_DEFAULT)
+        self.model_parallel_size = param_dict.get(MODEL_PARLLEL_SIZE, MODEL_PARLLEL_SIZE_DEFAULT)
         if self.model_parallel_size < 1:
-            raise ElasticityConfigError(
-                "Model-Parallel size cannot be less than 1, "
-                f"given model-parallel size: {self.model_parallel_size}")
+            raise ElasticityConfigError("Model-Parallel size cannot be less than 1, "
+                                        f"given model-parallel size: {self.model_parallel_size}")
 
-        self.num_gpus_per_node = param_dict.get(NUM_GPUS_PER_NODE,
-                                                NUM_GPUS_PER_NODE_DEFAULT)
+        self.num_gpus_per_node = param_dict.get(NUM_GPUS_PER_NODE, NUM_GPUS_PER_NODE_DEFAULT)
         if self.num_gpus_per_node < 1:
-            raise ElasticityConfigError(
-                "Number of GPUs per node cannot be less than 1, "
-                f"given number of GPUs per node: {self.num_gpus_per_node}")
+            raise ElasticityConfigError("Number of GPUs per node cannot be less than 1, "
+                                        f"given number of GPUs per node: {self.num_gpus_per_node}")
 
         self.min_time = param_dict.get(MIN_TIME, MIN_TIME_DEFAULT)
         if self.min_time < 0:
-            raise ElasticityConfigError(
-                f"Elasticity min time needs to be >= 0: given {self.min_time}")
+            raise ElasticityConfigError(f"Elasticity min time needs to be >= 0: given {self.min_time}")
 
         self.version = param_dict.get(VERSION, VERSION_DEFAULT)
-        self.prefer_larger_batch_size = param_dict.get(PREFER_LARGER_BATCH,
-                                                       PREFER_LARGER_BATCH_DEFAULT)
-        self.ignore_non_elastic_batch_info = param_dict.get(
-            IGNORE_NON_ELASTIC_BATCH_INFO,
-            IGNORE_NON_ELASTIC_BATCH_INFO_DEFAULT)
+        self.prefer_larger_batch_size = param_dict.get(PREFER_LARGER_BATCH, PREFER_LARGER_BATCH_DEFAULT)
+        self.ignore_non_elastic_batch_info = param_dict.get(IGNORE_NON_ELASTIC_BATCH_INFO,
+                                                            IGNORE_NON_ELASTIC_BATCH_INFO_DEFAULT)
 
     def repr(self):
         return self.__dict__
 
     def __repr__(self):
         return json.dumps(self.__dict__, sort_keys=True, indent=4)
```

### Comparing `deepspeed-0.8.3/deepspeed/elasticity/constants.py` & `deepspeed-0.9.0/deepspeed/elasticity/constants.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-"""
-Copyright 2020 The Microsoft DeepSpeed Team
-"""
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 #########################################
 # Elasticity
 #########################################
 ''' Elasticity Utility in DeepSpeed can be used to create highly elastic jobs compatible
 with a large number of GPUs. For elastic jobs, DeepSpeed will provide a batch size that
 can support a large number of GPUs based on the user specified parameters
```

### Comparing `deepspeed-0.8.3/deepspeed/elasticity/elastic_agent.py` & `deepspeed-0.9.0/deepspeed/elasticity/elastic_agent.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,8 +1,11 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 from torch.distributed.elastic.agent.server.local_elastic_agent import LocalElasticAgent
 from typing import Any, Dict, Optional, Tuple
 from datetime import datetime
 from torch.distributed.elastic.agent.server.api import log, _get_socket_with_port
 from torch.distributed.elastic.metrics import put_metric
 from torch.distributed.elastic.agent.server.api import (
@@ -19,29 +22,28 @@
 import shutil
 import copy
 from contextlib import closing
 import subprocess
 
 
 class DSElasticAgent(LocalElasticAgent):
+
     def __init__(
         self,
         spec: WorkerSpec,
         env: Dict,
         start_method="spawn",
         exit_barrier_timeout: float = 300,
         log_dir: Optional[str] = None,
     ):
         super().__init__(spec, start_method, exit_barrier_timeout, log_dir)
         self.ds_env = env
 
     @staticmethod
-    def _set_master_addr_port(store: Store,
-                              master_addr: Optional[str],
-                              master_port: Optional[int]):
+    def _set_master_addr_port(store: Store, master_addr: Optional[str], master_port: Optional[int]):
         if master_port is None:
             sock = _get_socket_with_port()
             with closing(sock):
                 master_port = sock.getsockname()[1]
 
         if master_addr is None:
             # master_addr = _get_fq_hostname()
@@ -78,16 +80,15 @@
                 "ROLE_WORLD_SIZE": str(worker.role_world_size),
                 "MASTER_ADDR": master_addr,
                 "MASTER_PORT": str(master_port),
                 "TORCHELASTIC_RESTART_COUNT": str(restart_count),
                 "TORCHELASTIC_MAX_RESTARTS": str(spec.max_restarts),
                 "TORCHELASTIC_RUN_ID": spec.rdzv_handler.get_run_id(),
                 "TORCHELASTIC_USE_AGENT_STORE": str(use_agent_store),
-                "NCCL_ASYNC_ERROR_HANDLING": os.getenv("NCCL_ASYNC_ERROR_HANDLING",
-                                                       str(1)),
+                "NCCL_ASYNC_ERROR_HANDLING": os.getenv("NCCL_ASYNC_ERROR_HANDLING", str(1)),
             }
             worker_env_ds.update(worker_env_elastic)
             if "OMP_NUM_THREADS" in os.environ:
                 worker_env_ds["OMP_NUM_THREADS"] = os.environ["OMP_NUM_THREADS"]
 
             envs[local_rank] = worker_env_ds
             worker_args = list(spec.args)
@@ -116,59 +117,50 @@
 
     def _invoke_run(self, role: str = "default") -> RunResult:
         # NOTE: currently only works for a single role
 
         spec = self._worker_group.spec
         role = spec.role
 
-        log.info(
-            f"[{role}] starting workers for entrypoint: {spec.get_entrypoint_name()}")
+        log.info(f"[{role}] starting workers for entrypoint: {spec.get_entrypoint_name()}")
 
         self._initialize_workers(self._worker_group)
         monitor_interval = spec.monitor_interval
         rdzv_handler = spec.rdzv_handler
 
         participants = rdzv_handler._state_holder.state.participants
 
         while True:
             assert self._worker_group.state != WorkerState.INIT
             time.sleep(monitor_interval)
             run_result = self._monitor_workers(self._worker_group)
             state = run_result.state
             self._worker_group.state = state
 
-            expire_time = datetime.utcnow() - (
-                rdzv_handler._settings.keep_alive_interval *
-                rdzv_handler._settings.keep_alive_max_attempt)
+            expire_time = datetime.utcnow() - (rdzv_handler._settings.keep_alive_interval *
+                                               rdzv_handler._settings.keep_alive_max_attempt)
             _dead_nodes = [
-                node for node,
-                last_heartbeat in
-                rdzv_handler._state_holder.state.last_heartbeats.items()
+                node for node, last_heartbeat in rdzv_handler._state_holder.state.last_heartbeats.items()
                 if last_heartbeat < expire_time
             ]
 
             put_metric(f"workers.{role}.remaining_restarts", self._remaining_restarts)
             put_metric(f"workers.{role}.{state.name.lower()}", 1)
 
             if state == WorkerState.SUCCEEDED:
-                log.info(
-                    f"[{role}] worker group successfully finished."
-                    f" Waiting {self._exit_barrier_timeout} seconds for other agents to finish."
-                )
+                log.info(f"[{role}] worker group successfully finished."
+                         f" Waiting {self._exit_barrier_timeout} seconds for other agents to finish.")
                 self._exit_barrier()
                 return run_result
-            elif state in {
-                    WorkerState.UNHEALTHY,
-                    WorkerState.FAILED
-            } or len(participants) > len(rdzv_handler._state_holder.state.participants):
+            elif state in {WorkerState.UNHEALTHY, WorkerState.FAILED
+                           } or len(participants) > len(rdzv_handler._state_holder.state.participants):
                 if self._remaining_restarts > 0:
-                    log.info(
-                        f"[{role}] Worker group {state.name}. "
-                        f"{self._remaining_restarts}/{spec.max_restarts} attempts left;"
-                        f" will restart worker group")
+                    log.info(f"[{role}] Worker group {state.name}. "
+                             f"{self._remaining_restarts}/{spec.max_restarts} attempts left;"
+                             f" will restart worker group")
                     self._remaining_restarts -= 1
                     # rdzv_handler._state_holder.state.restart = False
                     self._restart_workers(self._worker_group)
                     participants = rdzv_handler._state_holder.state.participants
 
                 else:
                     self._stop_workers(self._worker_group)
```

### Comparing `deepspeed-0.8.3/deepspeed/elasticity/elasticity.py` & `deepspeed-0.9.0/deepspeed/elasticity/elasticity.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,12 @@
-"""
-Copyright 2020 The Microsoft DeepSpeed Team
-"""
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+
 import os
 import json
 import numpy as np
 import math
 from packaging import version as pkg_version
 
 from .config import ElasticityConfig, ElasticityConfigError, ElasticityError, \
@@ -13,52 +15,16 @@
     MINIMUM_DEEPSPEED_VERSION, DEEPSPEED_ELASTICITY_CONFIG
 from ..git_version_info import version as __version__
 from ..utils import logger
 
 # Thirty eight smallest highly composite numbers. The list should
 # be enough to support up to 720K batch size.
 HCN_LIST = [
-    1,
-    2,
-    4,
-    6,
-    12,
-    24,
-    36,
-    48,
-    60,
-    120,
-    180,
-    240,
-    360,
-    720,
-    840,
-    1260,
-    1680,
-    2520,
-    5040,
-    7560,
-    10080,
-    15120,
-    20160,
-    25200,
-    27720,
-    45360,
-    50400,
-    55440,
-    83160,
-    110880,
-    166320,
-    221760,
-    277200,
-    332640,
-    498960,
-    554400,
-    665280,
-    720720
+    1, 2, 4, 6, 12, 24, 36, 48, 60, 120, 180, 240, 360, 720, 840, 1260, 1680, 2520, 5040, 7560, 10080, 15120, 20160,
+    25200, 27720, 45360, 50400, 55440, 83160, 110880, 166320, 221760, 277200, 332640, 498960, 554400, 665280, 720720
 ]
 
 
 def get_candidate_batch_sizes(base_list, max_acceptable_batch_size):
     candidate_batch_size = []
     for base in base_list:
         if base >= max_acceptable_batch_size:
@@ -90,35 +56,27 @@
                 if max_gpus % i == 0:
                     valid_gpus.append(i)
     valid_gpus = set(valid_gpus)
     valid_gpus = sorted(list(valid_gpus))
     return valid_gpus
 
 
-def get_best_candidates(candidate_batch_sizes,
-                        micro_batches,
-                        min_gpus,
-                        max_gpus,
-                        prefer_larger):
+def get_best_candidates(candidate_batch_sizes, micro_batches, min_gpus, max_gpus, prefer_larger):
 
     max_valid_gpus = 0
     valid_gpus = None
     final_batch_size = int(min(micro_batches))
 
     for batch_size in candidate_batch_sizes:
 
-        current_valid_gpus = get_valid_gpus(batch_size,
-                                            micro_batches,
-                                            min_gpus,
-                                            max_gpus)
-
-        if (len(current_valid_gpus) > max_valid_gpus
-                or (len(current_valid_gpus) == max_valid_gpus and
-                    ((prefer_larger and batch_size > final_batch_size) or
-                     (not prefer_larger and batch_size < final_batch_size)))):
+        current_valid_gpus = get_valid_gpus(batch_size, micro_batches, min_gpus, max_gpus)
+
+        if (len(current_valid_gpus) > max_valid_gpus or (len(current_valid_gpus) == max_valid_gpus and
+                                                         ((prefer_larger and batch_size > final_batch_size) or
+                                                          (not prefer_larger and batch_size < final_batch_size)))):
             max_valid_gpus = len(current_valid_gpus)
             valid_gpus = current_valid_gpus
             final_batch_size = batch_size
 
     return final_batch_size, valid_gpus
 
 
@@ -153,23 +111,18 @@
 
     lcm = np.lcm.reduce(micro_batches)
 
     base_list = []
     base_list.extend(micro_batches)
     base_list.append(lcm)
 
-    candidate_batch_sizes = get_candidate_batch_sizes(base_list,
-                                                      max_acceptable_batch_size)
+    candidate_batch_sizes = get_candidate_batch_sizes(base_list, max_acceptable_batch_size)
 
-    final_batch_size, valid_gpus = get_best_candidates(
-        candidate_batch_sizes,
-        micro_batches,
-        min_gpus,
-        max_gpus,
-        prefer_larger)
+    final_batch_size, valid_gpus = get_best_candidates(candidate_batch_sizes, micro_batches, min_gpus, max_gpus,
+                                                       prefer_larger)
 
     return final_batch_size, valid_gpus
 
 
 def _get_compatible_gpus_v02(micro_batches,
                              max_acceptable_batch_size,
                              current_num_gpus,
@@ -199,19 +152,20 @@
                     candidate_microbatch = micro_batch
                 if prefer_larger and candidate_microbatch < micro_batch:
                     candidate_microbatch = micro_batch
         return candidate_microbatch
 
     dp_size_per_node = num_gpus_per_node // model_parallel_size
 
-    final_batch_size, valid_world_size = _get_compatible_gpus_v01(micro_batches,
-                             int(max_acceptable_batch_size/dp_size_per_node),
-                             int(min_gpus/num_gpus_per_node),
-                             int(max_gpus/num_gpus_per_node), # Passing number of max nodes as Elasticity v2 works at node level
-                             prefer_larger=prefer_larger)
+    final_batch_size, valid_world_size = _get_compatible_gpus_v01(
+        micro_batches,
+        int(max_acceptable_batch_size / dp_size_per_node),
+        int(min_gpus / num_gpus_per_node),
+        int(max_gpus / num_gpus_per_node),  # Passing number of max nodes as Elasticity v2 works at node level
+        prefer_larger=prefer_larger)
 
     final_batch_size = int(final_batch_size) * dp_size_per_node
     valid_dp_world_size = [i * dp_size_per_node for i in valid_world_size]
     if current_num_gpus // model_parallel_size in valid_dp_world_size:
         candidate_microbatch = get_microbatch(final_batch_size)
         return final_batch_size, valid_dp_world_size, candidate_microbatch
 
@@ -252,46 +206,35 @@
 
 
 def ensure_immutable_elastic_config(runtime_elastic_config_dict: dict):
     """
     Ensure the resource scheduler saw the same elastic config we are using at runtime
     """
     if DEEPSPEED_ELASTICITY_CONFIG in os.environ:
-        scheduler_elastic_config_dict = json.loads(
-            os.environ[DEEPSPEED_ELASTICITY_CONFIG])
+        scheduler_elastic_config_dict = json.loads(os.environ[DEEPSPEED_ELASTICITY_CONFIG])
         scheduler_elastic_config = ElasticityConfig(scheduler_elastic_config_dict)
         runtime_elastic_config = ElasticityConfig(runtime_elastic_config_dict)
         err_str = "Elastic config '{}={}' seen by resource scheduler does not match config passed to runtime {}={}"
         if runtime_elastic_config.max_acceptable_batch_size != scheduler_elastic_config.max_acceptable_batch_size:
             raise ElasticityConfigError(
-                err_str.format('max_acceptable_batch_size',
-                               scheduler_elastic_config.max_acceptable_batch_size,
-                               'max_acceptable_batch_size',
-                               runtime_elastic_config.max_acceptable_batch_size))
+                err_str.format('max_acceptable_batch_size', scheduler_elastic_config.max_acceptable_batch_size,
+                               'max_acceptable_batch_size', runtime_elastic_config.max_acceptable_batch_size))
         if runtime_elastic_config.micro_batches != scheduler_elastic_config.micro_batches:
             raise ElasticityConfigError(
-                err_str.format('micro_batches',
-                               scheduler_elastic_config.micro_batches,
-                               'micro_batches',
+                err_str.format('micro_batches', scheduler_elastic_config.micro_batches, 'micro_batches',
                                runtime_elastic_config.micro_batches))
         if runtime_elastic_config.version != scheduler_elastic_config.version:
             raise ElasticityConfigError(
-                err_str.format('version',
-                               scheduler_elastic_config.version,
-                               'version',
-                               runtime_elastic_config.version))
+                err_str.format('version', scheduler_elastic_config.version, 'version', runtime_elastic_config.version))
     else:
         logger.warning("Unable to find DEEPSPEED_ELASTICITY_CONFIG environment variable, cannot " \
             "guarantee resource scheduler will scale this job using compatible GPU counts.")
 
 
-def compute_elastic_config(ds_config: dict,
-                           target_deepspeed_version: str,
-                           world_size=0,
-                           return_microbatch=False):
+def compute_elastic_config(ds_config: dict, target_deepspeed_version: str, world_size=0, return_microbatch=False):
     """Core deepspeed elasticity API. Given an elastic config (similar to the example below)
     DeepSpeed will compute a total train batch size corresponding valid GPU count list that
     provides a high level of elasticity. Elasticity in this case means we are safe to scale
     the training job up/down across the GPU count list *without* any negative impacts on
     training convergence. This is achievable primarily due to DeepSpeed's gradient accumulation
     feature which allows us to decompose a global training batch size into:
     micro-batch-size * gradient-accumulation-steps * world-size.
@@ -393,16 +336,15 @@
             max_gpus=elastic_config.max_gpus,
             prefer_larger=elastic_config.prefer_larger_batch_size,
             num_gpus_per_node=num_gpus_per_node,
             model_parallel_size=model_parallel_size)
         # ensure batch size is int dtype
         final_batch_size = int(final_batch_size)
     else:
-        raise NotImplementedError(
-            f"Unable to find elastic logic for version: {elastic_config.version}")
+        raise NotImplementedError(f"Unable to find elastic logic for version: {elastic_config.version}")
 
     logger.info(f"Valid World Size (GPUs / Model Parallel Size): {valid_gpus}")
 
     if world_size > 0:
         if world_size not in valid_gpus:
             raise ElasticityIncompatibleWorldSize(f"World size ({world_size}) is not valid " \
         f"with the current list of valid GPU counts: {valid_gpus}")
```

### Comparing `deepspeed-0.8.3/deepspeed/env_report.py` & `deepspeed-0.9.0/deepspeed/env_report.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,8 +1,11 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import torch
 import deepspeed
 import subprocess
 import argparse
 from .ops.op_builder.all_ops import ALL_OPS
 from .git_version_info import installed_ops, torch_info
@@ -44,16 +47,15 @@
     print("-" * (max_dots + max_dots2 + len(h[0]) + len(h[1])))
     installed = f"{GREEN}[YES]{END}"
     no = f"{YELLOW}[NO]{END}"
     for op_name, builder in ALL_OPS.items():
         dots = "." * (max_dots - len(op_name))
         is_compatible = OKAY if builder.is_compatible(verbose) else no
         is_installed = installed if installed_ops[op_name] else no
-        dots2 = '.' * ((len(h[1]) + (max_dots2 - len(h[1]))) -
-                       (len(is_installed) - color_len))
+        dots2 = '.' * ((len(h[1]) + (max_dots2 - len(h[1]))) - (len(is_installed) - color_len))
         print(op_name, dots, is_installed, dots2, is_compatible)
     print("-" * (max_dots + max_dots2 + len(h[0]) + len(h[1])))
 
 
 def ninja_installed():
     try:
         import ninja  # noqa: F401
@@ -64,82 +66,60 @@
 
 def nvcc_version():
     import torch.utils.cpp_extension
     cuda_home = torch.utils.cpp_extension.CUDA_HOME
     if cuda_home is None:
         return f"{RED} [FAIL] cannot find CUDA_HOME via torch.utils.cpp_extension.CUDA_HOME={torch.utils.cpp_extension.CUDA_HOME} {END}"
     try:
-        output = subprocess.check_output([cuda_home + "/bin/nvcc",
-                                          "-V"],
-                                         universal_newlines=True)
+        output = subprocess.check_output([cuda_home + "/bin/nvcc", "-V"], universal_newlines=True)
     except FileNotFoundError:
         return f"{RED} [FAIL] nvcc missing {END}"
     output_split = output.split()
     release_idx = output_split.index("release")
     release = output_split[release_idx + 1].replace(',', '').split(".")
     return ".".join(release)
 
 
 def debug_report():
     max_dots = 33
 
-    report = [
-        ("torch install path",
-         torch.__path__),
-        ("torch version",
-         torch.__version__),
-        ("deepspeed install path",
-         deepspeed.__path__),
-        ("deepspeed info",
-         f"{deepspeed.__version__}, {deepspeed.__git_hash__}, {deepspeed.__git_branch__}"
-         )
-    ]
+    report = [("torch install path", torch.__path__), ("torch version", torch.__version__),
+              ("deepspeed install path", deepspeed.__path__),
+              ("deepspeed info", f"{deepspeed.__version__}, {deepspeed.__git_hash__}, {deepspeed.__git_branch__}")]
     if get_accelerator().device_name() == 'cuda':
         hip_version = getattr(torch.version, "hip", None)
-        report.extend([("torch cuda version",
-                        torch.version.cuda),
-                       ("torch hip version",
-                        hip_version),
-                       ("nvcc version",
-                        (None if hip_version else nvcc_version())),
-                       ("deepspeed wheel compiled w.",
-                        f"torch {torch_info['version']}, " +
-                        (f"hip {torch_info['hip_version']}"
-                         if hip_version else f"cuda {torch_info['cuda_version']}"))])
+        report.extend([("torch cuda version", torch.version.cuda), ("torch hip version", hip_version),
+                       ("nvcc version", (None if hip_version else nvcc_version())),
+                       ("deepspeed wheel compiled w.", f"torch {torch_info['version']}, " +
+                        (f"hip {torch_info['hip_version']}" if hip_version else f"cuda {torch_info['cuda_version']}"))
+                       ])
     else:
-        report.extend([("deepspeed wheel compiled w.",
-                        f"torch {torch_info['version']} ")])
+        report.extend([("deepspeed wheel compiled w.", f"torch {torch_info['version']} ")])
 
     print("DeepSpeed general environment info:")
     for name, value in report:
         print(name, "." * (max_dots - len(name)), value)
 
 
 def parse_arguments():
     parser = argparse.ArgumentParser()
-    parser.add_argument(
-        '--hide_operator_status',
-        action='store_true',
-        help=
-        'Suppress display of installation and compatibility statuses of DeepSpeed operators. '
-    )
-    parser.add_argument('--hide_errors_and_warnings',
+    parser.add_argument('--hide_operator_status',
                         action='store_true',
-                        help='Suppress warning and error messages.')
+                        help='Suppress display of installation and compatibility statuses of DeepSpeed operators. ')
+    parser.add_argument('--hide_errors_and_warnings', action='store_true', help='Suppress warning and error messages.')
     args = parser.parse_args()
     return args
 
 
 def main(hide_operator_status=False, hide_errors_and_warnings=False):
     if not hide_operator_status:
         op_report(verbose=not hide_errors_and_warnings)
     debug_report()
 
 
 def cli_main():
     args = parse_arguments()
-    main(hide_operator_status=args.hide_operator_status,
-         hide_errors_and_warnings=args.hide_errors_and_warnings)
+    main(hide_operator_status=args.hide_operator_status, hide_errors_and_warnings=args.hide_errors_and_warnings)
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `deepspeed-0.8.3/deepspeed/git_version_info_installed.py` & `deepspeed-0.9.0/deepspeed/git_version_info_installed.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,6 +1,6 @@
-version='0.8.3'
+version='0.9.0'
 git_hash='unknown'
 git_branch='unknown'
 installed_ops={'async_io': False, 'cpu_adagrad': False, 'cpu_adam': False, 'fused_adam': False, 'fused_lamb': False, 'quantizer': False, 'random_ltd': False, 'sparse_attn': False, 'spatial_inference': False, 'transformer': False, 'stochastic_transformer': False, 'transformer_inference': False, 'utils': False}
 compatible_ops={'async_io': False, 'cpu_adagrad': True, 'cpu_adam': True, 'fused_adam': True, 'fused_lamb': True, 'quantizer': True, 'random_ltd': True, 'sparse_attn': False, 'spatial_inference': True, 'transformer': True, 'stochastic_transformer': True, 'transformer_inference': True, 'utils': True}
 torch_info={'version': '1.9', 'bf16_support': False, 'cuda_version': '10.2', 'nccl_version': '2.7', 'hip_version': '0.0'}
```

### Comparing `deepspeed-0.8.3/deepspeed/inference/config.py` & `deepspeed-0.9.0/deepspeed/inference/config.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,25 +1,30 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import torch
 from deepspeed.runtime.config_utils import DeepSpeedConfigModel
 from deepspeed.runtime.zero.config import DeepSpeedZeroConfig
 from pydantic import Field
 from pydantic import validator
 from typing import Dict, Union
 from enum import Enum
 
 
 class DtypeEnum(Enum):
     # The torch dtype must always be the first value (so we return torch.dtype)
     fp16 = torch.float16, "torch.float16", "fp16", "float16", "half"
-    bf16 = torch.bfloat16, "torch.bfloat16", "bf16", "bfloat16"
     fp32 = torch.float32, "torch.float32", "fp32", "float32", "float"
     int8 = torch.int8, "torch.int8", "int8"
 
+    # bf16 not supported
+    # bf16 = torch.bfloat16, "torch.bfloat16", "bf16", "bfloat16"
+
     # Copied from https://stackoverflow.com/a/43210118
     # Allows us to use multiple values for each Enum index and returns first
     # listed value when Enum is called
     def __new__(cls, *values):
         obj = object.__new__(cls)
         # first value is canonical value
         obj._value_ = values[0]
@@ -188,14 +193,19 @@
 
     base_dir: str = None
     """
     This shows the root directory under which all the checkpoint files exists.
     This can be passed through the json config too.
     """
 
+    set_empty_params: bool = False
+    """
+    specifying whether the inference-module is created with empty or real Tensor
+    """
+
     save_mp_checkpoint_path: str = None
     """
     The path for which we want to save the loaded model with a checkpoint. This
     feature is used for adjusting the parallelism degree to help alleviate the
     model loading overhead. It does not save any new checkpoint if no path is
     passed.
     """
@@ -218,58 +228,56 @@
     may be different than what the mp size that you want to use during
     inference.
     """
 
     replace_method: str = Field(
         "auto",
         deprecated=True,
-        deprecated_msg=
-        "This parameter is no longer needed, please remove from your call to DeepSpeed-inference"
-    )
+        deprecated_msg="This parameter is no longer needed, please remove from your call to DeepSpeed-inference")
 
     injection_policy: Dict = Field(None, alias="injection_dict")
     """
     Dictionary mapping a client nn.Module to its corresponding injection
     policy. e.g., `{BertLayer : deepspeed.inference.HFBertLayerPolicy}`
     """
 
     injection_policy_tuple: tuple = None
     """ TODO: Add docs """
 
-    config: Dict = Field(
-        None,
-        alias="args")  # todo: really no need for this field if we can refactor
+    config: Dict = Field(None, alias="args")  # todo: really no need for this field if we can refactor
 
     max_out_tokens: int = Field(1024, alias="max_tokens")
     """
     This argument shows the maximum number of tokens inference-engine can work
     with, including the input and output tokens. Please consider increasing it
     to the required token-length required for your use-case.
     """
 
+    min_out_tokens: int = Field(1, alias="min_tokens")
+    """
+    This argument communicates to the runtime the minimum number of tokens you
+    expect you will need to generate. This will cause the runtime to error
+    if it unable to provide this and provide context on the memory pressure
+    rather than seg-faulting or providing corrupted output.
+    """
+
+    transposed_mode: bool = Field(False, alias="transposed_mode")
+
     mp_size: int = Field(1, deprecated=True, new_param="tensor_parallel.tp_size")
     """
     Desired model parallel size, default is 1 meaning no model parallelism.
     Deprecated, please use the ``tensor_parallel` config to control model
     parallelism.
     """
     mpu: object = Field(None, deprecated=True, new_param="tensor_parallel.mpu")
     ep_size: int = Field(1, deprecated=True, new_param="moe.ep_size")
-    ep_group: object = Field(None,
-                             alias="expert_group",
-                             deprecated=True,
-                             new_param="moe.ep_group")
-    ep_mp_group: object = Field(None,
-                                alias="expert_mp_group",
-                                deprecated=True,
-                                new_param="moe.ep_mp_group")
+    ep_group: object = Field(None, alias="expert_group", deprecated=True, new_param="moe.ep_group")
+    ep_mp_group: object = Field(None, alias="expert_mp_group", deprecated=True, new_param="moe.ep_mp_group")
     moe_experts: list = Field([1], deprecated=True, new_param="moe.moe_experts")
-    moe_type: MoETypeEnum = Field(MoETypeEnum.standard,
-                                  deprecated=True,
-                                  new_param="moe.type")
+    moe_type: MoETypeEnum = Field(MoETypeEnum.standard, deprecated=True, new_param="moe.type")
 
     @validator("moe")
     def moe_backward_compat(cls, field_value, values):
         if isinstance(field_value, bool):
             return DeepSpeedMoEConfig(moe=field_value)
         return field_value
```

### Comparing `deepspeed-0.8.3/deepspeed/inference/engine.py` & `deepspeed-0.9.0/deepspeed/inference/engine.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,10 +1,12 @@
-'''
-Copyright 2021 The Microsoft DeepSpeed Team
-'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+
 import torch
 import time
 import os
 
 from deepspeed import comm as dist
 from deepspeed.utils.logging import log_dist
 
@@ -28,14 +30,66 @@
 
 DS_INFERENCE_ENABLED = False
 from torch import nn
 
 INFERENCE_MODEL_TIMER = "model-forward-inference"
 
 
+def build_bloom_alibi_tensor(attention_mask: torch.Tensor, num_heads: int, dtype: torch.dtype) -> torch.Tensor:
+    """
+    Link to paper: https://arxiv.org/abs/2108.12409 Alibi tensor is not causal as the original paper mentions, it
+    relies on a translation invariance of softmax for quick implementation: with l being a tensor, and a fixed value
+    `softmax(l+a) = softmax(l)`. Based on
+    https://github.com/ofirpress/attention_with_linear_biases/blob/a35aaca144e0eb6b789dfcb46784c4b8e31b7983/fairseq/models/transformer.py#L742
+    TODO @thomasw21 this doesn't work as nicely due to the masking strategy, and so masking varies slightly.
+
+    Args:
+    Returns tensor shaped (batch_size * num_heads, 1, max_seq_len)
+        attention_mask (`torch.Tensor`):
+            Token-wise attention mask, this should be of shape (batch_size, max_seq_len).
+        num_heads (`int`, *required*):
+            number of heads
+        dtype (`torch.dtype`, *optional*, default=`torch.bfloat16`):
+            dtype of the output tensor
+    """
+    import math
+    batch_size, seq_length = attention_mask.shape
+    closest_power_of_2 = 2**math.floor(math.log2(num_heads))
+    base = torch.tensor(2**(-(2**-(math.log2(closest_power_of_2) - 3))),
+                        device=attention_mask.device,
+                        dtype=torch.float32)
+    powers = torch.arange(1, 1 + closest_power_of_2, device=attention_mask.device, dtype=torch.int32)
+    slopes = torch.pow(base, powers)
+
+    if closest_power_of_2 != num_heads:
+        extra_base = torch.tensor(2**(-(2**-(math.log2(2 * closest_power_of_2) - 3))),
+                                  device=attention_mask.device,
+                                  dtype=torch.float32)
+        num_remaining_heads = min(closest_power_of_2, num_heads - closest_power_of_2)
+        extra_powers = torch.arange(1, 1 + 2 * num_remaining_heads, 2, device=attention_mask.device, dtype=torch.int32)
+        slopes = torch.cat([slopes, torch.pow(extra_base, extra_powers)], dim=0)
+
+    # Note: alibi will added to the attention bias that will be applied to the query, key product of attention
+    # => therefore alibi will have to be of shape (batch_size, num_heads, query_length, key_length)
+    # => here we set (batch_size=1, num_heads=num_heads, query_length=1, key_length=max_length)
+    # => the query_length dimension will then be broadcasted correctly
+    # This is more or less identical to T5's relative position bias:
+    # https://github.com/huggingface/transformers/blob/f681437203baa7671de3174b0fa583c349d9d5e1/src/transformers/models/t5/modeling_t5.py#L527
+    arange_tensor = ((attention_mask.cumsum(dim=-1) - 1) * attention_mask)[:, None, :]
+    alibi = slopes[..., None] * arange_tensor
+    if dist.is_initialized():
+        num_heads_per_rank = int(num_heads / dist.get_world_size())
+        offset = dist.get_rank() * num_heads_per_rank
+        alibi = alibi.view(batch_size, num_heads, 1, seq_length)
+        alibi = alibi[:, offset:num_heads_per_rank + offset, :, :]
+        return alibi.reshape(batch_size * num_heads_per_rank, 1, seq_length).to(dtype)
+    else:
+        return alibi.reshape(batch_size * num_heads, 1, seq_length).to(dtype)
+
+
 class InferenceEngine(Module):
     inference_mp_group = None
     inference_ep_group = None
     expert_mp_group = None
 
     def __init__(self, model, config):
         """
@@ -76,36 +130,40 @@
         self.ep_group = None  # config.moe.ep_group
         self.expert_mp_group = None  # config.moe.ep_mp_group
 
         self.cuda_graph_created = False
         self.checkpoint_engine = TorchCheckpointEngine()
         quantization_setting = None
         self._init_quantization_setting(
-            quantization_setting
-        )  # todo: update with the new quant config for weight quant
+            quantization_setting)  # todo: update with the new quant config for weight quant
         self.model_profile_enabled = False
         self._model_times = []
 
-        # This is a hack to remove the prepare_mask function on HF side for BLOOM architecture
-        self.remove_mask_prepare_for_bloom()
+        if not self.injection_dict and config.replace_with_kernel_inject:
+            # This is a hack to remove the prepare_mask function on HF side for BLOOM architecture
+            self.remove_mask_prepare_for_bloom()
+
+        if self.injection_dict or not config.replace_with_kernel_inject:
+            # This is a hack to redefine the alibi func due to TP
+            if config.tensor_parallel.tp_size > 1:
+                self.build_alibi_tensor()
 
         if get_accelerator().device_name() == 'cuda' and config.enable_cuda_graph:
             assert pkg_version.parse(torch.__version__) >= pkg_version.parse("1.10"), \
                 "If you want to use cuda graph, please upgrade torch to at least v1.10"
 
         if config.checkpoint and not config.replace_with_kernel_inject:
             self._load_checkpoint(config.checkpoint)
 
         # convert model to intended dtype
         if config.dtype:
             self._convert_to_dtype(config)
 
         if self.mpu:
-            config.tensor_parallel.tp_size = dist.get_world_size(
-                group=self.mpu.get_model_parallel_group())
+            config.tensor_parallel.tp_size = dist.get_world_size(group=self.mpu.get_model_parallel_group())
             self.mp_group = self.mpu.get_model_parallel_group()
         elif config.tensor_parallel.tp_size > 1:
             self._create_model_parallel_group(config)
             config.tensor_parallel.tp_group = self.mp_group
 
         if isinstance(self.module, torch.nn.Module):
             moe, _ = has_moe_layers(self.module)
@@ -145,16 +203,15 @@
                         config.injection_policy_tuple = injection_policy
                     self._apply_injection_policy(config, client_module)
 
         device = get_accelerator().current_device_name()
         self.module.to(device)
 
         if config.tensor_parallel.tp_size > 1:
-            _rng_state = get_accelerator().get_rng_state().to(
-                get_accelerator().current_device_name())
+            _rng_state = get_accelerator().get_rng_state().to(get_accelerator().current_device_name())
             dist.broadcast(_rng_state, 0)
             get_accelerator().set_rng_state(_rng_state.cpu())
 
         if config.tensor_parallel.tp_size > 1:
             assert not config.enable_cuda_graph, "Cuda graph is not supported for model parallelism"
 
         # Check if local CUDA graphs can be created in replacement modules
@@ -168,23 +225,26 @@
         self.use_cuda_events = use_cuda_events
         if self.use_cuda_events:
             self.timers = SynchronizedWallClockTimer()
 
     # todo: remove this once all the config dicts are centralized from top level pydantic config
     def _get_model_config_generate(self, config):
         # this is being passed to replace_transformer_layer(config=self.user_model_config_dict)
-        self.config = getattr(self.module,
-                              'config',
-                              None) if config.config is None else config.config
+        self.config = getattr(self.module, 'config', None) if config.config is None else config.config
 
     def remove_mask_prepare_for_bloom(self):
         if hasattr(self.module, 'transformer'):
             if hasattr(self.module.transformer, '_prepare_attn_mask'):
                 self.module.transformer._prepare_attn_mask = lambda attention_mask, *args, **kwargs: attention_mask
 
+    def build_alibi_tensor(self):
+        if hasattr(self.module, 'transformer'):
+            if hasattr(self.module.transformer, 'build_alibi_tensor'):
+                self.module.transformer.build_alibi_tensor = build_bloom_alibi_tensor
+
     def _pre_forward_hook(self, module, *inputs, **kwargs):
         if self.use_cuda_events:
             self.timers(INFERENCE_MODEL_TIMER).start()
         else:
             get_accelerator().synchronize()
             self._start = time.time()
 
@@ -219,28 +279,25 @@
         for e in moe_experts:
             self.ep_group.update({e: None})
             self.expert_mp_group.update({e: None})
         for moe_ep_size in self.ep_group.keys():
             num_ep_groups = dist.get_world_size() // moe_ep_size
             for i in range(num_ep_groups):
                 ep_cnt = i * moe_ep_size
-                size = dist.get_world_size(
-                ) if moe_ep_size > dist.get_world_size() else moe_ep_size
+                size = dist.get_world_size() if moe_ep_size > dist.get_world_size() else moe_ep_size
                 ranks = list(range(ep_cnt, ep_cnt + size))
                 _ep_group = dist.new_group(ranks)
                 if dist.get_rank() in ranks:
                     self.ep_group.update({moe_ep_size: _ep_group})
 
             if dist.get_world_size() > moe_ep_size:
                 num_expert_mp_groups = dist.get_world_size() // num_ep_groups
                 expert_mp_size = dist.get_world_size() // moe_ep_size
                 for i in range(num_expert_mp_groups):
-                    expert_mp_comm_ranks = [
-                        i + nr * moe_ep_size for nr in range(expert_mp_size)
-                    ]
+                    expert_mp_comm_ranks = [i + nr * moe_ep_size for nr in range(expert_mp_size)]
                     _expert_mp_group = dist.new_group(expert_mp_comm_ranks)
                     if dist.get_rank() in expert_mp_comm_ranks:
                         self.expert_mp_group.update({moe_ep_size: _expert_mp_group})
 
     def _init_quantization_setting(self, quantization_setting):
         self.quantize_bits = 8
         self.mlp_extra_grouping = False
@@ -249,73 +306,56 @@
             self.mlp_extra_grouping, \
             self.quantize_groups = quantization_setting
         elif quantization_setting is not None:
             self.quantize_groups = quantization_setting
         log_dist(
             f"quantize_bits = {self.quantize_bits} "
             f"mlp_extra_grouping = {self.mlp_extra_grouping}, "
-            f"quantize_groups = {self.quantize_groups}",
-            [0])
+            f"quantize_groups = {self.quantize_groups}", [0])
 
     # TODO: remove this function and add this functionality to pydantic config checking
     def _validate_args(self, mpu, replace_with_kernel_inject):
         # TODO: to support SD pipeline we need to avoid this check for now
         if replace_with_kernel_inject and not isinstance(self.module, Module):
             raise ValueError(f"model must be a torch.nn.Module, got {type(self.module)}")
-        if not isinstance(self._config.tensor_parallel.tp_size,
-                          int) or self._config.tensor_parallel.tp_size < 1:
-            raise ValueError(
-                f"mp_size must be an int >= 1, got {self._config.tensor_parallel.tp_size}"
-            )
+        if not isinstance(self._config.tensor_parallel.tp_size, int) or self._config.tensor_parallel.tp_size < 1:
+            raise ValueError(f"mp_size must be an int >= 1, got {self._config.tensor_parallel.tp_size}")
 
         if mpu:
             methods = ["get_model_parallel_group", "get_data_parallel_group"]
             for method in methods:
                 if not hasattr(mpu, method):
                     raise ValueError(f"mpu is missing {method}")
-        if self._config.checkpoint is not None and not isinstance(
-                self._config.checkpoint,
-            (str,
-             dict)):
-            raise ValueError(
-                f"checkpoint must be None, str or dict, got {type(self._config.checkpoint)}"
-            )
+        if self._config.checkpoint is not None and not isinstance(self._config.checkpoint, (str, dict)):
+            raise ValueError(f"checkpoint must be None, str or dict, got {type(self._config.checkpoint)}")
 
         supported_dtypes = [None, torch.half, torch.int8, torch.float]
         if self._config.dtype not in supported_dtypes:
-            raise ValueError(
-                f"{self._config.dtype} not supported, valid dtype: {supported_dtypes}")
+            raise ValueError(f"{self._config.dtype} not supported, valid dtype: {supported_dtypes}")
 
         if self.injection_dict is not None and not isinstance(self.injection_dict, dict):
-            raise ValueError(
-                f"injection_dict must be None or a dict, got: {self.injection_dict}")
+            raise ValueError(f"injection_dict must be None or a dict, got: {self.injection_dict}")
 
     def load_model_with_checkpoint(self, r_module):
         self.mp_replace = ReplaceWithTensorSlicing(
-            mp_group=self.mp_group,
-            mp_size=self._config.tensor_parallel.tp_size)  #, out_dim=0, in_dim=1)
+            mp_group=self.mp_group, mp_size=self._config.tensor_parallel.tp_size)  #, out_dim=0, in_dim=1)
         error_msgs = []
 
         def load(module, state_dict, prefix):
             args = (state_dict, prefix, {}, True, [], [], error_msgs)
             if hasattr(module, 'weight'):
                 if 'query_key_value' in prefix:
-                    module.weight = self.mp_replace.qkv_copy(
-                        module.weight.data,
-                        state_dict[prefix + 'weight'])
+                    module.weight = self.mp_replace.qkv_copy(module.weight.data, state_dict[prefix + 'weight'])
                 else:
-                    module.weight = self.mp_replace.copy(module.weight.data,
-                                                         state_dict[prefix + 'weight'])
+                    module.weight = self.mp_replace.copy(module.weight.data, state_dict[prefix + 'weight'])
             else:
-                module.norm.weight = self.mp_replace.copy(module.norm.weight.data,
-                                                          state_dict[prefix + 'weight'])
+                module.norm.weight = self.mp_replace.copy(module.norm.weight.data, state_dict[prefix + 'weight'])
             if prefix + 'bias' in self.key_list:
                 if hasattr(module, 'norm'):
-                    module.norm.bias = self.mp_replace.copy(module.norm.bias,
-                                                            state_dict[prefix + 'bias'])
+                    module.norm.bias = self.mp_replace.copy(module.norm.bias, state_dict[prefix + 'bias'])
                 else:
                     data = state_dict[prefix + 'bias']
                     data = data.to(get_accelerator().current_device_name())
                     module.bias = self.mp_replace.copy(module.bias, data)
 
         layer_policies = {
             nn.Linear: load,
@@ -327,53 +367,40 @@
 
         def load_module_recursive(module, prefix='', level=0):
             for name, child in module.named_children():
                 if child.__class__ in layer_policies:
                     checking_key = prefix + name + '.'
                     if not any(checking_key in item for item in self.key_list):
                         continue
-                    if len(list(child.parameters())) > 0 and list(
-                            child.parameters())[0].numel() == 0:
+                    if len(list(child.parameters())) > 0 and list(child.parameters())[0].numel() == 0:
                         if len(child.weight.ds_shape) == 1:
-                            child = Normalize(dim=child.weight.ds_shape[-1],
-                                              dtype=child.weight.dtype,
-                                              eps=child.eps)
+                            child = Normalize(dim=child.weight.ds_shape[-1], dtype=child.weight.dtype, eps=child.eps)
                             setattr(module, name, child)
                     load(child, self.sd, prefix + name + '.')
                 else:
-                    load_module_recursive(child,
-                                          prefix if level == 0 else prefix + name + '.',
-                                          level + 1)
+                    load_module_recursive(child, prefix if level == 0 else prefix + name + '.', level + 1)
 
         load_module_recursive(r_module)
 
     def _apply_injection_policy(self, config, client_module=None):
         # client_module is only passed when using the injection_dict method.
         checkpoint_dir = config.checkpoint
-        checkpoint = SDLoaderFactory.get_sd_loader_json(
-            checkpoint_dir,
-            self.checkpoint_engine) if checkpoint_dir is not None else None
+        checkpoint = SDLoaderFactory.get_sd_loader_json(checkpoint_dir,
+                                                        self.checkpoint_engine) if checkpoint_dir is not None else None
 
         generic_injection(self.module,
-                          fp16=(config.dtype == torch.half)
-                          or (config.dtype == torch.int8),
+                          fp16=(config.dtype == torch.half) or (config.dtype == torch.int8),
                           enable_cuda_graph=config.enable_cuda_graph)
 
         if isinstance(self.module, torch.nn.Module):
             # config is our DeepSpeedInferenceConfig and self.config is the HF model config
-            replace_transformer_layer(client_module,
-                                      self.module,
-                                      checkpoint,
-                                      config,
-                                      self.config)
+            replace_transformer_layer(client_module, self.module, checkpoint, config, self.config)
 
     def _get_all_ckpt_names(self, checkpoints_path, tag):
-        ckpt_file_pattern = self._get_ckpt_name(checkpoints_path,
-                                                tag,
-                                                mp_placeholder="*")
+        ckpt_file_pattern = self._get_ckpt_name(checkpoints_path, tag, mp_placeholder="*")
         import glob
 
         ckpt_files = glob.glob(ckpt_file_pattern)
         ckpt_files.sort()
         return ckpt_files
 
     def _get_ckpt_name(self, checkpoints_path, tag, mp_placeholder=None):
@@ -388,91 +415,85 @@
             "mp_rank_" + mp_rank_str + "_model_states.pt",
         )
         return ckpt_name
 
     def _load_checkpoint(self, load_dir, load_module_strict=True, tag=None):
         is_pipe_parallel = isinstance(self.module, PipelineModule)
         if is_pipe_parallel:
-            raise RuntimeError(
-                'pipeline parallelism is currently not supported in inference.')
+            raise RuntimeError('pipeline parallelism is currently not supported in inference.')
         if not isinstance(load_dir, dict) and os.path.isdir(load_dir):
             if tag is None:
                 latest_path = os.path.join(load_dir, "latest")
                 if os.path.isfile(latest_path):
                     with open(latest_path, "r") as fd:
                         tag = fd.read().strip()
 
             ckpt_list = self._get_all_ckpt_names(load_dir, tag)
             sd_loader = SDLoaderFactory.get_sd_loader(ckpt_list, self.checkpoint_engine)
         else:
-            sd_loader = SDLoaderFactory.get_sd_loader_json(load_dir,
-                                                           self.checkpoint_engine)
+            sd_loader = SDLoaderFactory.get_sd_loader_json(load_dir, self.checkpoint_engine)
 
         if type(sd_loader) is list:
             self.sd = torch.load(sd_loader[0], map_location='cpu')
             self.key_list = list(self.sd.keys())
 
             self.load_model_with_checkpoint(self.module)
 
             for i in range(1, len(sd_loader)):
                 if not dist.is_initialized() or dist.get_rank() == 0:
                     print(f"loading checkpoint ({i})")
-                self.sd = torch.load(sd_loader[i],
-                                     map_location=get_accelerator().device_name())
+                self.sd = torch.load(sd_loader[i], map_location=get_accelerator().device_name())
                 self.key_list = list(self.sd.keys())
                 self.load_model_with_checkpoint(self.module)
         else:
             mp_rank = 0 if self.mpu is None else self.mpu.get_model_parallel_rank()
 
             load_path, checkpoint, quantize_config = sd_loader.load(self._config.tensor_parallel.tp_size,
-                                                    mp_rank,
-                                                    is_pipe_parallel=is_pipe_parallel,
-                                                    quantize=(self._config.dtype is torch.int8),
-                                                    quantize_groups=self.quantize_groups,
-                                                    mlp_extra_grouping=self.mlp_extra_grouping)
+                                                                    mp_rank,
+                                                                    is_pipe_parallel=is_pipe_parallel,
+                                                                    quantize=(self._config.dtype is torch.int8),
+                                                                    quantize_groups=self.quantize_groups,
+                                                                    mlp_extra_grouping=self.mlp_extra_grouping)
 
             self.quantization_scales, self.quantize_merge_count = quantize_config
 
             moe, _ = has_moe_layers(self.module)
             if moe:
                 from deepspeed.runtime.engine import DeepSpeedEngine
                 old_moe_load = False
                 if not isinstance(checkpoint['num_experts'], list):
                     old_moe_load = True
-                DeepSpeedEngine.load_moe_state_dict(
-                    load_dir,
-                    tag,
-                    state_dict=checkpoint[self._choose_module_key(checkpoint)],
-                    old_moe_load=old_moe_load,
-                    model=self.module,
-                    mpu=self.mpu,
-                    checkpoint_engine=self.checkpoint_engine)
-
-            self.module.load_state_dict(
-                state_dict=checkpoint[self._choose_module_key(checkpoint)],
-                strict=load_module_strict)
+                DeepSpeedEngine.load_moe_state_dict(load_dir,
+                                                    tag,
+                                                    state_dict=checkpoint[self._choose_module_key(checkpoint)],
+                                                    old_moe_load=old_moe_load,
+                                                    model=self.module,
+                                                    mpu=self.mpu,
+                                                    checkpoint_engine=self.checkpoint_engine)
+
+            self.module.load_state_dict(state_dict=checkpoint[self._choose_module_key(checkpoint)],
+                                        strict=load_module_strict)
 
     def _choose_module_key(self, sd):
-        assert not ('module' in sd and 'model' in sd), "checkpoint has both 'model' and 'module' keys, not sure how to proceed"
+        assert not ('module' in sd
+                    and 'model' in sd), "checkpoint has both 'model' and 'module' keys, not sure how to proceed"
         assert 'module' in sd or 'model' in sd, "checkpoint contains neither 'model' or 'module' keys, not sure how to proceed"
         if 'module' in sd:
             return 'module'
         elif 'model' in sd:
             return 'model'
 
     def _convert_to_dtype(self, config):
         if not isinstance(self.module, torch.nn.Module):
             return
 
         if False:  #config.dtype is torch.int8 and self.quantization_scales is None:
             quantizer = WeightQuantization(mlp_extra_grouping=self.mlp_extra_grouping)
-            model, self.quantization_scales = quantizer.model_quantize(self.module,
-                                                                        self.injection_dict,
-                                                                        self.quantize_bits,
-                                                                        self.quantize_groups)
+            model, self.quantization_scales = quantizer.model_quantize(self.module, self.injection_dict,
+                                                                       self.quantize_bits, self.quantize_groups)
         elif config.dtype == torch.half:
             self.module.half()
         elif config.dtype == torch.bfloat16:
             self.module.bfloat16()
         elif config.dtype == torch.float:
             self.module.float()
 
@@ -505,19 +526,18 @@
         self._cuda_graphs.replay()
         return self.static_output
 
     def model_times(self):
         assert self.model_profile_enabled, "model profiling is not enabled"
         model_times = self._model_times
         if self._config.enable_cuda_graph and len(self._model_times) == 0:
-            raise ValueError(
-                "Model times are empty and cuda graph is enabled. If "
-                "this is a GPT-style model this combo is not supported. If this is a "
-                "BERT-style model this is a bug, please report it. "
-                f"Model type is: {type(self.module)}")
+            raise ValueError("Model times are empty and cuda graph is enabled. If "
+                             "this is a GPT-style model this combo is not supported. If this is a "
+                             "BERT-style model this is a bug, please report it. "
+                             f"Model type is: {type(self.module)}")
         self._model_times = []
         return model_times
 
     def _module_match(self, module):
         for policy in generic_policies:
             policy = policy()
             if policy.match_replaced(module):
@@ -528,35 +548,32 @@
         if isinstance(module, torch.nn.Module):
             return False
         else:
             sub_module_cuda_graph = False
             for name in module.__dict__.keys():
                 sub_module = getattr(module, name)
 
-                if self._module_match(sub_module) and hasattr(sub_module,
-                                                              "enable_cuda_graph"):
+                if self._module_match(sub_module) and hasattr(sub_module, "enable_cuda_graph"):
                     sub_module_cuda_graph = True
 
             return sub_module_cuda_graph
 
     def forward(self, *inputs, **kwargs):
         """Execute forward propagation
 
         Arguments:
             *inputs: Variable length input list
             **kwargs: variable length keyword arguments
         """
         start = None
-        if self.model_profile_enabled and get_accelerator().device_name(
-        ) == 'cuda' and self._config.enable_cuda_graph:
+        if self.model_profile_enabled and get_accelerator().device_name() == 'cuda' and self._config.enable_cuda_graph:
             get_accelerator().synchronize()
             start = time.time()
 
-        if get_accelerator().device_name(
-        ) == 'cuda' and self._config.enable_cuda_graph and not self.local_cuda_graph:
+        if get_accelerator().device_name() == 'cuda' and self._config.enable_cuda_graph and not self.local_cuda_graph:
             if self.cuda_graph_created:
                 outputs = self._graph_replay(*inputs, **kwargs)
             else:
                 self._create_cuda_graph(*inputs, **kwargs)
                 outputs = self._graph_replay(*inputs, **kwargs)
         else:
             outputs = self.module(*inputs, **kwargs)
@@ -576,13 +593,11 @@
         if "generation_config" in kwargs:
             gen_config = kwargs["generation_config"]
             num_beams = getattr(gen_config, "num_beams", 1)
         if "num_beams" in kwargs:
             num_beams = kwargs["num_beams"]
 
         if num_beams > 1:
-            raise NotImplementedError(
-                "DeepSpeed does not support `num_beams` > 1, if this is important to you please "
-                "add your request to: https://github.com/microsoft/DeepSpeed/issues/2506"
-            )
+            raise NotImplementedError("DeepSpeed does not support `num_beams` > 1, if this is important to you please "
+                                      "add your request to: https://github.com/microsoft/DeepSpeed/issues/2506")
 
         return self.module.generate(*inputs, **kwargs)
```

### Comparing `deepspeed-0.8.3/deepspeed/launcher/launch.py` & `deepspeed-0.9.0/deepspeed/launcher/launch.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,8 +1,11 @@
-# Copyright 2020 The Microsoft DeepSpeed Team
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 """
 DeepSpeed launcher, this is similar to torch's distributed.launch but supports
 additional features such as arbitrary gpu exclusion.
 
 deepspeed.launcher.launch is intended to be run on a single worker node and
 will spawn several worker sub-processes depending on how many devices/ranks
 are on the worker.
@@ -12,14 +15,15 @@
 import subprocess
 import os
 import json
 import base64
 import time
 import signal
 import psutil
+import distutils
 from collections import defaultdict
 from typing import Dict
 from argparse import ArgumentParser, REMAINDER
 from ..constants import TORCH_DISTRIBUTED_DEFAULT_PORT
 from ..nebula.constants import DLTS_POD_ENV_PATH
 from ..utils import logger
 from ..elasticity import is_torch_elastic_compatible
@@ -48,59 +52,59 @@
                         " --master_addr can simply be 127.0.0.1")
     parser.add_argument("--master_port",
                         default=TORCH_DISTRIBUTED_DEFAULT_PORT,
                         type=int,
                         help="Master node (rank 0)'s free port that needs to "
                         "be used for communication during distributed "
                         "training")
-    parser.add_argument("--world_info",
-                        default="None",
-                        type=str,
-                        help="world info base64 encoded dictionary")
+    parser.add_argument("--world_info", default="None", type=str, help="world info base64 encoded dictionary")
 
     parser.add_argument("--module",
                         action="store_true",
                         help="Change each process to interpret the launch "
                         "script as a Python module, executing with the same "
                         "behavior as 'python -m'.")
 
     parser.add_argument("--no_python",
                         action="store_true",
                         help="Skip prepending the training script with "
                         "'python' - just execute it directly.")
 
-    parser.add_argument("--enable_elastic_training",
-                        action="store_true",
-                        help="Enable elastic training support.")
+    parser.add_argument("--enable_elastic_training", action="store_true", help="Enable elastic training support.")
 
-    parser.add_argument("--min_elastic_nodes",
-                        type=int,
-                        default=-1,
-                        help="Min number of nodes in elastic training.")
+    parser.add_argument("--min_elastic_nodes", type=int, default=-1, help="Min number of nodes in elastic training.")
 
-    parser.add_argument("--max_elastic_nodes",
-                        type=int,
-                        default=-1,
-                        help="Max number of nodes in elastic training.")
+    parser.add_argument("--max_elastic_nodes", type=int, default=-1, help="Max number of nodes in elastic training.")
 
     parser.add_argument("--no_local_rank",
                         action="store_true",
                         help="Do not pass local_rank as an argument when calling "
                         "the user's training script.")
 
     parser.add_argument("--save_pid",
                         type=int,
                         default=0,
                         help="main launching process pid, for internal pid tracking")
 
-    parser.add_argument(
-        "--enable_each_rank_log",
-        default="None",
-        type=str,
-        help="redirect the stdout and stderr from each rank into different log files")
+    parser.add_argument("--enable_each_rank_log",
+                        default="None",
+                        type=str,
+                        help="redirect the stdout and stderr from each rank into different log files")
+
+    parser.add_argument("--bind_cores_to_rank",
+                        action="store_true",
+                        help="Bind each rank to different cores of the host. "
+                        "This improves host efficiency especially for CPU backend")
+
+    parser.add_argument("--bind_core_list",
+                        type=str,
+                        default=None,
+                        help="List of cores to bind to with comma separated list of "
+                        "numbers and range. i.e. 1,3-5,7 => [1,3,4,5,7].  When not "
+                        "specified, all cores on system would be used rank binding")
 
     # positional
     parser.add_argument("training_script",
                         type=str,
                         help="The full path to the single GPU training "
                         "program/script to be launched in parallel, "
                         "followed by all the arguments for the "
@@ -122,14 +126,97 @@
         except psutil.NoSuchProcess:
             pass
     gone, alive = psutil.wait_procs(children, timeout=30)
     for p in alive:
         p.kill()
 
 
+def parse_range(rng):
+    try:
+        value = int(rng)
+        return range(value, value + 1)
+    except ValueError:
+        # value is not a single number
+        parts = rng.split('-')
+        if len(parts) != 2:
+            raise ValueError("Bad range: '%s', range must be either a number or two number separated by dash" %
+                             (rng, ))
+        start = int(parts[0])
+        end = int(parts[1])
+        if start > end:
+            raise ValueError("Bad range: '%s', range end must larger than or equal to start" % (rng, ))
+        return range(start, end + 1)
+
+
+# parse comma and dash separated range list into list
+# i.e. "0,2-4,6" --> [0, 2, 3, 4, 6]
+# rules:
+# 1. Range list numser be comma sepeaated, each item are either a single number,
+#    or a range marked by two numbers (both number are included in the range)
+# 2. Sub ranges must be in ascend order and not overlap with each other
+# 3. No space in the range expression
+def parse_range_list(range_str):
+    number_list = []
+    last = -1
+    range_list = range_str.split(',')
+    for sub_range in range_list:
+        sub_number_list = parse_range(sub_range)
+        if sub_number_list[0] <= last:
+            raise ValueError(
+                "Bad range: '%s', sub ranges must not overlap with each other and should be in ascend order" %
+                (range_str, ))
+        last = sub_number_list[-1]
+        number_list.extend(sub_number_list)
+    return number_list
+
+
+# return a list of list for cores to numa mapping
+# [
+#     [ cores for numa 0 ]
+#     [ cores belong to numa 1 ]
+#     ...
+# ]
+def get_numa_cores():
+    ret = []
+    output = subprocess.check_output(['numactl', '--hardware']).decode("utf-8")
+    lines = output.split('\n')
+    for line in lines:
+        if line.startswith('available:'):
+            num_numas = int(line.split(' ')[1])
+            break
+    for numa in range(num_numas):
+        for line in lines:
+            if line.startswith(f'node {numa} cpus:'):
+                cores = line.split(' ')[3:]
+                ret.append([int(core) for core in cores])
+    return ret
+
+
+def check_for_numactl_pkg():
+    libs = dict(
+        dpkg=["-l", "numactl", "apt"],
+        pacman=["-Q", "numactl", "pacman"],
+        rpm=["-q", "numactl", "yum"],
+    )
+
+    found = False
+    for pkgmgr, data in libs.items():
+        flag, lib, tool = data
+        path = distutils.spawn.find_executable(pkgmgr)
+        if path is not None:
+            cmd = f"{pkgmgr} {flag} {lib}"
+            result = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)
+            if result.wait() == 0:
+                found = True
+            else:
+                print(f"please install the {lib} package with {tool}")
+            break
+    return found
+
+
 def main():
     args = parse_args()
     current_env = os.environ.copy()
 
     for k in current_env.keys():
         if "NCCL" in k:
             logger.info(f"{args.node_rank} {k}={current_env[k]}")
@@ -141,17 +228,15 @@
 
     logger.info(f"WORLD INFO DICT: {world_info}")
     node_list = list(world_info.keys())
     args.nnodes = len(node_list)
     local_node = node_list[args.node_rank]
     local_gpu_ids = world_info[local_node]
     num_local_procs = len(local_gpu_ids)
-    logger.info(
-        f"nnodes={args.nnodes}, num_local_procs={num_local_procs}, node_rank={args.node_rank}"
-    )
+    logger.info(f"nnodes={args.nnodes}, num_local_procs={num_local_procs}, node_rank={args.node_rank}")
 
     global_rank_mapping = defaultdict(list)
     curr_global_rank = 0
     dist_world_size = 0
     for node_id in node_list:
         gids = world_info[node_id]
         dist_world_size += len(gids)
@@ -189,70 +274,96 @@
             args.enable_elastic_training = False
 
     if os.path.exists(DLTS_POD_ENV_PATH):
         with open(DLTS_POD_ENV_PATH) as file:
             lines = file.readlines()
             lines = [line.rstrip() for line in lines]
             for line in lines:
-                if line.startswith('export FC_TASKROLE_NAME') or line.startswith(
-                        'export FC_TASK_INDEX'):
+                if line.startswith('export FC_TASKROLE_NAME') or line.startswith('export FC_TASK_INDEX'):
                     key_val = line.split()[1]
                     key, val = key_val.split('=')
                     current_env[key] = val
 
     processes = []
     cmd = []
 
     if not args.enable_elastic_training:
         if args.enable_each_rank_log != "None":
             # prepare the log path and the file name prefix
             if os.path.isfile(args.enable_each_rank_log):
-                raise ValueError(
-                    f"{args.enable_each_rank_log} should not be a file, it should be a directory."
-                )
+                raise ValueError(f"{args.enable_each_rank_log} should not be a file, it should be a directory.")
             if not os.path.exists(args.enable_each_rank_log):
                 try:
                     os.makedirs(args.enable_each_rank_log)
                 except Exception as e:
                     print(e)
-                    raise ValueError(
-                        f"unable to create directory {args.enable_each_rank_log} for each rank log."
-                    )
+                    raise ValueError(f"unable to create directory {args.enable_each_rank_log} for each rank log.")
             log_name_prefix = time.strftime("%Y%m%d%H%M%S", time.localtime())
 
         for local_rank in range(0, num_local_procs):
             # each process's rank
             dist_rank = global_rank_mapping[local_node][local_rank]
             current_env["RANK"] = str(dist_rank)
             current_env["LOCAL_RANK"] = str(local_rank)
 
             # spawn the processes
             cmd = []
+            if args.bind_cores_to_rank:
+                check_for_numactl_pkg()
+                if 'KMP_AFFINITY' in os.environ.keys():
+                    raise ValueError("Environment variable KMP_AFFINITY conflicts with numactl "
+                                     "because it interfere with how many CPU cores numactl can set. "
+                                     "Unset KMP_AFFINITY before launching deepspeed.\n\n"
+                                     "\t$ unset KMP_AFFINITY\n"
+                                     "\t$ deepspeed <deepspeed command parameters>")
+                if args.bind_core_list != None:
+                    core_list = parse_range_list(args.bind_core_list)
+                    total_cores = len(core_list)
+                else:
+                    total_cores = psutil.cpu_count(logical=False)
+                    core_list = range(total_cores)
+                cores_per_rank = total_cores // num_local_procs
+                assert cores_per_rank >= 1, "At least one core needs to be assigned to each rank"
+                core_list_for_rank = core_list[cores_per_rank * local_rank:cores_per_rank * (local_rank + 1)]
+                current_env["OMP_NUM_THREADS"] = f"{cores_per_rank}"
+                cmd.append("numactl")
+
+                # check if all cores belong to same numa, if true, bind process to that numa domain with -m parameter
+                numa_cores = get_numa_cores()
+                num_numas = len(numa_cores)
+                for i in range(num_numas):
+                    if set(core_list_for_rank) <= set(numa_cores[i]):
+                        cmd.append("-m")
+                        cmd.append(f"{i}")
+                        break
+
+                cmd.append("-C")
+                core_list_str = f"{core_list_for_rank[0]}"
+                for core_id in core_list_for_rank[1:]:
+                    core_list_str = f"{core_list_str},{core_id}"
+                cmd.append(f"{core_list_str}")
             if not args.no_python:
-                cmd = [sys.executable, "-u"]
+                cmd.append(sys.executable)
+                cmd.append("-u")
                 if args.module:
                     cmd.append("-m")
             else:
                 if args.module:
                     raise ValueError("Don't use both the '--no_python' flag"
                                      " and the '--module' flag at the same time.")
             cmd.append(args.training_script)
             # A user may not want to pass local_rank as a keyword arg so we make this optional.
             if not args.no_local_rank:
                 cmd.append(f"--local_rank={local_rank}")
             cmd += args.training_script_args
 
             if args.enable_each_rank_log != "None":
-                log_file = os.path.join(args.enable_each_rank_log,
-                                        f"{log_name_prefix}_rank{dist_rank}.log")
+                log_file = os.path.join(args.enable_each_rank_log, f"{log_name_prefix}_rank{dist_rank}.log")
                 log_fd = open(log_file, 'w')
-                process = subprocess.Popen(cmd,
-                                           env=current_env,
-                                           stdout=log_fd,
-                                           stderr=log_fd)
+                process = subprocess.Popen(cmd, env=current_env, stdout=log_fd, stderr=log_fd)
             else:
                 process = subprocess.Popen(cmd, env=current_env)
 
             processes.append(process)
     else:
         from ..elasticity import DSElasticAgent
         from torch.distributed.elastic.rendezvous import RendezvousParameters
@@ -260,15 +371,15 @@
         import torch.distributed.elastic.rendezvous.registry as rdzv_registry
         from torch.distributed.elastic.multiprocessing import Std
 
         if args.min_elastic_nodes == -1:
             args.min_elastic_nodes = 1
         if args.max_elastic_nodes == -1:
             args.max_elastic_nodes = args.nnodes
-        assert args.max_elastic_nodes > 0 and  args.min_elastic_nodes > 0 , "Max and Min nodes should be positive"
+        assert args.max_elastic_nodes > 0 and args.min_elastic_nodes > 0, "Max and Min nodes should be positive"
 
         current_env["NCCL_ASYNC_ERROR_HANDLING"] = str(1)
 
         # Get config and arguments
         cmd = []
         if not args.no_python:
             cmd = [sys.executable, "-u"]
@@ -283,16 +394,15 @@
         cmd_args = cmd[1:]
 
         rdzv_configs: Dict[str, str] = {'timeout': 100}
         run_id = os.environ.get("ELASTIC_RUN_ID", ELASTIC_TRAINING_ID_DEFAULT)
 
         # Creating config for rendezvous class
         rdzv_parameters = RendezvousParameters(backend='c10d',
-                                               endpoint=args.master_addr + ":" +
-                                               str(args.master_port),
+                                               endpoint=args.master_addr + ":" + str(args.master_port),
                                                run_id=run_id,
                                                min_nodes=args.min_elastic_nodes,
                                                max_nodes=args.max_elastic_nodes,
                                                **rdzv_configs)
 
         spec = WorkerSpec(
             role='trainer',
```

### Comparing `deepspeed-0.8.3/deepspeed/launcher/multinode_runner.py` & `deepspeed-0.9.0/deepspeed/launcher/multinode_runner.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,22 +1,26 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import os
 import sys
 import shutil
 import subprocess
 import warnings
 from shlex import split
 from abc import ABC, abstractmethod
 from deepspeed.accelerator import get_accelerator
 from ..utils import logger
 from .constants import PDSH_MAX_FAN_OUT, MVAPICH_TMP_HOSTFILE
 
 
 class MultiNodeRunner(ABC):
+
     def __init__(self, args, world_info_base64):
         self.args = args
         self.validate_args()
         self.user_arguments = self.parse_user_args()
         self.user_script = args.user_script
         self.world_info_base64 = world_info_base64
         self.exports = {}
@@ -41,62 +45,48 @@
         return self.__class__.__name__
 
     def validate_args(self):
         """Validate self.args"""
 
 
 class PDSHRunner(MultiNodeRunner):
+
     def __init__(self, args, world_info_base64):
         super().__init__(args, world_info_base64)
 
     def backend_exists(self):
         return shutil.which('pdsh')
 
     @property
     def name(self):
         return "pdsh"
 
     def parse_user_args(self):
-        return list(
-            map(lambda x: x if x.startswith("-") else f"'{x}'",
-                self.args.user_args))
+        return list(map(lambda x: x if x.startswith("-") else f"'{x}'", self.args.user_args))
 
     def get_cmd(self, environment, active_resources):
         environment['PDSH_RCMD_TYPE'] = 'ssh'
 
         active_workers = ",".join(active_resources.keys())
         logger.info("Running on the following workers: %s" % active_workers)
 
         # PDSH flags for max node fan out and specific hosts to launch on
         # See https://linux.die.net/man/1/pdsh for flag details
-        pdsh_cmd_args = [
-            'pdsh',
-            '-S',
-            '-f',
-            str(PDSH_MAX_FAN_OUT),
-            '-w',
-            active_workers
-        ] + split(self.args.launcher_args)
+        pdsh_cmd_args = ['pdsh', '-S', '-f', str(PDSH_MAX_FAN_OUT), '-w', active_workers] + split(
+            self.args.launcher_args)
 
         exports = ""
         for key, val in self.exports.items():
             exports += "export {}={}; ".format(key, val)
 
         # https://linux.die.net/man/1/pdsh
         # %n will be replaced by pdsh command
         deepspeed_launch = [
-            exports,
-            f"cd {os.path.abspath('.')};",
-            sys.executable,
-            "-u",
-            "-m",
-            "deepspeed.launcher.launch",
-            f'--world_info={self.world_info_base64}',
-            "--node_rank=%n",
-            f"--master_addr={self.args.master_addr}",
+            exports, f"cd {os.path.abspath('.')};", sys.executable, "-u", "-m", "deepspeed.launcher.launch",
+            f'--world_info={self.world_info_base64}', "--node_rank=%n", f"--master_addr={self.args.master_addr}",
             f"--master_port={self.args.master_port}"
         ]
         if self.args.no_python:
             deepspeed_launch.append("--no_python")
         if self.args.module:
             deepspeed_launch.append("--module")
         if self.args.no_local_rank:
@@ -107,19 +97,19 @@
             deepspeed_launch.append("--enable_elastic_training")
             deepspeed_launch.append(f"--max_elastic_nodes={self.args.max_elastic_nodes}")
             deepspeed_launch.append(f"--min_elastic_nodes={self.args.min_elastic_nodes}")
 
         cmd_to_search = [i + "\\" for i in deepspeed_launch[2:6]]
 
         kill_command = pdsh_cmd_args + ["pkill -f ", " ".join(cmd_to_search)[:-2]]
-        return pdsh_cmd_args + deepspeed_launch + [self.user_script
-                                                   ] + self.user_arguments, kill_command
+        return pdsh_cmd_args + deepspeed_launch + [self.user_script] + self.user_arguments, kill_command
 
 
 class OpenMPIRunner(MultiNodeRunner):
+
     def __init__(self, args, world_info_base64, resource_pool):
         super().__init__(args, world_info_base64)
         self.resource_pool = resource_pool
         self.add_export('UCX_TLS', 'tcp')
 
     def backend_exists(self):
         #TODO: if IB is available we should suggestion mvapich
@@ -129,19 +119,17 @@
     def name(self):
         return "openmpi"
 
     def validate_args(self):
         super().validate_args()
         #TODO: Allow for include/exclude at node-level but not gpu-level
         if self.args.include != "" or self.args.exclude != "":
-            raise ValueError(
-                f"{self.name} backend does not support worker include/exclusion")
+            raise ValueError(f"{self.name} backend does not support worker include/exclusion")
         if self.args.num_nodes != -1 or self.args.num_gpus != -1:
-            raise ValueError(
-                f"{self.name} backend does not support limiting num nodes/gpus")
+            raise ValueError(f"{self.name} backend does not support limiting num nodes/gpus")
 
     def get_cmd(self, environment, active_resources):
         total_process_count = sum(self.resource_pool.values())
 
         mpirun_cmd = [
             'mpirun',
             '-n',
@@ -162,19 +150,19 @@
 
         python_exec = []
         if not self.args.no_python:
             python_exec = [sys.executable, "-u"]
             if self.args.module:
                 python_exec.append("-m")
 
-        return mpirun_cmd + export_cmd + python_exec + [self.user_script
-                                                        ] + self.user_arguments
+        return mpirun_cmd + export_cmd + python_exec + [self.user_script] + self.user_arguments
 
 
 class MPICHRunner(MultiNodeRunner):
+
     def __init__(self, args, world_info_base64, resource_pool):
         super().__init__(args, world_info_base64)
         self.resource_pool = resource_pool
 
     def backend_exists(self):
         #TODO: if IB is available we should suggestion mpich
         return shutil.which('mpirun')  #mpich_info
@@ -183,20 +171,18 @@
     def name(self):
         return "mpich"
 
     def validate_args(self):
         super().validate_args()
         #TODO: Allow for include/exclude at node-level but not gpu-level
         if self.args.include != "" or self.args.exclude != "":
-            raise ValueError(
-                f"{self.name} backend does not support worker include/exclusion")
+            raise ValueError(f"{self.name} backend does not support worker include/exclusion")
 
         if self.args.num_nodes != -1 or self.args.num_gpus != -1:
-            raise ValueError(
-                f"{self.name} backend does not support limiting num nodes/gpus")
+            raise ValueError(f"{self.name} backend does not support limiting num nodes/gpus")
 
     def get_cmd(self, environment, active_resources):
         devices_per_node = self.resource_pool.values()
         total_process_count = sum(devices_per_node)
         process_per_node = list(devices_per_node)[0]
 
         mpirun_cmd = [
@@ -216,27 +202,29 @@
             python_exec = [sys.executable, "-u"]
             if self.args.module:
                 python_exec.append("-m")
         return mpirun_cmd + python_exec + [self.user_script] + self.user_arguments
 
 
 class SlurmRunner(MultiNodeRunner):
+
     def __init__(self, args, world_info_base64, resource_pool):
         super().__init__(args, world_info_base64)
         self.resource_pool = resource_pool
 
     def backend_exists(self):
         return shutil.which('sinfo')
 
     @property
     def name(self):
         return 'slurm'
 
     def get_cmd(self, environment, active_resources):
-        assert not getattr(self.args, 'detect_nvlink_pairs', False), "slurm backend does not support remapping visible devices"
+        assert not getattr(self.args, 'detect_nvlink_pairs',
+                           False), "slurm backend does not support remapping visible devices"
         total_process_count = sum(self.resource_pool.values())
         srun_cmd = [
             'srun',
             '-n',
             f'{total_process_count}',
         ] + split(self.args.launcher_args)
 
@@ -257,20 +245,20 @@
             srun_cmd.append(f'{self.args.num_gpus}')
 
         exports = '--export=ALL'
         for key, val in self.exports.items():
             exports += f",{key}={val}"
 
         python_exec = [sys.executable, "-u"]
-        command = srun_cmd + [exports] + python_exec + [self.user_script
-                                                        ] + self.user_arguments
+        command = srun_cmd + [exports] + python_exec + [self.user_script] + self.user_arguments
         return command
 
 
 class MVAPICHRunner(MultiNodeRunner):
+
     def __init__(self, args, world_info_base64, resource_pool):
         super().__init__(args, world_info_base64)
         self.resource_pool = resource_pool
 
         # Disable the CMA kernel module, not available on Ubuntu systems
         self.add_export('MV2_SMP_USE_CMA', '0')
 
@@ -299,32 +287,28 @@
             warnings.warn("mpiname does not exist, mvapich is not installed properly")
         else:
             results = subprocess.check_output('mpiname', shell=True)
             mpiname_results = results.decode('utf-8').strip()
             if "MVAPICH2-GDR" in mpiname_results:
                 exists = True
             else:
-                warnings.warn(
-                    f"Expected MVAPICH2-GDR as return for mpiname but received {mpiname_results}"
-                )
+                warnings.warn(f"Expected MVAPICH2-GDR as return for mpiname but received {mpiname_results}")
         return exists
 
     @property
     def name(self):
         return "mvapich"
 
     def validate_args(self):
         super().validate_args()
         #TODO: Allow for include/exclude at node-level but not gpu-level
         if self.args.include != "" or self.args.exclude != "":
-            raise ValueError(
-                f"{self.name} backend does not support worker include/exclusion")
+            raise ValueError(f"{self.name} backend does not support worker include/exclusion")
         if self.args.num_nodes != -1 or self.args.num_gpus != -1:
-            raise ValueError(
-                f"{self.name} backend does not support limiting num nodes/gpus")
+            raise ValueError(f"{self.name} backend does not support limiting num nodes/gpus")
 
     def get_cmd(self, environment, active_resources):
         devices_per_node = self.resource_pool.values()
         total_process_count = sum(devices_per_node)
         process_per_node = list(devices_per_node)[0]
         if not all([n == process_per_node for n in devices_per_node]):
             raise ValueError("mvapich requires same number of devices per node")
@@ -349,9 +333,8 @@
 
         python_exec = []
         if not self.args.no_python:
             python_exec = [sys.executable, "-u"]
             if self.args.module:
                 python_exec.append("-m")
 
-        return mpirun_cmd + export_cmd + python_exec + [self.user_script
-                                                        ] + self.user_arguments
+        return mpirun_cmd + export_cmd + python_exec + [self.user_script] + self.user_arguments
```

### Comparing `deepspeed-0.8.3/deepspeed/launcher/runner.py` & `deepspeed-0.9.0/deepspeed/launcher/runner.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,8 +1,11 @@
-# Copyright 2020 The Microsoft DeepSpeed Team
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 """
 DeepSpeed runner is the main front-end to launching multi-worker
 training jobs with DeepSpeed. By default this uses pdsh to parallel
 ssh into multiple worker nodes and launch all the necessary processes
 per rank for training.
 """
 
@@ -32,17 +35,16 @@
 EXPORT_ENVS += NEBULA_EXPORT_ENVS
 DEEPSPEED_ENVIRONMENT_NAME = ".deepspeed_env"
 DEEPSPEED_ENVIRONMENT_PATHS = [os.path.expanduser("~"), '.']
 PDSH_MAX_FAN_OUT = 1024
 
 
 def parse_args(args=None):
-    parser = argparse.ArgumentParser(
-        description="DeepSpeed runner to help launch distributed "
-        "multi-node/multi-gpu training jobs.")
+    parser = argparse.ArgumentParser(description="DeepSpeed runner to help launch distributed "
+                                     "multi-node/multi-gpu training jobs.")
 
     parser.add_argument("-H",
                         "--hostfile",
                         type=str,
                         default=DLTS_HOSTFILE,
                         help="Hostfile path (in MPI style) that defines the "
                         "resource pool available to the job (e.g., "
@@ -105,20 +107,19 @@
 
     parser.add_argument("--master_addr",
                         default="",
                         type=str,
                         help="(optional) IP address of node 0, will be "
                         "inferred via 'hostname -I' if not specified.")
 
-    parser.add_argument(
-        "--launcher",
-        default=PDSH_LAUNCHER,
-        type=str,
-        help="(optional) choose launcher backend for multi-node "
-        "training. Options currently include PDSH, OpenMPI, MVAPICH, SLURM, MPICH.")
+    parser.add_argument("--launcher",
+                        default=PDSH_LAUNCHER,
+                        type=str,
+                        help="(optional) choose launcher backend for multi-node "
+                        "training. Options currently include PDSH, OpenMPI, MVAPICH, SLURM, MPICH.")
 
     parser.add_argument("--launcher_args",
                         default="",
                         type=str,
                         help="(optional) pass launcher specific arguments as a "
                         "single quoted argument.")
 
@@ -143,45 +144,48 @@
                         help="Do not perform ssh check in multi-node launcher model")
 
     parser.add_argument("--force_multi",
                         action="store_true",
                         help="Force multi-node launcher mode, helps in cases where user "
                         "wants to launch on single remote node.")
 
-    parser.add_argument(
-        "--save_pid",
-        action="store_true",
-        help="Save file containing launcher process id (pid) at /tmp/<main-pid>.ds, "
-        "where <main-pid> is the pid of the first process that invoked `deepspeed`. "
-        "Useful when launching deepspeed processes programmatically.")
-
-    parser.add_argument(
-        "--enable_each_rank_log",
-        default="None",
-        type=str,
-        help="redirect the stdout and stderr from each rank into different log files")
-
-    parser.add_argument(
-        "--autotuning",
-        default="",
-        choices=["tune",
-                 "run"],
-        type=str,
-        help="Run DeepSpeed autotuner to discover optimal configuration parameters "
-        "before running job.")
+    parser.add_argument("--save_pid",
+                        action="store_true",
+                        help="Save file containing launcher process id (pid) at /tmp/<main-pid>.ds, "
+                        "where <main-pid> is the pid of the first process that invoked `deepspeed`. "
+                        "Useful when launching deepspeed processes programmatically.")
+
+    parser.add_argument("--enable_each_rank_log",
+                        default="None",
+                        type=str,
+                        help="redirect the stdout and stderr from each rank into different log files")
+
+    parser.add_argument("--autotuning",
+                        default="",
+                        choices=["tune", "run"],
+                        type=str,
+                        help="Run DeepSpeed autotuner to discover optimal configuration parameters "
+                        "before running job.")
 
     parser.add_argument("--elastic_training",
                         action="store_true",
                         help="Enable elastic training support in DeepSpeed.")
 
-    parser.add_argument("user_script",
-                        type=str,
-                        help="User script to launch, followed by any required "
+    parser.add_argument("user_script", type=str, help="User script to launch, followed by any required "
                         "arguments.")
     parser.add_argument('user_args', nargs=argparse.REMAINDER)
+    parser.add_argument("--bind_cores_to_rank",
+                        action="store_true",
+                        help="Bind each rank to different cores of the host")
+    parser.add_argument("--bind_core_list",
+                        type=str,
+                        default=None,
+                        help="List of cores to bind to with comma separated list of "
+                        "numbers and range. i.e. 1,3-5,7 => [1,3,4,5,7].  When not "
+                        "specified, all cores on system would be used rank binding")
     return parser.parse_args(args=args)
 
 
 def fetch_hostfile(hostfile_path):
     if not os.path.isfile(hostfile_path):
         logger.warning("Unable to find hostfile, will proceed with training "
                        "with local resources only.")
@@ -209,29 +213,23 @@
             # hostfile comment or empty line, ignore
             continue
         elif match:
             host = match.group(1)
             num_slots = int(match.group(2))
             if host in resource_pool:
                 logger.error(f"Bad hostfile text: {hostfile_lines}")
-                raise ValueError(
-                    f"Hostfile contains multiple entries for {host}, unable to proceed with launching"
-                )
+                raise ValueError(f"Hostfile contains multiple entries for {host}, unable to proceed with launching")
             resource_pool[host] = num_slots
         else:
             logger.error(f"Bad hostfile text: {hostfile_lines}")
-            raise ValueError(
-                "Hostfile contains a bad entry: {line}, unable to proceed with launching"
-            )
+            raise ValueError("Hostfile contains a bad entry: {line}, unable to proceed with launching")
 
     if len(resource_pool) == 0:
         logger.error(f"Bad hostfile text: {hostfile_lines}")
-        raise ValueError(
-            "Hostfile is empty or not formatted correctly, unable to proceed with launching."
-        )
+        raise ValueError("Hostfile is empty or not formatted correctly, unable to proceed with launching.")
 
     return resource_pool
 
 
 def _stable_remove_duplicates(data):
     # Create a new list in the same order as original but with duplicates
     # removed, should never be more than ~16 elements so simple is best
@@ -333,17 +331,15 @@
 
 
 def parse_inclusion_exclusion(resource_pool, inclusion, exclusion):
     active_resources = collections.OrderedDict()
     for hostname, slots in resource_pool.items():
         active_resources[hostname] = list(range(slots))
 
-    return parse_resource_filter(active_resources,
-                                 include_str=inclusion,
-                                 exclude_str=exclusion)
+    return parse_resource_filter(active_resources, include_str=inclusion, exclude_str=exclusion)
 
 
 def encode_world_info(world_info):
     world_info_json = json.dumps(world_info).encode('utf-8')
     world_info_base64 = base64.urlsafe_b64encode(world_info_json).decode('utf-8')
     return world_info_base64
 
@@ -385,16 +381,15 @@
 
     resource_pool = fetch_hostfile(args.hostfile)
 
     # respect CUDA_VISIBLE_DEVICES for a single node and no explicit resource filters
     cuda_visible_devices = os.environ.get("CUDA_VISIBLE_DEVICES", "")
     if not resource_pool and len(cuda_visible_devices):
         detected_str = f"Detected CUDA_VISIBLE_DEVICES={cuda_visible_devices}"
-        if len(args.include) or len(
-                args.exclude) or args.num_nodes > 1 or args.num_gpus > 0:
+        if len(args.include) or len(args.exclude) or args.num_nodes > 1 or args.num_gpus > 0:
             print(
                 f"{detected_str} but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed."
             )
         else:
             args.include = f"localhost:{cuda_visible_devices}"
             print(f"{detected_str}: setting --include={args.include}")
         del os.environ["CUDA_VISIBLE_DEVICES"]
@@ -412,28 +407,25 @@
         resource_pool['localhost'] = device_count
         args.master_addr = "127.0.0.1"
         multi_node_exec = False
 
     if not multi_node_exec and args.num_nodes > 1:
         raise ValueError("Num nodes is >1 but no extra nodes available via hostfile")
 
-    active_resources = parse_inclusion_exclusion(resource_pool,
-                                                 args.include,
-                                                 args.exclude)
+    active_resources = parse_inclusion_exclusion(resource_pool, args.include, args.exclude)
     env = os.environ.copy()
 
     # validate that passwordless-ssh is workly properly with this hostfile
     if multi_node_exec and not args.no_ssh_check:
         first_host = list(active_resources.keys())[0]
         try:
-            subprocess.check_call(
-                f'ssh -o PasswordAuthentication=no {first_host} hostname',
-                stderr=subprocess.DEVNULL,
-                stdout=subprocess.DEVNULL,
-                shell=True)
+            subprocess.check_call(f'ssh -o PasswordAuthentication=no {first_host} hostname',
+                                  stderr=subprocess.DEVNULL,
+                                  stdout=subprocess.DEVNULL,
+                                  shell=True)
         except subprocess.CalledProcessError:
             raise RuntimeError(
                 f"Using hostfile at {args.hostfile} but host={first_host} was not reachable via ssh. If you are running with a single node please remove {args.hostfile} or setup passwordless ssh."
             )
 
     if not args.master_addr:
         assert multi_node_exec
@@ -477,37 +469,35 @@
     # encode world info as base64 to make it easier to pass via command line
     world_info_base64 = encode_world_info(active_resources)
 
     multi_node_exec = args.force_multi or len(active_resources) > 1
 
     if not multi_node_exec:
         deepspeed_launch = [
-            sys.executable,
-            "-u",
-            "-m",
-            "deepspeed.launcher.launch",
-            f"--world_info={world_info_base64}",
-            f"--master_addr={args.master_addr}",
-            f"--master_port={args.master_port}"
+            sys.executable, "-u", "-m", "deepspeed.launcher.launch", f"--world_info={world_info_base64}",
+            f"--master_addr={args.master_addr}", f"--master_port={args.master_port}"
         ]
         if args.no_python:
             deepspeed_launch.append("--no_python")
         if args.module:
             deepspeed_launch.append("--module")
         if args.no_local_rank:
             deepspeed_launch.append("--no_local_rank")
         if args.save_pid:
             deepspeed_launch += ["--save_pid", f"{os.getpid()}"]
         if args.enable_each_rank_log:
-            deepspeed_launch.append(
-                f"--enable_each_rank_log={args.enable_each_rank_log}")
+            deepspeed_launch.append(f"--enable_each_rank_log={args.enable_each_rank_log}")
         if args.elastic_training:
             deepspeed_launch.append("--enable_elastic_training")
             deepspeed_launch.append(f"--max_elastic_nodes={args.max_elastic_nodes}")
             deepspeed_launch.append(f"--min_elastic_nodes={args.min_elastic_nodes}")
+        if args.bind_cores_to_rank:
+            deepspeed_launch.append("--bind_cores_to_rank")
+        if args.bind_core_list != None:
+            deepspeed_launch.append(f"--bind_core_list={args.bind_core_list}")
         cmd = deepspeed_launch + [args.user_script] + args.user_args
     else:
         args.launcher = args.launcher.lower()
         if args.launcher == PDSH_LAUNCHER:
             runner = PDSHRunner(args, world_info_base64)
         elif args.launcher == OPENMPI_LAUNCHER:
             runner = OpenMPIRunner(args, world_info_base64, resource_pool)
```

### Comparing `deepspeed-0.8.3/deepspeed/model_implementations/diffusers/unet.py` & `deepspeed-0.9.0/deepspeed/model_implementations/diffusers/unet.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,15 +1,18 @@
-'''
-Copyright 2022 The Microsoft DeepSpeed Team
-'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+
 import torch
 from ..features.cuda_graph import CUDAGraph
 
 
 class DSUNet(CUDAGraph, torch.nn.Module):
+
     def __init__(self, unet, enable_cuda_graph=True):
         super().__init__(enable_cuda_graph=enable_cuda_graph)
         self.unet = unet
         # SD pipeline accesses this attribute
         self.in_channels = unet.in_channels
         self.device = self.unet.device
         self.dtype = self.unet.dtype
@@ -55,9 +58,16 @@
         self.static_kwargs = kwargs
 
         with torch.cuda.graph(self._cuda_graphs):
             self.static_output = self._forward(*self.static_inputs, **self.static_kwargs)
 
         self.cuda_graph_created = True
 
-    def _forward(self, sample, timestamp, encoder_hidden_states, return_dict=True):
-        return self.unet(sample, timestamp, encoder_hidden_states, return_dict)
+    def _forward(self, sample, timestamp, encoder_hidden_states, return_dict=True, cross_attention_kwargs=None):
+        if cross_attention_kwargs:
+            return self.unet(sample,
+                             timestamp,
+                             encoder_hidden_states,
+                             return_dict,
+                             cross_attention_kwargs=cross_attention_kwargs)
+        else:
+            return self.unet(sample, timestamp, encoder_hidden_states, return_dict)
```

### Comparing `deepspeed-0.8.3/deepspeed/model_implementations/diffusers/vae.py` & `deepspeed-0.9.0/deepspeed/model_implementations/diffusers/vae.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,18 +1,22 @@
-'''
-Copyright 2022 The Microsoft DeepSpeed Team
-'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+
 import torch
 from ..features.cuda_graph import CUDAGraph
 
 
 class DSVAE(CUDAGraph, torch.nn.Module):
+
     def __init__(self, vae, enable_cuda_graph=True):
         super().__init__(enable_cuda_graph=enable_cuda_graph)
         self.vae = vae
+        self.config = vae.config
         self.device = self.vae.device
         self.dtype = self.vae.dtype
         self.vae.requires_grad_(requires_grad=False)
         self.decoder_cuda_graph_created = False
         self.encoder_cuda_graph_created = False
         self.all_cuda_graph_created = False
 
@@ -40,16 +44,15 @@
 
         # create cuda_graph and assign static_inputs and static_outputs
         self._decoder_cuda_graph = torch.cuda.CUDAGraph()
         self.static_decoder_inputs = inputs
         self.static_decoder_kwargs = kwargs
 
         with torch.cuda.graph(self._decoder_cuda_graph):
-            self.static_decoder_output = self._decode(*self.static_decoder_inputs,
-                                                      **self.static_decoder_kwargs)
+            self.static_decoder_output = self._decode(*self.static_decoder_inputs, **self.static_decoder_kwargs)
 
         self.decoder_cuda_graph_created = True
 
     def decode(self, *inputs, **kwargs):
         if self.enable_cuda_graph:
             if self.decoder_cuda_graph_created:
                 outputs = self._graph_replay_decoder(*inputs, **kwargs)
@@ -84,16 +87,15 @@
 
         # create cuda_graph and assign static_inputs and static_outputs
         self._encoder_cuda_graph = torch.cuda.CUDAGraph()
         self.static_encoder_inputs = inputs
         self.static_encoder_kwargs = kwargs
 
         with torch.cuda.graph(self._encoder_cuda_graph):
-            self.static_encoder_output = self._encode(*self.static_encoder_inputs,
-                                                      **self.static_encoder_kwargs)
+            self.static_encoder_output = self._encode(*self.static_encoder_inputs, **self.static_encoder_kwargs)
 
         self.encoder_cuda_graph_created = True
 
     def encode(self, *inputs, **kwargs):
         if self.enable_cuda_graph:
             if self.encoder_cuda_graph_created:
                 outputs = self._graph_replay_encoder(*inputs, **kwargs)
```

### Comparing `deepspeed-0.8.3/deepspeed/model_implementations/features/cuda_graph.py` & `deepspeed-0.9.0/deepspeed/model_implementations/features/cuda_graph.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,14 +1,17 @@
-'''
-Copyright 2023 The Microsoft DeepSpeed Team
-'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+
 from abc import ABC, abstractmethod
 
 
 class CUDAGraph(ABC):
+
     def __init__(self, enable_cuda_graph=False):
         super().__init__()
         self.enable_cuda_graph = enable_cuda_graph
 
     @abstractmethod
     def _create_cuda_graph(self):
         """
```

### Comparing `deepspeed-0.8.3/deepspeed/model_implementations/transformers/clip_encoder.py` & `deepspeed-0.9.0/deepspeed/model_implementations/transformers/clip_encoder.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,16 +1,19 @@
-'''
-Copyright 2022 The Microsoft DeepSpeed Team
-'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+
 import torch
 from deepspeed.accelerator import get_accelerator
 from ..features.cuda_graph import CUDAGraph
 
 
 class DSClipEncoder(CUDAGraph, torch.nn.Module):
+
     def __init__(self, enc, enable_cuda_graph=False):
         super().__init__(enable_cuda_graph=enable_cuda_graph)
         enc.text_model._build_causal_attention_mask = self._build_causal_attention_mask
         self.enc = enc
         self.device = self.enc.device
         self.dtype = self.enc.dtype
         self.cuda_graph_created = [False, False]
@@ -18,19 +21,15 @@
         self.static_kwargs = [None, None]
         self.static_output = [None, None]
         self._cuda_graphs = [None, None]
         self.iter = 0
         self.config = self.enc.config
 
     def _build_causal_attention_mask(self, bsz, seq_len, dtype):
-        mask = torch.empty(bsz,
-                           seq_len,
-                           seq_len,
-                           dtype=dtype,
-                           device=get_accelerator().current_device_name())
+        mask = torch.empty(bsz, seq_len, seq_len, dtype=dtype, device=get_accelerator().current_device_name())
         mask.fill_(torch.tensor(torch.finfo(dtype).min))
         mask.triu_(1)
         mask = mask.unsqueeze(1)
         return mask
 
     def _graph_replay(self, *inputs, **kwargs):
         for i in range(len(inputs)):
@@ -65,15 +64,14 @@
 
         # create cuda_graph and assign static_inputs and static_outputs
         self._cuda_graphs[self.iter] = torch.cuda.CUDAGraph()
         self.static_inputs[self.iter] = inputs
         self.static_kwargs[self.iter] = kwargs
 
         with torch.cuda.graph(self._cuda_graphs[self.iter]):
-            self.static_output[self.iter] = self._forward(
-                *self.static_inputs[self.iter],
-                **self.static_kwargs[self.iter])
+            self.static_output[self.iter] = self._forward(*self.static_inputs[self.iter],
+                                                          **self.static_kwargs[self.iter])
 
         self.cuda_graph_created[self.iter] = True
 
     def _forward(self, *inputs, **kwargs):
         return self.enc(*inputs, **kwargs)
```

### Comparing `deepspeed-0.8.3/deepspeed/model_implementations/transformers/ds_bert.py` & `deepspeed-0.9.0/deepspeed/model_implementations/transformers/ds_megatron_gpt.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,23 +1,20 @@
-'''
-Copyright 2022 The Microsoft DeepSpeed Team
-'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 from deepspeed.model_implementations.transformers.ds_transformer import DeepSpeedTransformerInference
 
 
-class DeepSpeedBERTInference(DeepSpeedTransformerInference):
-    """Initialize the DeepSpeed BERT Transformer Layer.
+class DeepSpeedMegatronGPTInference(DeepSpeedTransformerInference):
+    """Initialize the DeepSpeed Megatron GPT Transformer Layer.
     """
+
     def __init__(self,
                  config,
                  mp_group=None,
                  quantize_scales=None,
                  quantize_groups=1,
                  merge_count=1,
                  mlp_extra_grouping=False):
-        super().__init__(config,
-                         mp_group,
-                         quantize_scales,
-                         quantize_groups,
-                         merge_count,
-                         mlp_extra_grouping)
+        super().__init__(config, mp_group, quantize_scales, quantize_groups, merge_count, mlp_extra_grouping)
```

### Comparing `deepspeed-0.8.3/deepspeed/model_implementations/transformers/ds_bloom.py` & `deepspeed-0.9.0/deepspeed/model_implementations/transformers/ds_gpt.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,23 +1,20 @@
-'''
-Copyright 2022 The Microsoft DeepSpeed Team
-'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 from deepspeed.model_implementations.transformers.ds_transformer import DeepSpeedTransformerInference
 
 
-class DeepSpeedBloomInference(DeepSpeedTransformerInference):
-    """Initialize the DeepSpeed Bloom Transformer Layer.
+class DeepSpeedGPTInference(DeepSpeedTransformerInference):
+    """Initialize the DeepSpeed GPT Transformer Layer.
     """
+
     def __init__(self,
                  config,
                  mp_group=None,
                  quantize_scales=None,
                  quantize_groups=1,
                  merge_count=1,
                  mlp_extra_grouping=False):
-        super().__init__(config,
-                         mp_group,
-                         quantize_scales,
-                         quantize_groups,
-                         merge_count,
-                         mlp_extra_grouping)
+        super().__init__(config, mp_group, quantize_scales, quantize_groups, merge_count, mlp_extra_grouping)
```

### Comparing `deepspeed-0.8.3/deepspeed/model_implementations/transformers/ds_gpt.py` & `deepspeed-0.9.0/deepspeed/model_implementations/transformers/ds_bloom.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,23 +1,20 @@
-'''
-Copyright 2022 The Microsoft DeepSpeed Team
-'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 from deepspeed.model_implementations.transformers.ds_transformer import DeepSpeedTransformerInference
 
 
-class DeepSpeedGPTInference(DeepSpeedTransformerInference):
-    """Initialize the DeepSpeed GPT Transformer Layer.
+class DeepSpeedBloomInference(DeepSpeedTransformerInference):
+    """Initialize the DeepSpeed Bloom Transformer Layer.
     """
+
     def __init__(self,
                  config,
                  mp_group=None,
                  quantize_scales=None,
                  quantize_groups=1,
                  merge_count=1,
                  mlp_extra_grouping=False):
-        super().__init__(config,
-                         mp_group,
-                         quantize_scales,
-                         quantize_groups,
-                         merge_count,
-                         mlp_extra_grouping)
+        super().__init__(config, mp_group, quantize_scales, quantize_groups, merge_count, mlp_extra_grouping)
```

### Comparing `deepspeed-0.8.3/deepspeed/model_implementations/transformers/ds_transformer.py` & `deepspeed-0.9.0/deepspeed/model_implementations/transformers/ds_transformer.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-'''
-Copyright 2022 The Microsoft DeepSpeed Team
-'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import torch
 import torch.nn as nn
 from deepspeed import comm as dist
 from deepspeed.utils.logging import log_dist
 
 from deepspeed.ops.transformer.inference.ds_mlp import DeepSpeedMLP
@@ -52,45 +53,34 @@
             builder = InferenceBuilder()
             inference_cuda_module = builder.load()
 
         if DeepSpeedTransformerInference.layer_id == 1:
             log_dist(f"DeepSpeed-Inference config: {self.config.__dict__}", [0])
 
         if self.config.bigscience_bloom:
-            self.attention = BloomSelfAttention(self.config,
-                                                mp_group,
-                                                quantize_scales,
-                                                quantize_groups,
-                                                merge_count)
+            self.attention = BloomSelfAttention(self.config, mp_group, quantize_scales, quantize_groups, merge_count)
         else:
-            self.attention = DeepSpeedSelfAttention(self.config,
-                                                    mp_group,
-                                                    quantize_scales,
-                                                    quantize_groups,
+            self.attention = DeepSpeedSelfAttention(self.config, mp_group, quantize_scales, quantize_groups,
                                                     merge_count)
-        self.mlp = DeepSpeedMLP(self.config,
-                                mp_group,
-                                quantize_scales,
-                                quantize_groups,
-                                merge_count,
+        self.mlp = DeepSpeedMLP(self.config, mp_group, quantize_scales, quantize_groups, merge_count,
                                 mlp_extra_grouping)
 
-        device = get_accelerator().current_device_name(
-        )  # if config.bigscience_bloom else 'cpu'
-        self.norm_w = nn.Parameter(torch.empty(self.config.hidden_size,
-                                               dtype=data_type,
-                                               device=device),
-                                   requires_grad=False)
-        self.norm_b = nn.Parameter(torch.empty(self.config.hidden_size,
-                                               dtype=data_type,
-                                               device=device),
-                                   requires_grad=False)
+        device = get_accelerator().current_device_name()  # if config.bigscience_bloom else 'cpu'
+        if self.config.set_empty_params:
+            self.norm_w = None
+            self.norm_b = None
+        else:
+            self.norm_w = nn.Parameter(torch.empty(self.config.hidden_size, dtype=data_type, device=device),
+                                       requires_grad=False)
+            self.norm_b = nn.Parameter(torch.empty(self.config.hidden_size, dtype=data_type, device=device),
+                                       requires_grad=False)
         self.layer_past = None
         self.allocate_workspace = inference_cuda_module.allocate_workspace_fp32 if (not config.fp16) else \
                                 inference_cuda_module.allocate_workspace_fp16
+        self._alloc_workspace = True
 
     @classmethod
     def reset_cache(cls):
         if inference_cuda_module is not None:
             inference_cuda_module.reset_cache()
 
     def forward(
@@ -110,33 +100,33 @@
             encoder_attention_mask=None,
             use_cache=False,
             alibi=None,
             output_attentions=False,
             # TODO(arashb): 'layer_head_mask' and 'past_key_value' are only added to satisfy the OPT models API.
             # This needs to be redesigned later!
             layer_head_mask=None,
-            past_key_value=None):
+            past_key_value=None,
+            **kwargs):
 
         if x is not None:
             input = x
+        if "hidden_states" in kwargs:
+            input = kwargs["hidden_states"]
 
-        input_mask = (input_mask if attn_mask is None else
-                      attn_mask) if attention_mask is None else attention_mask
+        input_mask = (input_mask if attn_mask is None else attn_mask) if attention_mask is None else attention_mask
 
         # Allocate memory only on first layer forward
-        if self.config.layer_id == 0:
-            self.allocate_workspace(self.config.hidden_size,
-                                    self.config.heads,
+        if self.config.layer_id == 0 and self._alloc_workspace:
+            self.allocate_workspace(self.config.hidden_size, self.config.heads,
                                     input.size()[1],
-                                    input.size()[0],
-                                    DeepSpeedTransformerInference.layer_id,
-                                    self.config.mp_size,
+                                    input.size()[0], DeepSpeedTransformerInference.layer_id, self.config.mp_size,
                                     self.config.bigscience_bloom,
-                                    dist.get_rank() if dist.is_initialized() else 0,
-                                    self.config.max_out_tokens)
+                                    dist.get_rank() if dist.is_initialized() else 0, self.config.max_out_tokens,
+                                    self.config.min_out_tokens)
+            self._alloc_workspace = False
 
         get_present = (get_present or get_key_value or use_cache)
         input_mask = input_mask if attention_mask is None else attention_mask
 
         # We set the prev key/value to None when there is a prompt
         if input.shape[1] > 1:
             self.layer_past = None
@@ -167,18 +157,15 @@
                                               alibi)
 
             presents = (key, value)
             self.layer_past = presents if layer_past is None else None
             output = self.mlp(attention_output, input, inp_norm, self.attention.attn_ob)
 
             if not self.config.pre_layer_norm:
-                output = inference_cuda_module.layer_norm(output,
-                                                          self.norm_w,
-                                                          self.norm_b,
-                                                          self.config.epsilon)
+                output = inference_cuda_module.layer_norm(output, self.norm_w, self.norm_b, self.config.epsilon)
 
             output = output.to(input_type)
         if get_present:
             output = (output, presents)
 
         if self.config.return_single_tuple:
             return (output, )
```

### Comparing `deepspeed-0.8.3/deepspeed/module_inject/auto_tp.py` & `deepspeed-0.9.0/deepspeed/module_inject/auto_tp.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,17 +1,21 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 # Automatic Tensor Parallelism
 import re
 
 from torch import nn
 from .replace_policy import replace_policies
 
 
 class AutoTP():
+
     def in_module_list(module, module_list):
         for item in module_list:
             if type(item).__name__ == type(module).__name__:
                 return True
         return False
 
     def get_module_list(model):
@@ -24,26 +28,15 @@
                     elif not AutoTP.in_module_list(module, mlist):
                         mlist = mlist + [module]
             else:
                 mlist = mlist + AutoTP.get_module_list(child)
         return mlist
 
     def supported(model):
-        unsupported = [
-            'bloom',
-            'codegen',
-            'deberta',
-            'flaubert',
-            'fsmt',
-            'gpt2',
-            'led',
-            'longformer',
-            'xlm',
-            'xlnet'
-        ]
+        unsupported = ['codegen', 'deberta', 'flaubert', 'fsmt', 'gpt2', 'led', 'longformer', 'xlm', 'xlnet']
         model = str(model)
         key = re.search(r": (.*?)Model", model)
         if key is None:
             key = re.search(r": (.*?)Stack", model)
         if key is None:
             key = re.match(r"(.*?)Model", model)
         assert key is not None, "Not able to determine model policy automatically. Please provide policy."
@@ -52,16 +45,15 @@
         return True
 
     def get_layers(parent, module):
         layer_list = []
         for key, submodule in module._modules.items():
             if isinstance(submodule, nn.Linear):
                 layer_list = layer_list + [parent + "." + key]
-            elif isinstance(submodule,
-                            nn.LayerNorm) or key == 'LayerNorm' or key == 'layer_norm':
+            elif isinstance(submodule, nn.LayerNorm) or key == 'LayerNorm' or key == 'layer_norm':
                 layer_list = layer_list + ["ln"]
             else:
                 layer_list = layer_list + AutoTP.get_layers(key, submodule)
         return layer_list
 
     def update_policy_list(policy_list, new_module, new_gems):
         if len(policy_list):
@@ -98,17 +90,15 @@
         module_list = AutoTP.get_module_list(model)
         assert AutoTP.supported(model), "AutoTP not supported for model. Please use kernel injection since container policy for model exists." \
         if AutoTP.kernel_supported(module_list) else "AutoTP not supported for model. Please provide policy."
         for module in module_list:
             for key, submodule in module._modules.items():
                 if isinstance(submodule, nn.Linear):
                     layer_list = layer_list + ["." + key]
-                elif isinstance(
-                        submodule,
-                        nn.LayerNorm) or key == 'LayerNorm' or key == 'layer_norm':
+                elif isinstance(submodule, nn.LayerNorm) or key == 'LayerNorm' or key == 'layer_norm':
                     layer_list = layer_list + ["ln"]
                 else:
                     layer_list = layer_list + AutoTP.get_layers(key, submodule)
             for i, layer in enumerate(layer_list):
                 if layer == 'ln':
                     if layer_list[i - 1] != 'ln':
                         gem_list = gem_list + [layer_list[i - 1]]
```

### Comparing `deepspeed-0.8.3/deepspeed/module_inject/containers/__init__.py` & `deepspeed-0.9.0/deepspeed/module_inject/containers/__init__.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,8 +1,11 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 from .bert import DS_BERTContainer, HFBertLayerPolicy
 from .bloom import DS_BloomContainer, BLOOMLayerPolicy, supported_models
 from .distil_bert import DS_DistilBERTContainer, HFDistilBertLayerPolicy
 from .gpt2 import DS_GPT2Container, HFGPT2LayerPolicy
 from .gptj import DS_GPTJContainer, HFGPTJLayerPolicy
 from .gptneo import DS_GPTNEOContainer, HFGPTNEOLayerPolicy
```

### Comparing `deepspeed-0.8.3/deepspeed/module_inject/containers/base.py` & `deepspeed-0.9.0/deepspeed/ops/transformer/inference/diffusers_attention.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,248 +1,195 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
 
-# Create a container object to save model-specific tensors using the policy file above.
-from abc import ABC
-import torch
+# DeepSpeed Team
 
-from deepspeed.ops.transformer.inference.config import DeepSpeedInferenceConfig
+import math
+import torch
+from torch.autograd import Function
+import torch.nn as nn
+from packaging import version as pkg_version
+from deepspeed.utils.logging import log_dist
 from deepspeed.accelerator import get_accelerator
+from deepspeed.ops.op_builder import InferenceBuilder
 
+# Cuda modules will be imported if needed
+inference_cuda_module = None
+minus_inf = -10000.0
+triton_flash_attn = None
+
+
+def load_triton_flash_attn():
+    global triton_flash_attn
+    try:
+        import triton
+    except ImportError:
+        raise ImportError("Please install triton 2.0+ or `pip install deepspeed[sd]`")
+
+    if pkg_version.parse(triton.__version__) < pkg_version.parse("2.0"):
+        raise ImportError("Please install triton 2.0+ or `pip install deepspeed[sd]`")
+
+    from .triton_ops import triton_flash_attn
+
+
+class DeepSpeedDiffusersAttentionFunction(Function):
+
+    @staticmethod
+    def forward(ctx, input, context, input_mask, config, attn_qkvw, attn_qw, attn_kw, attn_vw, attn_qkvb,
+                num_attention_heads_per_partition, norm_factor, hidden_size_per_partition, attn_ow, attn_ob,
+                do_out_bias, score_context_func, linear_func, triton_flash_attn_kernel):
+
+        def _transpose_for_context(x):
+            x = x.permute(0, 2, 1, 3)
+            new_x_layer_shape = x.size()[:-2] + \
+                                      (hidden_size_per_partition,)
+            return x.reshape(*new_x_layer_shape)
+
+        def _transpose_for_scores(x):
+            attention_head_size = x.shape[-1] // num_attention_heads_per_partition
+            new_x_shape = x.size()[:-1] + (num_attention_heads_per_partition, attention_head_size)
+            x = x.reshape(*new_x_shape)
+            x = x.permute(0, 2, 1, 3)
+            return x.contiguous()
+
+        def selfAttention_fp(input, context, input_mask):
+            if config.fp16 and input.dtype == torch.float32:
+                input = input.half()
+            head_size = input.shape[-1] // config.heads
+            do_flash_attn = (head_size <= 128)
+            scale = (1 / norm_factor) * (1 / norm_factor)
+            if do_flash_attn and context == None:
+                qkv_out = linear_func(input, attn_qkvw, attn_qkvb if attn_qkvb is not None else attn_qkvw, attn_qkvb
+                                      is not None, do_flash_attn, config.heads)
+
+                context_layer = triton_flash_attn_kernel(qkv_out[0], qkv_out[1], qkv_out[2], scale,
+                                                         input.shape[-2] % 128 == 0)
+                context_layer = _transpose_for_context(context_layer[:, :, :, :head_size])
+
+            else:
+                do_flash_attn = False
+                if context is not None:
+                    query = torch.matmul(input, attn_qw)
+                    key = torch.matmul(context, attn_kw)
+                    value = torch.matmul(context, attn_vw)
+                else:
+                    qkv = torch.matmul(input, attn_qkvw)
+                    query, key, value = qkv.chunk(3, dim=-1)
+                    query = query.contiguous()
+                    key = key.contiguous()
+                    value = value.contiguous()
+                query, key, value = inference_cuda_module.pad_transform_fp16(query, key, value, config.heads,
+                                                                             do_flash_attn)
+                attention_scores = (torch.matmul(query, key.transpose(-1, -2)) * scale).softmax(dim=-1)
+                context_layer = _transpose_for_context(torch.matmul(attention_scores, value))
+
+            output = linear_func(context_layer, attn_ow, attn_ob, do_out_bias, False, config.heads)
+            return output
+
+        output = selfAttention_fp(input, context, input_mask)
+
+        return output
+
+    @staticmethod
+    def backward(ctx, grad_output, grad_output1, grad_output2, grad_output3):
+        raise RuntimeError('You are running with DeepSpeed Inference mode. \
+                            Please switch to Training mode for running backward!')
+
+
+class DeepSpeedDiffusersAttention(nn.Module):
+    """Initialize the DeepSpeed Transformer Layer.
+        Arguments:
+            layer_id: The layer index starting from 0, e.g. if model has 24 transformer layers,
+                layer_id will be 0,1,2...23 when each layer object is instantiated
+            config: An object of DeepSpeedInferenceConfig
+    """
+    layer_id = 0
+
+    def __init__(
+        self,
+        config,
+    ):
+        super(DeepSpeedDiffusersAttention, self).__init__()
 
-class BaseConvolutionContainer(ABC):
-    # not implemented
-    def __init__(self):
-        pass
-
-
-class BaseTransformerContainer(ABC):
-    def __init__(self, policy, config, model_config, layer_id, child):
-        self.policy = policy
         self.config = config
-        self.model_config = model_config
-        self.layer_id = layer_id
-        self.child = child
-
-        self.megatron_v2 = self.policy.is_megatron_v2
-        self.scale_attention = self.policy.scale_attention
-        self.ckpt_load_enabled = False
-
-        # configuration for models. todo: can this be moved to a pydantic model config?
-        self.hidden_size = None
-        self.num_attention_heads = None
-        self.mp_size = self.config.tensor_parallel.tp_size
-        self.pre_layer_norm = self.policy.pre_attn_norm
-        self.fp16 = False
-        self.attn_linear_layer = self.policy.linear_layer
-        self.mlp_linear_layer = self.policy.linear_layer
-        self.layer_norm_eps = self.model_config.layer_norm_eps if \
-            hasattr(self.model_config, 'layer_norm_eps') else (self.model_config.layer_norm_epsilon if \
-            hasattr(self.model_config, 'layer_norm_epsilon') else self.model_config.layernorm_epsilon if \
-            hasattr(self.model_config, 'layernorm_epsilon') else 1.0e-12)
-        self.return_tuple = self.config.return_tuple
-        self.triangular_masking = True
-        self.local_attention = ((self.model_config.attention_layers[self.layer_id]
-                                 == "local") if hasattr(self.model_config,
-                                                        'attention_layers') else False)
-        self.window_size = getattr(self.model_config, "window_size", 1)
-        self.mlp_act_func_type = self.policy.mlp_act_func_type
-        self.training_mp_size = self.config.training_mp_size
-        self.bigscience_bloom = False
-        self.max_out_tokens = self.config.max_out_tokens
-        self.scale_attn_by_inverse_layer_idx = getattr(
-            self.config,
-            "scale_attn_by_inverse_layer_idx",
-            False)
-        self.use_mup = self.policy.use_mup
-        self.return_single_tuple = False
-        self.rotary_dim = self.model_config.rotary_dim if hasattr(self.model_config, 'rotary_dim') \
-                          else self.child.attention.rotary_ndims if \
-                          hasattr(self.child, 'attention') and hasattr(self.child.attention,'rotary_ndims') else -1
-        self.mlp_after_attn = (self.rotary_dim is None or self.rotary_dim < 0)
-
-        # Attention tensors
-        self.qkvw = None
-        self.qkvb = None
-        self.dense_w = None
-        self.dense_b = None
-        # MLP tensors
-        self._h4h_w = None
-        self._h4h_b = None
-        self._4hh_w = None
-        self._4hh_b = None
-        # LayerNorm tensors
-        self.attn_nw = None
-        self.attn_nb = None
-        self.input_nw = None
-        self.input_nb = None
-
-    def create_ds_model_config(self):
-        self.set_hidden_heads(*self.policy.get_hidden_heads())
-        assert self.num_attention_heads % self.mp_size == 0,\
-                "To run the model parallel across the GPUs, the attention_heads require to be divisible by the world_size!" +\
-                "This is because the attention computation is partitioned evenly among the parallel GPUs."
-
-        self.ds_model_config = DeepSpeedInferenceConfig(
-            hidden_size=self.hidden_size,
-            heads=self.num_attention_heads,
-            layer_norm_eps=self.layer_norm_eps,
-            fp16=self.fp16,
-            pre_layer_norm=self.pre_layer_norm,
-            mp_size=self.mp_size,
-            q_int8=self.quantize,
-            return_tuple=self.return_tuple,
-            triangular_masking=self.triangular_masking,
-            local_attention=self.local_attention,
-            window_size=self.window_size,
-            rotary_dim=self.rotary_dim,
-            mlp_after_attn=self.mlp_after_attn,
-            mlp_act_func_type=self.mlp_act_func_type,
-            training_mp_size=self.training_mp_size,
-            bigscience_bloom=self.bigscience_bloom,
-            max_out_tokens=self.max_out_tokens,
-            scale_attn_by_inverse_layer_idx=self.scale_attn_by_inverse_layer_idx,
-            use_mup=self.use_mup,
-            return_single_tuple=self.return_single_tuple,
-        )
-
-        return self.ds_model_config
-
-    def initialize_tensors(self):
-        # Set the tensors from policy (user module) to container (DS module)
-        self.set_attention(*self.policy.attention())
-        self.set_mlp(*self.policy.mlp())
-        self.set_layernorm(*self.policy.layernorm())
-
-    def convert_to_required_dtype(self, dtype):
-        # Note: converting tensors to fp16 requires that we do it in-place using self.__dict__ and not make a list/dict copy
-        if dtype == torch.half:
-            for k, v in self.__dict__.items():
-                # The list comprehension is used for MoE tensor lists
-                if isinstance(v, list) and all((isinstance(tensor, torch.Tensor) \
-                   or isinstance(tensor, torch.nn.Parameter)) for tensor in v):
-                    self.__dict__[k] = [moe_tensor.half() for moe_tensor in v]
-
-                if isinstance(v, torch.Tensor) or isinstance(v, torch.nn.Parameter):
-                    self.__dict__[k] = v.half()
-
-    def set_dtype(self, fp16=False):
-        self.fp16 = fp16
-
-    def set_moe(self, moe=False):
-        self.moe = moe
-
-    def set_tensor_parallel_config(self, mp_size, mp_group):
-        self.mp_size = mp_size
-        self.mp_group = mp_group
-
-    def set_quantization_config(self, quantize, quantizer):
-        self.quantize = quantize
-        self.quantizer = quantizer
-
-    def set_hidden_heads(self, hidden_size, num_attention_heads):
-        self.hidden_size = hidden_size
-        self.num_attention_heads = num_attention_heads
-
-    def set_attention(self, qkvw, qkvb, dense_w, dense_b):
-        self.qkvw = qkvw
-        self.qkvb = qkvb
-        self.dense_w = dense_w
-        self.dense_b = dense_b
-
-    def set_mlp(self, _h4h_w, _h4h_b, _4hh_w, _4hh_b):
-        self._h4h_w = _h4h_w
-        self._h4h_b = _h4h_b
-        self._4hh_w = _4hh_w
-        self._4hh_b = _4hh_b
-
-    def set_layernorm(self, attn_nw, attn_nb, input_nw, input_nb):
-        self.attn_nw = attn_nw
-        self.attn_nb = attn_nb
-        self.input_nw = input_nw
-        self.input_nb = input_nb
-
-    def apply_weight_quantization(self):
-        # quantize attention weights
-        self.attention_quantization()
-
-        # quantize mlp weights
-        self.mlp_quantization()
-
-    def attention_quantization(self):
-        self.module.attention.attn_qkvw = self.quantizer.quantize(
-            self.module.attention.attn_qkvw)
-        self.module.attention.attn_ow = self.quantizer.quantize(
-            self.module.attention.attn_ow)
-
-    def mlp_quantization(self):
-        self.module.mlp.inter_w = self.quantizer.quantize(self.module.mlp.inter_w)
-        self.module.mlp.output_w = self.quantizer.quantize(self.module.mlp.output_w)
-
-    def apply_tensor_parallelism(self, mp_replace):
-        # setup the new Attention module
-        self.attention_qkv_mp(mp_replace)
-        self.attention_o_mp(mp_replace)
-
-        # setup the new MLP module
-        self.mlp_inter_mp(mp_replace)
-        self.mlp_output_mp(mp_replace)
-
-        # Apply weight quantization
-        self.apply_weight_quantization()
-
-    def attention_qkv_mp(self, mp_replace):
-        self.module.attention.attn_qkvw = mp_replace.qkv_copy(
-            self.module.attention.attn_qkvw,
-            self.qkvw)
-        self.module.attention.attn_qkvb = mp_replace.qkv_copy(
-            self.module.attention.attn_qkvb,
-            self.qkvb)
-
-    def attention_o_mp(self, mp_replace):
-        self.module.attention.attn_ow = mp_replace.copy(self.module.attention.attn_ow,
-                                                        self.dense_w)
-        self.module.attention.attn_ob = mp_replace.copy(self.module.attention.attn_ob,
-                                                        self.dense_b)
-
-    def mlp_inter_mp(self, mp_replace):
-        self.module.mlp.inter_w = mp_replace.copy(self.module.mlp.inter_w, self._h4h_w)
-        self.module.mlp.inter_b = mp_replace.copy(self.module.mlp.inter_b, self._h4h_b)
-
-    def mlp_output_mp(self, mp_replace):
-        self.module.mlp.output_w = mp_replace.copy(self.module.mlp.output_w, self._4hh_w)
-        self.module.mlp.output_b = mp_replace.copy(self.module.mlp.output_b, self._4hh_b)
-
-    def copy_data_to_new_module(self):
-        if self.attn_nw is None:
-            self.module.mlp.attn_nw = self.attn_nw
-            self.module.mlp.attn_nb = self.attn_nb
-        else:
-            self.module.mlp.attn_nw.data.copy_(
-                self.attn_nw.to(get_accelerator().current_device_name()))
-            self.module.mlp.attn_nb.data.copy_(
-                self.attn_nb.to(get_accelerator().current_device_name()))
-
-        self.module.norm_w.data.copy_(
-            self.input_nw.to(get_accelerator().current_device_name()))
-        self.module.norm_b.data.copy_(
-            self.input_nb.to(get_accelerator().current_device_name()))
-
-    def transpose(self):
-        self.transpose_attention()
-        self.transpose_mlp()
-
-    def transpose_attention(self):
-        if self.attn_linear_layer:
-            self.qkvw = self.transpose_impl(self.qkvw.data)
-            self.dense_w = self.transpose_impl(self.dense_w.data)
-
-    def transpose_mlp(self):
-        if self.mlp_linear_layer:
-            self._h4h_w = self.transpose_impl(self._h4h_w.data)
-            self._4hh_w = self.transpose_impl(self._4hh_w.data)
-
-    def transpose_impl(self, data):
-        data = data.contiguous()
-        data.reshape(-1).copy_(data.transpose(-1, -2).contiguous().reshape(-1))
-        data = data.reshape(data.shape[-1], data.shape[-2])
-        data.to(get_accelerator().current_device_name())
-        return data
+        self.config.layer_id = DeepSpeedDiffusersAttention.layer_id
+        DeepSpeedDiffusersAttention.layer_id += 1
+        device = get_accelerator().current_device_name() if config.bigscience_bloom else 'cpu'
+        qkv_size_per_partition = (self.config.hidden_size // self.config.mp_size) * 3
+
+        data_type = torch.int8 if config.q_int8 else torch.half if config.fp16 else torch.float
+        data_type_fp = torch.half if config.fp16 else torch.float
+        global inference_cuda_module
+        if inference_cuda_module is None:
+            builder = InferenceBuilder()
+            inference_cuda_module = builder.load()
+
+        if DeepSpeedDiffusersAttention.layer_id == 1:
+            log_dist(f"DeepSpeed-Attention config: {self.config.__dict__}", [0])
+
+        self.attn_qkvw = nn.Parameter(torch.empty(self.config.hidden_size,
+                                                  qkv_size_per_partition,
+                                                  dtype=data_type,
+                                                  device=device),
+                                      requires_grad=False)
+        self.attn_kw = nn.Parameter(torch.empty(self.config.hidden_size,
+                                                self.config.hidden_size,
+                                                dtype=data_type,
+                                                device=device),
+                                    requires_grad=False)
+        self.attn_vw = nn.Parameter(torch.empty(self.config.hidden_size,
+                                                self.config.hidden_size,
+                                                dtype=data_type,
+                                                device=device),
+                                    requires_grad=False)
+        self.attn_qw = nn.Parameter(torch.empty(self.config.hidden_size,
+                                                self.config.hidden_size,
+                                                dtype=data_type,
+                                                device=device),
+                                    requires_grad=False)
+        self.attn_qkvb = nn.Parameter(torch.empty(qkv_size_per_partition, dtype=data_type_fp, device=device),
+                                      requires_grad=False)
+        out_size_per_partition = self.config.hidden_size // self.config.mp_size
+        self.attn_ow = nn.Parameter(torch.empty(out_size_per_partition,
+                                                self.config.hidden_size,
+                                                dtype=data_type,
+                                                device=device),
+                                    requires_grad=False)
+
+        self.attn_ob = nn.Parameter(torch.empty(self.config.hidden_size, dtype=data_type_fp, device=device),
+                                    requires_grad=False)
+        self.do_out_bias = True
+
+        if triton_flash_attn is None:
+            load_triton_flash_attn()
+        self.triton_flash_attn_kernel = triton_flash_attn()
+        self.num_attention_heads_per_partition = self.config.heads // self.config.mp_size
+        self.hidden_size_per_partition = self.config.hidden_size // self.config.mp_size
+        self.hidden_size_per_attention_head = self.config.hidden_size // self.config.heads
+
+        self.norm_factor = math.sqrt(math.sqrt(self.config.hidden_size // self.config.heads))
+
+        if self.config.scale_attn_by_inverse_layer_idx is True:
+            self.norm_factor *= math.sqrt(self.config.layer_id + 1)
+            # https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/gpt2/modeling_gpt2.py#L191
+
+        self.score_context_func = inference_cuda_module.softmax_context_fp32 if (not config.fp16) else \
+                                    inference_cuda_module.softmax_context_fp16
+        self.linear_func = inference_cuda_module.linear_layer_fp16 if config.fp16 else \
+                                    inference_cuda_module.linear_layer_fp32
+        self.allocate_workspace = inference_cuda_module.allocate_workspace_fp32 if not (config.fp16) else \
+                                    inference_cuda_module.allocate_workspace_fp16
+
+    def forward(self, input, context=None, input_mask=None):
+        if self.config.layer_id == 0:
+            self.allocate_workspace(self.config.hidden_size, self.config.heads,
+                                    input.size()[1],
+                                    input.size()[0], DeepSpeedDiffusersAttention.layer_id, self.config.mp_size, False,
+                                    0, self.config.max_out_tokens)
+        output = DeepSpeedDiffusersAttentionFunction.apply(input, context, input_mask, self.config, self.attn_qkvw,
+                                                           self.attn_qw, self.attn_kw, self.attn_vw, self.attn_qkvb,
+                                                           self.num_attention_heads_per_partition, self.norm_factor,
+                                                           self.hidden_size_per_partition, self.attn_ow, self.attn_ob,
+                                                           self.do_out_bias, self.score_context_func, self.linear_func,
+                                                           self.triton_flash_attn_kernel)
+
+        return output
```

### Comparing `deepspeed-0.8.3/deepspeed/module_inject/containers/base_moe.py` & `deepspeed-0.9.0/deepspeed/module_inject/containers/base_moe.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,28 +1,30 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 # Create a container object to save model-specific tensors using the policy file above.
 from .base import *
 from deepspeed import comm as dist
 import deepspeed.ops.transformer as transformer_inference
 from deepspeed.accelerator import get_accelerator
 
 
 class BaseTransformerMoEContainer(BaseTransformerContainer):
+
     def __init__(self, **kwargs):
         # Call the init function of the parent class to initialize the tensors and configs from parent class
         super().__init__(**kwargs)
 
         self.num_experts = self.policy.get_num_experts()
         self.ep_world_size = dist.get_world_size()
         self.local_ep_size = 1 if self.num_experts < self.ep_world_size else self.num_experts // self.ep_world_size
 
-        self.layer_norm_eps = self.config.layer_norm_eps if hasattr(
-            self.config,
-            'layer_norm_eps') else 1e-12,
+        self.layer_norm_eps = self.config.layer_norm_eps if hasattr(self.config, 'layer_norm_eps') else 1e-12,
 
         # MoE models will have a list of mlp related tensors
         self._h4h_w = []
         self._h4h_b = []
         self._4hh_w = []
         self._4hh_b = []
 
@@ -98,44 +100,31 @@
         # setup the new MLP module
         self.mlp_mp()
 
     def mlp_mp(self):
         gpu_index = dist.get_rank()
         for ep_index in range(self.local_ep_size):
             # mlp inter
-            self.module.mlp[ep_index].inter_w.data = self._h4h_w[
-                gpu_index * self.local_ep_size + ep_index].to(
-                    get_accelerator().current_device_name())
-            self.module.mlp[ep_index].inter_b.data = self._h4h_b[
-                gpu_index * self.local_ep_size + ep_index].to(
-                    get_accelerator().current_device_name())
+            self.module.mlp[ep_index].inter_w.data = self._h4h_w[gpu_index * self.local_ep_size + ep_index].to(
+                get_accelerator().current_device_name())
+            self.module.mlp[ep_index].inter_b.data = self._h4h_b[gpu_index * self.local_ep_size + ep_index].to(
+                get_accelerator().current_device_name())
 
             # mlp output
-            self.module.mlp[ep_index].output_w.data = self._4hh_w[
-                gpu_index * self.local_ep_size + ep_index].to(
-                    get_accelerator().current_device_name())
-            self.module.mlp[ep_index].output_b.data = self._4hh_b[
-                gpu_index * self.local_ep_size + ep_index].to(
-                    get_accelerator().current_device_name())
+            self.module.mlp[ep_index].output_w.data = self._4hh_w[gpu_index * self.local_ep_size + ep_index].to(
+                get_accelerator().current_device_name())
+            self.module.mlp[ep_index].output_b.data = self._4hh_b[gpu_index * self.local_ep_size + ep_index].to(
+                get_accelerator().current_device_name())
 
     def copy_data_to_new_module(self):
-        self.module.attn_nw.data = self.attn_nw.to(
-            get_accelerator().current_device_name())
-        self.module.attn_nb.data = self.attn_nb.to(
-            get_accelerator().current_device_name())
-
-        self.module.norm_w.data.copy_(
-            self.input_nw.to(get_accelerator().current_device_name()))
-        self.module.norm_b.data.copy_(
-            self.input_nb.to(get_accelerator().current_device_name()))
+        self.module.attn_nw.data = self.attn_nw.to(get_accelerator().current_device_name())
+        self.module.attn_nb.data = self.attn_nb.to(get_accelerator().current_device_name())
+
+        self.module.norm_w.data.copy_(self.input_nw.to(get_accelerator().current_device_name()))
+        self.module.norm_b.data.copy_(self.input_nb.to(get_accelerator().current_device_name()))
 
         if self.config.moe.type == 'residual':
-            self.module.res_mlp.inter_w.data = self._res_h4h_w.to(
-                get_accelerator().current_device_name())
-            self.module.res_mlp.inter_b.data = self._res_h4h_b.to(
-                get_accelerator().current_device_name())
-            self.module.res_mlp.output_w.data = self._res_4hh_w.to(
-                get_accelerator().current_device_name())
-            self.module.res_mlp.output_b.data = self._res_4hh_b.to(
-                get_accelerator().current_device_name())
-            self.module.res_coef.data = self._res_coef.to(
-                get_accelerator().current_device_name())
+            self.module.res_mlp.inter_w.data = self._res_h4h_w.to(get_accelerator().current_device_name())
+            self.module.res_mlp.inter_b.data = self._res_h4h_b.to(get_accelerator().current_device_name())
+            self.module.res_mlp.output_w.data = self._res_4hh_w.to(get_accelerator().current_device_name())
+            self.module.res_mlp.output_b.data = self._res_4hh_b.to(get_accelerator().current_device_name())
+            self.module.res_coef.data = self._res_coef.to(get_accelerator().current_device_name())
```

### Comparing `deepspeed-0.8.3/deepspeed/module_inject/containers/bert.py` & `deepspeed-0.9.0/deepspeed/module_inject/containers/bert.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,17 +1,21 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 from .base import *
 from deepspeed.model_implementations.transformers.ds_bert import DeepSpeedBERTInference
 import torch
 from torch.nn.parameter import Parameter
 from ..policy import TransformerPolicy
 
 
 class DS_BERTContainer(BaseTransformerContainer):
+
     def __init__(self, **kwargs):
         super().__init__(**kwargs)
 
         # All model specific things should be defined here instead of the base class.
         self.return_tuple = True
         self.triangular_masking = False
 
@@ -19,14 +23,15 @@
         _config = config if config is not None else self.ds_model_config
         self.module = DeepSpeedBERTInference(_config, mp_group=self.mp_group)
         self.module.config.scale_attention = self.scale_attention
         return self.module
 
 
 class HFBertLayerPolicy(TransformerPolicy):
+
     def __init__(self, client_module, inference=False):
         super().__init__(inference, pre_attn_norm=False)
         self.client_module = client_module
         self.cuda_graph_supported = True
 
         if HFBertLayerPolicy._orig_layer_class is None:
             try:
@@ -35,27 +40,35 @@
                     transformers.models.bert.modeling_bert.BertLayer,
                     transformers.models.roberta.modeling_roberta.RobertaLayer
                 ]
             except:
                 HFBertLayerPolicy._orig_layer_class = None
 
     def get_hidden_heads(self):
+        if self.pre_attn_norm:
+            attention_layernorm = self.client_module.PostAttentionLayerNorm
+        else:
+            attention_layernorm = self.client_module.attention.output.LayerNorm
         return self.client_module.attention.self.query.weight.shape[1], \
-                self.client_module.attention.self.num_attention_heads
+                self.client_module.attention.self.num_attention_heads, \
+                attention_layernorm.eps
 
-    def attention(self):
+    def get_q_k_v(self):
+        return None
+
+    def attention(self, enable_training=False):
         qw = self.client_module.attention.self.query.weight
         qb = self.client_module.attention.self.query.bias
         kw = self.client_module.attention.self.key.weight
         kb = self.client_module.attention.self.key.bias
         vw = self.client_module.attention.self.value.weight
         vb = self.client_module.attention.self.value.bias
 
-        qkvw = Parameter(torch.cat((qw, kw, vw), dim=0), requires_grad=False)
-        qkvb = Parameter(torch.cat((qb, kb, vb), dim=0), requires_grad=False)
+        qkvw = Parameter(torch.cat((qw, kw, vw), dim=0), requires_grad=enable_training)
+        qkvb = Parameter(torch.cat((qb, kb, vb), dim=0), requires_grad=enable_training)
 
         return qkvw, \
                qkvb, \
                self.client_module.attention.output.dense.weight, \
                self.client_module.attention.output.dense.bias, \
 
     def mlp(self):
@@ -75,7 +88,10 @@
         else:
             attention_layernorm = self.client_module.attention.output.LayerNorm
             transformer_layernorm = self.client_module.output.LayerNorm
         return attention_layernorm.weight, \
                attention_layernorm.bias, \
                transformer_layernorm.weight, \
                transformer_layernorm.bias
+
+    def get_lora_params(self):
+        return []
```

### Comparing `deepspeed-0.8.3/deepspeed/module_inject/containers/bloom.py` & `deepspeed-0.9.0/deepspeed/module_inject/containers/bloom.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,40 +1,40 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 from .base import *
 from .features.meta_tensor import MetaTensorContainer
 from deepspeed.model_implementations.transformers.ds_bloom import DeepSpeedBloomInference
 from ..policy import TransformerPolicy
 from ..policy import transformer_param_names
 from ..policy import maybe_copy
 
 supported_models = {None}
 
 
 class DS_BloomContainer(MetaTensorContainer, BaseTransformerContainer):
+
     def __init__(self, **kwargs):
         super().__init__(**kwargs)
 
         # All model specific things should be defined here instead of the base class.
         self.bigscience_bloom = True
 
     def create_module(self, config=None):
         _config = config if config is not None else self.ds_model_config
 
         self.module = DeepSpeedBloomInference(_config, mp_group=self.mp_group)
         self.module.config.scale_attention = self.scale_attention
         return self.module
 
-    def attention_qkv_mp(self, mp_replace):
-        self.module.attention.attn_qkvw = mp_replace.copy(
-            self.module.attention.attn_qkvw,
-            self.qkvw)
-        self.module.attention.attn_qkvb = mp_replace.copy(
-            self.module.attention.attn_qkvb,
-            self.qkvb)
+    def attention_qkv_mp(self, mp_replace, reversed_dim=False):
+        self.module.attention.attn_qkvw = mp_replace.copy(self.module.attention.attn_qkvw, self.qkvw)
+        self.module.attention.attn_qkvb = mp_replace.copy(self.module.attention.attn_qkvb, self.qkvb)
 
     def load_params(self, module, sd, weight_quantizer, mp_replace, prefix):
         param_names = (
             'self_attention.query_key_value.weight', \
             'self_attention.query_key_value.bias', \
             'self_attention.dense.weight', \
             'self_attention.dense.bias', \
@@ -54,66 +54,47 @@
                        mp_replace,
                        transformer_param_names[i],
                        prefix + param_names[i],
                        qkv=True,
                        megatron_v2=self.policy.is_megatron_v2,
                        split_qkv=self.policy.split_qkv)
         for i in range(2, 4):
-            maybe_copy(module.attention,
-                       sd,
-                       weight_quantizer,
-                       mp_replace,
-                       transformer_param_names[i],
+            maybe_copy(module.attention, sd, weight_quantizer, mp_replace, transformer_param_names[i],
                        prefix + param_names[i])
         for i in range(4, 10):
-            maybe_copy(module.mlp,
-                       sd,
-                       weight_quantizer,
-                       mp_replace,
-                       transformer_param_names[i],
+            maybe_copy(module.mlp, sd, weight_quantizer, mp_replace, transformer_param_names[i],
                        prefix + param_names[i])
         for i in range(10, 12):
-            maybe_copy(module,
-                       sd,
-                       weight_quantizer,
-                       mp_replace,
-                       transformer_param_names[i],
-                       prefix + param_names[i])
+            maybe_copy(module, sd, weight_quantizer, mp_replace, transformer_param_names[i], prefix + param_names[i])
 
 
 class BLOOMLayerPolicy(TransformerPolicy):
     _orig_layer_class = None
 
-    def __init__(self,
-                 client_module,
-                 inference=True,
-                 use_load_prefix=True,
-                 split_qkv=False):
-        super().__init__(inference,
-                         linear_layer=True,
-                         use_load_prefix=use_load_prefix,
-                         split_qkv=split_qkv)
+    def __init__(self, client_module, inference=True, use_load_prefix=True, split_qkv=False):
+        super().__init__(inference, linear_layer=True, use_load_prefix=use_load_prefix, split_qkv=split_qkv)
         self.client_module = client_module
         try:
             import transformers
             BLOOMLayerPolicy._orig_layer_class = transformers.models.bloom.modeling_bloom.BloomBlock
             global supported_models
-            supported_models.update(
-                {transformers.models.bloom.modeling_bloom.BloomModel})
+            supported_models.update({transformers.models.bloom.modeling_bloom.BloomModel})
         except Exception as e:
-            print(
-                f"WARNING! Setting BLOOMLayerPolicy._orig_layer_class to None due to Exception: {e}"
-            )
+            print(f"WARNING! Setting BLOOMLayerPolicy._orig_layer_class to None due to Exception: {e}")
             BLOOMLayerPolicy._orig_layer_class = None
 
     def get_hidden_heads(self):
         return self.client_module.self_attention.hidden_size, \
-                self.client_module.self_attention.num_heads
+                self.client_module.self_attention.num_heads, \
+                self.client_module.input_layernorm.eps
 
-    def attention(self):
+    def get_q_k_v(self):
+        return None
+
+    def attention(self, enable_training=False):
         return self.client_module.self_attention.query_key_value.weight, \
                 self.client_module.self_attention.query_key_value.bias, \
                 self.client_module.self_attention.dense.weight, \
                 self.client_module.self_attention.dense.bias,
 
     def mlp(self):
         return self.client_module.mlp.dense_h_to_4h.weight, \
@@ -122,7 +103,10 @@
                self.client_module.mlp.dense_4h_to_h.bias
 
     def layernorm(self):
         return self.client_module.post_attention_layernorm.weight, \
                self.client_module.post_attention_layernorm.bias, \
                self.client_module.input_layernorm.weight, \
                self.client_module.input_layernorm.bias
+
+    def get_lora_params(self):
+        return []
```

### Comparing `deepspeed-0.8.3/deepspeed/module_inject/containers/clip.py` & `deepspeed-0.9.0/deepspeed/module_inject/containers/clip.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,45 +1,54 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 from .base import *
 from deepspeed.model_implementations.transformers.ds_gpt import DeepSpeedGPTInference
 import torch
 from torch.nn.parameter import Parameter
 from ..policy import TransformerPolicy
 
 
 class DS_CLIPContainer(BaseTransformerContainer):
+
     def __init__(self, **kwargs):
         super().__init__(**kwargs)
 
         # All model specific things should be defined here instead of the base class.
 
     def create_module(self, config=None):
         _config = config if config is not None else self.ds_model_config
         self.module = DeepSpeedGPTInference(_config, mp_group=self.mp_group)
         self.module.config.scale_attention = self.scale_attention
         return self.module
 
 
 class HFCLIPLayerPolicy(TransformerPolicy):
+
     def __init__(self, client_module, inference=False):
         super().__init__(inference, pre_attn_norm=True, scale_attention=True)
         self.client_module = client_module
         self.cuda_graph_supported = True
 
         if HFCLIPLayerPolicy._orig_layer_class is None:
             try:
                 import transformers
                 HFCLIPLayerPolicy._orig_layer_class = transformers.models.clip.modeling_clip.CLIPEncoderLayer
             except:
                 HFCLIPLayerPolicy._orig_layer_class = None
 
     def get_hidden_heads(self):
         return self.client_module.self_attn.q_proj.weight.shape[1], \
-                self.client_module.self_attn.num_heads
+                self.client_module.self_attn.num_heads, \
+                self.client_module.layer_norm1.eps
+
+    def get_q_k_v(self):
+        return None
 
     def attention(self):
         qw = self.client_module.self_attn.q_proj.weight
         qb = self.client_module.self_attn.q_proj.bias
         kw = self.client_module.self_attn.k_proj.weight
         kb = self.client_module.self_attn.k_proj.bias
         vw = self.client_module.self_attn.v_proj.weight
@@ -60,7 +69,10 @@
                self.client_module.mlp.fc2.bias
 
     def layernorm(self):
         return self.client_module.layer_norm2.weight, \
                self.client_module.layer_norm2.bias, \
                self.client_module.layer_norm1.weight, \
                self.client_module.layer_norm1.bias
+
+    def get_lora_params(self):
+        return []
```

### Comparing `deepspeed-0.8.3/deepspeed/module_inject/containers/distil_bert.py` & `deepspeed-0.9.0/deepspeed/module_inject/containers/distil_bert.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,17 +1,21 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 from .base import *
 from deepspeed.model_implementations.transformers.ds_bert import DeepSpeedBERTInference
 import torch
 from torch.nn.parameter import Parameter
 from ..policy import TransformerPolicy
 
 
 class DS_DistilBERTContainer(BaseTransformerContainer):
+
     def __init__(self, **kwargs):
         super().__init__(**kwargs)
 
         # All model specific things should be defined here instead of the base class.
         self.triangular_masking = False
         self.return_single_tuple = True
 
@@ -37,26 +41,30 @@
                     transformers.models.distilbert.modeling_distilbert.TransformerBlock,
                 ]
             except:
                 HFDistilBertLayerPolicy._orig_layer_class = None
 
     def get_hidden_heads(self):
         return self.client_module.attention.q_lin.weight.shape[1], \
-                self.client_module.attention.n_heads
+                self.client_module.attention.n_heads, \
+                self.client_module.sa_layer_norm.eps
 
-    def attention(self):
+    def get_q_k_v(self):
+        return None
+
+    def attention(self, enable_training=False):
         qw = self.client_module.attention.q_lin.weight
         qb = self.client_module.attention.q_lin.bias
         kw = self.client_module.attention.k_lin.weight
         kb = self.client_module.attention.k_lin.bias
         vw = self.client_module.attention.v_lin.weight
         vb = self.client_module.attention.v_lin.bias
 
-        qkvw = Parameter(torch.cat((qw, kw, vw), dim=0))
-        qkvb = Parameter(torch.cat((qb, kb, vb), dim=0))
+        qkvw = Parameter(torch.cat((qw, kw, vw), dim=0), requires_grad=enable_training)
+        qkvb = Parameter(torch.cat((qb, kb, vb), dim=0), requires_grad=enable_training)
 
         return qkvw, \
                qkvb, \
                self.client_module.attention.out_lin.weight, \
                self.client_module.attention.out_lin.bias
 
     def mlp(self):
@@ -69,7 +77,10 @@
     def layernorm(self):
         attention_layernorm = self.client_module.sa_layer_norm
         transformer_layernorm = self.client_module.output_layer_norm
         return attention_layernorm.weight, \
                attention_layernorm.bias, \
                transformer_layernorm.weight, \
                transformer_layernorm.bias
+
+    def get_lora_params(self):
+        return []
```

### Comparing `deepspeed-0.8.3/deepspeed/module_inject/containers/features/meta_tensor.py` & `deepspeed-0.9.0/deepspeed/module_inject/containers/features/meta_tensor.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,30 +1,34 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 from abc import ABC, abstractmethod
 
 
 class MetaTensorContainer(ABC):
+
     def __init__(self, **kwargs):
         super().__init__(**kwargs)
         self.is_meta = False
         self.ckpt_load_enabled = True
 
-    def initialize_tensors(self):
-        super().initialize_tensors()
+    def initialize_tensors(self, enable_training=False):
+        super().initialize_tensors(enable_training=enable_training)
         self.is_meta = self.qkvw.is_meta
 
-    def apply_tensor_parallelism(self, mp_replace):
+    def apply_tensor_parallelism(self, mp_replace=None, mp_group=None, tp_size=None):
         if self.is_meta:
             if self.qkvb is None:
                 self.module.attention.attn_qkvb = None
             if self.dense_b is None:
                 self.module.attention.attn_ob = None
         else:
-            super().apply_tensor_parallelism(mp_replace)
+            super().apply_tensor_parallelism(mp_replace, mp_group, tp_size)
 
     def copy_data_to_new_module(self):
         if self.is_meta:
             if self.attn_nw is None:
                 self.module.mlp.attn_nw = self.attn_nw
                 self.module.mlp.attn_nb = self.attn_nb
         else:
@@ -49,10 +53,9 @@
                 is used please see
                 https://github.com/microsoft/DeepSpeed/blob/master/deepspeed/module_inject/load_checkpoint.py
             2. `split_qkv` (Default: True): we use this flag when splitting
                 the qkv parameter into heads. If it is False, it means the heads
                 of q, k, and v are stored together and needs to split in the
                 DeepSpeed-Inference API.
         """
-        raise NotImplementedError(
-            "A load_params() function must be defined in the model container \
+        raise NotImplementedError("A load_params() function must be defined in the model container \
                                   when inheriting the MetaTensorContainer feature")
```

### Comparing `deepspeed-0.8.3/deepspeed/module_inject/containers/gpt2.py` & `deepspeed-0.9.0/deepspeed/module_inject/containers/gpt2.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,15 +1,19 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 from .base import *
 from deepspeed.model_implementations.transformers.ds_gpt import DeepSpeedGPTInference
 from ..policy import TransformerPolicy
 
 
 class DS_GPT2Container(BaseTransformerContainer):
+
     def __init__(self, **kwargs):
         super().__init__(**kwargs)
 
         # All model specific things should be defined here instead of the base class.
 
     def create_module(self, config=None):
         _config = config if config is not None else self.ds_model_config
@@ -29,17 +33,21 @@
             import transformers
             HFGPT2LayerPolicy._orig_layer_class = transformers.models.gpt2.modeling_gpt2.GPT2Block
         except:
             HFGPT2LayerPolicy._orig_layer_class = None
 
     def get_hidden_heads(self):
         return self.client_module.attn.embed_dim, \
-                self.client_module.attn.num_heads
+                self.client_module.attn.num_heads, \
+                self.client_module.ln_1.eps
 
-    def attention(self):
+    def get_q_k_v(self):
+        return None
+
+    def attention(self, enable_training=False):
         return  self.client_module.attn.c_attn.weight, \
                 self.client_module.attn.c_attn.bias, \
                 self.client_module.attn.c_proj.weight, \
                 self.client_module.attn.c_proj.bias
 
     def mlp(self):
         return self.client_module.mlp.c_fc.weight, \
@@ -48,7 +56,10 @@
                self.client_module.mlp.c_proj.bias
 
     def layernorm(self):
         return self.client_module.ln_2.weight, \
                self.client_module.ln_2.bias, \
                self.client_module.ln_1.weight, \
                self.client_module.ln_1.bias
+
+    def get_lora_params(self):
+        return []
```

### Comparing `deepspeed-0.8.3/deepspeed/module_inject/containers/gptj.py` & `deepspeed-0.9.0/deepspeed/module_inject/containers/gptj.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,21 +1,25 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 from .base import *
 from .features.meta_tensor import MetaTensorContainer
 from deepspeed.model_implementations.transformers.ds_gpt import DeepSpeedGPTInference
 import torch
 from torch.nn.parameter import Parameter
 from ..policy import TransformerPolicy
 from ..policy import transformer_param_names
 from ..policy import maybe_copy
 from ..policy import maybe_copy_qkv
 
 
 class DS_GPTJContainer(MetaTensorContainer, BaseTransformerContainer):
+
     def __init__(self, **kwargs):
         super().__init__(**kwargs)
 
         # All model specific things should be defined here instead of the base class.
 
     def create_module(self, config=None):
         _config = config if config is not None else self.ds_model_config
@@ -32,44 +36,28 @@
             'mlp.fc_in.weight', \
             'mlp.fc_in.bias', \
             'mlp.fc_out.weight', \
             'mlp.fc_out.bias', \
             'ln_1.weight', \
             'ln_1.bias'
         )
-        maybe_copy_qkv(
-            module.attention,
-            sd,
-            weight_quantizer,
-            mp_replace,
-            'attn_qkvw',
-            [prefix + param_names[0],
-             prefix + param_names[1],
-             prefix + param_names[2]],
-            split_qkv=self.policy.split_qkv)
-        for i in range(3, 4):
-            maybe_copy(module.attention,
+        maybe_copy_qkv(module.attention,
                        sd,
                        weight_quantizer,
                        mp_replace,
-                       transformer_param_names[i - 1],
+                       'attn_qkvw', [prefix + param_names[0], prefix + param_names[1], prefix + param_names[2]],
+                       split_qkv=self.policy.split_qkv)
+        for i in range(3, 4):
+            maybe_copy(module.attention, sd, weight_quantizer, mp_replace, transformer_param_names[i - 1],
                        prefix + param_names[i])
         for i in range(4, 8):
-            maybe_copy(module.mlp,
-                       sd,
-                       weight_quantizer,
-                       mp_replace,
-                       transformer_param_names[i],
+            maybe_copy(module.mlp, sd, weight_quantizer, mp_replace, transformer_param_names[i],
                        prefix + param_names[i])
         for i in range(8, 10):
-            maybe_copy(module,
-                       sd,
-                       weight_quantizer,
-                       mp_replace,
-                       transformer_param_names[i + 2],
+            maybe_copy(module, sd, weight_quantizer, mp_replace, transformer_param_names[i + 2],
                        prefix + param_names[i])
 
 
 class HFGPTJLayerPolicy(TransformerPolicy):
     _orig_layer_class = None
 
     def __init__(self, client_module, inference=True):
@@ -79,22 +67,26 @@
             import transformers
             HFGPTJLayerPolicy._orig_layer_class = transformers.models.gptj.modeling_gptj.GPTJBlock
         except:
             HFGPTJLayerPolicy._orig_layer_class = None
 
     def get_hidden_heads(self):
         return self.client_module.attn.q_proj.weight.shape[1], \
-                self.client_module.attn.num_attention_heads
+                self.client_module.attn.num_attention_heads, \
+                self.client_module.ln_1.eps
+
+    def get_q_k_v(self):
+        return None
 
-    def attention(self):
+    def attention(self, enable_training=False):
         qw = self.client_module.attn.q_proj.weight
         kw = self.client_module.attn.k_proj.weight
         vw = self.client_module.attn.v_proj.weight
 
-        qkvw = Parameter(torch.cat((qw, kw, vw), dim=0), requires_grad=False)
+        qkvw = Parameter(torch.cat((qw, kw, vw), dim=0), requires_grad=enable_training)
 
         return qkvw, \
                None, \
                self.client_module.attn.out_proj.weight, \
                None,
 
     def mlp(self):
@@ -104,7 +96,10 @@
                self.client_module.mlp.fc_out.bias
 
     def layernorm(self):
         return None, \
                None, \
                self.client_module.ln_1.weight, \
                self.client_module.ln_1.bias
+
+    def get_lora_params(self):
+        return []
```

### Comparing `deepspeed-0.8.3/deepspeed/module_inject/containers/gptneo.py` & `deepspeed-0.9.0/deepspeed/module_inject/containers/gptneo.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,21 +1,25 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 from .base import *
 from .features.meta_tensor import MetaTensorContainer
 from deepspeed.model_implementations.transformers.ds_gpt import DeepSpeedGPTInference
 import torch
 from torch.nn.parameter import Parameter
 from ..policy import TransformerPolicy
 from ..policy import transformer_param_names
 from ..policy import maybe_copy
 from ..policy import maybe_copy_qkv
 
 
 class DS_GPTNEOContainer(MetaTensorContainer, BaseTransformerContainer):
+
     def __init__(self, **kwargs):
         super().__init__(**kwargs)
 
         # All model specific things should be defined here instead of the base class.
 
     def create_module(self, config=None):
         _config = config if config is not None else self.ds_model_config
@@ -35,67 +39,56 @@
             'mlp.c_proj.weight', \
             'mlp.c_proj.bias', \
             'ln_2.weight', \
             'ln_2.bias', \
             'ln_1.weight', \
             'ln_1.bias'
         )
-        maybe_copy_qkv(
-            module.attention,
-            sd,
-            weight_quantizer,
-            mp_replace,
-            'attn_qkvw',
-            [prefix + param_names[0],
-             prefix + param_names[1],
-             prefix + param_names[2]],
-            split_qkv=self.policy.split_qkv)
-        for i in range(3, 5):
-            maybe_copy(module.attention,
+        maybe_copy_qkv(module.attention,
                        sd,
                        weight_quantizer,
                        mp_replace,
-                       transformer_param_names[i - 1],
+                       'attn_qkvw', [prefix + param_names[0], prefix + param_names[1], prefix + param_names[2]],
+                       split_qkv=self.policy.split_qkv)
+        for i in range(3, 5):
+            maybe_copy(module.attention, sd, weight_quantizer, mp_replace, transformer_param_names[i - 1],
                        prefix + param_names[i])
         for i in range(5, 11):
-            maybe_copy(module.mlp,
-                       sd,
-                       weight_quantizer,
-                       mp_replace,
-                       transformer_param_names[i - 1],
+            maybe_copy(module.mlp, sd, weight_quantizer, mp_replace, transformer_param_names[i - 1],
                        prefix + param_names[i])
         for i in range(11, 13):
-            maybe_copy(module,
-                       sd,
-                       weight_quantizer,
-                       mp_replace,
-                       transformer_param_names[i - 1],
+            maybe_copy(module, sd, weight_quantizer, mp_replace, transformer_param_names[i - 1],
                        prefix + param_names[i])
 
 
 class HFGPTNEOLayerPolicy(TransformerPolicy):
+
     def __init__(self, client_module, inference=True):
         super().__init__(inference, scale_attention=False)
         self.client_module = client_module
         try:
             import transformers
             HFGPTNEOLayerPolicy._orig_layer_class = transformers.models.gpt_neo.modeling_gpt_neo.GPTNeoBlock
         except:
             HFGPTNEOLayerPolicy._orig_layer_class = None
 
     def get_hidden_heads(self):
         return self.client_module.attn.attention.q_proj.weight.shape[1], \
-                self.client_module.attn.attention.num_heads
+                self.client_module.attn.attention.num_heads, \
+                self.client_module.ln_1.eps
+
+    def get_q_k_v(self):
+        return None
 
-    def attention(self):
+    def attention(self, enable_training=False):
         qw = self.client_module.attn.attention.q_proj.weight
         kw = self.client_module.attn.attention.k_proj.weight
         vw = self.client_module.attn.attention.v_proj.weight
 
-        qkvw = Parameter(torch.cat((qw, kw, vw), dim=0), requires_grad=False)
+        qkvw = Parameter(torch.cat((qw, kw, vw), dim=0), requires_grad=enable_training)
 
         return qkvw, \
                None, \
                self.client_module.attn.attention.out_proj.weight, \
                self.client_module.attn.attention.out_proj.bias
 
     def mlp(self):
@@ -105,7 +98,10 @@
                self.client_module.mlp.c_proj.bias
 
     def layernorm(self):
         return self.client_module.ln_2.weight, \
                self.client_module.ln_2.bias, \
                self.client_module.ln_1.weight, \
                self.client_module.ln_1.bias
+
+    def get_lora_params(self):
+        return []
```

### Comparing `deepspeed-0.8.3/deepspeed/module_inject/containers/gptneox.py` & `deepspeed-0.9.0/deepspeed/module_inject/containers/gptneox.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,23 +1,25 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 from .base import *
 from .features.meta_tensor import MetaTensorContainer
 from .features.megatron import MegatronContainer
 from deepspeed.model_implementations.transformers.ds_gpt import DeepSpeedGPTInference
 import torch
 from ..policy import TransformerPolicy
 from ..policy import transformer_param_names
 from ..policy import maybe_copy
 from packaging import version as pkg_version
 
 
-class DS_GPTNEOXContainer(MetaTensorContainer,
-                          MegatronContainer,
-                          BaseTransformerContainer):
+class DS_GPTNEOXContainer(MetaTensorContainer, MegatronContainer, BaseTransformerContainer):
+
     def __init__(self, **kwargs):
         super().__init__(**kwargs)
 
         # All model specific things should be defined here instead of the base class.
 
     def create_module(self, config=None):
         _config = config if config is not None else self.ds_model_config
@@ -53,34 +55,21 @@
                        transformer_param_names[i],
                        prefix + param_names[i],
                        qkv=True,
                        megatron_v2=self.policy.is_megatron_v2,
                        split_qkv=self.policy.split_qkv,
                        heads=self.policy.client_module.attention.num_attention_heads)
         for i in range(2, 4):
-            maybe_copy(module.attention,
-                       sd,
-                       weight_quantizer,
-                       mp_replace,
-                       transformer_param_names[i],
+            maybe_copy(module.attention, sd, weight_quantizer, mp_replace, transformer_param_names[i],
                        prefix + param_names[i])
         for i in range(4, 10):
-            maybe_copy(module.mlp,
-                       sd,
-                       weight_quantizer,
-                       mp_replace,
-                       transformer_param_names[i],
+            maybe_copy(module.mlp, sd, weight_quantizer, mp_replace, transformer_param_names[i],
                        prefix + param_names[i])
         for i in range(10, 12):
-            maybe_copy(module,
-                       sd,
-                       weight_quantizer,
-                       mp_replace,
-                       transformer_param_names[i],
-                       prefix + param_names[i])
+            maybe_copy(module, sd, weight_quantizer, mp_replace, transformer_param_names[i], prefix + param_names[i])
 
 
 class GPTNEOXLayerPolicy(TransformerPolicy):
     _orig_layer_class = None
     version = 0
 
     def __init__(self, client_module, inference=True, megatron_v2=True, split_qkv=False):
@@ -99,17 +88,21 @@
     def get_hidden_heads(self):
         if GPTNEOXLayerPolicy.version == 0:
             attention = self.client_module.attention
         else:
             attention = self.client_module.self_attention
 
         return self.client_module.attention.query_key_value.weight.shape[1], \
-                self.client_module.attention.num_attention_heads
+                self.client_module.attention.num_attention_heads, \
+                self.client_module.input_layernorm.eps
 
-    def attention(self):
+    def get_q_k_v(self):
+        return None
+
+    def attention(self, enable_training=False):
         if GPTNEOXLayerPolicy.version == 0:
             attention = self.client_module.attention
         else:
             attention = self.client_module.self_attention
 
         return attention.query_key_value.weight, \
                attention.query_key_value.bias, \
@@ -123,7 +116,10 @@
                self.client_module.mlp.dense_4h_to_h.bias
 
     def layernorm(self):
         return self.client_module.post_attention_layernorm.weight, \
                self.client_module.post_attention_layernorm.bias, \
                self.client_module.input_layernorm.weight, \
                self.client_module.input_layernorm.bias
+
+    def get_lora_params(self):
+        return []
```

### Comparing `deepspeed-0.8.3/deepspeed/module_inject/containers/megatron_gpt.py` & `deepspeed-0.9.0/deepspeed/module_inject/containers/megatron_gpt.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,18 +1,22 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 from .base import *
 from .features.megatron import MegatronContainer
 from deepspeed.model_implementations.transformers.ds_megatron_gpt import DeepSpeedMegatronGPTInference
 import torch
 from ..policy import TransformerPolicy
 from packaging import version as pkg_version
 
 
 class DS_MegatronGPTContainer(MegatronContainer, BaseTransformerContainer):
+
     def __init__(self, **kwargs):
         super().__init__(**kwargs)
 
         # All model specific things should be defined here instead of the base class.
 
     def create_module(self, config=None):
         _config = config if config is not None else self.ds_model_config
@@ -32,17 +36,15 @@
     _orig_layer_class = None
     version = 0
     moe_type = 'standard'
     megatron_v2 = True
     use_mup = False
 
     def __init__(self, client_module, inference=True):
-        super().__init__(inference,
-                         megatron_v2=MegatronLayerPolicy.megatron_v2,
-                         use_mup=MegatronLayerPolicy.use_mup)
+        super().__init__(inference, megatron_v2=MegatronLayerPolicy.megatron_v2, use_mup=MegatronLayerPolicy.use_mup)
         self.client_module = client_module
         # we use megatron version to differentiate between the old and new
         # megatron-lm source code
         if MegatronLayerPolicy._orig_layer_class is None:
             if pkg_version.parse(torch.__version__) <= pkg_version.parse("1.2"):
                 MegatronLayerPolicy._orig_layer_class = None
             else:
@@ -50,17 +52,21 @@
                     from megatron.model.transformer import ParallelTransformerLayer
                     MegatronLayerPolicy._orig_layer_class = ParallelTransformerLayer
                 except ImportError:
                     MegatronLayerPolicy._orig_layer_class = None
 
     def get_hidden_heads(self):
         return self.client_module.attention.query_key_value.weight.shape[1], \
-                self.client_module.attention.num_attention_heads
+                self.client_module.attention.num_attention_heads, \
+                self.client_module.input_layernorm.eps
 
-    def attention(self):
+    def get_q_k_v(self):
+        return None
+
+    def attention(self, enable_training=False):
         if self.inference:
             if MegatronLayerPolicy.version == 0:
                 attention = self.client_module.attention
             else:
                 attention = self.client_module.self_attention
 
         return attention.query_key_value.weight, \
@@ -100,7 +106,10 @@
                    self.client_module.mlp.dense_4h_to_h.bias
 
     def layernorm(self):
         return self.client_module.post_attention_layernorm.weight, \
                self.client_module.post_attention_layernorm.bias, \
                self.client_module.input_layernorm.weight, \
                self.client_module.input_layernorm.bias
+
+    def get_lora_params(self):
+        return []
```

### Comparing `deepspeed-0.8.3/deepspeed/module_inject/containers/megatron_gpt_moe.py` & `deepspeed-0.9.0/deepspeed/module_inject/containers/megatron_gpt_moe.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,19 +1,23 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 from .base import *
 from .base_moe import *
 from .features.megatron import MegatronContainer
 from deepspeed.model_implementations.transformers.ds_megatron_gpt import DeepSpeedMegatronGPTInference
 import torch
 from .megatron_gpt import MegatronLayerPolicy
 from packaging import version as pkg_version
 
 
 class DS_MegatronGPTMoEContainer(MegatronContainer, BaseTransformerMoEContainer):
+
     def __init__(self, policy, config, model_config, layer_id):
         super().__init__(policy, config, model_config, layer_id)
 
         # All model specific things should be defined here instead of the base class.
 
     def create_module(self, config=None):
         _config = config if config is not None else self.ds_model_config
```

### Comparing `deepspeed-0.8.3/deepspeed/module_inject/containers/opt.py` & `deepspeed-0.9.0/deepspeed/module_inject/containers/opt.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,22 +1,27 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 from .base import *
 from .features.meta_tensor import MetaTensorContainer
 from deepspeed.model_implementations.transformers.ds_opt import DeepSpeedOPTInference
 import torch
 from torch.nn.parameter import Parameter
 from ..policy import TransformerPolicy
 from ..policy import transformer_param_names
 from ..policy import maybe_copy
 from ..policy import maybe_copy_qkv
+from ..policy import maybe_get_lora
 from deepspeed.utils.types import ActivationFuncType
 
 
 class DS_OPTContainer(MetaTensorContainer, BaseTransformerContainer):
+
     def __init__(self, **kwargs):
         super().__init__(**kwargs)
 
         # All model specific things should be defined here instead of the base class.
 
     def create_module(self, config=None):
         _config = config if config is not None else self.ds_model_config
@@ -46,80 +51,68 @@
 
         for i in range(0, 6, 3):
             maybe_copy_qkv(module.attention,
                            sd,
                            weight_quantizer,
                            mp_replace,
                            transformer_param_names[i // 3],
-                           [
-                               prefix + param_names[i],
-                               prefix + param_names[i + 1],
-                               prefix + param_names[i + 2]
-                           ],
+                           [prefix + param_names[i], prefix + param_names[i + 1], prefix + param_names[i + 2]],
                            split_qkv=self.policy.split_qkv)
         for i in range(6, 8):
-            maybe_copy(module.attention,
-                       sd,
-                       weight_quantizer,
-                       mp_replace,
-                       transformer_param_names[i - 4],
+            maybe_copy(module.attention, sd, weight_quantizer, mp_replace, transformer_param_names[i - 4],
                        prefix + param_names[i])
         for i in range(8, 14):
-            maybe_copy(module.mlp,
-                       sd,
-                       weight_quantizer,
-                       mp_replace,
-                       transformer_param_names[i - 4],
+            maybe_copy(module.mlp, sd, weight_quantizer, mp_replace, transformer_param_names[i - 4],
                        prefix + param_names[i])
         for i in range(14, 16):
-            maybe_copy(module,
-                       sd,
-                       weight_quantizer,
-                       mp_replace,
-                       transformer_param_names[i - 4],
+            maybe_copy(module, sd, weight_quantizer, mp_replace, transformer_param_names[i - 4],
                        prefix + param_names[i])
 
 
 class HFOPTLayerPolicy(TransformerPolicy):
     _orig_layer_class = None
 
     def __init__(self, client_module, inference=True, use_load_prefix=True):
         super().__init__(inference,
                          linear_layer=True,
                          mlp_act_func_type=ActivationFuncType.ReLU,
                          pre_attn_norm=True,
                          use_load_prefix=use_load_prefix)
         self.client_module = client_module
-
         try:
             import transformers
             HFOPTLayerPolicy._orig_layer_class = transformers.models.opt.modeling_opt.OPTDecoderLayer
-            if isinstance(TransformerPolicy.hf_model_config,
-                          transformers.models.opt.configuration_opt.OPTConfig):
-                self.pre_attn_norm = TransformerPolicy.hf_model_config.do_layer_norm_before
         except:
             HFOPTLayerPolicy._orig_layer_class = None
 
     def get_hidden_heads(self):
         return self.client_module.self_attn.embed_dim, \
-                self.client_module.self_attn.num_heads
+                self.client_module.self_attn.num_heads, \
+                self.client_module.self_attn_layer_norm.eps
 
-    def attention(self):
+    def get_q_k_v(self):
+        return self.client_module.self_attn.q_proj.weight, \
+               self.client_module.self_attn.q_proj.bias, \
+               self.client_module.self_attn.k_proj.weight, \
+               self.client_module.self_attn.k_proj.bias, \
+               self.client_module.self_attn.v_proj.weight, \
+               self.client_module.self_attn.v_proj.bias
+
+    def attention(self, enable_training=False):
         qw = self.client_module.self_attn.q_proj.weight
         qb = self.client_module.self_attn.q_proj.bias
 
         kw = self.client_module.self_attn.k_proj.weight
         kb = self.client_module.self_attn.k_proj.bias
 
         vw = self.client_module.self_attn.v_proj.weight
         vb = self.client_module.self_attn.v_proj.bias
 
-        qkvw = Parameter(torch.cat((qw, kw, vw), dim=0), requires_grad=False)
-        qkvb = Parameter(torch.cat((qb, kb, vb), dim=0), requires_grad=False)
-
+        qkvw = Parameter(torch.cat((qw, kw, vw), dim=0), requires_grad=enable_training)
+        qkvb = Parameter(torch.cat((qb, kb, vb), dim=0), requires_grad=enable_training)
         return qkvw, \
                qkvb, \
                self.client_module.self_attn.out_proj.weight, \
                self.client_module.self_attn.out_proj.bias
 
     def mlp(self):
         return self.client_module.fc1.weight, \
@@ -128,7 +121,20 @@
                self.client_module.fc2.bias
 
     def layernorm(self):
         return self.client_module.final_layer_norm.weight, \
                self.client_module.final_layer_norm.bias, \
                self.client_module.self_attn_layer_norm.weight, \
                self.client_module.self_attn_layer_norm.bias
+
+    def get_lora_params(self):
+        all_lora_params = []
+        for p in [
+            self.client_module.fc1, \
+            self.client_module.fc2, \
+            self.client_module.self_attn.q_proj, \
+            self.client_module.self_attn.k_proj, \
+            self.client_module.self_attn.v_proj, \
+            self.client_module.self_attn.out_proj, \
+            ]:
+            all_lora_params.append(maybe_get_lora(p))
+        return all_lora_params
```

### Comparing `deepspeed-0.8.3/deepspeed/module_inject/containers/unet.py` & `deepspeed-0.9.0/deepspeed/module_inject/containers/unet.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,18 +1,21 @@
-'''
-Copyright 2022 The Microsoft DeepSpeed Team
-'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+
 import torch
 from torch.nn.parameter import Parameter
 
 from ..policy import DSPolicy
 from ...model_implementations.diffusers.unet import DSUNet
 
 
 class UNetPolicy(DSPolicy):
+
     def __init__(self):
         super().__init__()
         try:
             import diffusers
             self._orig_layer_class = diffusers.models.unet_2d_condition.UNet2DConditionModel
         except ImportError:
             self._orig_layer_class = None
```

### Comparing `deepspeed-0.8.3/deepspeed/module_inject/containers/vae.py` & `deepspeed-0.9.0/deepspeed/module_inject/containers/vae.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,15 +1,18 @@
-'''
-Copyright 2022 The Microsoft DeepSpeed Team
-'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+
 from ..policy import DSPolicy
 from ...model_implementations.diffusers.vae import DSVAE
 
 
 class VAEPolicy(DSPolicy):
+
     def __init__(self):
         super().__init__()
         try:
             import diffusers
             if hasattr(diffusers.models.vae, "AutoencoderKL"):
                 self._orig_layer_class = diffusers.models.vae.AutoencoderKL
             else:
```

### Comparing `deepspeed-0.8.3/deepspeed/module_inject/inject.py` & `deepspeed-0.9.0/deepspeed/module_inject/inject.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,38 +1,33 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import copy
 import torch
 from deepspeed.ops.transformer import DeepSpeedTransformerLayer, DeepSpeedTransformerConfig
 
 
-def module_inject(layer_obj,
-                  model,
-                  config,
-                  micro_batch_size,
-                  max_seq_length,
-                  seed,
-                  preln,
-                  fp16=True):
+def module_inject(layer_obj, model, config, micro_batch_size, max_seq_length, seed, preln, fp16=True):
     for name, child in model.named_children():
         if isinstance(child, layer_obj):
             print('REPLACING BertLayer')
 
-            cuda_config = DeepSpeedTransformerConfig(
-                batch_size=micro_batch_size,
-                max_seq_length=max_seq_length,
-                hidden_size=config.hidden_size,
-                heads=config.num_attention_heads,
-                attn_dropout_ratio=config.attention_probs_dropout_prob,
-                hidden_dropout_ratio=config.hidden_dropout_prob,
-                num_hidden_layers=config.num_hidden_layers,
-                initializer_range=config.initializer_range,
-                seed=seed,
-                fp16=fp16,
-                pre_layer_norm=preln)
+            cuda_config = DeepSpeedTransformerConfig(batch_size=micro_batch_size,
+                                                     max_seq_length=max_seq_length,
+                                                     hidden_size=config.hidden_size,
+                                                     heads=config.num_attention_heads,
+                                                     attn_dropout_ratio=config.attention_probs_dropout_prob,
+                                                     hidden_dropout_ratio=config.hidden_dropout_prob,
+                                                     num_hidden_layers=config.num_hidden_layers,
+                                                     initializer_range=config.initializer_range,
+                                                     seed=seed,
+                                                     fp16=fp16,
+                                                     pre_layer_norm=preln)
 
             new_module = DeepSpeedTransformerLayer(cuda_config)
 
             # copy relevant state from child -> new module
             qw = child.attention.self.query.weight
             qb = child.attention.self.query.bias
             kw = child.attention.self.key.weight
@@ -67,22 +62,15 @@
                 transformer_LayerNorm = child.output.LayerNorm
             new_module.norm_w.data = transformer_LayerNorm.weight
             new_module.norm_b.data = transformer_LayerNorm.bias
 
             setattr(model, name, copy.deepcopy(new_module))
 
         else:
-            module_inject(layer_obj,
-                          child,
-                          config,
-                          micro_batch_size,
-                          max_seq_length,
-                          seed,
-                          preln,
-                          fp16)
+            module_inject(layer_obj, child, config, micro_batch_size, max_seq_length, seed, preln, fp16)
 
     return model
 
 
 def test_hi():
     from turing.nvidia_modelingpreln import BertConfig as BertConfigPreLN
     from turing.nvidia_modelingpreln import BertForQuestionAnswering as BertForQuestionAnsweringPreLN
```

### Comparing `deepspeed-0.8.3/deepspeed/module_inject/layers.py` & `deepspeed-0.9.0/deepspeed/module_inject/layers.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,19 +1,23 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import torch
 from deepspeed import comm as dist
 from torch import nn
 from torch.nn import functional as F
 
 from torch.nn.parameter import Parameter
 from deepspeed.accelerator import get_accelerator
 
 
 class LinearAllreduce(nn.Module):
+
     def __init__(self, weight, bias=None, mp_group=None):
         super(LinearAllreduce, self).__init__()
         self.weight = weight
         self.bias = bias
         self.mp_group = mp_group
 
     def forward(self, input):
@@ -22,24 +26,23 @@
             dist.all_reduce(output, group=self.mp_group)
         if self.bias is not None:
             output += self.bias
         return output
 
 
 class LinearLayer(nn.Module):
+
     def __init__(self, weight_shape=None, dtype=torch.half, weight=None, bias=None):
         super(LinearLayer, self).__init__()
         if weight is not None:
             self.weight = weight
             self.bias = bias
         else:
             self.weight = Parameter(
-                torch.empty(weight_shape,
-                            dtype=dtype,
-                            device=get_accelerator().current_device_name()))
+                torch.empty(weight_shape, dtype=dtype, device=get_accelerator().current_device_name()))
 
             self.bias = Parameter(
                 torch.empty(weight_shape[0],
                             dtype=dtype,
                             device=get_accelerator().current_device_name())) \
                 if bias is not None else None
 
@@ -47,55 +50,63 @@
         output = torch.matmul(input, self.weight.transpose(-1, -2))
         if self.bias is not None:
             output += self.bias
         return output
 
 
 class Normalize(nn.Module):
-    def __init__(self, dim, dtype=torch.float, eps=1e-5):
+
+    def __init__(self, dim=None, dtype=torch.float, eps=1e-5, weight=None, bias=None):
         super(Normalize, self).__init__()
-        self.norm = nn.LayerNorm(dim,
-                                 eps=eps).to(dtype).to(
-                                     get_accelerator().current_device_name())
-        self.weight = self.norm.weight
-        self.bias = self.norm.bias
+        if weight is not None:
+            self.weight = weight
+            self.bias = bias
+        else:
+            self.norm = nn.LayerNorm(dim, eps=eps).to(dtype).to(get_accelerator().current_device_name())
+            self.weight = self.norm.weight
+            self.bias = self.norm.bias
+
+        self.eps = eps
 
     def forward(self, input):
-        return self.norm(input)
+        return nn.functional.layer_norm(input, input.shape[-1:], self.weight, self.bias, eps=self.eps)
 
 
 class EmbeddingLayer(nn.Module):
-    def __init__(self, weight_shape, dtype=torch.half):
+
+    def __init__(self, weight_shape=None, dtype=torch.half, weight=None, bias=None):
         super(EmbeddingLayer, self).__init__()
-        self.weight = Parameter(
-            torch.empty(weight_shape[0],
-                        weight_shape[1],
-                        dtype=dtype,
-                        device=get_accelerator().current_device_name()))
+        if weight is None:
+            self.weight = Parameter(
+                torch.empty(weight_shape[0],
+                            weight_shape[1],
+                            dtype=dtype,
+                            device=get_accelerator().current_device_name()))
+        else:
+            self.weight = weight
 
     def forward(self, input):
         return F.embedding(input, self.weight)
 
 
 class OPTEmbedding(EmbeddingLayer):
     """
     This module learns positional embeddings up to a fixed maximum size.
     """
-    def __init__(self, weight_shape):
+
+    def __init__(self, weight_shape=None, weight=None, bias=None):
         # OPT is set up so that if padding_idx is specified then offset the embedding ids by 2
         # and adjust num_embeddings appropriately. Other models don't have this hack
         self.offset = 2
-        super().__init__(weight_shape)
+        super().__init__(weight_shape, weight=weight)
 
     def forward(self, attention_mask: torch.LongTensor, past_key_values_length: int = 0):
         """`input_ids_shape` is expected to be [bsz x seqlen]."""
         attention_mask = attention_mask.long()
 
         # create positions depending on attention_mask
-        positions = (torch.cumsum(attention_mask,
-                                  dim=1).type_as(attention_mask) *
-                     attention_mask).long() - 1
+        positions = (torch.cumsum(attention_mask, dim=1).type_as(attention_mask) * attention_mask).long() - 1
 
         # cut positions if `past_key_values_length` is > 0
         positions = positions[:, past_key_values_length:]
 
         return super().forward(positions + self.offset)
```

### Comparing `deepspeed-0.8.3/deepspeed/module_inject/load_checkpoint.py` & `deepspeed-0.9.0/deepspeed/module_inject/load_checkpoint.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,8 +1,11 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 from torch import nn
 from deepspeed.model_implementations.transformers.ds_bloom import DeepSpeedBloomInference
 from deepspeed.model_implementations.transformers.ds_gpt import DeepSpeedGPTInference
 from deepspeed.model_implementations.transformers.ds_bert import DeepSpeedBERTInference
 from deepspeed.model_implementations.transformers.ds_megatron_gpt import DeepSpeedMegatronGPTInference
 from deepspeed.model_implementations.transformers.ds_opt import DeepSpeedOPTInference
@@ -46,18 +49,16 @@
         args = (sd[0], prefix, {}, True, [], [], error_msgs)
 
         if hasattr(module, 'weight'):
             module.weight = mp_replace.copy(module.weight.data, sd[0][prefix + 'weight'])
         if prefix + 'bias' in sd[0].keys():
             if module.bias.data.is_meta:
                 # meta tensor cannot be casted or copied to, so we need to replace it with a normal tensor here
-                module.bias = torch.nn.parameter.Parameter(
-                    data=torch.empty_like(module.bias.data,
-                                          device="cpu"),
-                    requires_grad=module.bias.data.requires_grad)
+                module.bias = torch.nn.parameter.Parameter(data=torch.empty_like(module.bias.data, device="cpu"),
+                                                           requires_grad=module.bias.data.requires_grad)
             module.bias = mp_replace.copy(module.bias.data, sd[0][prefix + 'bias'])
         args = None
         gc.collect()
 
     def load_transformer_layer(module, prefix):
         if ckpt_type == "tp":
 
@@ -67,147 +68,107 @@
                         if type(sd[0][prefix + n]) is list:
                             tmp_data, scale = sd[0][prefix + n]
                             tmp_data = tmp_data
                             scale = scale.to(get_accelerator().current_device_name())
                             # set the quantizer number of groups using the checkpoint scale shape
                             weight_quantizer.num_groups = scale.shape[0]
                         else:
-                            tmp_data = sd[0][prefix + n].to(
-                                get_accelerator().current_device_name())
+                            tmp_data = sd[0][prefix + n].to(get_accelerator().current_device_name())
                             scale = None
                         src_shape = tmp_data.shape
                         dst_shape = p.shape
                         inner_dim = 1 if tmp_data.dtype == torch.int8 else 0
                         outer_dim = 0 if tmp_data.dtype == torch.int8 else 1
                         if (len(src_shape) == 2 and len(dst_shape) == 2):
-                            if (src_shape[inner_dim] == dst_shape[0]
-                                    and src_shape[outer_dim] == dst_shape[1]):
+                            if (src_shape[inner_dim] == dst_shape[0] and src_shape[outer_dim] == dst_shape[1]):
                                 if tmp_data.dtype != torch.int8:
                                     p = weight_quantizer.quantize(
-                                        transpose(tmp_data) if weight_quantizer.
-                                        q_int8 else tmp_data)
+                                        transpose(tmp_data) if weight_quantizer.q_int8 else tmp_data)
                                 else:
-                                    p = torch.nn.parameter.Parameter(tmp_data,
-                                                                     requires_grad=False)
+                                    p = torch.nn.parameter.Parameter(tmp_data, requires_grad=False)
                                     p.scale = scale
                                 setattr(module, n, p)
                             else:
-                                dim = inner_dim if src_shape[inner_dim] != dst_shape[
-                                    0] else outer_dim
+                                dim = inner_dim if src_shape[inner_dim] != dst_shape[0] else outer_dim
                                 dim1 = 0 if src_shape[inner_dim] != dst_shape[0] else 1
                                 if src_shape[dim] > dst_shape[dim1]:
-                                    weight_partition = torch.split(
-                                        tmp_data,
-                                        dst_shape[dim1],
-                                        dim=dim)[rank].to(
-                                            get_accelerator().current_device_name())
+                                    weight_partition = torch.split(tmp_data, dst_shape[dim1], dim=dim)[rank].to(
+                                        get_accelerator().current_device_name())
                                     assert tmp_data.dtype != torch.int8 or scale.numel() > weight_quantizer.num_groups * (rank+1), \
                                         '''ERROR: We require the quantization scales for larger TP-size when loading INT8 checkpoint!\
                                            Please use the FP16 checkpoint to generate INT8 checkpoint with the sharding parameters!'''
-                                    scale = scale.view(
-                                        -1)[weight_quantizer.num_groups *
-                                            (rank + 1):].reshape(
-                                                weight_quantizer.num_groups,
-                                                -1).contiguous()
+                                    scale = scale.view(-1)[weight_quantizer.num_groups * (rank + 1):].reshape(
+                                        weight_quantizer.num_groups, -1).contiguous()
                                 else:
                                     assert tmp_data.dtype != torch.int8, \
                                         '''Merging of the checkpoints are not supported when using INT8 checkpoint! \
                                           Please use a as many GPUs as TP-size for the checkpoint'''
                                     all_data = [
-                                        sd[j][prefix +
-                                              n] if type(sd[j][prefix + n]) is list else
-                                        sd[j][prefix + n].to(
-                                            get_accelerator().current_device_name())
-                                        for j in range(len(sd))
+                                        sd[j][prefix + n] if type(sd[j][prefix + n]) is list else sd[j][prefix + n].to(
+                                            get_accelerator().current_device_name()) for j in range(len(sd))
                                     ]
                                     # Check if the weight tensor is for the QKV parameter
-                                    if src_shape[1] == (3 *
-                                                        src_shape[0]) // ckpt_mp_size:
+                                    if src_shape[1] == (3 * src_shape[0]) // ckpt_mp_size:
                                         qkv_size = src_shape[outer_dim] // 3
                                         src_split = [
-                                            torch.split(src[0].data,
-                                                        qkv_size,
-                                                        dim=outer_dim)
-                                            for src in all_data
+                                            torch.split(src[0].data, qkv_size, dim=outer_dim) for src in all_data
                                         ]
 
                                         weight_partition = torch.cat([
-                                            torch.cat([qkv_s[i] for qkv_s in src_split],
-                                                      axis=outer_dim)
+                                            torch.cat([qkv_s[i] for qkv_s in src_split], axis=outer_dim)
                                             for i in range(len(src_split[0]))
                                         ],
                                                                      dim=dim)
                                     else:
                                         weight_partition = torch.cat([
-                                            ad[0].to(
-                                                get_accelerator().current_device_name())
-                                            if type(ad) is list else ad
-                                            for ad in all_data
+                                            ad[0].to(get_accelerator().current_device_name())
+                                            if type(ad) is list else ad for ad in all_data
                                         ],
                                                                      dim=dim)
                                     if tmp_data.dtype == torch.int8:
-                                        scale = torch.cat([
-                                            ad[1].to(
-                                                get_accelerator().current_device_name())
-                                            for ad in all_data
-                                        ],
-                                                          dim=dim)
+                                        scale = torch.cat(
+                                            [ad[1].to(get_accelerator().current_device_name()) for ad in all_data],
+                                            dim=dim)
 
                                 if tmp_data.dtype != torch.int8:
                                     weight_partition = weight_quantizer.quantize(
                                         transpose(weight_partition), \
                                         parallel_dim=(0 if dim == 1 else 1)) if weight_quantizer.q_int8 else \
                                         weight_quantizer.quantize(weight_partition)
                                 else:
-                                    weight_partition = torch.nn.parameter.Parameter(
-                                        weight_partition,
-                                        requires_grad=False)
+                                    weight_partition = torch.nn.parameter.Parameter(weight_partition,
+                                                                                    requires_grad=False)
                                     weight_partition.scale = scale
                                 setattr(module, n, weight_partition)
                         else:
                             if src_shape[0] == dst_shape[0]:
                                 p.data.copy_(tmp_data)
                             else:
                                 if src_shape[0] > dst_shape[0]:
-                                    bias_split = torch.split(
-                                        tmp_data,
-                                        dst_shape[-1])[rank].to(get_accelerator(
-                                        ).current_device_name()).contiguous()
+                                    bias_split = torch.split(tmp_data, dst_shape[-1])[rank].to(
+                                        get_accelerator().current_device_name()).contiguous()
                                     p.data.copy_(bias_split)
                                 else:
                                     # Check if the weight tensor is for the QKV parameter
-                                    if src_shape[0] == (3 * r_module.config.hidden_size
-                                                        ) // ckpt_mp_size:
+                                    if src_shape[0] == (3 * r_module.config.hidden_size) // ckpt_mp_size:
                                         qkv_size = src_shape[0] // 3
                                         src_split = [
-                                            torch.split(sd[j][prefix + n],
-                                                        qkv_size,
-                                                        dim=0) for j in range(len(sd))
+                                            torch.split(sd[j][prefix + n], qkv_size, dim=0) for j in range(len(sd))
                                         ]
 
                                         p.data.copy_(
-                                            torch.cat(
-                                                [
-                                                    torch.cat([
-                                                        qkv_s[i] for qkv_s in src_split
-                                                    ],
-                                                              axis=0)
-                                                    for i in range(len(src_split[0]))
-                                                ],
-                                                dim=0).to(get_accelerator(
-                                                ).current_device_name()).contiguous())
+                                            torch.cat([
+                                                torch.cat([qkv_s[i] for qkv_s in src_split], axis=0)
+                                                for i in range(len(src_split[0]))
+                                            ],
+                                                      dim=0).to(get_accelerator().current_device_name()).contiguous())
                                     else:
                                         p.data.copy_(
-                                            torch.cat(
-                                                [
-                                                    sd[j][prefix + n]
-                                                    for j in range(len(sd))
-                                                ],
-                                                dim=0).to(get_accelerator(
-                                                ).current_device_name()).contiguous())
+                                            torch.cat([sd[j][prefix + n] for j in range(len(sd))],
+                                                      dim=0).to(get_accelerator().current_device_name()).contiguous())
 
             load_parameters(module, prefix)
             for n, child in module.named_children():
                 load_parameters(child, prefix + n + '.')
         else:
             container.load_params(module, sd[0], weight_quantizer, mp_replace, prefix)
 
@@ -245,38 +206,33 @@
                         child.weight.ds_id in all_ds_ids):
                         prefix1 = all_ds_ids[child.weight.ds_id]
                         if child.__class__ is nn.Linear:
                             child = LinearLayer(weight=all_ds_ids[child.weight.ds_id])
                             setattr(module, name, child)
                     continue
                 child_params = list(child.parameters())
-                if len(child_params) > 0 and (child_params[0].numel() == 0
-                                              or child_params[0].is_meta):
+                if len(child_params) > 0 and (child_params[0].numel() == 0 or child_params[0].is_meta):
                     if child.weight.is_meta:
                         ds_shape = child.weight.shape
                     else:
                         ds_shape = child.weight.ds_shape
                     if child.__class__ is nn.LayerNorm:
-                        child = Normalize(dim=ds_shape[-1],
-                                          dtype=child.weight.dtype,
-                                          eps=child.eps)
+                        child = Normalize(dim=ds_shape[-1], dtype=child.weight.dtype, eps=child.eps)
                         setattr(module, name, child)
                     elif child.__class__ is nn.Linear:
-                        child = LinearLayer(weight_shape=child.weight.shape,
-                                            bias=child.bias)
+                        child = LinearLayer(weight_shape=child.weight.shape, bias=child.bias)
                         setattr(module, name, child)
                     elif child.__class__ is OPTLearnedPositionalEmbedding:
                         child = OPTEmbedding(weight_shape=ds_shape)
                         setattr(module, name, child)
                     else:
                         ds_id = None
                         if hasattr(child.weight, 'ds_id'):
                             ds_id = child.weight.ds_id
-                        child = EmbeddingLayer(weight_shape=ds_shape,
-                                               dtype=child.weight.dtype)
+                        child = EmbeddingLayer(weight_shape=ds_shape, dtype=child.weight.dtype)
                         if ds_id is not None:
                             all_ds_ids[ds_id] = child.weight
                         setattr(module, name, child)
                 layer_policies[child.__class__](child, prefix + name + '.')
             else:
                 load_module_recursive(
                     child,
```

### Comparing `deepspeed-0.8.3/deepspeed/module_inject/module_quantize.py` & `deepspeed-0.9.0/deepspeed/module_inject/module_quantize.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,8 +1,11 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import torch
 
 
 def quantize_transformer_layer(orig_layer_impl, model, megatron=False, preln=False):
     """ Quantize bert-style transformer layers with DeepSpeed's transformer layer
     Arguments:
@@ -14,57 +17,46 @@
         preln (bool): does the original layer implementation do pre or post layer norm?
 
         Note: For Bert kind of models, we inject based on the DeepSpeed-Example models, if not setting huggingface flag.
 
     Returns:
         Updated nn.module with quantized transformer layers
     """
+
     def quantize_weight(weight):
         return weight.to(torch.int8)
 
     def megatron_layer_quantize(layer):
-        layer.attention.query_key_value.weight.data = quantize_weight(
-            layer.attention.query_key_value.weight.data)
-        layer.attention.dense.weight.data = quantize_weight(
-            layer.attention.dense.weight.data)
-        layer.mlp.dense_h_to_4h.weight.data = quantize_weight(
-            layer.mlp.dense_h_to_4h.weight.data)
-        layer.mlp.dense_4h_to_h.weight.data = quantize_weight(
-            layer.mlp.dense_4h_to_h.weight.data)
+        layer.attention.query_key_value.weight.data = quantize_weight(layer.attention.query_key_value.weight.data)
+        layer.attention.dense.weight.data = quantize_weight(layer.attention.dense.weight.data)
+        layer.mlp.dense_h_to_4h.weight.data = quantize_weight(layer.mlp.dense_h_to_4h.weight.data)
+        layer.mlp.dense_4h_to_h.weight.data = quantize_weight(layer.mlp.dense_4h_to_h.weight.data)
 
     def bert_layer_quantize(layer):
-        layer.attention.self.query.weight.data = quantize_weight(
-            layer.attention.self.query.weight.data)
-        layer.attention.self.key.weight.data = quantize_weight(
-            layer.attention.self.key.weight.data)
-        layer.attention.self.value.weight.data = quantize_weight(
-            layer.attention.self.value.weight.data)
-        layer.attention.output.dense.weight.data = quantize_weight(
-            layer.attention.output.dense.weight.data)
+        layer.attention.self.query.weight.data = quantize_weight(layer.attention.self.query.weight.data)
+        layer.attention.self.key.weight.data = quantize_weight(layer.attention.self.key.weight.data)
+        layer.attention.self.value.weight.data = quantize_weight(layer.attention.self.value.weight.data)
+        layer.attention.output.dense.weight.data = quantize_weight(layer.attention.output.dense.weight.data)
         if preln:
-            layer.intermediate.dense_act.weight.data = quantize_weight(
-                layer.intermediate.dense_act.weight.data)
+            layer.intermediate.dense_act.weight.data = quantize_weight(layer.intermediate.dense_act.weight.data)
         else:
-            layer.intermediate.dense.weight.data = quantize_weight(
-                layer.intermediate.dense.weight.data)
+            layer.intermediate.dense.weight.data = quantize_weight(layer.intermediate.dense.weight.data)
         layer.output.dense.weight.data = quantize_weight(layer.output.dense.weight.data)
 
     def quantize_fn(child):
         if megatron:
             # Quantize megatron GPT2 / GPT3 trained model
             megatron_layer_quantize(child)
         else:
             # Quantize either DeepSpeed or HuggingFace trained model
             bert_layer_quantize(child)
 
         return child
 
-    return quantize_module(model=model,
-                           orig_class=orig_layer_impl,
-                           quantize_fn=quantize_fn)
+    return quantize_module(model=model, orig_class=orig_layer_impl, quantize_fn=quantize_fn)
 
 
 def quantize_module(model, orig_class, quantize_fn):
     policy = {orig_class: quantize_fn}
     return _quantize_module(model, policy)
```

### Comparing `deepspeed-0.8.3/deepspeed/module_inject/policy.py` & `deepspeed-0.9.0/deepspeed/module_inject/policy.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,10 +1,12 @@
-'''
-Copyright 2022 The Microsoft DeepSpeed Team
-'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+
 from abc import ABC, abstractmethod
 from deepspeed.utils.types import ActivationFuncType
 import torch
 from deepspeed.accelerator import get_accelerator
 
 transformer_param_names = (
         'attn_qkvw', \
@@ -66,23 +68,30 @@
         self.use_mup = use_mup
         self.mlp_act_func_type = mlp_act_func_type
         self.pre_attn_norm = pre_attn_norm
         self.use_load_prefix = use_load_prefix
         self.split_qkv = split_qkv
 
     @abstractmethod
-    def attention(self):
+    def attention(self, enable_training=False):
         """
         Returns attention qkv and dense parameters
         weight: (3*hidden, hidden) and (hidden, hidden)
         bias: (3*hidden) and (hidden)
         """
         raise NotImplementedError
 
     @abstractmethod
+    def get_q_k_v(self):
+        """
+        return all q,k,v parameters without merging them together
+        """
+        raise NotImplementedError
+
+    @abstractmethod
     def get_hidden_heads(self):
         """
         return hidden_size and number of heads
         """
         raise NotImplementedError
 
     @abstractmethod
@@ -99,14 +108,22 @@
         """
         Returns LayerNorms used in transformer layer
         Post-Attention and pre/post layer norm
         gamma and beta with shape: (hidden)
         """
         raise NotImplementedError
 
+    @abstractmethod
+    def get_lora_params(self):
+        """
+        Returns lora parameters used in transformer layer
+
+        """
+        raise NotImplementedError
+
 
 # TODO (lekurile): This function exists in base container as well, consolidate as some point
 def transpose(data):
     with torch.no_grad():
         data = data.contiguous()
         data1 = data.transpose(-1, -2).reshape(-1)
         data.reshape(-1).copy_(data1)
@@ -120,23 +137,18 @@
     outer_dim = -1
     attention_head_size = x.shape[outer_dim] // heads
     new_x_shape = x.size()[:outer_dim] + (heads, attention_head_size)
     x_1 = x.view(*new_x_shape)
     (q, k, v) = torch.split(x_1, (x_1.shape[-1] // 3), dim=-1)
     if len(q.shape) > 2:
         new_shape = (q.shape[0], ) + (-1, )
-        return torch.cat((q.reshape(new_shape),
-                          k.reshape(new_shape),
-                          v.reshape(new_shape)),
+        return torch.cat((q.reshape(new_shape), k.reshape(new_shape), v.reshape(new_shape)),
                          dim=outer_dim).reshape(x.shape)
     else:
-        return torch.cat((q.reshape(-1),
-                          k.reshape(-1),
-                          v.reshape(-1)),
-                         dim=-1).reshape(x.shape)
+        return torch.cat((q.reshape(-1), k.reshape(-1), v.reshape(-1)), dim=-1).reshape(x.shape)
 
 
 # This checks if the parameter exits in the checkpoint file and maybe copies it into the corresponding destination tensor.
 # Note that not all parameters are saved in one checkpoint, that's why we always need to check if they exist!
 def maybe_copy(module,
                sd,
                weight_quantizer,
@@ -152,42 +164,31 @@
         tmp = sd[src_name]
         if len(dst.shape) == 1:
             if split_qkv:
                 dst = mp_replace.qkv_copy(dst, tmp)
             else:
                 dst = mp_replace.copy(dst, tmp)
             if qkv and megatron_v2:
-                dst = torch.nn.parameter.Parameter(
-                    _transpose(dst,
-                               heads=heads,
-                               mp_replace=mp_replace).contiguous())
+                dst = torch.nn.parameter.Parameter(_transpose(dst, heads=heads, mp_replace=mp_replace).contiguous())
         else:
             if split_qkv:
                 dst = mp_replace.qkv_copy(dst, weight_quantizer.quantize(tmp if weight_quantizer.q_int8 else \
                                                 (transpose(tmp).contiguous())), int8=weight_quantizer.q_int8)
             else:
                 if qkv and megatron_v2:
-                    tmp = _transpose(transpose(tmp),
-                                     heads=heads,
-                                     mp_replace=mp_replace).contiguous()
+                    tmp = _transpose(transpose(tmp), heads=heads, mp_replace=mp_replace).contiguous()
                     if weight_quantizer.q_int8:
                         tmp = transpose(tmp)
                 dst = mp_replace.copy(dst, weight_quantizer.quantize(tmp if weight_quantizer.q_int8 else \
                                                 transpose(tmp)), int8=weight_quantizer.q_int8)
         setattr(module, dst_name, dst)
 
 
 # Extending the maybe_copy function for when the q, k, and v are in separate parameters!
-def maybe_copy_qkv(module,
-                   sd,
-                   weight_quantizer,
-                   mp_replace,
-                   dst_name,
-                   src_names,
-                   split_qkv=False):
+def maybe_copy_qkv(module, sd, weight_quantizer, mp_replace, dst_name, src_names, split_qkv=False):
     if src_names[0] in sd:
         q = sd[src_names[0]]
         k = sd[src_names[1]]
         v = sd[src_names[2]]
         qkv_data = torch.cat((q, k, v), dim=0)
         dst = getattr(module, dst_name)
         if len(dst.shape) == 1:
@@ -199,7 +200,23 @@
             if split_qkv:
                 dst = mp_replace.qkv_copy(dst, weight_quantizer.quantize(qkv_data.to(get_accelerator().device_name()) if weight_quantizer.q_int8 else \
                                                 ((transpose(qkv_data)).contiguous())), int8=weight_quantizer.q_int8)
             else:
                 dst = mp_replace.copy(dst, weight_quantizer.quantize(qkv_data.to(get_accelerator().device_name()) if weight_quantizer.q_int8 else \
                                                 transpose(qkv_data)), int8=weight_quantizer.q_int8)
         setattr(module, dst_name, dst)
+
+
+def pack_lora_weights(p):
+    return [
+        p.lora_right_weight, \
+        p.lora_left_weight, \
+        p.lora_scaling
+    ]
+
+
+def maybe_get_lora(p):
+    if hasattr(p, 'lora_right_weight'):
+        lora_param = pack_lora_weights(p)
+    else:
+        lora_param = []
+    return lora_param
```

### Comparing `deepspeed-0.8.3/deepspeed/module_inject/replace_module.py` & `deepspeed-0.9.0/deepspeed/module_inject/replace_module.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,8 +1,11 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import os
 import torch
 import tqdm
 import deepspeed
 import deepspeed.ops.transformer as transformer_inference
 from deepspeed.ops.transformer.inference.diffusers_attention import DeepSpeedDiffusersAttention
@@ -19,14 +22,15 @@
 from .load_checkpoint import load_model_with_checkpoint
 import time
 
 from .utils import policy_to_ds_container
 
 
 class ReplaceWithTensorSlicing:
+
     def __init__(self, mp_group=None, mp_size=1, out_dim=1, in_dim=0):
         if mp_group is not None:
             self.gpu_index = dist.get_rank(group=mp_group)
         else:
             self.gpu_index = 0
         self.out_dim = out_dim
         self.in_dim = in_dim
@@ -54,87 +58,71 @@
                 dst = torch.nn.parameter.Parameter(dst, requires_grad=False)
                 if hasattr(src, 'scale'):
                     dst.scale = src.scale
                 return dst
             if self.out_dim == 1:
                 self.merge_assert(src_shape[outer_dim], dst_shape[self.out_dim])
                 qkv_size = dst_shape[self.out_dim] // 3
-                qkv_split = [
-                    torch.split(src_s,
-                                qkv_size,
-                                dim=outer_dim) for src_s in src_split
-                ]
+                qkv_split = [torch.split(src_s, qkv_size, dim=outer_dim) for src_s in src_split]
 
                 weight_split = [
-                    torch.cat([qkv_s[i] for qkv_s in qkv_split],
-                              axis=outer_dim) for i in range(len(qkv_split[0]))
+                    torch.cat([qkv_s[i] for qkv_s in qkv_split], axis=outer_dim) for i in range(len(qkv_split[0]))
                 ]
-                dst = dst.reshape(-1).data.copy_(
-                    weight_split[self.gpu_index].contiguous().reshape(-1)).reshape(
-                        weight_split[self.gpu_index].shape)
+                dst = dst.reshape(-1).data.copy_(weight_split[self.gpu_index].contiguous().reshape(-1)).reshape(
+                    weight_split[self.gpu_index].shape)
             else:
-                dst.data.copy_(src_split[self.gpu_index].to(
-                    get_accelerator().current_device_name()).contiguous())
+                dst.data.copy_(src_split[self.gpu_index].to(get_accelerator().current_device_name()).contiguous())
         else:
             if src_shape[0] == dst_shape[0]:
                 return torch.nn.parameter.Parameter(src)
             if self.out_dim == 1:
                 qkv_size = dst_shape[0] // 3
                 qkv_split = [torch.split(src_s, qkv_size, dim=0) for src_s in src_split]
-                bias_split = [
-                    torch.cat([qkv_s[i] for qkv_s in qkv_split],
-                              axis=0) for i in range(len(qkv_split[0]))
-                ]
+                bias_split = [torch.cat([qkv_s[i] for qkv_s in qkv_split], axis=0) for i in range(len(qkv_split[0]))]
                 dst.data.copy_(bias_split[self.gpu_index].contiguous())
             else:
                 dst.data.copy_(src_split[self.gpu_index].contiguous())
 
         dst = torch.nn.parameter.Parameter(dst, requires_grad=False)
         if hasattr(src, 'scale'):
             dst.scale = src.scale
         return dst
 
-    def copy(self, dst, src, int8=False):
+    def copy(self, dst, src, int8=False, allocat_tensor=False):
         if src is None:
             return src
         assert not dst.data.is_meta  # the torch.Tensor.copy_ method used below will silently fail on meta tensors
+        if allocat_tensor:
+            dst = torch.empty_like(dst)
         outer_dim = 0 if int8 else 1
         inner_dim = 1 if int8 else 0
         src_shape = src.shape
         dst_shape = dst.shape
         if (len(src_shape) == 2 and len(dst_shape) == 2):
 
-            if src_shape[inner_dim] == dst_shape[
-                    self.in_dim] and src_shape[outer_dim] == dst_shape[self.out_dim]:
+            if src_shape[inner_dim] == dst_shape[self.in_dim] and src_shape[outer_dim] == dst_shape[self.out_dim]:
                 dst = dst.reshape(-1).data.copy_(src.data.reshape(-1)).reshape(src.shape)
             else:
                 if src_shape[inner_dim] != dst_shape[self.in_dim]:
                     self.merge_assert(src_shape[inner_dim], dst_shape[self.in_dim])
-                    weight_split = torch.split(
-                        src,
-                        dst_shape[self.in_dim],
-                        dim=inner_dim)[self.gpu_index].contiguous()
+                    dst.data.copy_(src[:, self.gpu_index * dst_shape[self.in_dim]: (self.gpu_index + 1) * dst_shape[self.in_dim]] if inner_dim == 1 else \
+                                   src[self.gpu_index * dst_shape[self.in_dim]: (self.gpu_index + 1) * dst_shape[self.in_dim], :])
                 else:
                     self.merge_assert(src_shape[outer_dim], dst_shape[self.out_dim])
-                    weight_split = torch.split(
-                        src.data,
-                        dst_shape[self.out_dim],
-                        dim=outer_dim)[self.gpu_index].contiguous()
-                dst = dst.reshape(-1).data.copy_(weight_split.reshape(-1)).reshape(
-                    weight_split.shape)
+                    dst.data.copy_(src[:, self.gpu_index * dst_shape[self.out_dim]: (self.gpu_index + 1) * dst_shape[self.out_dim]] if outer_dim == 1 else \
+                                   src[self.gpu_index * dst_shape[self.out_dim]: (self.gpu_index + 1) * dst_shape[self.out_dim], :])
         else:
             if src_shape[0] == dst_shape[0]:
-                dst.data.copy_(src)
+                dst = src
             else:
-                bias_split = torch.split(src.data,
-                                         dst_shape[-1])[self.gpu_index].contiguous()
-                dst.data.copy_(bias_split)
+                dst.data.copy_(src[self.gpu_index * dst_shape[-1]:(self.gpu_index + 1) * dst_shape[-1]])
         dst = torch.nn.parameter.Parameter(dst, requires_grad=False)
         if hasattr(src, 'scale'):
             dst.scale = src.scale
+
         return dst
 
 
 def get_transformer_name(replaced_module):
     from .containers import supported_models
     from torch.nn import ModuleList
     transformer_name = ''
@@ -146,75 +134,59 @@
                     transformer_name += name
                     break
             break
     return transformer_name
 
 
 class GroupQuantizer:
+
     def __init__(self, q_int8=True, group_size=1, num_bits=8, num_groups=0):
         self.group_size = group_size
         self.num_bits = num_bits
         self.q_int8 = q_int8
 
         self.num_groups = num_groups
 
     def quantize(self, inputs, qkv=True, count=1, parallel_dim=0):
         if not self.q_int8 or not qkv:
             inputs = torch.nn.Parameter(inputs, requires_grad=False)
             inputs.scale = torch.empty(1)
             return inputs
         q_range = 2**self.num_bits
-        num_groups = self.num_groups if self.num_groups > 0 else inputs.shape[
-            0] // self.group_size
+        num_groups = self.num_groups if self.num_groups > 0 else inputs.shape[0] // self.group_size
         inputs = inputs.to(get_accelerator().current_device_name())
         input_flat = inputs.reshape(num_groups, -1).contiguous()
         input_min = torch.min(input_flat, dim=1, keepdim=True)[0].float()
         input_max = torch.max(input_flat, dim=1, keepdim=True)[0].float()
         scale = torch.max(input_min.abs(), input_max.abs()) * 2.0 / (q_range)
         input_flat = (input_flat / scale).round().clamp(-q_range // 2, q_range // 2 - 1)
         inputs_q = input_flat.reshape(inputs.shape).to(torch.int8).contiguous()
         out = torch.nn.Parameter(inputs_q, requires_grad=False)
         inputs_split = inputs.split(inputs.shape[parallel_dim] // 2, dim=parallel_dim)
-        input_flat = [
-            inputs_split[i].reshape(num_groups,
-                                    -1).contiguous() for i in range(2)
-        ]
-        input_min = [
-            torch.min(input_flat[i],
-                      dim=1,
-                      keepdim=True)[0].float() for i in range(2)
-        ]
-        input_max = [
-            torch.max(input_flat[i],
-                      dim=1,
-                      keepdim=True)[0].float() for i in range(2)
-        ]
-        scale1 = [
-            (torch.max(input_min[i].abs(),
-                       input_max[i].abs()) * 2.0 / (q_range)).squeeze().unsqueeze(0)
-            for i in range(2)
-        ]
-
-        out.scale = torch.cat([scale.squeeze().unsqueeze(0),
-                               scale1[0],
-                               scale1[1]],
-                              dim=0).reshape(num_groups,
-                                             -1).contiguous()
+        input_flat = [inputs_split[i].reshape(num_groups, -1).contiguous() for i in range(2)]
+        input_min = [torch.min(input_flat[i], dim=1, keepdim=True)[0].float() for i in range(2)]
+        input_max = [torch.max(input_flat[i], dim=1, keepdim=True)[0].float() for i in range(2)]
+        scale1 = [(torch.max(input_min[i].abs(), input_max[i].abs()) * 2.0 / (q_range)).squeeze().unsqueeze(0)
+                  for i in range(2)]
+
+        out.scale = torch.cat([scale.squeeze().unsqueeze(0), scale1[0], scale1[1]], dim=0).reshape(num_groups,
+                                                                                                   -1).contiguous()
         return out
 
 
 def _module_match(module):
     for policy in generic_policies:
         policy = policy()
         if policy.match(module):
             return policy
     return None
 
 
 def generic_injection(module, fp16=False, enable_cuda_graph=True):
+
     def replace_attn(child, policy):
         policy_attn = policy.attention(child)
         if policy_attn is None:
             return child
         if len(policy_attn) == 5:
             qkvw, attn_ow, attn_ob, hidden_size, heads = policy_attn
         else:
@@ -242,16 +214,15 @@
             attn_module.attn_qkvw = None
             attn_module.attn_qw.data = transpose(qw.data)
             attn_module.attn_kw.data = transpose(kw.data)
             attn_module.attn_vw.data = transpose(vw.data)
 
         attn_module.attn_qkvb = None
         attn_module.attn_ow.data = transpose(attn_ow.data)
-        attn_module.attn_ob.data.copy_(
-            attn_ob.data.to(get_accelerator().current_device_name()))
+        attn_module.attn_ob.data.copy_(attn_ob.data.to(get_accelerator().current_device_name()))
         return attn_module
 
     def replace_attn_block(child, policy):
         config = Diffusers2DTransformerConfig()
         return DeepSpeedDiffusersTransformerBlock(child, config)
 
     if isinstance(module, torch.nn.Module):
@@ -274,46 +245,39 @@
         #replace_transformer_layer(None,
         #                          module.text_encoder,
         #                          training=False,
         #                          replace_with_kernel_inject=True,
         #                          triangular_masking=True,
         #                          max_out_tokens=8192)
         from ..model_implementations.transformers.clip_encoder import DSClipEncoder
-        cg_encoder = DSClipEncoder(module.text_encoder,
-                                   enable_cuda_graph=enable_cuda_graph)
+        cg_encoder = DSClipEncoder(module.text_encoder, enable_cuda_graph=enable_cuda_graph)
         setattr(module, 'text_encoder', cg_encoder)
         for name in module.__dict__.keys():
             sub_module = getattr(module, name)
             policy = _module_match(sub_module)
 
             if policy is not None:
 
                 def _replace_module(module, policy):
                     for name, child in module.named_children():
                         _replace_module(child, policy)
                         if child.__class__ in new_policies:
-                            replaced_module = new_policies[child.__class__](child,
-                                                                            policy)
+                            replaced_module = new_policies[child.__class__](child, policy)
                             setattr(module, name, replaced_module)
 
                 _replace_module(sub_module, policy)
-                new_module = policy.apply(sub_module,
-                                          enable_cuda_graph=enable_cuda_graph)
+                new_module = policy.apply(sub_module, enable_cuda_graph=enable_cuda_graph)
                 print(f"**** found and replaced {name} w. {type(new_module)}")
                 setattr(module, name, new_module)
 
 
 container_g = None
 
 
-def replace_transformer_layer(orig_layer_impl,
-                              model,
-                              checkpoint_dict,
-                              config,
-                              model_config):
+def replace_transformer_layer(orig_layer_impl, model, checkpoint_dict, config, model_config):
     """ Replace bert-style transformer layers with DeepSpeed's transformer layer
     Arguments:
         orig_layer_impl (torch.nn.Module): the original transformer layer implementation to look for,
             e.g., transformers.modeling_bert.BertLayer.
         model (torch.nn.Module): user's nn.module representing their model
         checkpoint_dict: Dictionary for checkpoint passed from the Inference Engine
         config: top-level DS Inference config defined in inference/config.py
@@ -330,23 +294,18 @@
     '''
         linear_layer_setting (tuple of modules) [Optional]: shows which two classes are used for linear layers and embedding layers
     '''
     micro_batch_size = -1
     seed = -1
     local_rank = -1
 
-    mp_replace = ReplaceWithTensorSlicing(
-        mp_group=config.tensor_parallel.tp_group,
-        mp_size=config.tensor_parallel.tp_size)  #, out_dim=0, in_dim=1)
-
-    def replace_with_policy(child,
-                            policy_cls,
-                            triangular_masking,
-                            inference=False,
-                            layer_id=0):
+    mp_replace = ReplaceWithTensorSlicing(mp_group=config.tensor_parallel.tp_group,
+                                          mp_size=config.tensor_parallel.tp_size)  #, out_dim=0, in_dim=1)
+
+    def replace_with_policy(child, policy_cls, triangular_masking, inference=False, layer_id=0):
         policy = policy_cls(child, inference=inference)
         if not policy.cuda_graph_supported:
             # policy says cuda graph is not supported raise an error if set
             assert not config.enable_cuda_graph, "cuda graph is not supported with this model, please disable"
 
         from deepspeed.moe.layer import MoE
         moe = False
@@ -360,16 +319,15 @@
                                             model_config=model_config,
                                             layer_id=layer_id,
                                             child=child)
         _container.set_dtype(fp16)
         _container.set_moe(moe)
 
         # 2. Set the tensor parallelism config
-        _container.set_tensor_parallel_config(config.tensor_parallel.tp_size,
-                                              config.tensor_parallel.tp_group)
+        _container.set_tensor_parallel_config(config.tensor_parallel.tp_size, config.tensor_parallel.tp_group)
 
         # 3. Initialize tensors
         _container.initialize_tensors()
 
         # 4. deal with data types -- needs refactor to use dtype instead of fp16
         if fp16:
             _container.convert_to_required_dtype(dtype=torch.half)
@@ -407,81 +365,88 @@
 
         def _replace(child, name, conv_linear_layer):
             mp_replace = ReplaceWithTensorSlicing(mp_group=mp_group)
             weight_shape = child.weight.shape
             if name in all_reduce_linears:
                 new_weight = torch.empty((
                     weight_shape[1] if conv_linear_layer else weight_shape[0],
-                    (weight_shape[0] if conv_linear_layer else weight_shape[1]) //
-                    mp_size,
+                    (weight_shape[0] if conv_linear_layer else weight_shape[1]) // mp_size,
                 ),
                                          device=child.weight.device,
                                          dtype=child.weight.dtype)
                 if conv_linear_layer:
                     child.weight.data = child.weight.data.transpose(-1, -2).contiguous()
                 data = mp_replace.copy(new_weight, child.weight.data)
-                new_bias = torch.empty((weight_shape[0]),
-                                       device=child.weight.device,
-                                       dtype=child.weight.dtype)
+                new_bias = torch.empty((weight_shape[0]), device=child.weight.device, dtype=child.weight.dtype)
                 if child.bias is not None:
                     new_bias.data.copy_(child.bias.data)
                 return LinearAllreduce(data, child.bias if child.bias is None else \
                             torch.nn.parameter.Parameter(new_bias.to(get_accelerator().current_device_name())), mp_group)
             else:
                 new_weight = torch.empty((
-                    (weight_shape[1] if conv_linear_layer else weight_shape[0]) //
-                    mp_size,
+                    (weight_shape[1] if conv_linear_layer else weight_shape[0]) // mp_size,
                     weight_shape[0] // mp_size if conv_linear_layer else weight_shape[1],
                 ),
                                          device=child.weight.device,
                                          dtype=child.weight.dtype)
                 if conv_linear_layer:
                     child.weight.data = child.weight.data.transpose(-1, -2).contiguous()
                 data = mp_replace.copy(new_weight, child.weight.data)
 
                 new_bias = torch.empty((weight_shape[0] // mp_size),
                                        device=child.weight.device,
                                        dtype=child.weight.dtype)
-                bias_data = None if child.bias is None else mp_replace.copy(
-                    new_bias,
-                    child.bias.data).to(get_accelerator().current_device_name())
-                return LinearLayer(weight=data.to(
-                    get_accelerator().current_device_name()),
-                                   bias=bias_data)
+                bias_data = None if child.bias is None else mp_replace.copy(new_bias, child.bias.data).to(
+                    get_accelerator().current_device_name())
+                return LinearLayer(weight=data.to(get_accelerator().current_device_name()), bias=bias_data)
 
         def _slice_embedding(child, name, conv_linear_layer):
             mp_replace = ReplaceWithTensorSlicing(mp_group=mp_group)
-            new_weight = torch.empty((child.weight.shape[0],
-                                      child.weight.shape[1] // mp_size),
+            new_weight = torch.empty((child.weight.shape[0], child.weight.shape[1] // mp_size),
                                      device=child.weight.device,
                                      dtype=child.weight.dtype)
             data = mp_replace.copy(new_weight,
                                    child.weight.ds_tensor.data if hasattr(child.weight, 'ds_tensor') else \
                                    child.weight.data)
-            new_embedding = nn.Embedding(child.weight.shape[0],
-                                         child.weight.shape[1] // mp_size)
+            new_embedding = nn.Embedding(child.weight.shape[0], child.weight.shape[1] // mp_size)
             new_embedding.weight.data.copy_(data)
             return new_embedding
 
         def update_mp_params(child):
             if hasattr(child, 'n_heads'):
+                assert child.n_heads % mp_size == 0, "n_heads ({}) must be divisible by mp_size ({})".format(
+                    child.n_heads, mp_size)
                 child.n_heads = child.n_heads // mp_size
             if hasattr(child, 'inner_dim'):
+                assert child.inner_dim % mp_size == 0, "inner_dim ({}) must be divisible by mp_size ({})".format(
+                    child.inner_dim, mp_size)
                 child.inner_dim = child.inner_dim // mp_size
             if hasattr(child, 'num_heads'):
+                assert child.num_heads % mp_size == 0, "num_heads ({}) must be divisible by mp_size ({})".format(
+                    child.num_heads, mp_size)
                 child.num_heads = child.num_heads // mp_size
             if hasattr(child, 'num_attention_heads'):
+                assert child.num_attention_heads % mp_size == 0, "num_attention_heads ({}) must be divisible by mp_size ({})".format(
+                    child.num_attention_heads, mp_size)
                 child.num_attention_heads = child.num_attention_heads // mp_size
             if hasattr(child, 'num_attn_heads'):
+                assert child.num_attn_heads % mp_size == 0, "num_attn_heads ({}) must be divisible by mp_size ({})".format(
+                    child.num_attn_heads, mp_size)
                 child.num_attn_heads = child.num_attn_heads // mp_size
             if hasattr(child, 'all_head_size'):
+                assert child.all_head_size % mp_size == 0, "all_head_size ({}) must be divisible by mp_size ({})".format(
+                    child.all_head_size, mp_size)
                 child.all_head_size = child.all_head_size // mp_size
             if hasattr(child, 'embed_dim'):
+                assert child.embed_dim % mp_size == 0, "embed_dim must ({}) be divisible by mp_size ({})".format(
+                    child.embed_dim, mp_size)
                 child.embed_dim = child.embed_dim // mp_size
             if hasattr(child, 'hidden_size'):
+                assert child.hidden_size % mp_size == 0, "hidden_size ({}) must be divisible by mp_size ({})".format(
+                    child.hidden_size, mp_size)
                 child.hidden_size = child.hidden_size // mp_size
 
         conv_linear_layer = False
         if linear_layer_setting is not None:
             linear_policies = {linear_layer_setting[0]: _replace}
             if len(linear_layer_setting) == 2:
                 linear_policies.update({linear_layer_setting[1]: _slice_embedding})
@@ -495,20 +460,16 @@
                     linear_policies = {nn.Linear: _replace}
             else:
                 linear_policies = {nn.Linear: _replace, nn.Embedding: _slice_embedding}
 
         def _replace_module(r_module, prev_name=''):
             for name, child in r_module.named_children():
                 if child.__class__ in linear_policies:
-                    setattr(
-                        r_module,
-                        name,
-                        linear_policies[child.__class__](child,
-                                                         prev_name + '.' + name,
-                                                         conv_linear_layer))
+                    setattr(r_module, name, linear_policies[child.__class__](child, prev_name + '.' + name,
+                                                                             conv_linear_layer))
                 else:
                     update_mp_params(child)
                     _replace_module(child, name)
             return r_module
 
         return _replace_module(module)
 
@@ -547,74 +508,60 @@
         ckpt_list = checkpoint["tp"] if type(checkpoint) is dict else checkpoint
         ckpt_type = checkpoint_dict.get('parallelization', 'pp')
         ckpt_mp_size = checkpoint_dict.get('tp_size', len(ckpt_list))
         ckpt_mp_size = checkpoint_dict.get('mp_size', ckpt_mp_size)
         base_dir1 = checkpoint_dict.get('base_dir', config.base_dir)
 
         if ckpt_type == 'pp' and type(checkpoint) is list:
-            pbar = tqdm.tqdm(total=len(checkpoint),
-                             desc=f"Loading {len(checkpoint)} checkpoint shards")
+            pbar = tqdm.tqdm(total=len(checkpoint), desc=f"Loading {len(checkpoint)} checkpoint shards")
 
             for i in range(len(checkpoint)):
-                sd = [
-                    torch.load(os.path.join(base_dir1,
-                                            checkpoint[i]),
-                               map_location='cpu')
-                ]
+                sd = [torch.load(os.path.join(base_dir1, checkpoint[i]), map_location='cpu')]
                 load_model_with_checkpoint(replaced_module,
                                            sd,
                                            mp_replace,
                                            ckpt_type,
                                            ckpt_mp_size,
                                            quantizer,
                                            container=container_g)
                 pbar.update(1)
         else:
             import gc
             num_checkpoints = len(ckpt_list) // ckpt_mp_size
             tp_split_size = (world_size / ckpt_mp_size)
             sd_offset = int(rank / tp_split_size)
             sd_count = int((rank + max(1, tp_split_size)) / tp_split_size) - sd_offset
-            pbar = tqdm.tqdm(total=num_checkpoints,
-                             desc=f"Loading {num_checkpoints} checkpoint shards")
+            pbar = tqdm.tqdm(total=num_checkpoints, desc=f"Loading {num_checkpoints} checkpoint shards")
             for i in range(num_checkpoints):
                 pbar.update(1)
                 ckpt_index = i * ckpt_mp_size + sd_offset
                 ckpt_files = [
-                    os.path.join(base_dir1,
-                                 ckpt_list[ckpt_index +
-                                           j]) if base_dir1 else ckpt_list[ckpt_index +
-                                                                           j]
+                    os.path.join(base_dir1, ckpt_list[ckpt_index + j]) if base_dir1 else ckpt_list[ckpt_index + j]
                     for j in range(sd_count)
                 ]
-                sds = [
-                    torch.load(ckpt_file,
-                               map_location='cpu') for ckpt_file in ckpt_files
-                ]
+                sds = [torch.load(ckpt_file, map_location='cpu') for ckpt_file in ckpt_files]
                 load_model_with_checkpoint(replaced_module,
                                            sds,
                                            mp_replace,
                                            ckpt_type,
                                            ckpt_mp_size,
                                            quantizer,
                                            int(rank % tp_split_size),
                                            container=container_g)
                 sds = [None for _ in sds]
                 gc.collect()
 
             if "non_tp" in checkpoint:
-                pbar = tqdm.tqdm(
-                    total=len(checkpoint["non_tp"]),
-                    desc=f"Loading {len(checkpoint['non_tp'])} checkpoint shards")
+                pbar = tqdm.tqdm(total=len(checkpoint["non_tp"]),
+                                 desc=f"Loading {len(checkpoint['non_tp'])} checkpoint shards")
 
                 for i in range(len(checkpoint["non_tp"])):
                     pbar.update(1)
                     ckpt_file = os.path.join(base_dir1,
-                                             checkpoint["non_tp"][i]
-                                             ) if base_dir1 else checkpoint["non_tp"][i]
+                                             checkpoint["non_tp"][i]) if base_dir1 else checkpoint["non_tp"][i]
                     sds = [torch.load(ckpt_file, map_location='cpu')]
                     load_model_with_checkpoint(replaced_module,
                                                sds,
                                                mp_replace,
                                                ckpt_type,
                                                ckpt_mp_size,
                                                quantizer,
@@ -645,77 +592,59 @@
         non_tp_ckpt_name = f'non-tp.pt'
         ckpt_files = [non_tp_ckpt_name]
         os.makedirs(config.save_mp_checkpoint_path, exist_ok=True)
 
         if not dist.is_initialized() or dist.get_rank() == 0:
             print("Saving tp-sharded checkpoints")
             torch.save(
-                OrderedDict({
-                    k: v
-                    for k,
-                    v in dict(replaced_module.state_dict()).items()
-                    if transformer_name not in k
-                }),
-                f'{config.save_mp_checkpoint_path}/{non_tp_ckpt_name}')
+                OrderedDict({k: v
+                             for k, v in dict(replaced_module.state_dict()).items()
+                             if transformer_name not in k}), f'{config.save_mp_checkpoint_path}/{non_tp_ckpt_name}')
             ckpt_config = json.dumps({
-                'type':
-                ckpt_name,
-                'base_dir':
-                f'{config.save_mp_checkpoint_path}',
+                'type': ckpt_name,
+                'base_dir': f'{config.save_mp_checkpoint_path}',
                 'checkpoints': {
-                    "non_tp":
-                    ckpt_files,
-                    "tp": [
-                        f'tp_{r:0>2d}_{m:0>2d}.pt' for m in range(num_partitions)
-                        for r in range(world_size)
-                    ]
+                    "non_tp": ckpt_files,
+                    "tp": [f'tp_{r:0>2d}_{m:0>2d}.pt' for m in range(num_partitions) for r in range(world_size)]
                 },
-                'version':
-                1.0,
-                'parallelization':
-                'tp',
-                'tp_size':
-                world_size,
-                'dtype':
-                'int8' if quantize else ('float16' if fp16 else 'float32')
+                'version': 1.0,
+                'parallelization': 'tp',
+                'tp_size': world_size,
+                'dtype': 'int8' if quantize else ('float16' if fp16 else 'float32')
             })
-            with open(f"{config.save_mp_checkpoint_path}/ds_inference_config.json",
-                      "w") as cfg:
+            with open(f"{config.save_mp_checkpoint_path}/ds_inference_config.json", "w") as cfg:
                 cfg.write(ckpt_config)
 
         rep_sd = replaced_module.state_dict()
         for n, p in replaced_module.named_parameters():
             if hasattr(p, 'scale'):
                 rep_sd[n] = [p, p.scale]
         keys = list(rep_sd.keys())
         partition_size = (len(keys) // num_partitions + 1)
         for m in range(num_partitions):
             torch.save(
                 OrderedDict({
-                    k: [rep_sd[k],
-                        rep_sd[k].scale] if hasattr(rep_sd[k],
-                                                    'scale') else rep_sd[k]
-                    for k in keys[m * partition_size:(m + 1) * partition_size]
-                    if transformer_name in k
-                }),
-                f'{config.save_mp_checkpoint_path}/tp_{rank:0>2d}_{m:0>2d}.pt')
+                    k: [rep_sd[k], rep_sd[k].scale] if hasattr(rep_sd[k], 'scale') else rep_sd[k]
+                    for k in keys[m * partition_size:(m + 1) * partition_size] if transformer_name in k
+                }), f'{config.save_mp_checkpoint_path}/tp_{rank:0>2d}_{m:0>2d}.pt')
 
     return replaced_module
 
 
 def revert_transformer_layer(orig_layer_impl, model, config, preln=False):
     """ Revert DeepSpeed's transformer layer back to original bert-style transformer layer
     Arguments:
         orig_layer_impl (torch.nn.Module): the original transformer layer implementation that was replaced,
             e.g., transformers.modeling_bert.BertLayer.
         model (torch.nn.Module): user's nn.module representing their model
         config (dict): model config containing hidden size, attention heads, etc.
     Returns:
         Updated nn.module with original bert-style transformer layers
     """
+
     def replace_fn(child, _replace_policy, layer_id):
         #from turing.nvidia_modelingpreln import BertLayer
         orig_module = orig_layer_impl(config)
 
         # copy relevant state from child -> original module
         qkvw = child.attn_qkvw.data
         qkvb = child.attn_qkvb.data
@@ -809,17 +738,15 @@
         model (torch.nn.Module): model to augment
         policies (dict): Mapping of source class to replacement function.
     Returns:
         Modified ``model``.
     """
     for name, child in model.named_children():
         if child.__class__ in policies:
-            replaced_module = policies[child.__class__][0](child,
-                                                           policies[child.__class__][-1],
-                                                           layer_id)
+            replaced_module = policies[child.__class__][0](child, policies[child.__class__][-1], layer_id)
             setattr(model, name, replaced_module)
             if isinstance(model, PipelineModule):
                 assert hasattr(model, 'forward_funcs'),\
                     "we require pipe-module to have the list of fwd_functions"
                 model.forward_funcs[model.fwd_map[name]] = replaced_module
             layer_id += 1
         else:
```

### Comparing `deepspeed-0.8.3/deepspeed/module_inject/replace_policy.py` & `deepspeed-0.9.0/deepspeed/module_inject/replace_policy.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,10 +1,12 @@
-'''
-Copyright 2020 The Microsoft DeepSpeed Team
-'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+
 from .containers import HFGPT2LayerPolicy
 from .containers import HFBertLayerPolicy
 from .containers import BLOOMLayerPolicy
 from .containers import HFGPTJLayerPolicy
 from .containers import HFGPTNEOLayerPolicy
 from .containers import GPTNEOXLayerPolicy
 from .containers import HFOPTLayerPolicy
@@ -12,21 +14,13 @@
 from .containers import HFDistilBertLayerPolicy
 from .containers import HFCLIPLayerPolicy
 from .containers import UNetPolicy
 from .containers import VAEPolicy
 
 # transformer-based policies
 replace_policies = [
-    HFBertLayerPolicy,
-    HFGPTNEOLayerPolicy,
-    GPTNEOXLayerPolicy,
-    HFGPTJLayerPolicy,
-    MegatronLayerPolicy,
-    HFGPT2LayerPolicy,
-    BLOOMLayerPolicy,
-    HFOPTLayerPolicy,
-    HFCLIPLayerPolicy,
-    HFDistilBertLayerPolicy
+    HFBertLayerPolicy, HFGPTNEOLayerPolicy, GPTNEOXLayerPolicy, HFGPTJLayerPolicy, MegatronLayerPolicy,
+    HFGPT2LayerPolicy, BLOOMLayerPolicy, HFOPTLayerPolicy, HFCLIPLayerPolicy, HFDistilBertLayerPolicy
 ]
 
 # non-transformer-based policies
 generic_policies = [UNetPolicy, VAEPolicy]
```

### Comparing `deepspeed-0.8.3/deepspeed/module_inject/utils.py` & `deepspeed-0.9.0/deepspeed/module_inject/utils.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,8 +1,11 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 from deepspeed.utils import log_dist
 
 
 # helper function to map between DS policies and DS containers
 def policy_to_ds_container(**kwargs):
     from .containers import HFGPT2LayerPolicy, DS_GPT2Container
```

### Comparing `deepspeed-0.8.3/deepspeed/moe/layer.py` & `deepspeed-0.9.0/deepspeed/moe/layer.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-'''
-Copyright 2020 The Microsoft DeepSpeed Team
-'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import torch
 
 from deepspeed.utils import log_dist
 
 from deepspeed.utils import groups
 from .sharded_moe import MOELayer, TopKGate
@@ -27,14 +28,15 @@
         use_residual (bool, optional): default=False, make this MoE layer a Residual MoE (https://arxiv.org/abs/2201.05596) layer.
         noisy_gate_policy (str, optional): default=None, noisy gate policy, valid options are 'Jitter', 'RSample' or 'None'.
         drop_tokens (bool, optional): default=True, whether to drop tokens - (setting to False is equivalent to infinite capacity).
         use_rts (bool, optional): default=True, whether to use Random Token Selection.
         use_tutel (bool, optional): default=False, whether to use Tutel optimizations (if installed).
         enable_expert_tensor_parallelism (bool, optional): default=False, whether to use tensor parallelism for experts
     """
+
     def __init__(self,
                  hidden_size,
                  expert,
                  num_experts=1,
                  ep_size=1,
                  k=1,
                  capacity_factor=1.,
@@ -61,23 +63,16 @@
             f'Creating MoE layer with num_experts: {num_experts} | num_local_experts: {self.num_local_experts} | expert_parallel_size: {self.ep_size}',
             [0])
 
         assert noisy_gate_policy is None or noisy_gate_policy in ['None', 'Jitter', 'RSample'], \
             'Unsupported noisy_gate_policy: ' + noisy_gate_policy
 
         experts = Experts(expert, self.num_local_experts, self.expert_group_name)
-        self.deepspeed_moe = MOELayer(TopKGate(hidden_size,
-                                               num_experts,
-                                               k,
-                                               capacity_factor,
-                                               eval_capacity_factor,
-                                               min_capacity,
-                                               noisy_gate_policy,
-                                               drop_tokens,
-                                               use_rts),
+        self.deepspeed_moe = MOELayer(TopKGate(hidden_size, num_experts, k, capacity_factor, eval_capacity_factor,
+                                               min_capacity, noisy_gate_policy, drop_tokens, use_rts),
                                       experts,
                                       self.expert_group_name,
                                       self.ep_size,
                                       self.num_local_experts,
                                       use_tutel=use_tutel)
         if self.use_residual:
             self.mlp = expert
@@ -86,28 +81,24 @@
 
     def set_deepspeed_parallelism(self):
         self._create_process_groups()
 
     def _create_process_groups(self):
         # Create process group for a layer if needed
         if self.expert_group_name not in groups._get_expert_parallel_group_dict():
-            print(
-                f"No existing process group found, creating a new group named: {self.expert_group_name}"
-            )
+            print(f"No existing process group found, creating a new group named: {self.expert_group_name}")
             if (groups.mpu is None) or (not self.enable_expert_tensor_parallelism):
                 # Condition 1 - no groups.mpu means no tensor parallelism
                 # Condition 2 - disabling expert tensor parallelism on purpose
                 groups._create_expert_and_data_parallel(self.ep_size)
             else:
                 # expert tensor parallelism is enabled
-                groups._create_expert_data_and_model_parallel(self.ep_size,
-                                                              mpu=groups.mpu)
+                groups._create_expert_data_and_model_parallel(self.ep_size, mpu=groups.mpu)
         # Set the group handle for the MOELayer (deepspeed_moe) object
-        self.deepspeed_moe._set_ep_group(
-            groups._get_expert_parallel_group(self.expert_group_name))
+        self.deepspeed_moe._set_ep_group(groups._get_expert_parallel_group(self.expert_group_name))
 
     def forward(self, hidden_states, used_token=None):
         """ MoE forward
 
         Arguments:
             hidden_states (Tensor): input to the layer
             used_token (Tensor, optional): default: None, mask only used tokens
```

### Comparing `deepspeed-0.8.3/deepspeed/moe/mappings.py` & `deepspeed-0.9.0/deepspeed/moe/mappings.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-'''
-Copyright 2022 The Microsoft DeepSpeed Team
-'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 # The file has been adapted from the following Megatron-LM file:
 # https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/mpu/mappings.py
 # Git commit hash: 9dc3c42a84aa656f583703cf8b6b4f79f712b796
 # We retain the following copyright from the original files:
 
 # Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.
@@ -28,43 +29,40 @@
     """Gather tensors and concatenate them along a dimension"""
     mpu = deepspeed.utils.groups.mpu
 
     input_ = input_.contiguous()
     # Size and dimension.
     rank = mpu.get_tensor_model_parallel_rank()
 
-    tensor_list = [
-        torch.empty_like(input_)
-        for _ in range(mpu.get_tensor_model_parallel_world_size())
-    ]
+    tensor_list = [torch.empty_like(input_) for _ in range(mpu.get_tensor_model_parallel_world_size())]
     tensor_list[rank] = input_
-    deepspeed.comm.all_gather(tensor_list,
-                              input_,
-                              group=mpu.get_tensor_model_parallel_group())
+    deepspeed.comm.all_gather(tensor_list, input_, group=mpu.get_tensor_model_parallel_group())
 
     # Note: torch.cat already creates a contiguous tensor.
     output = torch.cat(tensor_list, dim=dim).contiguous()
 
     return output
 
 
 def _drop_tokens(input_, dim=0):
     """Divide a tensor among the tensor parallel ranks"""
     mpu = deepspeed.utils.groups.mpu
 
     total_chunks = mpu.get_tensor_model_parallel_world_size()
     this_chunk = mpu.get_tensor_model_parallel_rank()
-    assert input_.shape[dim] % total_chunks == 0, f"input dimension {dim} ({input_.shape[dim]}) is not divisible by tensor parallel world size ({total_chunks})"
+    assert input_.shape[
+        dim] % total_chunks == 0, f"input dimension {dim} ({input_.shape[dim]}) is not divisible by tensor parallel world size ({total_chunks})"
     chunk_size = input_.shape[dim] // total_chunks
 
     return torch.narrow(input_, dim, this_chunk * chunk_size, chunk_size)
 
 
 class _GatherTokens(torch.autograd.Function):
     """All gather tokens among the tensor parallel ranks"""
+
     @staticmethod
     def symbolic(graph, input_, dim):
         return _gather_tokens(input_, dim)
 
     @staticmethod
     def forward(ctx, input_, dim):
         ctx.dim = dim
```

### Comparing `deepspeed-0.8.3/deepspeed/moe/sharded_moe.py` & `deepspeed-0.9.0/deepspeed/moe/sharded_moe.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,15 +1,18 @@
-'''
-Copyright 2021 The Microsoft DeepSpeed Team
-'''
-# The file has been adapted from two fairscale files:
-# (1) https://github.com/facebookresearch/fairscale/blob/master/fairscale/nn/moe/moe_layer.py
-# (2) https://github.com/facebookresearch/fairscale/blob/master/fairscale/nn/moe/top2gate.py
-# Git commit hash: 34df606902a240567a0d898037ece55c2f1336cf
-# We retain the following license from the original files:
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+"""
+The file has been adapted from two fairscale files:
+ (1) https://github.com/facebookresearch/fairscale/blob/master/fairscale/nn/moe/moe_layer.py
+ (2) https://github.com/facebookresearch/fairscale/blob/master/fairscale/nn/moe/top2gate.py
+ Git commit hash: 34df606902a240567a0d898037ece55c2f1336cf
+ We retain the following license from the original files:
+"""
 
 # Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.
 #
 # This source code is licensed under the BSD license found in the
 # LICENSE file in the root directory of this source tree.
 
 from deepspeed.utils.timer import SynchronizedWallClockTimer
@@ -56,19 +59,17 @@
     Returns:
         a jittered x.
     """
     if epsilon == 0:
         return x
     uniform = uniform_map.get(device)
     if uniform is None:
-        uniform = torch.distributions.uniform.Uniform(
-            low=torch.tensor(1.0 - epsilon,
-                             device=device),
-            high=torch.tensor(1.0 + epsilon,
-                              device=device)).rsample  # type: ignore
+        uniform = torch.distributions.uniform.Uniform(low=torch.tensor(1.0 - epsilon, device=device),
+                                                      high=torch.tensor(1.0 + epsilon,
+                                                                        device=device)).rsample  # type: ignore
         uniform_map[device] = uniform
     return x * uniform(x.shape)
 
 
 def gumbel_rsample(shape: Tuple, device: torch.device) -> Tensor:
     gumbel = gumbel_map.get(device)
     if gumbel is None:
@@ -83,14 +84,15 @@
 
 # einsum dimensions: (g)roup, (s)equence, (e)xpert, (m)odel, (c)apacity
 # See https://arxiv.org/pdf/2006.16668.pdf for details.
 
 
 # Based on https://github.com/pytorch/pytorch/pull/40762
 class _AllToAll(torch.autograd.Function):
+
     @staticmethod
     def forward(
             ctx: Any,
             # TODO: replace with DS process group
             group: torch.distributed.ProcessGroup,
             input: Tensor) -> Tensor:  # type: ignore
         ctx.group = group
@@ -177,33 +179,26 @@
 def top1gating(logits: Tensor,
                capacity_factor: float,
                min_capacity: int,
                used_token: Tensor = None,
                noisy_gate_policy: Optional[str] = None,
                drop_tokens: bool = True,
                use_rts: bool = True,
-               use_tutel: bool = False) -> Tuple[Tensor,
-                                                 Tensor,
-                                                 Tensor,
-                                                 Tensor]:
+               use_tutel: bool = False) -> Tuple[Tensor, Tensor, Tensor, Tensor]:
     """Implements Top1Gating on logits."""
     if noisy_gate_policy == 'RSample':
         logits_w_noise = logits + gumbel_rsample(logits.shape, device=logits.device)
     # everything is in fp32 in this function
     gates = F.softmax(logits, dim=1)
 
-    capacity = _capacity(gates,
-                         torch.tensor(capacity_factor),
-                         torch.tensor(min_capacity))
+    capacity = _capacity(gates, torch.tensor(capacity_factor), torch.tensor(min_capacity))
 
     # Create a mask for 1st's expert per token
     # noisy gating
-    indices1_s = torch.argmax(
-        logits_w_noise if noisy_gate_policy == 'RSample' else gates,
-        dim=1)
+    indices1_s = torch.argmax(logits_w_noise if noisy_gate_policy == 'RSample' else gates, dim=1)
     num_experts = int(gates.shape[1])
     mask1 = F.one_hot(indices1_s, num_classes=num_experts)
 
     # mask only used tokens
     if used_token is not None:
         mask1 = einsum("s,se->se", used_token, mask1)
 
@@ -221,26 +216,24 @@
     ce = torch.mean(mask1.float(), dim=0)
     l_aux = torch.sum(me * ce) * num_experts
 
     # Random Token Selection
     if use_rts:
         uniform = exp_selection_uniform_map.get(logits.device)
         if uniform is None:
-            uniform = torch.distributions.uniform.Uniform(
-                low=torch.tensor(0.0,
-                                 device=logits.device),
-                high=torch.tensor(1.0,
-                                  device=logits.device)).rsample
+            uniform = torch.distributions.uniform.Uniform(low=torch.tensor(0.0, device=logits.device),
+                                                          high=torch.tensor(1.0, device=logits.device)).rsample
             exp_selection_uniform_map[logits.device] = uniform
 
         mask1_rand = mask1 * uniform(mask1.shape)
     else:
         mask1_rand = mask1
 
-    assert logits.shape[0] >= min_capacity, "No. of tokens (batch-size) should be greater than min_capacity. Either set min_capacity to 0 or increase your batch size."
+    assert logits.shape[
+        0] >= min_capacity, "No. of tokens (batch-size) should be greater than min_capacity. Either set min_capacity to 0 or increase your batch size."
 
     top_idx = _top_idx(mask1_rand, capacity)
 
     new_mask1 = mask1 * torch.zeros_like(mask1).scatter_(0, top_idx, 1)
     mask1 = new_mask1
 
     if use_tutel:
@@ -254,15 +247,21 @@
         locations1 = tutel_moe.fast_cumsum_sub_one(mask1)
     else:
         locations1 = torch.cumsum(mask1, dim=0) - 1
 
     if use_tutel:
         gates1_s = (gates * mask1).sum(dim=1)
         locations1_s = torch.sum(locations1 * mask1, dim=1)
-        return l_aux, capacity, num_experts, [indices1_s,], [locations1_s,], [gates1_s,], exp_counts
+        return l_aux, capacity, num_experts, [
+            indices1_s,
+        ], [
+            locations1_s,
+        ], [
+            gates1_s,
+        ], exp_counts
 
     # Store the capacity location for each token
     locations1_s = torch.sum(locations1 * mask1, dim=1)
 
     # Normalize gate probabilities
     mask1_float = mask1.float()
     gates = gates * mask1_float
@@ -271,27 +270,20 @@
     combine_weights = einsum("se,sc->sec", gates, locations1_sc)
 
     dispatch_mask = combine_weights.bool()
 
     return l_aux, combine_weights, dispatch_mask, exp_counts
 
 
-def top2gating(logits: Tensor,
-               capacity_factor: float,
-               min_capacity: int) -> Tuple[Tensor,
-                                           Tensor,
-                                           Tensor,
-                                           Tensor]:
+def top2gating(logits: Tensor, capacity_factor: float, min_capacity: int) -> Tuple[Tensor, Tensor, Tensor, Tensor]:
     """Implements Top2Gating on logits."""
     # everything is in fp32 in this function
     gates = F.softmax(logits, dim=1)
 
-    capacity = _capacity(gates,
-                         torch.tensor(capacity_factor * 2),
-                         torch.tensor(min_capacity))
+    capacity = _capacity(gates, torch.tensor(capacity_factor * 2), torch.tensor(min_capacity))
 
     # Create a mask for 1st's expert per token
     indices1_s = torch.argmax(gates, dim=1)
     num_experts = int(gates.shape[1])
     mask1 = F.one_hot(indices1_s, num_classes=num_experts)
 
     # Create a mask for 2nd's expert per token using Gumbel-max trick
@@ -389,49 +381,38 @@
         self.noisy_gate_policy = noisy_gate_policy
         self.timers = SynchronizedWallClockTimer()
         self.wall_clock_breakdown = False
         self.gate_time = 0.0
         self.drop_tokens = drop_tokens
         self.use_rts = use_rts
 
-    def forward(
-            self,
-            input: torch.Tensor,
-            used_token: torch.Tensor = None,
-            use_tutel: bool = False) -> Tuple[Tensor,
-                                              Tensor,
-                                              Tensor]:  # type: ignore
+    def forward(self,
+                input: torch.Tensor,
+                used_token: torch.Tensor = None,
+                use_tutel: bool = False) -> Tuple[Tensor, Tensor, Tensor]:  # type: ignore
 
         if self.wall_clock_breakdown:
             self.timers('TopKGate').start()
 
         if self.wg.weight.dtype != torch.float32:
             self.wg = self.wg.float()
         input_fp32 = input.float()
         # input jittering
         if self.noisy_gate_policy == 'Jitter' and self.training:
             input_fp32 = multiplicative_jitter(input_fp32, device=input.device)
         logits = self.wg(input_fp32)
 
         if self.k == 1:
-            gate_output = top1gating(
-                logits,
-                self.capacity_factor if self.training else self.eval_capacity_factor,
-                self.min_capacity,
-                used_token,
-                self.noisy_gate_policy if self.training else None,
-                self.drop_tokens,
-                self.use_rts,
-                use_tutel)
+            gate_output = top1gating(logits, self.capacity_factor if self.training else self.eval_capacity_factor,
+                                     self.min_capacity, used_token, self.noisy_gate_policy if self.training else None,
+                                     self.drop_tokens, self.use_rts, use_tutel)
 
         else:
-            gate_output = top2gating(
-                logits,
-                self.capacity_factor if self.training else self.eval_capacity_factor,
-                self.min_capacity)
+            gate_output = top2gating(logits, self.capacity_factor if self.training else self.eval_capacity_factor,
+                                     self.min_capacity)
 
         if self.wall_clock_breakdown:
             self.timers('TopKGate').stop()
             self.gate_time = self.timers('TopKGate').elapsed(reset=False)
 
         return gate_output
 
@@ -449,14 +430,15 @@
 
     Args:
         gate (torch.nn.Module):
             gate network
         expert (torch.nn.Module):
             expert network
     """
+
     def __init__(self,
                  gate: Module,
                  experts: Module,
                  ep_group_name,
                  ep_size,
                  num_local_experts: int,
                  use_tutel: bool = False) -> None:
@@ -477,17 +459,16 @@
 
         if self.use_tutel:
             logger.info('Using Tutel optimizations.')
         elif use_tutel and not TUTEL_INSTALLED:
             logger.warning("Tutel optimization requested but not installed. "
                            "Proceeding without Tutel.")
         elif use_tutel and TUTEL_INSTALLED and gate.k != 1:
-            logger.warning(
-                "To enable Tutel optimization, use top-1 instead of top-2 gate. "
-                "Proceeding without Tutel.")
+            logger.warning("To enable Tutel optimization, use top-1 instead of top-2 gate. "
+                           "Proceeding without Tutel.")
 
     def _set_ep_group(self, ep_group):
         self.ep_group = ep_group
 
     def forward(self, *input: Tensor, **kwargs: Any) -> Tensor:
 
         if self.wall_clock_breakdown:
@@ -502,26 +483,20 @@
         reshaped_input = input[0].reshape(-1, d_model)
 
         if self.use_tutel:
             self.l_aux, C, E, indices_, locations_, gates_, self.exp_counts = self.gate(reshaped_input, input[1], True)
             S, M = reshaped_input.size(0), reshaped_input.size(1)
 
             if not hasattr(self, '_tutel_dispatcher'):
-                self._tutel_dispatcher = tutel_moe.fast_dispatcher(
-                    E,
-                    C,
-                    M,
-                    dispatch_dtype=reshaped_input.dtype)
+                self._tutel_dispatcher = tutel_moe.fast_dispatcher(E, C, M, dispatch_dtype=reshaped_input.dtype)
             self._tutel_dispatcher.update(indices_, locations_, gates_, capacity=C)
             dispatched_input = self._tutel_dispatcher.encode(reshaped_input)
         else:
             self.l_aux, combine_weights, dispatch_mask, self.exp_counts = self.gate(reshaped_input, input[1])
-            dispatched_input = einsum("sec,sm->ecm",
-                                      dispatch_mask.type_as(input[0]),
-                                      reshaped_input)
+            dispatched_input = einsum("sec,sm->ecm", dispatch_mask.type_as(input[0]), reshaped_input)
 
         if self.wall_clock_breakdown:
             self.timers('falltoall').start()
 
         if groups._get_expert_model_parallel_world_size() == 1:
             # If the non-expert is tensor-parallel, it will create
             # duplicate tokens on the tensor-parallel ranks.
@@ -534,47 +509,40 @@
         dispatched_input = _AllToAll.apply(self.ep_group, dispatched_input)
 
         if self.wall_clock_breakdown:
             self.timers('falltoall').stop()
             self.time_falltoall = self.timers('falltoall').elapsed(reset=False)
 
         # Re-shape after all-to-all: ecm -> gecm
-        dispatched_input = dispatched_input.reshape(self.ep_size,
-                                                    self.num_local_experts,
-                                                    -1,
-                                                    d_model)
+        dispatched_input = dispatched_input.reshape(self.ep_size, self.num_local_experts, -1, d_model)
 
         expert_output = self.experts(dispatched_input)
 
         if self.wall_clock_breakdown:
             self.timers('salltoall').start()
 
         expert_output = _AllToAll.apply(self.ep_group, expert_output)
 
         if self.wall_clock_breakdown:
             self.timers('salltoall').stop()
             self.time_salltoall = self.timers('salltoall').elapsed(reset=False)
 
         # Re-shape back: gecm -> ecm
-        expert_output = expert_output.reshape(self.ep_size * self.num_local_experts,
-                                              -1,
-                                              d_model)
+        expert_output = expert_output.reshape(self.ep_size * self.num_local_experts, -1, d_model)
 
         if groups._get_expert_model_parallel_world_size() == 1:
             # the dropped duplicate tokens need to be gathered on each
             # tensor parallel rank again for the tensor-parallel
             # non-expert of the next layer.
             expert_output = gather_tokens(expert_output, dim=1)
 
         if self.use_tutel:
             combined_output = self._tutel_dispatcher.decode(expert_output.view(E * C, M))
         else:
-            combined_output = einsum("sec,ecm->sm",
-                                     combine_weights.type_as(input[0]),
-                                     expert_output)
+            combined_output = einsum("sec,ecm->sm", combine_weights.type_as(input[0]), expert_output)
 
         a = combined_output.reshape(input[0].shape)
 
         if self.wall_clock_breakdown:
             self.timers('moe').stop()
             self.time_moe = self.timers('moe').elapsed(reset=False)
```

### Comparing `deepspeed-0.8.3/deepspeed/moe/utils.py` & `deepspeed-0.9.0/deepspeed/moe/utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,11 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 from typing import List, Tuple, Dict
 import torch
 from .layer import MoE
 
 
 def has_moe_layers(m):
@@ -20,28 +23,26 @@
 def is_moe_param(param: torch.Tensor) -> bool:
     if hasattr(param, "allreduce") and not param.allreduce:
         return True
     return False
 
 
 def split_params_into_shared_and_expert_params(
-        params: List[torch.nn.Parameter]) -> Tuple[torch.nn.Parameter,
-                                                   torch.nn.Parameter]:
+        params: List[torch.nn.Parameter]) -> Tuple[torch.nn.Parameter, torch.nn.Parameter]:
     shared_params, expert_params = [], []
     for p in params:
         if is_moe_param(p):
             expert_params.append(p)
         else:
             shared_params.append(p)
     return shared_params, expert_params
 
 
 def split_params_grads_into_shared_and_expert_params(
-        group: List[torch.nn.Parameter]) -> Tuple[torch.nn.Parameter,
-                                                  torch.nn.Parameter]:
+        group: List[torch.nn.Parameter]) -> Tuple[torch.nn.Parameter, torch.nn.Parameter]:
     """Split grad of parameters into grads of non-expert params
     and grads of expert params. This is useful while computing
     grad-norms for clipping and overflow detection
 
         group (List[torch.nn.Parameter]):
     Args:
             The group of parameters to split
@@ -58,16 +59,15 @@
                 expert_grads.append(p.grad.to(p.dtype))
             else:
                 shared_grads.append(p.grad.to(p.dtype))
     return shared_grads, expert_grads
 
 
 def split_params_into_different_moe_groups_for_optimizer(param_groups: Tuple[Dict],
-                                                         max_group_size=178956971
-                                                         ) -> Tuple[Dict]:
+                                                         max_group_size=178956971) -> Tuple[Dict]:
     """Split parameters into different MoE groups for optimizer
 
     Args:
         param_groups (Tuple[Dict]):
             The list of parameter groups to split
 
     Returns:
@@ -97,16 +97,15 @@
             group_moe[param_group['name']][key]['name'] = key
             group_moe[param_group['name']][key]['moe'] = True
             for ori_key in param_group.keys():
                 if ori_key != 'name':
                     if ori_key == 'params':
                         group_moe[param_group['name']][key][ori_key] = []
                     else:
-                        group_moe[
-                            param_group['name']][key][ori_key] = param_group[ori_key]
+                        group_moe[param_group['name']][key][ori_key] = param_group[ori_key]
     # Assign param
     for param_group in param_groups:
         new_params = []
         for param in param_group['params']:
             if is_moe_param(param):
                 group_moe[param_group['name']][param.group_name]['params'].append(param)
                 # param_group['params'].remove(param)
```

### Comparing `deepspeed-0.8.3/deepspeed/monitor/config.py` & `deepspeed-0.9.0/deepspeed/monitor/config.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,25 +1,18 @@
-'''Copyright The Microsoft DeepSpeed Team'''
-"""
-Copyright (c) Microsoft Corporation
-Licensed under the MIT license.
-"""
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 from pydantic import root_validator
 from deepspeed.runtime.config_utils import DeepSpeedConfigModel
 
 
 def get_monitor_config(param_dict):
-    monitor_dict = {
-        key: param_dict.get(key,
-                            {})
-        for key in ("tensorboard",
-                    "wandb",
-                    "csv_monitor")
-    }
+    monitor_dict = {key: param_dict.get(key, {}) for key in ("tensorboard", "wandb", "csv_monitor")}
     return DeepSpeedMonitorConfig(**monitor_dict)
 
 
 class TensorBoardConfig(DeepSpeedConfigModel):
     """Sets parameters for TensorBoard monitor."""
 
     enabled: bool = False
@@ -74,14 +67,14 @@
     """ TensorBoard monitor, requires `tensorboard` package is installed. """
 
     wandb: WandbConfig = {}
     """ WandB monitor, requires `wandb` package is installed. """
 
     csv_monitor: CSVConfig = {}
     """ Local CSV output of monitoring data. """
+
     @root_validator
     def check_enabled(cls, values):
         values["enabled"] = False
-        if (values.get("tensorboard").enabled or values.get("wandb").enabled
-                or values.get("csv_monitor").enabled):
+        if (values.get("tensorboard").enabled or values.get("wandb").enabled or values.get("csv_monitor").enabled):
             values["enabled"] = True
         return values
```

### Comparing `deepspeed-0.8.3/deepspeed/monitor/csv_monitor.py` & `deepspeed-0.9.0/deepspeed/monitor/csv_monitor.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,16 +1,20 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 from .monitor import Monitor
 import os
 
 import deepspeed.comm as dist
 
 
 class csvMonitor(Monitor):
+
     def __init__(self, csv_config):
         super().__init__(csv_config)
         self.filenames = []
         self.enabled = csv_config.enabled
         self.output_path = csv_config.output_path
         self.job_name = csv_config.job_name
         self.log_dir = self.setup_log_dir()
```

### Comparing `deepspeed-0.8.3/deepspeed/monitor/monitor.py` & `deepspeed-0.9.0/deepspeed/monitor/monitor.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,17 +1,21 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 """
- Support different forms of monitoring such as wandb and tensorboard
+Support different forms of monitoring such as wandb and tensorboard
 """
 
 from abc import ABC, abstractmethod
 import deepspeed.comm as dist
 
 
 class Monitor(ABC):
+
     @abstractmethod
     def __init__(self, monitor_config):
         self.monitor_config = monitor_config
 
     @abstractmethod
     def write_events(self, event_list):
         pass
@@ -19,14 +23,15 @@
 
 from .wandb import WandbMonitor
 from .tensorboard import TensorBoardMonitor
 from .csv_monitor import csvMonitor
 
 
 class MonitorMaster(Monitor):
+
     def __init__(self, monitor_config):
         super().__init__(monitor_config)
         self.tb_monitor = None
         self.wandb_monitor = None
         self.csv_monitor = None
         self.enabled = monitor_config.enabled
```

### Comparing `deepspeed-0.8.3/deepspeed/monitor/tensorboard.py` & `deepspeed-0.9.0/deepspeed/monitor/tensorboard.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,32 +1,34 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 from .utils import check_tb_availability
 from .monitor import Monitor
 import os
 
 import deepspeed.comm as dist
 
 
 class TensorBoardMonitor(Monitor):
+
     def __init__(self, tensorboard_config):
         super().__init__(tensorboard_config)
         check_tb_availability()
 
         self.summary_writer = None
         self.enabled = tensorboard_config.enabled
         self.output_path = tensorboard_config.output_path
         self.job_name = tensorboard_config.job_name
 
         if self.enabled and dist.get_rank() == 0:
             self.get_summary_writer()
 
-    def get_summary_writer(self,
-                           base=os.path.join(os.path.expanduser("~"),
-                                             "tensorboard")):
+    def get_summary_writer(self, base=os.path.join(os.path.expanduser("~"), "tensorboard")):
         if self.enabled and dist.get_rank() == 0:
             from torch.utils.tensorboard import SummaryWriter
             if self.output_path is not None:
                 log_dir = os.path.join(self.output_path, self.job_name)
             # NOTE: This code path currently is never used since the default output_path is an empty string and not None. Saving it in case we want this functionality in the future.
             else:
                 if "DLWS_JOB_ID" in os.environ:
```

### Comparing `deepspeed-0.8.3/deepspeed/monitor/utils.py` & `deepspeed-0.9.0/deepspeed/monitor/utils.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,8 +1,11 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 
 def check_tb_availability():
     try:
         # torch.utils.tensorboard will fail if `tensorboard` is not available,
         # see their docs for more details: https://pytorch.org/docs/1.8.0/tensorboard.html
         import tensorboard  # noqa: F401
```

### Comparing `deepspeed-0.8.3/deepspeed/monitor/wandb.py` & `deepspeed-0.9.0/deepspeed/monitor/wandb.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,16 +1,20 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 from .utils import check_wandb_availability
 from .monitor import Monitor
 
 import deepspeed.comm as dist
 
 
 class WandbMonitor(Monitor):
+
     def __init__(self, wandb_config):
         super().__init__(wandb_config)
         check_wandb_availability()
         import wandb
 
         self.enabled = wandb_config.enabled
         self.group = wandb_config.group
```

### Comparing `deepspeed-0.8.3/deepspeed/nebula/config.py` & `deepspeed-0.9.0/deepspeed/nebula/config.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,18 +1,18 @@
-'''Copyright The Microsoft DeepSpeed Team'''
-"""
-Copyright (c) Microsoft Corporation
-Licensed under the MIT license.
-"""
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 from deepspeed.runtime.config_utils import get_scalar_param, DeepSpeedConfigObject
 from deepspeed.nebula.constants import *
 
 
 class DeepSpeedNebulaConfig(DeepSpeedConfigObject):
+
     def __init__(self, param_dict):
         super(DeepSpeedNebulaConfig, self).__init__()
 
         self.enabled = None
         self.persistent_storage_path = None
         self.persistent_time_interval = None
         self.num_of_version_in_retention = None
@@ -22,33 +22,22 @@
             nebula_dict = param_dict[NEBULA]
         else:
             nebula_dict = {}
 
         self._initialize(nebula_dict)
 
     def _initialize(self, nebula_dict):
-        self.enabled = get_scalar_param(nebula_dict,
-                                        NEBULA_ENABLED,
-                                        NEBULA_ENABLED_DEFAULT)
-
-        self.load_path = get_scalar_param(nebula_dict,
-                                          NEBULA_LOAD_PATH,
-                                          NEBULA_LOAD_PATH_DEFAULT)
+        self.enabled = get_scalar_param(nebula_dict, NEBULA_ENABLED, NEBULA_ENABLED_DEFAULT)
+
+        self.load_path = get_scalar_param(nebula_dict, NEBULA_LOAD_PATH, NEBULA_LOAD_PATH_DEFAULT)
 
-        self.enable_nebula_load = get_scalar_param(nebula_dict,
-                                                   NEBULA_ENABLE_NEBULA_LOAD,
+        self.enable_nebula_load = get_scalar_param(nebula_dict, NEBULA_ENABLE_NEBULA_LOAD,
                                                    NEBULA_ENABLE_NEBULA_LOAD_DEFAULT)
 
-        self.persistent_storage_path = get_scalar_param(
-            nebula_dict,
-            NEBULA_PERSISTENT_STORAGE_PATH,
-            NEBULA_PERSISTENT_STORAGE_PATH_DEFAULT)
-
-        self.persistent_time_interval = get_scalar_param(
-            nebula_dict,
-            NEBULA_PERSISTENT_TIME_INTERVAL,
-            NEBULA_PERSISTENT_TIME_INTERVAL_DEFAULT)
-
-        self.num_of_version_in_retention = get_scalar_param(
-            nebula_dict,
-            NEBULA_NUM_OF_VERSION_IN_RETENTION,
-            NEBULA_NUM_OF_VERSION_IN_RETENTION_DEFAULT)
+        self.persistent_storage_path = get_scalar_param(nebula_dict, NEBULA_PERSISTENT_STORAGE_PATH,
+                                                        NEBULA_PERSISTENT_STORAGE_PATH_DEFAULT)
+
+        self.persistent_time_interval = get_scalar_param(nebula_dict, NEBULA_PERSISTENT_TIME_INTERVAL,
+                                                         NEBULA_PERSISTENT_TIME_INTERVAL_DEFAULT)
+
+        self.num_of_version_in_retention = get_scalar_param(nebula_dict, NEBULA_NUM_OF_VERSION_IN_RETENTION,
+                                                            NEBULA_NUM_OF_VERSION_IN_RETENTION_DEFAULT)
```

### Comparing `deepspeed-0.8.3/deepspeed/nebula/constants.py` & `deepspeed-0.9.0/deepspeed/nebula/constants.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,12 +1,11 @@
-'''Copyright The Microsoft DeepSpeed Team'''
-"""
-Copyright (c) Microsoft Corporation
-Licensed under the MIT license.
-"""
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 #########################################
 # nebula
 #########################################
 # Nebula. By default, this feature is not enabled.
 # Users can configure in ds_config.json as below example:
 NEBULA_FORMAT = '''
@@ -59,29 +58,16 @@
 # now. When it comes to checkpoint 3, the 1 will be removed if
 # 1 has been persisted to disk.
 NEBULA_NUM_OF_VERSION_IN_RETENTION = "num_of_version_in_retention"
 NEBULA_NUM_OF_VERSION_IN_RETENTION_DEFAULT = 2
 
 # Neubla envs
 NEBULA_EXPORT_ENVS = [
-    'DLTS_JOB_ID',
-    'DLTS_NUM_WORKER',
-    'NEBULA_PERSISTENT_STORAGE_PATH',
-    'NEBULA_PERSISTENT_TIME_INTERVAL',
-    'AML_RUN_ID',
-    'AZUREML_RUN_TOKEN',
-    'AZUREML_WORKSPACE_SCOPE',
-    'AZUREML_EXPERIMENT_SCOPE',
-    'AZUREML_RUN_HISTORY_SERVICE_ENDPOINT',
-    'AZUREML_RUN_ID',
-    'NEBULA_MEMORY_BUFFER_SIZE',
-    'AZUREML_PARAMETER_ITPJOB_NAME',
-    'FC_TASKROLE_NAME',
-    'FC_TASK_INDEX',
-    'MASTER_HOST',
-    'LOCAL_HOST',
-    'AZUREML_BLOB_ACCOUNT_NAME',
-    'AZUREML_BLOB_ACCOUNT_KEY'
+    'DLTS_JOB_ID', 'DLTS_NUM_WORKER', 'NEBULA_PERSISTENT_STORAGE_PATH', 'NEBULA_PERSISTENT_TIME_INTERVAL',
+    'AML_RUN_ID', 'AZUREML_RUN_TOKEN', 'AZUREML_WORKSPACE_SCOPE', 'AZUREML_EXPERIMENT_SCOPE',
+    'AZUREML_RUN_HISTORY_SERVICE_ENDPOINT', 'AZUREML_RUN_ID', 'NEBULA_MEMORY_BUFFER_SIZE',
+    'AZUREML_PARAMETER_ITPJOB_NAME', 'FC_TASKROLE_NAME', 'FC_TASK_INDEX', 'MASTER_HOST', 'LOCAL_HOST',
+    'AZUREML_BLOB_ACCOUNT_NAME', 'AZUREML_BLOB_ACCOUNT_KEY'
 ]
 
 # ITP env files
 DLTS_POD_ENV_PATH = '/dlts-runtime/env/pod.env'
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/adagrad/cpu_adagrad.py` & `deepspeed-0.9.0/deepspeed/ops/adagrad/cpu_adagrad.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,40 +1,31 @@
-'''
-Copyright 2020 The Microsoft DeepSpeed Team
-'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import torch
 from deepspeed.ops.op_builder import CPUAdagradBuilder
 from deepspeed.utils.logging import should_log_le
 
 
 class DeepSpeedCPUAdagrad(torch.optim.Optimizer):
     optimizer_id = 0
 
-    def __init__(self,
-                 model_params,
-                 lr=1e-2,
-                 eps=1e-10,
-                 weight_decay=0,
-                 amsgrad=False,
-                 fp32_optimizer_states=True):
+    def __init__(self, model_params, lr=1e-2, eps=1e-10, weight_decay=0, amsgrad=False, fp32_optimizer_states=True):
 
         default_args = dict(lr=lr, eps=eps, weight_decay=weight_decay, amsgrad=amsgrad)
         super(DeepSpeedCPUAdagrad, self).__init__(model_params, default_args)
 
         self.opt_id = DeepSpeedCPUAdagrad.optimizer_id
         DeepSpeedCPUAdagrad.optimizer_id = DeepSpeedCPUAdagrad.optimizer_id + 1
         self.fp32_optimizer_states = fp32_optimizer_states
         self.ds_opt_adagrad = CPUAdagradBuilder().load()
 
-        self.ds_opt_adagrad.create_adagrad(self.opt_id,
-                                           lr,
-                                           eps,
-                                           weight_decay,
-                                           should_log_le("info"))
+        self.ds_opt_adagrad.create_adagrad(self.opt_id, lr, eps, weight_decay, should_log_le("info"))
 
     def __del__(self):
         # need to destroy the C++ object explicitly to avoid a memory leak when deepspeed.initialize
         # is used multiple times in the same process (notebook or pytest worker)
         self.ds_opt_adagrad.destroy_adagrad(self.opt_id)
 
     def __setstate__(self, state):
@@ -86,53 +77,33 @@
                     state['step'] = 0
 
                     #use full precision by default unless self.fp32_optimizer_states is off
                     state_dtype = torch.float if self.fp32_optimizer_states else p.dtype
 
                     #memory_format=torch.preserve_format)
                     # gradient variances
-                    state['exp_avg_sq'] = torch.zeros_like(p.data,
-                                                           dtype=state_dtype,
-                                                           device='cpu')
+                    state['exp_avg_sq'] = torch.zeros_like(p.data, dtype=state_dtype, device='cpu')
                     #memory_format=torch.preserve_format)
 
                 state['step'] += 1
 
                 if p.grad.is_sparse == True:
                     sparse_param = p.sparse_mask(p.grad)
                     sparse_exp_avg_sq = state['exp_avg_sq'].sparse_mask(p.grad)
-                    self.ds_opt_adagrad.adagrad_update(self.opt_id,
-                                                       state['step'],
-                                                       group['lr'],
-                                                       group['eps'],
-                                                       group['weight_decay'],
-                                                       sparse_param.values(),
-                                                       p.grad.values(),
+                    self.ds_opt_adagrad.adagrad_update(self.opt_id, state['step'], group['lr'], group['eps'],
+                                                       group['weight_decay'], sparse_param.values(), p.grad.values(),
                                                        sparse_exp_avg_sq.values())
                     p[sparse_param.indices()] = sparse_param.values()
-                    state['exp_avg_sq'][
-                        sparse_exp_avg_sq.indices()] = sparse_exp_avg_sq.values()
+                    state['exp_avg_sq'][sparse_exp_avg_sq.indices()] = sparse_exp_avg_sq.values()
                     if fp16_param_groups is not None:
-                        fp16_param_groups[group_id][param_id][
-                            sparse_param.indices()] = sparse_param.values()
+                        fp16_param_groups[group_id][param_id][sparse_param.indices()] = sparse_param.values()
                 else:
                     if fp16_param_groups is not None:
-                        self.ds_opt_adagrad.adagrad_update_copy(
-                            self.opt_id,
-                            state['step'],
-                            group['lr'],
-                            group['eps'],
-                            group['weight_decay'],
-                            p.data,
-                            p.grad.data,
-                            state['exp_avg_sq'],
-                            fp16_param_groups[group_id][param_id].data)
+                        self.ds_opt_adagrad.adagrad_update_copy(self.opt_id, state['step'], group['lr'], group['eps'],
+                                                                group['weight_decay'], p.data, p.grad.data,
+                                                                state['exp_avg_sq'],
+                                                                fp16_param_groups[group_id][param_id].data)
                     else:
-                        self.ds_opt_adagrad.adagrad_update(self.opt_id,
-                                                           state['step'],
-                                                           group['lr'],
-                                                           group['eps'],
-                                                           group['weight_decay'],
-                                                           p.data,
-                                                           p.grad.data,
+                        self.ds_opt_adagrad.adagrad_update(self.opt_id, state['step'], group['lr'], group['eps'],
+                                                           group['weight_decay'], p.data, p.grad.data,
                                                            state['exp_avg_sq'])
         return loss
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/adam/cpu_adam.py` & `deepspeed-0.9.0/deepspeed/ops/adam/cpu_adam.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-'''
-Copyright 2020 The Microsoft DeepSpeed Team
-'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import torch
 from cpuinfo import get_cpu_info
 from deepspeed.utils import logger
 from deepspeed.utils.logging import should_log_le
 from deepspeed.ops.op_builder import CPUAdamBuilder
 
@@ -12,16 +13,15 @@
 class DeepSpeedCPUAdam(torch.optim.Optimizer):
     optimizer_id = 0
 
     def __init__(self,
                  model_params,
                  lr=1e-3,
                  bias_correction=True,
-                 betas=(0.9,
-                        0.999),
+                 betas=(0.9, 0.999),
                  eps=1e-8,
                  weight_decay=0,
                  amsgrad=False,
                  adamw_mode=True,
                  fp32_optimizer_states=True):
         """Fast vectorized implementation of two variations of Adam optimizer on CPU:
 
@@ -72,40 +72,32 @@
                             eps=eps,
                             weight_decay=weight_decay,
                             bias_correction=bias_correction,
                             amsgrad=amsgrad)
         super(DeepSpeedCPUAdam, self).__init__(model_params, default_args)
 
         cpu_info = get_cpu_info()
-        self.cpu_vendor = cpu_info["vendor_id_raw"].lower(
-        ) if "vendor_id_raw" in cpu_info else "unknown"
+        self.cpu_vendor = cpu_info["vendor_id_raw"].lower() if "vendor_id_raw" in cpu_info else "unknown"
         if "amd" in self.cpu_vendor:
             for group_id, group in enumerate(self.param_groups):
                 for param_id, p in enumerate(group['params']):
                     if p.dtype == torch.half:
-                        logger.warning(
-                            "FP16 params for CPUAdam may not work on AMD CPUs")
+                        logger.warning("FP16 params for CPUAdam may not work on AMD CPUs")
                         break
                 else:
                     continue
                 break
 
         self.opt_id = DeepSpeedCPUAdam.optimizer_id
         DeepSpeedCPUAdam.optimizer_id = DeepSpeedCPUAdam.optimizer_id + 1
         self.adam_w_mode = adamw_mode
         self.fp32_optimizer_states = fp32_optimizer_states
         self.ds_opt_adam = CPUAdamBuilder().load()
 
-        self.ds_opt_adam.create_adam(self.opt_id,
-                                     lr,
-                                     betas[0],
-                                     betas[1],
-                                     eps,
-                                     weight_decay,
-                                     adamw_mode,
+        self.ds_opt_adam.create_adam(self.opt_id, lr, betas[0], betas[1], eps, weight_decay, adamw_mode,
                                      should_log_le("info"))
 
     def __del__(self):
         # need to destroy the C++ object explicitly to avoid a memory leak when deepspeed.initialize
         # is used multiple times in the same process (notebook or pytest worker)
         self.ds_opt_adam.destroy_adam(self.opt_id)
 
@@ -164,49 +156,26 @@
                     #print(f'group {group_id} param {param_id} = {p.numel()}')
                     state['step'] = 0
 
                     #use full precision by default unless self.fp32_optimizer_states is off
                     state_dtype = torch.float if self.fp32_optimizer_states else p.dtype
 
                     # gradient momentums
-                    state['exp_avg'] = torch.zeros_like(p.data,
-                                                        dtype=state_dtype,
-                                                        device=device)
+                    state['exp_avg'] = torch.zeros_like(p.data, dtype=state_dtype, device=device)
                     #memory_format=torch.preserve_format)
                     # gradient variances
-                    state['exp_avg_sq'] = torch.zeros_like(p.data,
-                                                           dtype=state_dtype,
-                                                           device=device)
+                    state['exp_avg_sq'] = torch.zeros_like(p.data, dtype=state_dtype, device=device)
                     #memory_format=torch.preserve_format)
 
                 state['step'] += 1
                 beta1, beta2 = group['betas']
 
                 if fp16_param_groups is not None:
-                    self.ds_opt_adam.adam_update_copy(
-                        self.opt_id,
-                        state['step'],
-                        group['lr'],
-                        beta1,
-                        beta2,
-                        group['eps'],
-                        group['weight_decay'],
-                        group['bias_correction'],
-                        p.data,
-                        p.grad.data,
-                        state['exp_avg'],
-                        state['exp_avg_sq'],
-                        fp16_param_groups[group_id][param_id].data)
+                    self.ds_opt_adam.adam_update_copy(self.opt_id, state['step'], group['lr'], beta1, beta2,
+                                                      group['eps'], group['weight_decay'], group['bias_correction'],
+                                                      p.data, p.grad.data, state['exp_avg'], state['exp_avg_sq'],
+                                                      fp16_param_groups[group_id][param_id].data)
                 else:
-                    self.ds_opt_adam.adam_update(self.opt_id,
-                                                 state['step'],
-                                                 group['lr'],
-                                                 beta1,
-                                                 beta2,
-                                                 group['eps'],
-                                                 group['weight_decay'],
-                                                 group['bias_correction'],
-                                                 p.data,
-                                                 p.grad.data,
-                                                 state['exp_avg'],
-                                                 state['exp_avg_sq'])
+                    self.ds_opt_adam.adam_update(self.opt_id, state['step'], group['lr'], beta1, beta2, group['eps'],
+                                                 group['weight_decay'], group['bias_correction'], p.data, p.grad.data,
+                                                 state['exp_avg'], state['exp_avg_sq'])
         return loss
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/adam/fused_adam.py` & `deepspeed-0.9.0/deepspeed/ops/adam/fused_adam.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,13 +1,15 @@
-'''
-Copyright 2020 The Microsoft DeepSpeed Team
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
 
+# DeepSpeed Team
+"""
 Copyright NVIDIA/apex
 This file is adapted from fused adam in NVIDIA/apex, commit a109f85
-'''
+"""
 
 import torch
 from .multi_tensor_apply import MultiTensorApply
 
 multi_tensor_applier = MultiTensorApply(2048 * 32)
 from deepspeed.accelerator import get_accelerator
 from deepspeed.ops.op_builder import FusedAdamBuilder
@@ -43,33 +45,29 @@
             method is called. (default: True)
 
     .. _Adam - A Method for Stochastic Optimization:
         https://arxiv.org/abs/1412.6980
     .. _On the Convergence of Adam and Beyond:
         https://openreview.net/forum?id=ryQu7f-RZ
     """
+
     def __init__(self,
                  params,
                  lr=1e-3,
                  bias_correction=True,
-                 betas=(0.9,
-                        0.999),
+                 betas=(0.9, 0.999),
                  eps=1e-8,
                  adam_w_mode=True,
                  weight_decay=0.,
                  amsgrad=False,
                  set_grad_none=True):
 
         if amsgrad:
             raise RuntimeError('FusedAdam does not support the AMSGrad variant.')
-        defaults = dict(lr=lr,
-                        bias_correction=bias_correction,
-                        betas=betas,
-                        eps=eps,
-                        weight_decay=weight_decay)
+        defaults = dict(lr=lr, bias_correction=bias_correction, betas=betas, eps=eps, weight_decay=weight_decay)
         super(FusedAdam, self).__init__(params, defaults)
         self.adam_w_mode = 1 if adam_w_mode else 0
         self.set_grad_none = set_grad_none
 
         fused_adam_cuda = FusedAdamBuilder().load()
         # Skip buffer
         self._dummy_overflow_buf = get_accelerator().IntTensor([0])
@@ -79,20 +77,15 @@
         if self.set_grad_none:
             for group in self.param_groups:
                 for p in group['params']:
                     p.grad = None
         else:
             super(FusedAdam, self).zero_grad()
 
-    def step(self,
-             closure=None,
-             grads=None,
-             output_params=None,
-             scale=None,
-             grad_norms=None):
+    def step(self, closure=None, grads=None, output_params=None, scale=None, grad_norms=None):
         """Performs a single optimization step.
 
         Arguments:
             closure (callable, optional): A closure that reevaluates the model
                 and returns the loss.
 
         The remaining arguments are deprecated, and are only retained (for the moment) for error-checking purposes.
@@ -117,16 +110,15 @@
             g_32, p_32, m_32, v_32 = [], [], [], []
 
             for p in group['params']:
                 if p.grad is None:
                     continue
                 if p.grad.data.is_sparse:
                     raise RuntimeError(
-                        'FusedAdam does not support sparse gradients, please consider SparseAdam instead'
-                    )
+                        'FusedAdam does not support sparse gradients, please consider SparseAdam instead')
 
                 state = self.state[p]
                 # State initialization
                 if len(state) == 0:
                     # DeepSpeed ZeRO 3 processes each subgroup a time, so we need to keep tracking step count for each tensor separately.
                     # While this is not an issue for ZeRO 1 & 2, since they apply a single optimizatin step to the whole param group at the same time.
                     # In order to keep backward compatibility for the existing checkpoints, we use group['state'] to initialize state['step'] if it exists.
@@ -147,39 +139,17 @@
                     m_32.append(state['exp_avg'])
                     v_32.append(state['exp_avg_sq'])
                 else:
                     raise RuntimeError('FusedAdam only support fp16 and fp32.')
 
             if (len(g_16) > 0):
                 state['step'] += 1
-                multi_tensor_applier(self.multi_tensor_adam,
-                                     self._dummy_overflow_buf,
-                                     [g_16,
-                                      p_16,
-                                      m_16,
-                                      v_16],
-                                     group['lr'],
-                                     beta1,
-                                     beta2,
-                                     group['eps'],
-                                     state['step'],
-                                     self.adam_w_mode,
-                                     bias_correction,
-                                     group['weight_decay'])
+                multi_tensor_applier(self.multi_tensor_adam, self._dummy_overflow_buf, [g_16, p_16, m_16, v_16],
+                                     group['lr'], beta1, beta2, group['eps'], state['step'], self.adam_w_mode,
+                                     bias_correction, group['weight_decay'])
             if (len(g_32) > 0):
                 state['step'] += 1
-                multi_tensor_applier(self.multi_tensor_adam,
-                                     self._dummy_overflow_buf,
-                                     [g_32,
-                                      p_32,
-                                      m_32,
-                                      v_32],
-                                     group['lr'],
-                                     beta1,
-                                     beta2,
-                                     group['eps'],
-                                     state['step'],
-                                     self.adam_w_mode,
-                                     bias_correction,
-                                     group['weight_decay'])
+                multi_tensor_applier(self.multi_tensor_adam, self._dummy_overflow_buf, [g_32, p_32, m_32, v_32],
+                                     group['lr'], beta1, beta2, group['eps'], state['step'], self.adam_w_mode,
+                                     bias_correction, group['weight_decay'])
 
         return loss
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/adagrad/cpu_adagrad.cpp` & `deepspeed-0.9.0/deepspeed/ops/csrc/adagrad/cpu_adagrad.cpp`

 * *Files 2% similar despite different names*

```diff
@@ -1,7 +1,12 @@
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
 #include "cpu_adagrad.h"
 #include <torch/extension.h>
 #include <iostream>
 #include <memory>
 #include <type_traits>
 #include <unordered_map>
 #if defined(__ENABLE_CUDA__)
@@ -169,15 +174,15 @@
     float* grads_ptr = (float*)grads_c.data_ptr();
     float* exp_avg_sq_ptr = (float*)exp_avg_sq_c.data_ptr();
 
     std::shared_ptr<Adagrad_Optimizer> opt =
         std::static_pointer_cast<Adagrad_Optimizer>(s_optimizers[optimizer_id]);
     opt->IncrementStep(step);
     opt->update_state(lr, epsilon, weight_decay);
-    opt->Step_8(params_ptr, grads_ptr, exp_avg_sq_ptr, params_c.size(0));
+    opt->Step_8(params_ptr, grads_ptr, exp_avg_sq_ptr, params_c.numel());
 
 #if defined(__ENABLE_CUDA__)
     opt->SynchronizeStreams();
 #endif
     return 0;
 }
 
@@ -205,15 +210,15 @@
     std::shared_ptr<Adagrad_Optimizer> opt =
         std::static_pointer_cast<Adagrad_Optimizer>(s_optimizers[optimizer_id]);
     opt->IncrementStep(step);
     opt->update_state(lr, epsilon, weight_decay);
     opt->Step_8(params_ptr,
                 grads_ptr,
                 exp_avg_sq_ptr,
-                params_c.size(0),
+                params_c.numel(),
                 gpu_params_ptr,
                 (params.options().dtype() == at::kHalf));
 
     opt->SynchronizeStreams();
 #else
     assert(false);
 #endif
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/adam/cpu_adam.cpp` & `deepspeed-0.9.0/deepspeed/ops/csrc/adam/cpu_adam.cpp`

 * *Files 1% similar despite different names*

```diff
@@ -1,7 +1,12 @@
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
 #include "cpu_adam.h"
 #include <torch/extension.h>
 #include <cassert>
 #include <iostream>
 #include <memory>
 #include <type_traits>
 #include <unordered_map>
@@ -226,15 +231,15 @@
     opt->IncrementStep(step, beta1, beta2);
     opt->update_state(lr, epsilon, weight_decay, bias_correction);
 
     opt->Step_8(params_ptr,
                 grads_ptr,
                 exp_avg_ptr,
                 exp_avg_sq_ptr,
-                params_c.size(0),
+                params_c.numel(),
                 nullptr,
                 (params.options().dtype() == at::kHalf));
 
 #if defined(__ENABLE_CUDA__)
     opt->SynchronizeStreams();
 #endif
     return 0;
@@ -271,15 +276,15 @@
         std::static_pointer_cast<Adam_Optimizer>(s_optimizers[optimizer_id]);
     opt->IncrementStep(step, beta1, beta2);
     opt->update_state(lr, epsilon, weight_decay, bias_correction);
     opt->Step_8(params_ptr,
                 grads_ptr,
                 exp_avg_ptr,
                 exp_avg_sq_ptr,
-                params_c.size(0),
+                params_c.numel(),
                 gpu_params_ptr,
                 (params.options().dtype() == at::kHalf));
 
     opt->SynchronizeStreams();
 #else
     assert(false);
 #endif
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/adam/multi_tensor_adam.cu` & `deepspeed-0.9.0/deepspeed/ops/csrc/adam/multi_tensor_adam.cu`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,15 @@
-/* Copyright 2020 The Microsoft DeepSpeed Team
-   Copyright NVIDIA/apex
-   This file is adapted from fused adam in NVIDIA/apex, commit a109f85
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
+/*
+Copyright NVIDIA/apex
+This file is adapted from fused adam in NVIDIA/apex, commit a109f85
 */
 
 #include <ATen/ATen.h>
 #include <ATen/AccumulateType.h>
 #include <ATen/cuda/CUDAContext.h>
 #include <ATen/cuda/Exceptions.h>
 // Another possibility:
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/adam/multi_tensor_apply.cuh` & `deepspeed-0.9.0/deepspeed/ops/csrc/adam/multi_tensor_apply.cuh`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,15 @@
-/* Copyright 2020 The Microsoft DeepSpeed Team
-   Copyright NVIDIA/apex
-   This file is adapted from fused adam in NVIDIA/apex, commit a109f85
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
+/*
+Copyright NVIDIA/apex
+This file is adapted from fused adam in NVIDIA/apex, commit a109f85
 */
 
 #include <ATen/ATen.h>
 #include <ATen/AccumulateType.h>
 #include <ATen/cuda/CUDAContext.h>
 #include <ATen/cuda/Exceptions.h>
 #include <c10/cuda/CUDAGuard.h>
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/aio/common/deepspeed_aio_common.cpp` & `deepspeed-0.9.0/deepspeed/ops/csrc/aio/common/deepspeed_aio_common.cpp`

 * *Files 2% similar despite different names*

```diff
@@ -1,11 +1,13 @@
-/*
-Copyright 2020 The Microsoft DeepSpeed Team
-Licensed under the MIT license.
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
+/*
 Functionality for swapping optimizer tensors to/from (NVMe) storage devices.
 */
 
 #include <assert.h>
 #include <stdlib.h>
 #include <string.h>
 
@@ -258,15 +260,15 @@
     std::string err_msg = file_op + std::string(" failed on ") + std::string(filename) +
                           " error = " + std::to_string(error_code);
     std::cerr << c_library_name << ":  " << err_msg << std::endl;
 }
 
 int open_file(const char* filename, const bool read_op)
 {
-    const int flags = read_op ? (O_RDONLY | __O_DIRECT) : (O_WRONLY | O_CREAT | __O_DIRECT);
+    const int flags = read_op ? (O_RDONLY | O_DIRECT) : (O_WRONLY | O_CREAT | O_DIRECT);
     const int mode = 0600;
     const auto fd = open(filename, flags, mode);
     if (fd == -1) {
         const auto error_code = errno;
         const auto error_msg = read_op ? " open for read " : " open for write ";
         report_file_error(filename, error_msg, error_code);
         return -1;
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/aio/common/deepspeed_aio_common.h` & `deepspeed-0.9.0/deepspeed/ops/csrc/aio/common/deepspeed_aio_common.h`

 * *Files 10% similar despite different names*

```diff
@@ -1,11 +1,13 @@
-/*
-Copyright 2020 The Microsoft DeepSpeed Team
-Licensed under the MIT license.
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
+/*
 Functionality for swapping optimizer tensors to/from (NVMe) storage devices.
 */
 
 #include <deepspeed_aio_utils.h>
 #include <stdlib.h>
 #include <memory>
 #include <string>
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/aio/common/deepspeed_aio_types.cpp` & `deepspeed-0.9.0/deepspeed/ops/csrc/aio/common/deepspeed_aio_types.cpp`

 * *Files 21% similar despite different names*

```diff
@@ -1,11 +1,13 @@
-/*
-Copyright 2020 The Microsoft DeepSpeed Team
-Licensed under the MIT license.
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
+/*
 Functionality for swapping optimizer tensors to/from (NVMe) storage devices.
 */
 
 #include <cmath>
 
 #include "deepspeed_aio_utils.h"
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/aio/common/deepspeed_aio_types.h` & `deepspeed-0.9.0/deepspeed/ops/csrc/aio/common/deepspeed_aio_types.h`

 * *Files 13% similar despite different names*

```diff
@@ -1,11 +1,13 @@
-/*
-Copyright 2020 The Microsoft DeepSpeed Team
-Licensed under the MIT license.
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
+/*
 Functionality for swapping optimizer tensors to/from (NVMe) storage devices.
 */
 
 #include <libaio.h>
 #include <stdlib.h>
 
 #include <string>
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/aio/common/deepspeed_aio_utils.cpp` & `deepspeed-0.9.0/deepspeed/ops/csrc/aio/common/deepspeed_aio_utils.cpp`

 * *Files 6% similar despite different names*

```diff
@@ -1,11 +1,13 @@
-/*
-Copyright 2020 The Microsoft DeepSpeed Team
-Licensed under the MIT license.
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
+/*
 Functionality for swapping optimizer tensors to/from (NVMe) storage devices.
 */
 
 #include <cmath>
 #include <iostream>
 
 #include "deepspeed_aio_utils.h"
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/aio/common/deepspeed_aio_utils.h` & `deepspeed-0.9.0/deepspeed/ops/csrc/aio/common/deepspeed_aio_utils.h`

 * *Files 8% similar despite different names*

```diff
@@ -1,11 +1,13 @@
-/*
-Copyright 2020 The Microsoft DeepSpeed Team
-Licensed under the MIT license.
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
+/*
 Functionality for swapping optimizer tensors to/from (NVMe) storage devices.
 */
 
 #pragma once
 
 #include <assert.h>
 #include <stdlib.h>
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/aio/py_lib/deepspeed_aio_thread.cpp` & `deepspeed-0.9.0/deepspeed/ops/csrc/aio/py_lib/deepspeed_aio_thread.cpp`

 * *Files 6% similar despite different names*

```diff
@@ -1,11 +1,13 @@
-/*
-Copyright 2020 The Microsoft DeepSpeed Team
-Licensed under the MIT license.
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
+/*
 Functionality for swapping optimizer tensors to/from (NVMe) storage devices.
 */
 
 #include "deepspeed_aio_thread.h"
 
 using namespace std;
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/aio/py_lib/deepspeed_aio_thread.h` & `deepspeed-0.9.0/deepspeed/ops/csrc/aio/py_lib/deepspeed_aio_thread.h`

 * *Files 11% similar despite different names*

```diff
@@ -1,11 +1,13 @@
-/*
-Copyright 2020 The Microsoft DeepSpeed Team
-Licensed under the MIT license.
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
+/*
 Functionality for swapping optimizer tensors to/from (NVMe) storage devices.
 */
 
 #include <condition_variable>
 #include <memory>
 #include <queue>
 #include "deepspeed_py_aio.h"
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/aio/py_lib/deepspeed_pin_tensor.cpp` & `deepspeed-0.9.0/deepspeed/ops/csrc/aio/py_lib/deepspeed_pin_tensor.cpp`

 * *Files 9% similar despite different names*

```diff
@@ -1,11 +1,13 @@
-/*
-Copyright 2023 The Microsoft DeepSpeed Team
-Licensed under the MIT license.
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
+/*
 Functionality for managing CPU tensors occupying page-locked memory.
 */
 
 #include "deepspeed_pin_tensor.h"
 
 using namespace std;
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/aio/py_lib/deepspeed_pin_tensor.h` & `deepspeed-0.9.0/deepspeed/ops/csrc/aio/py_lib/deepspeed_pin_tensor.h`

 * *Files 20% similar despite different names*

```diff
@@ -1,15 +1,18 @@
-/*
-Copyright 2023 The Microsoft DeepSpeed Team
-Licensed under the MIT license.
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
+/*
 Functionality for managing CPU tensors occupying page-locked memory.
 TODO: Implement a full-featured manager that
- 1. Avoid page-locked memory leaks
- 2. Minimize page-locked memory usage by reducing internal fragmentation
+1. Avoid page-locked memory leaks
+2. Minimize page-locked memory usage by reducing internal fragmentation
+Functionality for managing CPU tensors occupying page-locked memory.
 */
 
 #include <map>
 #include "deepspeed_py_aio.h"
 
 struct deepspeed_pin_tensor_t {
     std::map<void*, size_t> _locked_tensors;
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/aio/py_lib/deepspeed_py_aio.cpp` & `deepspeed-0.9.0/deepspeed/ops/csrc/aio/py_lib/deepspeed_py_aio.cpp`

 * *Files 2% similar despite different names*

```diff
@@ -1,7 +1,11 @@
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 /*
 Copyright 2020 The Microsoft DeepSpeed Team
 Licensed under the MIT license.
 
 Functionality for swapping optimizer tensors to/from (NVMe) storage devices.
 */
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/aio/py_lib/deepspeed_py_aio.h` & `deepspeed-0.9.0/deepspeed/ops/csrc/aio/py_lib/deepspeed_py_aio.h`

 * *Files 20% similar despite different names*

```diff
@@ -1,7 +1,11 @@
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 /*
 Copyright 2020 The Microsoft DeepSpeed Team
 Licensed under the MIT license.
 
 Functionality for swapping optimizer tensors to/from (NVMe) storage devices.
 */
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/aio/py_lib/deepspeed_py_aio_handle.cpp` & `deepspeed-0.9.0/deepspeed/ops/csrc/aio/py_lib/deepspeed_py_aio_handle.cpp`

 * *Files 1% similar despite different names*

```diff
@@ -1,7 +1,11 @@
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 /*
 Copyright 2020 The Microsoft DeepSpeed Team
 Licensed under the MIT license.
 
 Functionality for swapping optimizer tensors to/from (NVMe) storage devices.
 */
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/aio/py_lib/deepspeed_py_aio_handle.h` & `deepspeed-0.9.0/deepspeed/ops/csrc/aio/py_lib/deepspeed_py_aio_handle.h`

 * *Files 3% similar despite different names*

```diff
@@ -1,11 +1,13 @@
-/*
-Copyright 2020 The Microsoft DeepSpeed Team
-Licensed under the MIT license.
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
+/*
 Functionality for swapping optimizer tensors to/from (NVMe) storage devices.
 */
 
 #include <condition_variable>
 #include <memory>
 #include "deepspeed_aio_thread.h"
 #include "deepspeed_pin_tensor.h"
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/aio/py_lib/deepspeed_py_copy.cpp` & `deepspeed-0.9.0/deepspeed/ops/csrc/aio/py_lib/deepspeed_py_copy.cpp`

 * *Files 3% similar despite different names*

```diff
@@ -1,11 +1,13 @@
-/*
-Copyright 2020 The Microsoft DeepSpeed Team
-Licensed under the MIT license.
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
+/*
 Functionality for swapping optimizer tensors to/from (NVMe) storage devices.
 */
 
 #include "deepspeed_py_copy.h"
 #include <omp.h>
 
 #define ROUND_DOWN(size, step) ((size) & ~((step)-1))
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/aio/py_lib/py_ds_aio.cpp` & `deepspeed-0.9.0/deepspeed/ops/csrc/aio/py_lib/py_ds_aio.cpp`

 * *Files 8% similar despite different names*

```diff
@@ -1,11 +1,13 @@
-/*
-Copyright 2020 The Microsoft DeepSpeed Team
-Licensed under the MIT license.
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
+/*
 Functionality for swapping optimizer tensors to/from (NVMe) storage devices.
 */
 
 #include <torch/extension.h>
 #include "deepspeed_py_aio_handle.h"
 #include "deepspeed_py_copy.h"
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/common/custom_cuda_kernel.cu` & `deepspeed-0.9.0/deepspeed/ops/csrc/common/custom_cuda_kernel.cu`

 * *Files 23% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #include "custom_cuda_layers.h"
 
 __global__ void param_update_kernel(const float* input, __half* output, int size)
 {
     int id = blockIdx.x * blockDim.x + threadIdx.x;
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/includes/StopWatch.h` & `deepspeed-0.9.0/deepspeed/ops/csrc/includes/StopWatch.h`

 * *Files 11% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #pragma once
 #ifdef _WIN32
 #include <windows.h>
 #else
 #include <time.h>
 #endif
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/includes/Timer.h` & `deepspeed-0.9.0/deepspeed/ops/csrc/includes/Timer.h`

 * *Files 10% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #ifndef __TIMER_H__
 #define __TIMER_H__
 
 #include <cuda_runtime.h>
 #include <chrono>
 #include "cuda.h"
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/includes/context.h` & `deepspeed-0.9.0/deepspeed/ops/csrc/includes/context.h`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #pragma once
 
 #include <ATen/cuda/CUDAContext.h>
 #include <cuda_runtime_api.h>
 #include <cassert>
 #include <iostream>
@@ -39,36 +40,36 @@
 {
     return (std::max)(
         (std::min)((N + DS_CUDA_NUM_THREADS - 1) / DS_CUDA_NUM_THREADS, DS_MAXIMUM_NUM_BLOCKS),
         // Use at least 1 block, since CUDA does not allow empty block
         1);
 }
 
-class Context {
+class TrainingContext {
 public:
-    Context() : _workspace(nullptr), _seed(42), _curr_offset(0)
+    TrainingContext() : _workspace(nullptr), _seed(42), _curr_offset(0)
     {
         curandCreateGenerator(&_gen, CURAND_RNG_PSEUDO_DEFAULT);
         curandSetPseudoRandomGeneratorSeed(_gen, 123);
         if (cublasCreate(&_cublasHandle) != CUBLAS_STATUS_SUCCESS) {
             auto message = std::string("Fail to create cublas handle.");
             std::cerr << message << std::endl;
             throw std::runtime_error(message);
         }
     }
 
-    virtual ~Context()
+    virtual ~TrainingContext()
     {
         cublasDestroy(_cublasHandle);
         cudaFree(_workspace);
     }
 
-    static Context& Instance()
+    static TrainingContext& Instance()
     {
-        static Context _ctx;
+        static TrainingContext _ctx;
         return _ctx;
     }
 
     void SetWorkSpace(void* workspace)
     {
         if (!workspace) { throw std::runtime_error("Workspace is null."); }
         _workspace = workspace;
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/includes/conversion_utils.h` & `deepspeed-0.9.0/deepspeed/ops/csrc/includes/conversion_utils.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright 2022 The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #pragma once
 
 #include "ds_kernel_utils.h"
 
 #include <cuda_fp16.h>
 #include <stdint.h>
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/includes/cpu_adagrad.h` & `deepspeed-0.9.0/deepspeed/ops/csrc/includes/cpu_adagrad.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #pragma once
 
 #define NOMINMAX  // Windows idiosyncrasy
                   // https://stackoverflow.com/questions/4913922/possible-problems-with-nominmax-on-visual-c
 
 #include <stdio.h>
@@ -34,16 +35,16 @@
     Adagrad_Optimizer(float alpha = 1e-2, float eps = 1e-8, float weight_decay = 0)
         : _alpha(alpha), _eps(eps), _weight_decay(weight_decay)
     {
 #if defined(__ENABLE_CUDA__)
         cudaMallocHost((void**)_doubled_buffer, TILE * sizeof(float));
         cudaMallocHost((void**)(_doubled_buffer + 1), TILE * sizeof(float));
 
-        _streams[0] = Context::Instance().GetCurrentStream();
-        _streams[1] = Context::Instance().GetNewStream();
+        _streams[0] = TrainingContext::Instance().GetCurrentStream();
+        _streams[1] = TrainingContext::Instance().GetNewStream();
         _buf_index = false;
 #endif
     }
     ~Adagrad_Optimizer()
     {
 #if defined(__ENABLE_CUDA__)
         cudaFreeHost(_doubled_buffer[0]);
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/includes/cpu_adam.h` & `deepspeed-0.9.0/deepspeed/ops/csrc/includes/cpu_adam.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #pragma once
 
 #define NOMINMAX  // Windows idiosyncrasy
                   // https://stackoverflow.com/questions/4913922/possible-problems-with-nominmax-on-visual-c
 
 #include <stdio.h>
@@ -49,16 +50,16 @@
           _step(0),
           _adamw_mode(adamw_mode)
     {
 #if defined(__ENABLE_CUDA__)
         cudaMallocHost((void**)_doubled_buffer, TILE * sizeof(float));
         cudaMallocHost((void**)(_doubled_buffer + 1), TILE * sizeof(float));
 
-        _streams[0] = Context::Instance().GetCurrentStream();
-        _streams[1] = Context::Instance().GetNewStream();
+        _streams[0] = TrainingContext::Instance().GetCurrentStream();
+        _streams[1] = TrainingContext::Instance().GetNewStream();
         _buf_index = false;
 #endif
     }
     ~Adam_Optimizer()
     {
 #if defined(__ENABLE_CUDA__)
         cudaFreeHost(_doubled_buffer[0]);
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/includes/cublas_wrappers.h` & `deepspeed-0.9.0/deepspeed/ops/csrc/includes/cublas_wrappers.h`

 * *Files 11% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #pragma once
 
 #include <assert.h>
 #include <cublas_v2.h>
 #include <cuda.h>
 #include <cuda_fp16.h>
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/includes/custom_cuda_layers.h` & `deepspeed-0.9.0/deepspeed/ops/csrc/includes/custom_cuda_layers.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright 2022 The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #pragma once
 
 #include "ds_kernel_utils.h"
 
 #include <cuda.h>
 #include <cuda_fp16.h>
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/includes/dequantization_utils.h` & `deepspeed-0.9.0/deepspeed/ops/csrc/includes/dequantization_utils.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright 2022 The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #include "conversion_utils.h"
 #include "ds_kernel_utils.h"
 #include "quantization.h"
 #include "quantization_utils.h"
 
 namespace cg = cooperative_groups;
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/includes/dropout.h` & `deepspeed-0.9.0/deepspeed/ops/csrc/includes/dropout.h`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #pragma once
 
 #include <cuda.h>
 #include <cuda_fp16.h>
 #include <stdio.h>
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/includes/ds_kernel_utils.h` & `deepspeed-0.9.0/deepspeed/ops/csrc/includes/ds_kernel_utils.h`

 * *Files 10% similar despite different names*

```diff
@@ -1,10 +1,13 @@
-/*
-Copyright 2022 The Microsoft DeepSpeed Team
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
+/*
 Centralized header file for preprocessor macros and constants
 used throughout the codebase.
 */
 
 #pragma once
 
 #include <cuda.h>
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/includes/ds_transformer_cuda.h` & `deepspeed-0.9.0/deepspeed/ops/csrc/includes/ds_transformer_cuda.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #pragma once
 
 #include <cuda_runtime_api.h>
 #include <curand.h>
 #include <memory>
 #include <vector>
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/includes/feed_forward.h` & `deepspeed-0.9.0/deepspeed/ops/csrc/includes/feed_forward.h`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #ifndef __FEEDFORWARD_H__
 #define __FEEDFORWARD_H__
 
 #include <cuda.h>
 #include <cuda_fp16.h>
 #include <stdio.h>
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/includes/gelu.h` & `deepspeed-0.9.0/deepspeed/ops/csrc/includes/gelu.h`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #pragma once
 
 #include <cuda.h>
 #include <cuda_fp16.h>
 #include <stdio.h>
 #include "custom_cuda_layers.h"
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/includes/gemm_test.h` & `deepspeed-0.9.0/deepspeed/ops/csrc/includes/gemm_test.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #pragma once
 
 #include <cuda_fp16.h>
 #ifndef __HIP_PLATFORM_HCC__
 #include <cuda_profiler_api.h>
 #endif
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/includes/general_kernels.h` & `deepspeed-0.9.0/deepspeed/ops/csrc/includes/general_kernels.h`

 * *Files 20% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #include <cuda.h>
 #include <cuda_fp16.h>
 #include <stdio.h>
 #include <stdlib.h>
 
 #ifdef __HIP_PLATFORM_HCC__
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/includes/memory_access_utils.h` & `deepspeed-0.9.0/deepspeed/ops/csrc/includes/memory_access_utils.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright 2022 The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #pragma once
 
 #include <cuda.h>
 #include "ds_kernel_utils.h"
 
 /////////////////////////////// Memory Access Utils ///////////////////////////////
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/includes/normalize_layer.h` & `deepspeed-0.9.0/deepspeed/ops/csrc/includes/normalize_layer.h`

 * *Files 11% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #pragma once
 
 #include <cuda.h>
 #include <cuda_fp16.h>
 #include <stdio.h>
 #include <fstream>
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/includes/quantization.h` & `deepspeed-0.9.0/deepspeed/ops/csrc/includes/quantization.h`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #pragma once
 
 #include <cuda_fp16.h>
 #include "ds_kernel_utils.h"
 
 namespace quantize {
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/includes/quantization_utils.h` & `deepspeed-0.9.0/deepspeed/ops/csrc/includes/quantization_utils.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright 2022 The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #include <cassert>
 #include "conversion_utils.h"
 #include "ds_kernel_utils.h"
 #include "memory_access_utils.h"
 #include "quantization.h"
 #include "reduction_utils.h"
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/includes/reduction_utils.h` & `deepspeed-0.9.0/deepspeed/ops/csrc/includes/reduction_utils.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright 2022 The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #pragma once
 
 #include "conversion_utils.h"
 #include "ds_kernel_utils.h"
 #include "memory_access_utils.h"
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/includes/simd.h` & `deepspeed-0.9.0/deepspeed/ops/csrc/includes/simd.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #pragma once
 
 #if (__x86_64__ || __i386__)
 #include <cpuid.h>
 #include <x86intrin.h>
 #endif
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/includes/softmax.h` & `deepspeed-0.9.0/deepspeed/ops/csrc/includes/softmax.h`

 * *Files 10% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #pragma once
 
 #include <cuda.h>
 #include <cuda_fp16.h>
 #include <stdio.h>
 #include "custom_cuda_layers.h"
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/includes/strided_batch_gemm.h` & `deepspeed-0.9.0/deepspeed/ops/csrc/includes/strided_batch_gemm.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #pragma once
 
 #include <cuda.h>
 #include <cuda_fp16.h>
 #include <stdio.h>
 #include "context.h"
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/includes/type_shim.h` & `deepspeed-0.9.0/deepspeed/ops/csrc/includes/type_shim.h`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 /* Taken from NVIDIA/apex commit 855808f3fc268e9715d613f3c2e56469d8c986d8 */
 #include <ATen/ATen.h>
 
 // Forward/backward compatibility hack around
 // https://github.com/pytorch/pytorch/commit/3aeb78079bcd68282fe9117088e138b77318e288
 // pending more future-proof guidance from upstream.
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/lamb/fused_lamb_cuda.cpp` & `deepspeed-0.9.0/deepspeed/ops/csrc/lamb/fused_lamb_cuda.cpp`

 * *Files 6% similar despite different names*

```diff
@@ -1,8 +1,12 @@
-/* Copyright 2019 The Microsoft DeepSpeed Team */
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
 #include <torch/extension.h>
 
 // CUDA forward declaration
 void fused_lamb_cuda(at::Tensor& p,
                      at::Tensor& p_copy,
                      at::Tensor& m,
                      at::Tensor& v,
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/lamb/fused_lamb_cuda_kernel.cu` & `deepspeed-0.9.0/deepspeed/ops/csrc/lamb/fused_lamb_cuda_kernel.cu`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,12 @@
-/* Copyright 2019 The Microsoft DeepSpeed Team */
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
 #include <cuda.h>
 #include <cuda_runtime.h>
 #include <stdio.h>
 #include <cmath>
 #include "ATen/ATen.h"
 #include "ATen/TensorUtils.h"
 #include "ATen/cuda/CUDAContext.h"
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/quantization/dequantize.cu` & `deepspeed-0.9.0/deepspeed/ops/csrc/quantization/dequantize.cu`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright 2022 The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #include "dequantization_utils.h"
 #include "memory_access_utils.h"
 
 namespace cg = cooperative_groups;
 
 template <typename T, int numBits, dequantize::Type qType, int unroll, int threads>
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/quantization/fake_quantizer.cu` & `deepspeed-0.9.0/deepspeed/ops/csrc/quantization/fake_quantizer.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #include <math.h>
 #include "custom_cuda_layers.h"
 #include "memory_access_utils.h"
 
 namespace cg = cooperative_groups;
 
@@ -452,15 +453,15 @@
                                     int num_bits,
                                     cudaStream_t stream)
 {
     dim3 block_dim(1024);
     dim3 grid_dim(group_num);
 
     uint64_t inc = total_count / grid_dim.x / block_dim.x;
-    std::pair<uint64_t, uint64_t> seed = Context::Instance().IncrementOffset(inc);
+    std::pair<uint64_t, uint64_t> seed = TrainingContext::Instance().IncrementOffset(inc);
 
     sr_fake_quantize_kernel<<<grid_dim, block_dim, 0, stream>>>(
         vals, (total_count / group_num) / 4, group_num, num_bits, seed);
 }
 template void launch_sr_fake_quantize_kernel(float* vals,
                                              int total_count,
                                              int group_num,
@@ -1006,15 +1007,15 @@
                                          int num_bits,
                                          cudaStream_t stream)
 {
     dim3 block_dim(1024);
     dim3 grid_dim(group_num);
 
     uint64_t inc = total_count / grid_dim.x / block_dim.x;
-    std::pair<uint64_t, uint64_t> seed = Context::Instance().IncrementOffset(inc);
+    std::pair<uint64_t, uint64_t> seed = TrainingContext::Instance().IncrementOffset(inc);
 
     sr_fake_quantize_kernel<<<grid_dim, block_dim, 0, stream>>>(
         vals, (total_count / group_num) / 4, group_num, num_bits, seed);
 }
 template void launch_sr_fake_quantize_kernel_asym(float* vals,
                                                   int total_count,
                                                   int group_num,
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/quantization/pt_binding.cpp` & `deepspeed-0.9.0/deepspeed/ops/csrc/quantization/pt_binding.cpp`

 * *Files 7% similar despite different names*

```diff
@@ -1,7 +1,12 @@
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
 #include <ATen/cuda/CUDAContext.h>
 #include <torch/extension.h>
 #include <cassert>
 #include <vector>
 #include "quantization.h"
 
 template <typename T>
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/quantization/quantize.cu` & `deepspeed-0.9.0/deepspeed/ops/csrc/quantization/quantize.cu`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright 2022 The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #include "ds_kernel_utils.h"
 #include "memory_access_utils.h"
 #include "quantization.h"
 #include "quantization_utils.h"
 #include "reduction_utils.h"
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/random_ltd/gather_scatter.cu` & `deepspeed-0.9.0/deepspeed/ops/csrc/random_ltd/gather_scatter.cu`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright 2022 The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #include "custom_cuda_layers.h"
 #include "memory_access_utils.h"
 
 namespace cg = cooperative_groups;
 
 namespace td_data {
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/random_ltd/pt_binding.cpp` & `deepspeed-0.9.0/deepspeed/ops/csrc/random_ltd/pt_binding.cpp`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright 2022 The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #include <torch/extension.h>
 #include <vector>
 #include "custom_cuda_layers.h"
 
 torch::Tensor token_sort_(torch::Tensor& unsorted_token_ids, int64_t original_tokens)
 {
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/random_ltd/slice_attn_masks.cu` & `deepspeed-0.9.0/deepspeed/ops/csrc/random_ltd/slice_attn_masks.cu`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright 2022 The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #include "custom_cuda_layers.h"
 #include "memory_access_utils.h"
 
 namespace cg = cooperative_groups;
 
 template <typename T>
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/random_ltd/token_sort.cu` & `deepspeed-0.9.0/deepspeed/ops/csrc/random_ltd/token_sort.cu`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright 2022 The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #include <cassert>
 #include "custom_cuda_layers.h"
 #include "memory_access_utils.h"
 
 namespace cg = cooperative_groups;
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/sparse_attention/utils.cpp` & `deepspeed-0.9.0/deepspeed/ops/csrc/sparse_attention/utils.cpp`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,16 @@
-// DeepSpeed note, code taken & adapted from commit 9aa94789f13ada713af36cfd8cca2fc9a7f6b79a
-// https://github.com/ptillet/torch-blocksparse/blob/master/csrc/utils.cpp
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
+/*
+DeepSpeed note, code taken & adapted from commit 9aa94789f13ada713af36cfd8cca2fc9a7f6b79a
+ https:github.com/ptillet/torch-blocksparse/blob/master/csrc/utils.cpp
+*/
 
 #include <torch/extension.h>
 #include <string>
 #include <tuple>
 #include <vector>
 #ifdef _OPENMP
 #include <omp.h>
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/spatial/csrc/opt_bias_add.cu` & `deepspeed-0.9.0/deepspeed/ops/csrc/spatial/csrc/opt_bias_add.cu`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright 2022 The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #include <cassert>
 #include "memory_access_utils.h"
 #include "spatial_cuda_layers.h"
 
 /*
 Fused bias add variants
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/spatial/csrc/pt_binding.cpp` & `deepspeed-0.9.0/deepspeed/ops/csrc/spatial/csrc/pt_binding.cpp`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright 2022 The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #include <c10/cuda/CUDAStream.h>
 #include <torch/extension.h>
 #include <cstdio>
 #include <vector>
 #include "spatial_cuda_layers.h"
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/spatial/includes/spatial_cuda_layers.h` & `deepspeed-0.9.0/deepspeed/ops/csrc/spatial/includes/spatial_cuda_layers.h`

 * *Files 14% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright 2022 The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #pragma once
 
 #if __CUDA_ARCH__ >= 530
 #define HALF_PRECISION_AVAILABLE = 1
 #endif
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/transformer/cublas_wrappers.cu` & `deepspeed-0.9.0/deepspeed/ops/csrc/transformer/cublas_wrappers.cu`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #include "cublas_wrappers.h"
 
 #ifdef __HIP_PLATFORM_HCC__
 int cublas_gemm_ex(rocblas_handle handle,
                    rocblas_operation transa,
                    rocblas_operation transb,
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/transformer/dropout_kernels.cu` & `deepspeed-0.9.0/deepspeed/ops/csrc/transformer/dropout_kernels.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #include "custom_cuda_layers.h"
 
 const int unroll_factor = 4;
 
 __global__ void dropout_kernel(const int N,
                                const float ratio,
@@ -273,15 +274,15 @@
     dim3 block_dim = DS_CUDA_NUM_THREADS;
 
     if (dim > 512) {
         block_dim.x >>= 1;
         grid_dim.x <<= 1;
     }
     uint64_t inc = total_count / grid_dim.x / block_dim.x;
-    std::pair<uint64_t, uint64_t> seed = Context::Instance().IncrementOffset(inc);
+    std::pair<uint64_t, uint64_t> seed = TrainingContext::Instance().IncrementOffset(inc);
     if (bwd)
         dropout_kernel_bwd<<<grid_dim, block_dim, 0, stream>>>(
             total_count, ratio, vals, out, mask, seed);
     else
         dropout_kernel<<<grid_dim, block_dim, 0, stream>>>(
             total_count, ratio, out, vals, mask, seed);
 }
@@ -620,15 +621,15 @@
 
     int total_count = batch * dim / unroll_factor;
 
     dim3 grid_dim = DS_GET_BLOCKS(total_count);
     dim3 block_dim = DS_CUDA_NUM_THREADS;
 
     uint64_t inc = (batch * dim) / grid_dim.x / block_dim.x;
-    std::pair<uint64_t, uint64_t> seed = Context::Instance().IncrementOffset(inc);
+    std::pair<uint64_t, uint64_t> seed = TrainingContext::Instance().IncrementOffset(inc);
 
     dropout_kernel<<<grid_dim, block_dim, 0, stream>>>(
         total_count, dim, ratio, bias, out, mask, seed);
 }
 
 template void launch_dropout(float*,
                              const float* bias,
@@ -842,15 +843,15 @@
     assert(unroll_factor == 4);
 
     int total_count = batch * dim / unroll_factor;
     dim3 grid_dim = DS_GET_BLOCKS(total_count);
     dim3 block_dim = DS_CUDA_NUM_THREADS;
 
     uint64_t inc = (batch * dim) / grid_dim.x / block_dim.x;
-    std::pair<uint64_t, uint64_t> seed = Context::Instance().IncrementOffset(inc);
+    std::pair<uint64_t, uint64_t> seed = TrainingContext::Instance().IncrementOffset(inc);
 
     dropout_kernel<<<grid_dim, block_dim, 0, stream>>>(
         total_count, dim, ratio, input, residual, bias, out, mask, seed);
 }
 
 template void launch_dropout(float*,
                              const float*,
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/transformer/ds_transformer_cuda.cpp` & `deepspeed-0.9.0/deepspeed/ops/csrc/transformer/ds_transformer_cuda.cpp`

 * *Files 1% similar despite different names*

```diff
@@ -1,7 +1,12 @@
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
 #include <torch/extension.h>
 
 #include <cublas_v2.h>
 #include <cuda_fp16.h>
 #include <cuda_runtime.h>
 #include <type_traits>
 #include <unordered_map>
@@ -69,16 +74,16 @@
       _seq_length(seq_length),
       _training(true),
       _pre_or_postLayerNorm(pre_or_postLayerNorm),
       _attn_dropout_checkpoint(attn_dropout_checkpoint),
       _normalize_invertible(normalize_invertible),
       _gelu_checkpoint(gelu_checkpoint),
       _stochastic_mode(stochastic_mode),
-      _stream(Context::Instance().GetCurrentStream()),
-      _cublasHandle(Context::Instance().GetCublasHandle()),
+      _stream(TrainingContext::Instance().GetCurrentStream()),
+      _cublasHandle(TrainingContext::Instance().GetCublasHandle()),
       _qkv_linear(typename FeedForward<T>::Config(batch_size * seq_length,
                                                   3 * hidden_size,
                                                   hidden_size,
                                                   gemm_algos[0])),
       _attn_out_linear(typename FeedForward<T>::Config(batch_size * seq_length,
                                                        hidden_size,
                                                        hidden_size,
@@ -174,15 +179,15 @@
                                       T* gelu_inp_ptr,
                                       T* ff2_inp_ptr)
 {
     cublasSetStream(_cublasHandle, _stream);
 
     if (!_stochastic_mode) cudaStreamSynchronize(_stream);
 
-    T* workspace = static_cast<T*>(Context::Instance().GetWorkSpace());
+    T* workspace = static_cast<T*>(TrainingContext::Instance().GetWorkSpace());
     size_t small_buf_size = bsz * _seq_length * _hidden_size;
     T* buf_0 = workspace;
     T* buf_1 = buf_0 + small_buf_size;
     T* buf_2 = buf_1;
 
     if (_normalize_invertible) {
         add_res_ptr = buf_1 + 3 * small_buf_size;
@@ -334,15 +339,15 @@
                                        T* grad_norm_w_ptr,
                                        T* grad_norm_b_ptr)
 {
     cublasSetStream(_cublasHandle, _stream);
 
     if (!_stochastic_mode) cudaStreamSynchronize(_stream);
 
-    T* workspace = static_cast<T*>(Context::Instance().GetWorkSpace());
+    T* workspace = static_cast<T*>(TrainingContext::Instance().GetWorkSpace());
     size_t small_buf_size = bsz * _seq_length * _hidden_size;
     T* buf_0 = workspace;
     T* buf_1 = buf_0 + small_buf_size;
     T* buf_2 = buf_1 + small_buf_size;
     T* buf_3 = buf_2 + small_buf_size;
 
     T* ff2_buf = (_gelu_checkpoint ? buf_3 + (bsz * _seq_length * _intermediate_size)
@@ -600,33 +605,34 @@
                              bool pre_or_postLayerNorm,
                              bool test_gemm,
                              bool attn_dropout_checkpoint,
                              bool normalize_invertible,
                              bool gelu_checkpoint,
                              bool stochastic_mode)
 {
-    Context::Instance().SetSeed(seed);
-    Context::Instance().TestGemmFP16(
+    TrainingContext::Instance().SetSeed(seed);
+    TrainingContext::Instance().TestGemmFP16(
         test_gemm, batch_size, init_seq_length, num_heads, hidden_dim / num_heads);
 
-    auto layer = std::make_shared<BertTransformerLayer<T>>(layer_id,
-                                                           batch_size,
-                                                           hidden_dim,
-                                                           num_heads,
-                                                           intermediate_size,
-                                                           init_seq_length,
-                                                           attn_dropout_ratio,
-                                                           hidden_dropout_ratio,
-                                                           layer_norm_eps,
-                                                           pre_or_postLayerNorm,
-                                                           Context::Instance().GetGemmAlgos(),
-                                                           attn_dropout_checkpoint,
-                                                           normalize_invertible,
-                                                           gelu_checkpoint,
-                                                           stochastic_mode);
+    auto layer =
+        std::make_shared<BertTransformerLayer<T>>(layer_id,
+                                                  batch_size,
+                                                  hidden_dim,
+                                                  num_heads,
+                                                  intermediate_size,
+                                                  init_seq_length,
+                                                  attn_dropout_ratio,
+                                                  hidden_dropout_ratio,
+                                                  layer_norm_eps,
+                                                  pre_or_postLayerNorm,
+                                                  TrainingContext::Instance().GetGemmAlgos(),
+                                                  attn_dropout_checkpoint,
+                                                  normalize_invertible,
+                                                  gelu_checkpoint,
+                                                  stochastic_mode);
 
     s_transformer_layers[layer_id] = layer;
 
     std::string dtype = (std::is_same<T, __half>::value) ? "half" : "float";
 
     std::cout << "layer #" << layer_id << " is created with date type [" << dtype << "]."
               << std::endl;
@@ -716,15 +722,15 @@
                                                          seq_len,
                                                          layer->GetHiddenSize(),
                                                          layer->GetIntermediateSize(),
                                                          layer->GetNumHeads(),
                                                          layer->IsTrainingMode(),
                                                          layer->GeluCheckpoint())},
                                   options);
-    Context::Instance().SetWorkSpace((T*)workspace.data_ptr());
+    TrainingContext::Instance().SetWorkSpace((T*)workspace.data_ptr());
 
     auto inp_norm = ((prelayernorm || !normalize_invertible) ? torch::empty_like(input) : output);
     auto add_res = (normalize_invertible ? inp_norm : torch::empty_like(input));
     auto attn_o_inp = torch::empty_like(input);
     auto qkv_tf = torch::empty({(bsz * seq_len), output_w.size(0) * 3}, options);
 
     auto attn_prob_dropout_mask =
@@ -900,15 +906,15 @@
                                                          seq_len,
                                                          layer->GetHiddenSize(),
                                                          layer->GetIntermediateSize(),
                                                          layer->GetNumHeads(),
                                                          layer->IsTrainingMode(),
                                                          layer->GeluCheckpoint())},
                                   options);
-    Context::Instance().SetWorkSpace((T*)workspace.data_ptr());
+    TrainingContext::Instance().SetWorkSpace((T*)workspace.data_ptr());
 
     auto grad_input = torch::empty_like(input);
     auto grad_attn_qkvw = torch::empty_like(attn_qkvw);
     auto grad_attn_qkvb = torch::empty_like(attn_qkvb);
     auto grad_attn_ow = torch::empty_like(attn_ow);
     auto grad_attn_ob = torch::empty_like(attn_ob);
     auto grad_attn_nw = torch::empty_like(attn_nw);
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/transformer/gelu_kernels.cu` & `deepspeed-0.9.0/deepspeed/ops/csrc/transformer/gelu_kernels.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #include "custom_cuda_layers.h"
 
 inline __device__ float gelu(const float x)
 {
     const float sqrt_param = 0.79788456080286535587989211986876f;
     const float mul_param = 0.044715;
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/transformer/general_kernels.cu` & `deepspeed-0.9.0/deepspeed/ops/csrc/transformer/general_kernels.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #include "general_kernels.h"
 
 namespace cg = cooperative_groups;
 
 template <typename T>
 __global__ void column_sum_reduce(const T* __restrict__ inp,
@@ -157,15 +158,15 @@
                               int seq_length,
                               int hidden_dim,
                               cudaStream_t& stream)
 {
     int total_count = batch_size * seq_length * hidden_dim / 4;
     dim3 grid_dim = DS_GET_BLOCKS(total_count);  //(batch_size * seq_length);
 
-    dim3 block_dim = DS_CUDA_NUM_THREADS;  //(hidden_dim / 4);
+    dim3 block_dim = DS_CUDA_NUM_THREADS;        //(hidden_dim / 4);
 
     fused_add2_kernel<<<grid_dim, block_dim, 0, stream>>>(total_count, out, inp1, inp2);
 }
 
 template <>
 void launch_fused_add2<__half>(__half* out,
                                const __half* inp1,
@@ -174,15 +175,15 @@
                                int seq_length,
                                int hidden_dim,
                                cudaStream_t& stream)
 {
     int total_count = batch_size * seq_length * hidden_dim / 4;
     dim3 grid_dim = DS_GET_BLOCKS(total_count);  //(batch_size * seq_length);
 
-    dim3 block_dim = DS_CUDA_NUM_THREADS;  //(hidden_dim / 4);
+    dim3 block_dim = DS_CUDA_NUM_THREADS;        //(hidden_dim / 4);
 
     fused_add2_kernel<<<grid_dim, block_dim, 0, stream>>>(total_count, out, inp1, inp2);
 }
 
 __global__ void fused_add3_kernel(float* out,
                                   const float* inp1,
                                   const float* inp2,
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/transformer/inference/csrc/apply_rotary_pos_emb.cu` & `deepspeed-0.9.0/deepspeed/ops/csrc/transformer/inference/csrc/apply_rotary_pos_emb.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright 2022 The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #include "inference_cuda_layers.h"
 
 #ifndef __HIP_PLATFORM_HCC__
 #include <cuda_profiler_api.h>
 #endif
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/transformer/inference/csrc/dequantize.cu` & `deepspeed-0.9.0/deepspeed/ops/csrc/transformer/inference/csrc/dequantize.cu`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright 2022 The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #include "inference_cuda_layers.h"
 
 #define MAX_QUANTIZE_GROUPING 1024
 
 #define loop_unroll 1
 #define loop_unroll_bits 1
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/transformer/inference/csrc/gelu.cu` & `deepspeed-0.9.0/deepspeed/ops/csrc/transformer/inference/csrc/gelu.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright 2022 The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #include "conversion_utils.h"
 #include "inference_cuda_layers.h"
 #include "memory_access_utils.h"
 
 namespace cg = cooperative_groups;
 #define MAX_CAP 4
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/transformer/inference/csrc/layer_norm.cu` & `deepspeed-0.9.0/deepspeed/ops/csrc/transformer/inference/csrc/layer_norm.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright 2022 The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #include "conversion_utils.h"
 #include "ds_kernel_utils.h"
 #include "inference_cuda_layers.h"
 #include "memory_access_utils.h"
 #include "reduction_utils.h"
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp` & `deepspeed-0.9.0/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp`

 * *Files 12% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright 2022 The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #include <c10/cuda/CUDAStream.h>
 #include <torch/extension.h>
 #include <stdexcept>
 #include <vector>
 #include "inference_context.h"
 #include "inference_cublas_wrappers.h"
@@ -91,67 +92,69 @@
                            bsz,
                            heads,
                            seq_len,
                            soft_len,
                            head_offset,
                            mask_stride,
                            mp_size,
-                           Context::Instance().GetCurrentStream(async_op));
+                           InferenceContext::Instance().GetCurrentStream(async_op));
 
     return attn_scores_c;
 }
 
 template <typename T>
 void allocate_workspace(unsigned hidden_dim,
                         unsigned num_heads,
                         unsigned prompt_length,
                         unsigned batch_size,
                         unsigned num_layers,
                         unsigned mp_size = 1,
                         bool external_cache = false,
                         unsigned rank = 0,
-                        unsigned max_out_tokens = 1024)
+                        unsigned max_out_tokens = 1024,
+                        unsigned min_out_tokens = 1)
 {
-    Context::Instance().GenWorkSpace(num_layers,
-                                     num_heads,
-                                     batch_size,
-                                     prompt_length,
-                                     hidden_dim,
-                                     mp_size,
-                                     external_cache,
-                                     sizeof(T),
-                                     rank,
-                                     max_out_tokens);
+    InferenceContext::Instance().GenWorkSpace(num_layers,
+                                              num_heads,
+                                              batch_size,
+                                              prompt_length,
+                                              hidden_dim,
+                                              mp_size,
+                                              external_cache,
+                                              sizeof(T),
+                                              rank,
+                                              max_out_tokens,
+                                              min_out_tokens);
 }
 
 template <typename T>
 at::Tensor einsum_sec_sm_ecm(at::Tensor& Q, at::Tensor& W)
 {
     auto options = at::TensorOptions()
                        .dtype(Q.options().dtype())
                        .layout(at::kStrided)
                        .device(at::kCUDA)
                        .requires_grad(false);
-    T* workspace = (T*)Context::Instance().GetWorkSpace();
+    T* workspace = (T*)InferenceContext::Instance().GetWorkSpace();
     float alpha = 1;
     float gemm_beta = 0.0;
 
     /*
     // Reallocate memory if we received a new prompt
     if (!workspace || input.size(1) != 1) {
-        allocate_workspace<T>(W.size(1), Context::Instance().GetMaxTokenLenght(), Q.size(0), 1,
-    head_size); workspace = (T*)Context::Instance().GetWorkSpace();
+        allocate_workspace<T>(W.size(1), InferenceContext::Instance().GetMaxTokenLenght(),
+    Q.size(0), 1, head_size); workspace = (T*)InferenceContext::Instance().GetWorkSpace();
     }
     */
 
     auto O = at::from_blob(workspace, {Q.size(1), Q.size(2), W.size(1)}, options);
     unsigned m = W.size(1);
     unsigned n = Q.size(1) * Q.size(2);
     unsigned k = Q.size(0);
-    cublas_gemm_ex(Context::Instance().GetCublasHandle(),
+    cublas_gemm_ex(InferenceContext::Instance().GetCublasHandle(),
                    CUBLAS_OP_N,
                    CUBLAS_OP_T,
                    m,
                    n,
                    k,
                    &alpha,
                    &gemm_beta,
@@ -190,16 +193,17 @@
     float alpha = norm_factor;
     float gemm_beta = 0.0;
     auto attn_score = at::empty({bsz, heads, seq_len, soft_len}, options);
     int k = prev_value_cont.size(2) / heads;
 
     auto mask_stride = get_attn_mask_stride(attn_mask);
 
-    cublasSetStream(Context::Instance().GetCublasHandle(), Context::Instance().GetCurrentStream());
-    cublas_strided_batched_gemm(Context::Instance().GetCublasHandle(),
+    cublasSetStream(InferenceContext::Instance().GetCublasHandle(),
+                    InferenceContext::Instance().GetCurrentStream());
+    cublas_strided_batched_gemm(InferenceContext::Instance().GetCublasHandle(),
                                 soft_len,
                                 seq_len,
                                 k,
                                 &alpha,
                                 &gemm_beta,
                                 (T*)prev_key_cont.data_ptr(),
                                 (T*)query_cont.data_ptr(),
@@ -226,17 +230,17 @@
                            bsz,
                            heads,
                            seq_len,
                            soft_len,
                            0,
                            mask_stride,
                            1,
-                           Context::Instance().GetCurrentStream(false));
+                           InferenceContext::Instance().GetCurrentStream(false));
     alpha = 1.0;
-    cublas_strided_batched_gemm(Context::Instance().GetCublasHandle(),
+    cublas_strided_batched_gemm(InferenceContext::Instance().GetCublasHandle(),
                                 k,
                                 seq_len,
                                 soft_len,
                                 &alpha,
                                 &gemm_beta,
                                 (T*)prev_value_cont.data_ptr(),
                                 (T*)attn_score.data_ptr(),
@@ -359,29 +363,30 @@
                        int window_size,
                        at::Tensor& alibi,
                        int layer_id)
 {
     float layer_scale = alibi.sizes().size() > 1 ? std::max(1, layer_id) : 1.0;
     float alpha = norm_factor * norm_factor / layer_scale;
     float gemm_beta = 0.0;
-    T* workspace = (T*)Context::Instance().GetAttentionUnfusedWorkspace();
+    T* workspace = (T*)InferenceContext::Instance().GetAttentionUnfusedWorkspace();
 
-    cublasSetStream(Context::Instance().GetCublasHandle(), Context::Instance().GetCurrentStream());
-    cublas_strided_batched_gemm(Context::Instance().GetCublasHandle(),
+    cublasSetStream(InferenceContext::Instance().GetCublasHandle(),
+                    InferenceContext::Instance().GetCurrentStream());
+    cublas_strided_batched_gemm(InferenceContext::Instance().GetCublasHandle(),
                                 soft_len,
                                 seq_len,
                                 k,
                                 &alpha,
                                 &gemm_beta,
                                 (T*)prev_key_cont,
                                 (T*)query_cont,
                                 workspace,
                                 CUBLAS_OP_T,
                                 CUBLAS_OP_N,
-                                Context::Instance().GetMaxTokenLenght() * k,
+                                InferenceContext::Instance().GetMaxTokenLenght() * k,
                                 seq_len * k,
                                 seq_len * soft_len,
                                 bsz * heads,
 #ifdef __HIP_PLATFORM_HCC__
                                 rocblas_gemm_algo_standard);
 #else
                                 CUBLAS_GEMM_DEFAULT_TENSOR_OP);
@@ -395,37 +400,37 @@
                            local_attention,
                            window_size,
                            bsz,
                            seq_len,
                            soft_len,
                            heads);
     alpha = 1.0;
-    cublas_strided_batched_gemm(Context::Instance().GetCublasHandle(),
+    cublas_strided_batched_gemm(InferenceContext::Instance().GetCublasHandle(),
                                 k,
                                 seq_len,
                                 soft_len,
                                 &alpha,
                                 &gemm_beta,
                                 (T*)prev_value_cont,
                                 workspace,
                                 (T*)output,
                                 CUBLAS_OP_N,
                                 CUBLAS_OP_N,
-                                Context::Instance().GetMaxTokenLenght() * k,
+                                InferenceContext::Instance().GetMaxTokenLenght() * k,
                                 seq_len * soft_len,
                                 seq_len * k,
                                 bsz * heads,
 #ifdef __HIP_PLATFORM_HCC__
                                 rocblas_gemm_algo_standard);
 #else
                                 CUBLAS_GEMM_DEFAULT_TENSOR_OP);
 #endif
 }
 
-void reset_cache() { Context::Instance().reset_tokens(); }
+void reset_cache() { InferenceContext::Instance().reset_tokens(); }
 
 template <typename T>
 std::vector<at::Tensor> ds_softmax_context(at::Tensor& query_key_value,
                                            at::Tensor& attn_mask,
                                            int rotary_dim,
                                            bool rotate_half,
                                            bool rotate_every_two,
@@ -441,34 +446,35 @@
 {
     unsigned bsz = query_key_value.size(0);
     unsigned seq_len = query_key_value.size(1);
     unsigned hidden_dim = query_key_value.size(2) / 3;
 
     bool is_prompt = (seq_len > 1);
 
-    if (is_prompt) Context::Instance().reset_tokens(seq_len);
-    unsigned soft_len = Context::Instance().current_tokens();
+    if (is_prompt) InferenceContext::Instance().reset_tokens(seq_len);
+    unsigned soft_len = InferenceContext::Instance().current_tokens();
 
     int k = hidden_dim / heads;
     auto options = at::TensorOptions()
                        .dtype(query_key_value.options().dtype())
                        .layout(at::kStrided)
                        .device(at::kCUDA)
                        .requires_grad(false);
 
-    T* workspace = (T*)Context::Instance().GetWorkSpace();
+    T* workspace = (T*)InferenceContext::Instance().GetWorkSpace();
     size_t buf_size = bsz * seq_len * hidden_dim;
-    auto output = torch::from_blob(workspace + 4 * buf_size, {bsz, seq_len, hidden_dim}, options);
+    auto output = torch::from_blob(workspace + 3 * buf_size, {bsz, seq_len, hidden_dim}, options);
 
-    auto query_cont = workspace + 8 * buf_size;
-    size_t offset = 16 * (hidden_dim * bsz * Context::Instance().GetMaxTokenLenght()) +
-                    layer_id * 2 * bsz * Context::Instance().GetMaxTokenLenght() * hidden_dim;
+    auto query_cont = workspace + 4 * buf_size;
+    size_t offset =
+        10 * (hidden_dim * bsz * InferenceContext::Instance().GetMaxTokenLenght()) +
+        layer_id * 2 * bsz * InferenceContext::Instance().GetMaxTokenLenght() * hidden_dim;
     unsigned all_tokens = soft_len;
     auto kv_cache = workspace + offset + (hidden_dim / heads) * (is_prompt ? 0 : soft_len - 1);
-    size_t value_offset = bsz * Context::Instance().GetMaxTokenLenght() * hidden_dim;
+    size_t value_offset = bsz * InferenceContext::Instance().GetMaxTokenLenght() * hidden_dim;
 
     T* temp_buf = (T*)output.data_ptr() + at::numel(output);
     launch_bias_add_transform_0213<T>((T*)query_cont,
                                       kv_cache,
                                       kv_cache + value_offset,
                                       (T*)query_key_value.data_ptr(),
                                       nullptr,
@@ -477,30 +483,30 @@
                                       (is_prompt ? 0 : soft_len - 1),
                                       soft_len,
                                       hidden_dim,
                                       heads,
                                       rotary_dim,
                                       rotate_half,
                                       rotate_every_two,
-                                      Context::Instance().GetCurrentStream(),
+                                      InferenceContext::Instance().GetCurrentStream(),
                                       3,
-                                      Context::Instance().GetMaxTokenLenght());
+                                      InferenceContext::Instance().GetMaxTokenLenght());
     if (rotary_dim > 0 && rotate_half)
         launch_apply_rotary_pos_emb(query_cont,
                                     kv_cache,
                                     k,
                                     seq_len,
                                     rotary_dim,
                                     (is_prompt ? 0 : soft_len - 1),
                                     heads,
                                     bsz,
                                     rotate_half,
                                     rotate_every_two,
-                                    Context::Instance().GetCurrentStream(),
-                                    Context::Instance().GetMaxTokenLenght());
+                                    InferenceContext::Instance().GetCurrentStream(),
+                                    InferenceContext::Instance().GetMaxTokenLenght());
 
     attention_unfused<T>(workspace + offset,
                          (T*)query_cont,
                          attn_mask,
                          workspace + offset + value_offset,
                          temp_buf,
                          bsz,
@@ -517,21 +523,35 @@
                          layer_id);
     launch_transform4d_0213<T>((T*)output.data_ptr(),
                                temp_buf,
                                bsz,
                                heads,
                                seq_len,
                                output.size(2),
-                               Context::Instance().GetCurrentStream(false),
+                               InferenceContext::Instance().GetCurrentStream(false),
                                1);
 
-    if (layer_id == num_layers - 1) Context::Instance().advance_tokens();
-    auto prev_key = torch::from_blob(workspace + offset, {bsz, heads, all_tokens, k}, options);
+    if (layer_id == num_layers - 1) InferenceContext::Instance().advance_tokens();
+    auto prev_key = torch::from_blob(workspace + offset,
+                                     {bsz, heads, all_tokens, k},
+                                     {hidden_dim * InferenceContext::Instance().GetMaxTokenLenght(),
+                                      k * InferenceContext::Instance().GetMaxTokenLenght(),
+                                      k,
+                                      1},
+                                     options);
+
     auto prev_value =
-        torch::from_blob(workspace + offset + value_offset, {bsz, heads, all_tokens, k}, options);
+        torch::from_blob(workspace + offset + value_offset,
+                         {bsz, heads, all_tokens, k},
+                         {hidden_dim * InferenceContext::Instance().GetMaxTokenLenght(),
+                          k * InferenceContext::Instance().GetMaxTokenLenght(),
+                          k,
+                          1},
+                         options);
+
     return {output, prev_key, prev_value};
 }
 
 template <typename T>
 at::Tensor ds_bias_gelu(at::Tensor& input, at::Tensor& bias)
 {
     auto input_cont = input.contiguous();
@@ -539,15 +559,15 @@
     int bsz = input_cont.size(0) * input_cont.size(1);
     int intermediate_size = input_cont.size(2);
 
     launch_bias_gelu((T*)input_cont.data_ptr(),
                      (T*)bias.data_ptr(),
                      intermediate_size,
                      bsz,
-                     Context::Instance().GetCurrentStream());
+                     InferenceContext::Instance().GetCurrentStream());
     return input_cont;
 }
 
 at::Tensor ds_bias_geglu(at::Tensor& activation, at::Tensor& bias)
 {
     /*
     Used in FF of Stable diffusion
@@ -565,22 +585,22 @@
 
     if (activation.options().dtype() == torch::kFloat32) {
         launch_fused_bias_geglu((float*)output.data_ptr(),
                                 (const float*)activation.data_ptr(),
                                 (const float*)bias.data_ptr(),
                                 rows,
                                 channels,
-                                Context::Instance().GetCurrentStream());
+                                InferenceContext::Instance().GetCurrentStream());
     } else {
         launch_fused_bias_geglu((__half*)output.data_ptr(),
                                 (const __half*)activation.data_ptr(),
                                 (const __half*)bias.data_ptr(),
                                 rows,
                                 channels,
-                                Context::Instance().GetCurrentStream());
+                                InferenceContext::Instance().GetCurrentStream());
     }
 
     return output;
 }
 
 template <typename T>
 at::Tensor ds_bias_relu(at::Tensor& input, at::Tensor& bias)
@@ -590,15 +610,15 @@
     int bsz = input_cont.size(0) * input_cont.size(1);
     int intermediate_size = input_cont.size(2);
 
     launch_bias_relu((T*)input_cont.data_ptr(),
                      (T*)bias.data_ptr(),
                      intermediate_size,
                      bsz,
-                     Context::Instance().GetCurrentStream());
+                     InferenceContext::Instance().GetCurrentStream());
     return input_cont;
 }
 
 template <typename T>
 at::Tensor ds_bias_add(at::Tensor& input, at::Tensor& bias)
 {
     auto input_cont = input.contiguous();
@@ -606,15 +626,15 @@
     int bsz = input_cont.size(0) * input_cont.size(1);
     int hidden_size = input_cont.size(2);
 
     launch_bias_add((T*)input_cont.data_ptr(),
                     (T*)bias.data_ptr(),
                     hidden_size,
                     bsz,
-                    Context::Instance().GetCurrentStream());
+                    InferenceContext::Instance().GetCurrentStream());
     return input_cont;
 }
 
 template <typename T>
 at::Tensor ds_bias_residual(at::Tensor& input, at::Tensor& residual, at::Tensor& bias)
 {
     auto input_cont = input.contiguous();
@@ -623,15 +643,15 @@
     int bsz = input_cont.size(0) * input_cont.size(1);
     // launch_bias_residual((T*)input_cont.data_ptr(),
     //                      (T*)residual_cont.data_ptr(),
     //                      (T*)bias.data_ptr(),
     //                      bsz,
     //                      input_cont.size(2),
     //                      (bias.size(0) > 1),
-    //                      Context::Instance().GetCurrentStream());
+    //                      InferenceContext::Instance().GetCurrentStream());
     return input_cont;
 }
 
 at::Tensor ds_layer_norm(at::Tensor& input, at::Tensor& gamma, at::Tensor& beta, float epsilon)
 {
     const int rows = input.size(0) * input.size(1);
     const int elems_per_row = input.size(2);
@@ -641,24 +661,24 @@
         launch_fused_ln((__half*)output.data_ptr(),
                         (const __half*)input.data_ptr(),
                         (const __half*)gamma.data_ptr(),
                         (const __half*)beta.data_ptr(),
                         epsilon,
                         rows,
                         elems_per_row,
-                        Context::Instance().GetCurrentStream());
+                        InferenceContext::Instance().GetCurrentStream());
     } else {
         launch_fused_ln((float*)output.data_ptr(),
                         (const float*)input.data_ptr(),
                         (const float*)gamma.data_ptr(),
                         (const float*)beta.data_ptr(),
                         epsilon,
                         rows,
                         elems_per_row,
-                        Context::Instance().GetCurrentStream());
+                        InferenceContext::Instance().GetCurrentStream());
     }
 
     return output;
 }
 
 template <typename T>
 void ds_layer_norm_internal(T* workspace,
@@ -671,15 +691,15 @@
     launch_fused_ln(workspace,
                     (const T*)input.data_ptr(),
                     (const T*)gamma.data_ptr(),
                     (const T*)beta.data_ptr(),
                     epsilon,
                     bsz,
                     input.size(2),
-                    Context::Instance().GetCurrentStream());
+                    InferenceContext::Instance().GetCurrentStream());
 }
 
 /* Currently only used in unit testing */
 at::Tensor ds_layer_norm_residual(at::Tensor& input,
                                   at::Tensor& bias,
                                   at::Tensor& residual,
                                   at::Tensor& gamma,
@@ -696,26 +716,26 @@
                                  (const __half*)residual.data_ptr(),
                                  (const __half*)bias.data_ptr(),
                                  (const __half*)gamma.data_ptr(),
                                  (const __half*)beta.data_ptr(),
                                  epsilon,
                                  rows,
                                  elems_per_row,
-                                 Context::Instance().GetCurrentStream());
+                                 InferenceContext::Instance().GetCurrentStream());
     } else {
         launch_fused_residual_ln((float*)output.data_ptr(),
                                  (const float*)input.data_ptr(),
                                  (const float*)residual.data_ptr(),
                                  (const float*)bias.data_ptr(),
                                  (const float*)gamma.data_ptr(),
                                  (const float*)beta.data_ptr(),
                                  epsilon,
                                  rows,
                                  elems_per_row,
-                                 Context::Instance().GetCurrentStream());
+                                 InferenceContext::Instance().GetCurrentStream());
     }
 
     return output;
 }
 
 /* Currently only used in unit testing */
 std::vector<at::Tensor> ds_layer_norm_residual_store_pre_ln_res(at::Tensor& input,
@@ -737,61 +757,61 @@
                                                   (const __half*)residual.data_ptr(),
                                                   (const __half*)bias.data_ptr(),
                                                   (const __half*)gamma.data_ptr(),
                                                   (const __half*)beta.data_ptr(),
                                                   epsilon,
                                                   rows,
                                                   elems_per_row,
-                                                  Context::Instance().GetCurrentStream());
+                                                  InferenceContext::Instance().GetCurrentStream());
     } else {
         launch_fused_residual_ln_store_pre_ln_res((float*)norm_output.data_ptr(),
                                                   (float*)res_output.data_ptr(),
                                                   (const float*)input.data_ptr(),
                                                   (const float*)residual.data_ptr(),
                                                   (const float*)bias.data_ptr(),
                                                   (const float*)gamma.data_ptr(),
                                                   (const float*)beta.data_ptr(),
                                                   epsilon,
                                                   rows,
                                                   elems_per_row,
-                                                  Context::Instance().GetCurrentStream());
+                                                  InferenceContext::Instance().GetCurrentStream());
     }
 
     return {norm_output, res_output};
 }
 
 template <typename T>
 void quantized_gemm(void* output,
                     T* input,
                     at::Tensor& weight,
                     at::Tensor& qscale,
                     int groups,
                     int bsz,
                     int hidden_size)
 {
-    // T* weight16 = (T*)Context::Instance().GetWorkSpace() + 12 * hidden_size * bsz;
+    // T* weight16 = (T*)InferenceContext::Instance().GetWorkSpace() + 12 * hidden_size * bsz;
 
     auto options = at::TensorOptions()
                        .dtype(at::kHalf)
                        .layout(at::kStrided)
                        .device(at::kCUDA)
                        .requires_grad(false);
     auto tmp = torch::empty(weight.sizes(), options);
     T* weight16 = (T*)tmp.data_ptr();
     launch_dequantize(weight16,
                       (int8_t*)weight.data_ptr(),
                       (float*)qscale.data_ptr(),
                       weight.size(0),
                       weight.size(1),
                       groups,
-                      Context::Instance().GetCurrentStream());
+                      InferenceContext::Instance().GetCurrentStream());
 
     float alpha = (T)1.0;
     float gemm_beta = (T)0.0;
-    cublas_gemm_ex(Context::Instance().GetCublasHandle(),
+    cublas_gemm_ex(InferenceContext::Instance().GetCublasHandle(),
                    CUBLAS_OP_T,
                    CUBLAS_OP_N,
                    weight.size(0),
                    bsz,
                    weight.size(1),
                    &alpha,
                    &gemm_beta,
@@ -811,34 +831,35 @@
                               at::Tensor& weight,
                               at::Tensor& q_scale,
                               at::Tensor& bias,
                               at::Tensor& gamma,
                               at::Tensor& beta,
                               const float epsilon,
                               bool add_bias,
-                              bool q_int8)
+                              bool q_int8,
+                              bool transposed_mode)
 {
     int bsz = input.size(0) * input.size(1);
-    T* workspace = (T*)Context::Instance().GetWorkSpace();
+    T* workspace = (T*)InferenceContext::Instance().GetWorkSpace();
     workspace += (3 * bsz * input.size(2));
     ds_layer_norm_internal<T>(workspace, input, gamma, beta, epsilon);
 
     if (q_int8) {
         quantized_gemm<T>(
             output.data_ptr(), workspace, weight, q_scale, q_scale.size(0), bsz, input.size(2));
     } else {
         float alpha = (T)1.0;
         float gemm_beta = (T)0.0;
 
-        cublasSetStream(Context::Instance().GetCublasHandle(),
-                        Context::Instance().GetCurrentStream());
-        cublas_gemm_ex(Context::Instance().GetCublasHandle(),
-                       CUBLAS_OP_N,
+        cublasSetStream(InferenceContext::Instance().GetCublasHandle(),
+                        InferenceContext::Instance().GetCurrentStream());
+        cublas_gemm_ex(InferenceContext::Instance().GetCublasHandle(),
+                       (transposed_mode ? CUBLAS_OP_T : CUBLAS_OP_N),
                        CUBLAS_OP_N,
-                       weight.size(1),
+                       weight.size(transposed_mode ? 0 : 1),
                        bsz,
                        input.size(2),
                        &alpha,
                        &gemm_beta,
                        (T*)weight.data_ptr(),
                        workspace,
                        (T*)output.data_ptr(),
@@ -847,17 +868,17 @@
 #else
                        CUBLAS_GEMM_DEFAULT_TENSOR_OP);
 #endif
     }
     if (add_bias)
         launch_bias_add((T*)output.data_ptr(),
                         (T*)bias.data_ptr(),
-                        q_int8 ? weight.size(0) : weight.size(1),
+                        (transposed_mode || q_int8) ? weight.size(0) : weight.size(1),
                         bsz,
-                        Context::Instance().GetCurrentStream());
+                        InferenceContext::Instance().GetCurrentStream());
     return torch::from_blob(workspace, input.sizes(), input.options());
 }
 
 template <typename T>
 std::vector<at::Tensor> ds_qkv_gemm(at::Tensor& input,
                                     at::Tensor& weight,
                                     at::Tensor& q_scale,
@@ -866,29 +887,39 @@
                                     at::Tensor& beta,
                                     const float epsilon,
                                     bool add_bias,
                                     unsigned num_layers,
                                     bool external_cache,
                                     unsigned mp_size,
                                     unsigned rank,
-                                    bool q_int8)
+                                    bool q_int8,
+                                    bool transposed_mode)
 {
     int bsz = input.size(0) * input.size(1);
-    T* workspace = (T*)Context::Instance().GetWorkSpace();
-    int out_size = q_int8 ? weight.size(0) : weight.size(1);
+    T* workspace = (T*)InferenceContext::Instance().GetWorkSpace();
+    int out_size = (transposed_mode || q_int8) ? weight.size(0) : weight.size(1);
 
     auto options = at::TensorOptions()
                        .dtype(input.options().dtype())
                        .layout(at::kStrided)
                        .device(at::kCUDA)
                        .requires_grad(false);
 
     auto output = at::from_blob(workspace, {input.size(0), input.size(1), out_size}, options);
-    auto inp_norm = qkv_unfused_cublas<T>(
-        output, input, weight, q_scale, bias, gamma, beta, epsilon, add_bias, q_int8);
+    auto inp_norm = qkv_unfused_cublas<T>(output,
+                                          input,
+                                          weight,
+                                          q_scale,
+                                          bias,
+                                          gamma,
+                                          beta,
+                                          epsilon,
+                                          add_bias,
+                                          q_int8,
+                                          transposed_mode);
 
     return {output, inp_norm};
 }
 
 template <typename T>
 void quantized_gemm(at::Tensor& output,
                     at::Tensor& input,
@@ -908,19 +939,19 @@
     launch_dequantize((T*)weight16.data_ptr(),
                       (int8_t*)weight.data_ptr(),
                       (float*)qscale.data_ptr(),
                       weight.size(0),
                       weight.size(1),
                       groups,
                       merge_count,
-                      Context::Instance().GetCurrentStream());
+                      InferenceContext::Instance().GetCurrentStream());
 
     float alpha = (T)1.0;
     float gemm_beta = (T)0.0;
-    cublas_gemm_ex(Context::Instance().GetCublasHandle(),
+    cublas_gemm_ex(InferenceContext::Instance().GetCublasHandle(),
                    CUBLAS_OP_T,
                    CUBLAS_OP_N,
                    weight.size(0),
                    bsz,
                    input.size(2),
                    &alpha,
                    &gemm_beta,
@@ -959,47 +990,49 @@
 
     quantized_gemm<T>(output, inp_norm, weight, q_scale, groups, 0);
     if (add_bias)
         launch_bias_add((T*)output.data_ptr(),
                         (T*)bias.data_ptr(),
                         weight.size(1),
                         bsz,
-                        Context::Instance().GetCurrentStream());
+                        InferenceContext::Instance().GetCurrentStream());
 
     return output;
 }
 
 template <typename T>
 at::Tensor ds_linear_layer(at::Tensor& input,
                            at::Tensor& weight,
                            at::Tensor& bias,
                            bool add_bias,
                            bool do_flash_attn,
-                           int num_heads)
+                           int num_heads,
+                           bool transposed_mode)
 {
     auto input_cont = input.contiguous();
     auto options = at::TensorOptions()
                        .dtype(input_cont.options().dtype())
                        .layout(at::kStrided)
                        .device(at::kCUDA)
                        .requires_grad(false);
 
     int head_size = input_cont.size(2) / num_heads;
     int bsz = input.size(0) * input.size(1);
-    T* workspace = (T*)Context::Instance().GetWorkSpace();
+    T* workspace = (T*)InferenceContext::Instance().GetWorkSpace();
     auto output = at::from_blob(workspace, {input.size(0), input.size(1), weight.size(1)}, options);
 
     float alpha = (T)1.0;
     float gemm_beta = (T)0.0;
-    cublasSetStream(Context::Instance().GetCublasHandle(), Context::Instance().GetCurrentStream());
+    cublasSetStream(InferenceContext::Instance().GetCublasHandle(),
+                    InferenceContext::Instance().GetCurrentStream());
 
-    cublas_gemm_ex(Context::Instance().GetCublasHandle(),
+    cublas_gemm_ex(InferenceContext::Instance().GetCublasHandle(),
+                   (transposed_mode ? CUBLAS_OP_T : CUBLAS_OP_N),
                    CUBLAS_OP_N,
-                   CUBLAS_OP_N,
-                   weight.size(1),
+                   weight.size(transposed_mode ? 0 : 1),
                    bsz,
                    input_cont.size(2),
                    &alpha,
                    &gemm_beta,
                    (T*)weight.data_ptr(),
                    (T*)input_cont.data_ptr(),
                    (T*)output.data_ptr(),
@@ -1007,30 +1040,30 @@
                    rocblas_gemm_algo_standard);
 #else
                    CUBLAS_GEMM_DEFAULT_TENSOR_OP);
 #endif
     if (add_bias)
         launch_bias_add((T*)output.data_ptr(),
                         (T*)bias.data_ptr(),
-                        weight.size(1),
+                        weight.size(transposed_mode ? 0 : 1),
                         bsz,
-                        Context::Instance().GetCurrentStream());
+                        InferenceContext::Instance().GetCurrentStream());
     bool add_padding = (head_size % 32 != 0 && head_size < 64) || (head_size % 64 != 0);
     if (do_flash_attn) {
         if (add_padding) {
             int padded_head_size = head_size < 32 ? 32 : (head_size < 64 ? 64 : 128);
             auto padded_output = workspace + output.numel();
             auto final_output =
                 padded_output + (input.size(0) * input.size(1) * 3 * num_heads * padded_head_size);
             pad_data(padded_output,
                      workspace,
                      3 * bsz * num_heads,
                      head_size,
                      padded_head_size,
-                     Context::Instance().GetCurrentStream());
+                     InferenceContext::Instance().GetCurrentStream());
 
             launch_bias_add_transform_0213<T>(
                 final_output,
                 final_output + (input.size(0) * input.size(1) * num_heads * padded_head_size),
                 final_output + (input.size(0) * input.size(1) * 2 * num_heads * padded_head_size),
                 padded_output,
                 nullptr,
@@ -1039,15 +1072,15 @@
                 0,
                 input.size(1),
                 (num_heads * padded_head_size),
                 num_heads,
                 -1,
                 false,
                 false,
-                Context::Instance().GetCurrentStream(),
+                InferenceContext::Instance().GetCurrentStream(),
                 3,
                 input.size(1));
             return at::from_blob(final_output,
                                  {3, input.size(0), num_heads, input.size(1), padded_head_size},
                                  options);
             // return at::from_blob(padded_output, {input.size(0) * input.size(1), 3, num_heads,
             // padded_head_size}, options);
@@ -1064,15 +1097,15 @@
                 0,
                 input.size(1),
                 input_cont.size(2),
                 num_heads,
                 -1,
                 false,
                 false,
-                Context::Instance().GetCurrentStream(),
+                InferenceContext::Instance().GetCurrentStream(),
                 3,
                 input.size(1));
             return at::from_blob(
                 final_output, {3, input.size(0), num_heads, input.size(1), head_size}, options);
             // return at::from_blob(workspace, {input.size(0) * input.size(1), 3, num_heads,
             // head_size}, options);
         }
@@ -1082,41 +1115,41 @@
 }
 
 template <typename T>
 std::vector<at::Tensor> add_padding(at::Tensor& query, at::Tensor& key, at::Tensor& value)
 {
     int head_size = query.size(3);
     int padded_head_size = head_size < 32 ? 32 : (head_size < 64 ? 64 : 128);
-    T* workspace = (T*)Context::Instance().GetWorkSpace();
+    T* workspace = (T*)InferenceContext::Instance().GetWorkSpace();
     T* key_pad_ptr = workspace + padded_head_size * query.size(0) * query.size(1) * query.size(2);
     T* value_pad_ptr = key_pad_ptr + padded_head_size * query.size(0) * query.size(1) * 128;
     pad_head_seq(workspace,
                  (T*)query.data_ptr(),
                  query.size(0) * query.size(1),
                  query.size(2),
                  query.size(2),
                  head_size,
                  padded_head_size,
-                 Context::Instance().GetCurrentStream());
+                 InferenceContext::Instance().GetCurrentStream());
     pad_head_seq(key_pad_ptr,
                  (T*)key.data_ptr(),
                  query.size(0) * query.size(1),
                  key.size(2),
                  128,
                  head_size,
                  padded_head_size,
-                 Context::Instance().GetCurrentStream());
+                 InferenceContext::Instance().GetCurrentStream());
     pad_head_seq(value_pad_ptr,
                  (T*)value.data_ptr(),
                  query.size(0) * query.size(1),
                  key.size(2),
                  128,
                  head_size,
                  padded_head_size,
-                 Context::Instance().GetCurrentStream());
+                 InferenceContext::Instance().GetCurrentStream());
     return {
         at::from_blob(workspace,
                       {query.size(0), query.size(1), query.size(2), padded_head_size},
                       query.options()),
         at::from_blob(
             key_pad_ptr, {query.size(0), query.size(1), 128, padded_head_size}, query.options()),
         at::from_blob(
@@ -1130,44 +1163,44 @@
                                            int heads,
                                            bool add_padding)
 {
     int head_size = query.size(2) / heads;
     int key_value_length = add_padding ? 128 : key.size(1);
     int padded_head_size = add_padding ? (head_size < 32 ? 32 : (head_size < 64 ? 64 : 128))
                                        : head_size;
-    T* workspace = (T*)Context::Instance().GetWorkSpace();
+    T* workspace = (T*)InferenceContext::Instance().GetWorkSpace();
     T* key_pad_ptr = workspace + padded_head_size * query.size(0) * heads * query.size(1);
     T* value_pad_ptr = key_pad_ptr + padded_head_size * query.size(0) * heads * key_value_length;
     launch_pad_add_transform_0213(workspace,
                                   (T*)query.data_ptr(),
                                   query.size(0),
                                   query.size(2),
                                   query.size(1),
                                   query.size(1),
                                   heads,
                                   padded_head_size,
-                                  Context::Instance().GetCurrentStream());
+                                  InferenceContext::Instance().GetCurrentStream());
     launch_pad_add_transform_0213(key_pad_ptr,
                                   (T*)key.data_ptr(),
                                   key.size(0),
                                   key.size(2),
                                   key.size(1),
                                   key_value_length,
                                   heads,
                                   padded_head_size,
-                                  Context::Instance().GetCurrentStream());
+                                  InferenceContext::Instance().GetCurrentStream());
     launch_pad_add_transform_0213(value_pad_ptr,
                                   (T*)value.data_ptr(),
                                   value.size(0),
                                   value.size(2),
                                   value.size(1),
                                   key_value_length,
                                   heads,
                                   padded_head_size,
-                                  Context::Instance().GetCurrentStream());
+                                  InferenceContext::Instance().GetCurrentStream());
     return {
         at::from_blob(
             workspace, {query.size(0), heads, query.size(1), padded_head_size}, query.options()),
         at::from_blob(key_pad_ptr,
                       {query.size(0), heads, key_value_length, padded_head_size},
                       query.options()),
         at::from_blob(value_pad_ptr,
@@ -1192,52 +1225,53 @@
     auto output = at::empty({input_cont.size(0), input_cont.size(1), weight.size(1)}, options);
 
     quantized_gemm<T>(output, input_cont, weight, q_scale, groups, 0);
     launch_bias_add((T*)output.data_ptr(),
                     (T*)bias.data_ptr(),
                     weight.size(1),
                     bsz,
-                    Context::Instance().GetCurrentStream());
+                    InferenceContext::Instance().GetCurrentStream());
     return output;
 }
 
 template <typename T>
 at::Tensor ds_vector_matmul(at::Tensor& input,
                             at::Tensor& weight,
                             bool async_op,
                             at::Tensor& q_scale,
-                            bool q_int8)
+                            bool q_int8,
+                            bool transposed_mode)
 {
     auto options = at::TensorOptions()
                        .dtype(input.options().dtype())
                        .layout(at::kStrided)
                        .device(at::kCUDA)
                        .requires_grad(false);
     int out_size = q_int8 ? weight.size(0) : weight.size(1);
     int bsz = input.size(0) * input.size(1);
 
-    T* workspace = (T*)Context::Instance().GetWorkSpace();
+    T* workspace = (T*)InferenceContext::Instance().GetWorkSpace();
     auto output = at::from_blob(workspace, {input.size(0), input.size(1), out_size}, options);
     if (q_int8) {
         quantized_gemm<T>(output.data_ptr(),
                           (T*)input.data_ptr(),
                           weight,
                           q_scale,
                           q_scale.size(0),
                           bsz,
                           input.size(2));
     } else {
         float alpha = (T)1.0;
         float gemm_beta = (T)0.0;
-        cublasSetStream(Context::Instance().GetCublasHandle(),
-                        Context::Instance().GetCurrentStream(async_op));
-        cublas_gemm_ex(Context::Instance().GetCublasHandle(),
-                       CUBLAS_OP_N,
+        cublasSetStream(InferenceContext::Instance().GetCublasHandle(),
+                        InferenceContext::Instance().GetCurrentStream(async_op));
+        cublas_gemm_ex(InferenceContext::Instance().GetCublasHandle(),
+                       (transposed_mode ? CUBLAS_OP_T : CUBLAS_OP_N),
                        CUBLAS_OP_N,
-                       weight.size(1),
+                       weight.size(transposed_mode ? 0 : 1),
                        bsz,
                        input.size(2),
                        &alpha,
                        &gemm_beta,
                        (T*)weight.data_ptr(),
                        (T*)input.data_ptr(),
                        (T*)output.data_ptr(),
@@ -1282,47 +1316,48 @@
                               at::Tensor& beta,
                               const float epsilon,
                               bool preLayerNorm,
                               bool mlp_after_attn,
                               at::Tensor& q_scale,
                               at::Tensor& q_scale1,
                               bool q_int8,
-                              ActivationFuncType act_func_type)
+                              ActivationFuncType act_func_type,
+                              bool transposed_mode)
 {
     int bsz = input.size(0) * input.size(1);
-    T* inp_norm =
-        (T*)Context::Instance().GetWorkSpace() + torch::numel(input) + torch::numel(output);
+    T* inp_norm = (T*)InferenceContext::Instance().GetWorkSpace() + torch::numel(input) +
+                  torch::numel(output);
     T* intermediate = inp_norm + torch::numel(input);
 
     if (mlp_after_attn) {
         launch_fused_residual_ln((T*)inp_norm,
                                  (const T*)input.data_ptr(),
                                  (const T*)residual.data_ptr(),
                                  (const T*)input_bias.data_ptr(),
                                  (const T*)gamma.data_ptr(),
                                  (const T*)beta.data_ptr(),
                                  epsilon,
                                  bsz,
                                  input.size(2),
-                                 Context::Instance().GetCurrentStream());
+                                 InferenceContext::Instance().GetCurrentStream());
     } else {
         ds_layer_norm_internal(inp_norm, input, gamma, beta, epsilon);
     }
     if (q_int8) {
         quantized_gemm<T>(
             intermediate, inp_norm, weight, q_scale, q_scale.size(0), bsz, input.size(2));
     } else {
         float alpha = (T)1.0;
         float gemm_beta = (T)0.0;
-        cublasSetStream(Context::Instance().GetCublasHandle(),
-                        Context::Instance().GetCurrentStream());
-        cublas_gemm_ex(Context::Instance().GetCublasHandle(),
-                       CUBLAS_OP_N,
+        cublasSetStream(InferenceContext::Instance().GetCublasHandle(),
+                        InferenceContext::Instance().GetCurrentStream());
+        cublas_gemm_ex(InferenceContext::Instance().GetCublasHandle(),
+                       (transposed_mode ? CUBLAS_OP_T : CUBLAS_OP_N),
                        CUBLAS_OP_N,
-                       weight.size(1),
+                       weight.size(transposed_mode ? 0 : 1),
                        bsz,
                        input.size(2),
                        &alpha,
                        &gemm_beta,
                        (T*)weight.data_ptr(),
                        inp_norm,
                        intermediate,
@@ -1331,44 +1366,44 @@
 #else
                        CUBLAS_GEMM_DEFAULT_TENSOR_OP);
 #endif
     }
     if (act_func_type == ActivationFuncType::GELU) {
         launch_bias_gelu(intermediate,
                          (T*)bias.data_ptr(),
-                         q_int8 ? weight.size(0) : weight.size(1),
+                         (transposed_mode || q_int8) ? weight.size(0) : weight.size(1),
                          bsz,
-                         Context::Instance().GetCurrentStream());
+                         InferenceContext::Instance().GetCurrentStream());
     } else if (act_func_type == ActivationFuncType::ReLU) {
         launch_bias_relu(intermediate,
                          (T*)bias.data_ptr(),
-                         q_int8 ? weight.size(0) : weight.size(1),
+                         (transposed_mode || q_int8) ? weight.size(0) : weight.size(1),
                          bsz,
-                         Context::Instance().GetCurrentStream());
+                         InferenceContext::Instance().GetCurrentStream());
     }
 
     if (q_int8) {
         quantized_gemm<T>(output.data_ptr(),
                           intermediate,
                           weight1,
                           q_scale1,
                           q_scale1.size(0),
                           bsz,
                           input.size(2));
     } else {
         float alpha = (T)1.0;
         float gemm_beta = (T)0.0;
-        cublasSetStream(Context::Instance().GetCublasHandle(),
-                        Context::Instance().GetCurrentStream());
-        cublas_gemm_ex(Context::Instance().GetCublasHandle(),
+        cublasSetStream(InferenceContext::Instance().GetCublasHandle(),
+                        InferenceContext::Instance().GetCurrentStream());
+        cublas_gemm_ex(InferenceContext::Instance().GetCublasHandle(),
+                       (transposed_mode ? CUBLAS_OP_T : CUBLAS_OP_N),
                        CUBLAS_OP_N,
-                       CUBLAS_OP_N,
-                       weight1.size(1),
+                       weight1.size(transposed_mode ? 0 : 1),
                        bsz,
-                       weight1.size(0),
+                       weight1.size(transposed_mode ? 1 : 0),
                        &alpha,
                        &gemm_beta,
                        (T*)weight1.data_ptr(),
                        intermediate,
                        (T*)output.data_ptr(),
 #ifdef __HIP_PLATFORM_HCC__
                        rocblas_gemm_algo_standard);
@@ -1391,26 +1426,28 @@
                                     at::Tensor& beta,
                                     const float epsilon,
                                     bool preLayerNorm,
                                     bool mlp_after_attn,
                                     at::Tensor& q_scale,
                                     at::Tensor& q_scale1,
                                     bool q_int8,
-                                    int activation_type)
+                                    int activation_type,
+                                    bool transposed_mode)
 {
     auto options = at::TensorOptions()
                        .dtype(input.options().dtype())
                        .layout(at::kStrided)
                        .device(at::kCUDA)
                        .requires_grad(false);
 
-    int out_size = q_int8 ? weight_out.size(0) : weight_out.size(1);
-    auto output = at::from_blob((T*)Context::Instance().GetWorkSpace() + torch::numel(input),
-                                {input.size(0), input.size(1), out_size},
-                                options);
+    int out_size = (q_int8 || transposed_mode) ? weight_out.size(0) : weight_out.size(1);
+    auto output =
+        at::from_blob((T*)InferenceContext::Instance().GetWorkSpace() + torch::numel(input),
+                      {input.size(0), input.size(1), out_size},
+                      options);
     int bsz = input.size(0) * input.size(1);
 
     auto act_func_type = static_cast<ActivationFuncType>(activation_type);
     auto res_add = mlp_unfused_cublas<T>(output,
                                          mlp_after_attn ? input : residual,
                                          residual,
                                          input_bias,
@@ -1421,15 +1458,16 @@
                                          beta,
                                          epsilon,
                                          preLayerNorm,
                                          mlp_after_attn,
                                          q_scale,
                                          q_scale1,
                                          q_int8,
-                                         act_func_type);
+                                         act_func_type,
+                                         transposed_mode);
 
     return {output, res_add};
 }
 
 template <typename T>
 std::vector<at::Tensor> ds_mlp_gemm_int8(at::Tensor& input,
                                          at::Tensor& residual,
@@ -1457,40 +1495,42 @@
 
     auto residual_add = (preLayerNorm ? at::empty_like(input_cont) : inp_norm);
     quantized_gemm<T>(output, inp_norm, weight, q_scale, groups, 0);
     launch_bias_gelu((T*)output.data_ptr(),
                      (T*)bias.data_ptr(),
                      weight.size(1),
                      bsz,
-                     Context::Instance().GetCurrentStream());
+                     InferenceContext::Instance().GetCurrentStream());
 
     return {output, residual_add};
 }
 
 template <typename T>
 at::Tensor fused_gemm_gelu(at::Tensor& input,
                            at::Tensor& weight,
                            at::Tensor& weight_scale,
                            at::Tensor& bias,
                            at::Tensor& weight_out,
                            at::Tensor& weight_out_scale,
                            const float epsilon,
                            bool preLayerNorm,
                            bool q_int8,
-                           bool async_op)
+                           bool async_op,
+                           bool transposed_mode)
 {
     auto options = at::TensorOptions()
                        .dtype(input.options().dtype())
                        .layout(at::kStrided)
                        .device(at::kCUDA)
                        .requires_grad(false);
 
-    int intm_dim = q_int8 ? weight.size(0) : weight.size(1);
+    int intm_dim = (transposed_mode || q_int8) ? weight.size(0) : weight.size(1);
 
-    // auto output = at::from_blob((T*)Context::Instance().GetWorkSpace() + torch::numel(input),
+    // auto output = at::from_blob((T*)InferenceContext::Instance().GetWorkSpace() +
+    // torch::numel(input),
     //                            {input.size(0), input.size(1), out_size},
     //                            options);
     // T* intermediate = (T*)input.data_ptr() + torch::numel(input);
     auto intermediate = at::empty({input.size(0), input.size(1), intm_dim}, options);
 
     int bsz = input.size(0) * input.size(1);
 
@@ -1501,18 +1541,18 @@
                           (T*)input.data_ptr(),
                           weight,
                           weight_scale,
                           weight_scale.size(0),
                           bsz,
                           input.size(2));
     } else {
-        cublasSetStream(Context::Instance().GetCublasHandle(),
-                        Context::Instance().GetCurrentStream());
-        cublas_gemm_ex(Context::Instance().GetCublasHandle(),
-                       CUBLAS_OP_N,
+        cublasSetStream(InferenceContext::Instance().GetCublasHandle(),
+                        InferenceContext::Instance().GetCurrentStream());
+        cublas_gemm_ex(InferenceContext::Instance().GetCublasHandle(),
+                       (transposed_mode ? CUBLAS_OP_T : CUBLAS_OP_N),
                        CUBLAS_OP_N,
                        intm_dim,
                        bsz,
                        input.size(2),
                        &alpha,
                        &gemm_beta,
                        (T*)weight.data_ptr(),
@@ -1524,29 +1564,29 @@
                        CUBLAS_GEMM_DEFAULT_TENSOR_OP);
 #endif
     }
     launch_bias_gelu((T*)intermediate.data_ptr(),
                      (T*)bias.data_ptr(),
                      intm_dim,
                      bsz,
-                     Context::Instance().GetCurrentStream());
+                     InferenceContext::Instance().GetCurrentStream());
 
-    int out_size = q_int8 ? weight_out.size(0) : weight_out.size(1);
+    int out_size = (transposed_mode || q_int8) ? weight_out.size(0) : weight_out.size(1);
     auto output = at::empty({input.size(0), input.size(1), out_size}, options);
     if (q_int8) {
         quantized_gemm<T>(output.data_ptr(),
                           (T*)intermediate.data_ptr(),
                           weight_out,
                           weight_out_scale,
                           weight_out_scale.size(0),
                           bsz,
                           input.size(2));
     } else {
-        cublas_gemm_ex(Context::Instance().GetCublasHandle(),
-                       CUBLAS_OP_N,
+        cublas_gemm_ex(InferenceContext::Instance().GetCublasHandle(),
+                       (transposed_mode ? CUBLAS_OP_T : CUBLAS_OP_N),
                        CUBLAS_OP_N,
                        out_size,
                        bsz,
                        intm_dim,
                        &alpha,
                        &gemm_beta,
                        (T*)weight_out.data_ptr(),
@@ -1554,16 +1594,16 @@
                        (T*)output.data_ptr(),
 #ifdef __HIP_PLATFORM_HCC__
                        rocblas_gemm_algo_standard);
 #else
                        CUBLAS_GEMM_DEFAULT_TENSOR_OP);
 #endif
     }
-    // cudaEventRecord(Context::Instance().GetCompEvent(2),
-    //                Context::Instance().GetCurrentStream(true));
+    // cudaEventRecord(InferenceContext::Instance().GetCompEvent(2),
+    //                InferenceContext::Instance().GetCurrentStream(true));
     return output;
 }
 
 template <typename T>
 at::Tensor& residual_add_bias(at::Tensor& hidden_state,
                               at::Tensor& residual,
                               const at::Tensor& attention_output,
@@ -1582,26 +1622,26 @@
                              static_cast<T*>(attention_output.data_ptr()),
                              static_cast<T*>(final_bias.data_ptr()),
                              static_cast<T*>(attention_bias.data_ptr()),
                              bsz,
                              hidden_size,
                              mp_size,
                              preln,
-                             Context::Instance().GetCurrentStream());
+                             InferenceContext::Instance().GetCurrentStream());
     else
         launch_gptj_residual_add<T>(
             static_cast<T*>(residual.data_ptr()),
             static_cast<T*>(hidden_state.data_ptr()),
             static_cast<T*>(attention_output.data_ptr()),
             static_cast<T*>(final_bias.data_ptr()),
             static_cast<T*>((add_bias ? attention_bias.data_ptr() : nullptr)),
             hidden_size,
             bsz,
             mp_size,
-            Context::Instance().GetCurrentStream());
+            InferenceContext::Instance().GetCurrentStream());
     return residual;
 }
 
 std::vector<at::Tensor> apply_rotary_pos_emb(at::Tensor& mixed_query,
                                              at::Tensor& key_layer,
                                              unsigned rotary_dim,
                                              unsigned offset,
@@ -1623,29 +1663,29 @@
                                            seq_len,
                                            rotary_dim,
                                            offset,
                                            num_heads,
                                            bsz,
                                            rotate_half,
                                            rotate_every_two,
-                                           Context::Instance().GetCurrentStream(),
-                                           Context::Instance().GetMaxTokenLenght());
+                                           InferenceContext::Instance().GetCurrentStream(),
+                                           InferenceContext::Instance().GetMaxTokenLenght());
     else
         launch_apply_rotary_pos_emb<__half>((__half*)query_cont.data_ptr(),
                                             (__half*)key_cont.data_ptr(),
                                             head_size,
                                             seq_len,
                                             rotary_dim,
                                             offset,
                                             num_heads,
                                             bsz,
                                             rotate_half,
                                             rotate_every_two,
-                                            Context::Instance().GetCurrentStream(),
-                                            Context::Instance().GetMaxTokenLenght());
+                                            InferenceContext::Instance().GetCurrentStream(),
+                                            InferenceContext::Instance().GetMaxTokenLenght());
     return {query_cont, key_cont};
 }
 
 template <typename T>
 at::Tensor fused_gemm_gelu_int8(at::Tensor& input,
                                 at::Tensor& weight,
                                 at::Tensor& bias,
@@ -1666,24 +1706,24 @@
     int bsz = input_cont.size(0) * input_cont.size(1);
 
     quantized_gemm<T>(output, input_cont, weight, q_scale, groups, 0);
     launch_bias_gelu((T*)output.data_ptr(),
                      (T*)bias.data_ptr(),
                      weight.size(1),
                      bsz,
-                     Context::Instance().GetCurrentStream());
+                     InferenceContext::Instance().GetCurrentStream());
 
     return output;
 }
 
 at::Tensor moe_res_matmul(at::Tensor& moe_res, at::Tensor& coef, at::Tensor& output)
 {
     int M = moe_res.size(0) * moe_res.size(1);
     int N = moe_res.size(2);
-    Context::Instance().SynchComm();
+    InferenceContext::Instance().SynchComm();
     if (moe_res.scalar_type() == at::kFloat) {
         launch_moe_res_matmul<float>((float*)moe_res.data_ptr(),
                                      (float*)coef.data_ptr(),
                                      (float*)output.data_ptr(),
                                      M,
                                      N,
                                      at::cuda::getCurrentCUDAStream());
@@ -1694,14 +1734,18 @@
                                       M,
                                       N,
                                       at::cuda::getCurrentCUDAStream());
     }
     return output;
 }
 
+void ds_release_workspace() { InferenceContext::Instance().release_workspace(); }
+
+bool ds_retake_workspace() { return InferenceContext::Instance().retake_workspace(); }
+
 PYBIND11_MODULE(TORCH_EXTENSION_NAME, m)
 {
     m.def("softmax_fp32", &ds_softmax<float>, "DeepSpeed SoftMax with fp32 (CUDA)");
     m.def("softmax_fp16", &ds_softmax<__half>, "DeepSpeed SoftMax with fp16 (CUDA)");
     m.def(
         "softmax_context_fp32", &ds_softmax_context<float>, "DeepSpeed attention with fp32 (CUDA)");
     m.def("softmax_context_fp16",
@@ -1773,8 +1817,10 @@
     m.def("allocate_workspace_fp32",
           &allocate_workspace<float>,
           "DeepSpeed memory allocation for GPT inference with fp32 (CUDA)");
     m.def("allocate_workspace_fp16",
           &allocate_workspace<__half>,
           "DeepSpeed memory allocation for GPT inference with fp16 (CUDA)");
     m.def("reset_cache", &reset_cache, "Reset Cache for generation tasks");
+    m.def("release_workspace", &ds_release_workspace, "DeepSpeed Release Workspace");
+    m.def("retake_workspace", &ds_retake_workspace, "DeepSpeed Retake Workspace");
 }
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/transformer/inference/csrc/relu.cu` & `deepspeed-0.9.0/deepspeed/ops/csrc/transformer/inference/csrc/relu.cu`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright 2022 The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #include "conversion_utils.h"
 #include "inference_cuda_layers.h"
 #include "memory_access_utils.h"
 
 namespace cg = cooperative_groups;
 #define MAX_CAP 4
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu` & `deepspeed-0.9.0/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright 2022 The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #ifndef __HIP_PLATFORM_HCC__
 #include <cuda_profiler_api.h>
 #endif
 #include "inference_cuda_layers.h"
 namespace cg = cooperative_groups;
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/transformer/inference/includes/inference_context.h` & `deepspeed-0.9.0/deepspeed/ops/csrc/transformer/inference/includes/inference_context.h`

 * *Files 10% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright 2022 The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #pragma once
 
 #include <c10/cuda/CUDAStream.h>
 #include <cuda_runtime_api.h>
 #include <cassert>
 #include <iostream>
@@ -41,78 +42,82 @@
 {
     return std::max(
         std::min((N + DS_CUDA_NUM_THREADS - 1) / DS_CUDA_NUM_THREADS, DS_MAXIMUM_NUM_BLOCKS),
         // Use at least 1 block, since CUDA does not allow empty block
         1);
 }
 
-class Context {
+class InferenceContext {
 public:
-    Context()
+    InferenceContext()
         : _workspace(nullptr),
           _seed(42),
           _curr_offset(0),
           _stream(0),
           _free_memory_size(0),
           _num_tokens(1),
-          _attention_unfused_workspace_offset(0)
+          _attention_unfused_workspace_offset(0),
+          _workSpaceSize(0)
     {
+        _workSpaceSize = 0;
+        _workspace = 0;
         if (cublasCreate(&_cublasHandle) != CUBLAS_STATUS_SUCCESS) {
             auto message = std::string("Fail to create cublas handle.");
             std::cerr << message << std::endl;
             throw std::runtime_error(message);
         }
 #ifndef __HIP_PLATFORM_HCC__
         cublasSetMathMode(_cublasHandle, CUBLAS_TENSOR_OP_MATH);
 #endif
         cudaEventCreate(&_comp1_event);
         cudaEventCreate(&_comp2_event);
         cudaEventCreate(&_comp_event);
         cudaEventCreate(&_comm_event);
     }
 
-    virtual ~Context()
+    virtual ~InferenceContext()
     {
         cublasDestroy(_cublasHandle);
         cudaFree(_workspace);
         cudaEventDestroy(_comp1_event);
         cudaEventDestroy(_comp2_event);
         cudaEventDestroy(_comp_event);
         cudaEventDestroy(_comm_event);
     }
 
-    static Context& Instance()
+    static InferenceContext& Instance()
     {
-        static Context _ctx;
+        static InferenceContext _ctx;
         return _ctx;
     }
 
     void GenWorkSpace(const unsigned& num_layers,
                       const unsigned& num_heads,
                       const size_t& batch_size,
                       const size_t& prompt_len,
                       const size_t& hidden_dim,
                       const unsigned& mp_size,
                       const bool& external_cache,
                       const size_t& elem_size,
                       const unsigned& rank,
-                      unsigned max_out_tokens)
+                      unsigned max_out_tokens,
+                      unsigned min_out_tokens)
     {
         size_t total_size;
         if (!_free_memory_size) { cudaMemGetInfo(&_free_memory_size, &total_size); }
 
         // Flash attention requires padded heads and we'll conservatively allocate
         // for that here. Flash attention is only enabled for head size <= 128 right now
         const int head_size = hidden_dim / num_heads;
         const int padded_head_size = head_size <= 32 ? 32 : (head_size <= 64 ? 64 : 128);
         const int effective_head_size = (head_size > 128) ? head_size : padded_head_size;
 
-        size_t activation_size = 16 * (num_heads * effective_head_size) * batch_size;
+        size_t activation_size = 10 * (num_heads * effective_head_size) * batch_size;
         // Other sequence length dimension is added when the final workSpaceSize is calculated
-        size_t temp_size = batch_size * num_heads * max_out_tokens * 2;
+        size_t temp_size = batch_size * (num_heads / mp_size) * max_out_tokens;
         size_t cache_size =
             num_layers * batch_size * ((num_heads * effective_head_size) / mp_size) * 2;
         size_t minimal_requirements =
             temp_size + (_free_memory_size > GIGABYTE ? 500 : 100) * MEGABYTE;
         if (_free_memory_size < minimal_requirements) {
             printf("Requested:\t%lu\nFree:\t%lu\nTotal:\t%lu\n",
                    minimal_requirements,
@@ -124,33 +129,45 @@
         _max_seq_len = ((_free_memory_size - minimal_requirements) / elem_size) /
                        (activation_size + temp_size + cache_size);
         _max_seq_len = std::min((size_t)max_out_tokens, _max_seq_len);
         size_t workSpaceSize = ((external_cache ? (activation_size + temp_size)
                                                 : (activation_size + temp_size + cache_size))) *
                                _max_seq_len * elem_size;
         temp_size *= _max_seq_len * elem_size;
-        if (rank == 0 && !_workspace)
+
+        if (_max_seq_len < min_out_tokens) {
+            printf(
+                "Allocatable workspace available (%d tokens) is less than minimum requested "
+                "workspace (%d tokens)\n",
+                _max_seq_len,
+                min_out_tokens);
+            throw std::runtime_error("Workspace can't be allocated, not enough memory");
+        }
+
+        if (!_workspace) {
+            assert(_workspace == nullptr);
+            cudaMalloc(&_workspace, workSpaceSize);
+        } else if (_workSpaceSize < workSpaceSize) {
+            cudaFree(_workspace);
+            cudaMalloc(&_workspace, workSpaceSize);
+        }
+        if (rank == 0 && (!_workspace || _workSpaceSize < workSpaceSize))
             printf(
                 "------------------------------------------------------\n"
                 "Free memory : %f (GigaBytes)  \n"
                 "Total memory: %f (GigaBytes)  \n"
                 "Requested memory: %f (GigaBytes) \n"
                 "Setting maximum total tokens (input + output) to %lu \n"
+                "WorkSpace: %p \n"
                 "------------------------------------------------------\n",
                 (float)_free_memory_size / GIGABYTE,
                 (float)total_size / GIGABYTE,
                 (float)workSpaceSize / GIGABYTE,
-                _max_seq_len);
-        if (!_workspace) {
-            assert(_workspace == nullptr);
-            cudaMalloc(&_workspace, workSpaceSize);
-        } else if (_workSpaceSize < workSpaceSize) {
-            cudaFree(_workspace);
-            cudaMalloc(&_workspace, workSpaceSize);
-        }
+                _max_seq_len,
+                _workspace);
 
         if (!_workspace) {
             printf("Requested:\t%lu\nFree:\t%lu\nTotal:\t%lu\n",
                    workSpaceSize,
                    _free_memory_size,
                    total_size);
             throw std::runtime_error("Workspace is null.");
@@ -198,14 +215,25 @@
             if (!_stream) _stream = at::cuda::getStreamFromPool(true);
             return _stream;
         }
         cudaStream_t stream = at::cuda::getCurrentCUDAStream();
         return stream;
     }
 
+    void release_workspace()
+    {
+        cudaFree(_workspace);
+        _workspace = nullptr;
+    }
+    bool retake_workspace()
+    {
+        if (_workspace != nullptr || _workSpaceSize == 0) return true;
+        cudaMalloc(&_workspace, _workSpaceSize);
+        return _workspace != nullptr;
+    }
     cublasHandle_t GetCublasHandle() { return _cublasHandle; }
 
     std::pair<uint64_t, uint64_t> IncrementOffset(uint64_t offset_inc)
     {
         uint64_t offset = _curr_offset;
         _curr_offset += offset_inc;
         return std::pair<uint64_t, uint64_t>(_seed, offset);
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/transformer/inference/includes/inference_cublas_wrappers.h` & `deepspeed-0.9.0/deepspeed/ops/csrc/transformer/inference/includes/inference_cublas_wrappers.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright 2022 The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #pragma once
 
 #include <assert.h>
 #include <cublas_v2.h>
 #include <cuda.h>
 #include <cuda_fp16.h>
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/transformer/inference/includes/inference_cuda_layers.h` & `deepspeed-0.9.0/deepspeed/ops/csrc/transformer/inference/includes/inference_cuda_layers.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright 2022 The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #pragma once
 
 #include "ds_kernel_utils.h"
 
 #include <cuda.h>
 #include <cuda_fp16.h>
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/transformer/normalize_kernels.cu` & `deepspeed-0.9.0/deepspeed/ops/csrc/transformer/normalize_kernels.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #include "custom_cuda_layers.h"
 
 namespace cg = cooperative_groups;
 
 /*
 Fused bias add, residual (elementwise) add, and normalization layer.
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/transformer/softmax_kernels.cu` & `deepspeed-0.9.0/deepspeed/ops/csrc/transformer/softmax_kernels.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #include <math.h>
 #include "custom_cuda_layers.h"
 #include "general_kernels.h"
 
 namespace cg = cooperative_groups;
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/transformer/transform_kernels.cu` & `deepspeed-0.9.0/deepspeed/ops/csrc/transformer/transform_kernels.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-/*
-Copyright The Microsoft DeepSpeed Team
-*/
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
 
 #include "custom_cuda_layers.h"
 
 #define rows_trans 16
 #define cols_trans 16
 
 template <typename T>
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/csrc/utils/flatten_unflatten.cpp` & `deepspeed-0.9.0/deepspeed/ops/csrc/utils/flatten_unflatten.cpp`

 * *Files 8% similar despite different names*

```diff
@@ -1,11 +1,15 @@
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
 /*
-   Copyright 2020 The Microsoft DeepSpeed Team
-   Copyright NVIDIA/apex
-   This file is adapted from fused adam in NVIDIA/apex, commit a109f85
+Copyright NVIDIA/apex
+This file is adapted from fused adam in NVIDIA/apex, commit a109f85
 */
 
 #include <torch/csrc/utils/tensor_flatten.h>
 #include <torch/extension.h>
 // https://github.com/pytorch/pytorch/blob/master/torch/csrc/utils/tensor_flatten.h
 
 at::Tensor flatten(std::vector<at::Tensor> tensors)
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/lamb/fused_lamb.py` & `deepspeed-0.9.0/deepspeed/ops/lamb/fused_lamb.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,13 +1,15 @@
-'''
-Copyright 2019 The Microsoft DeepSpeed Team
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
 
+# DeepSpeed Team
+"""
 Copyright NVIDIA/apex
 This file is adapted from NVIDIA/apex/optimizer/fused_adam and implements the LAMB optimizer
-'''
+"""
 import types
 import torch
 from deepspeed.ops.op_builder import FusedLambBuilder
 
 
 class FusedLamb(torch.optim.Optimizer):
     """Implements the LAMB algorithm. Currently GPU-only.
@@ -31,20 +33,20 @@
         weight_decay (float, optional): weight decay (L2 penalty) (default: 0)
         max_grad_norm (float, optional): value used to clip global grad norm
             (default: 0.0)
         max_coeff(float, optional): maximum value of the lamb coefficient (default: 10.0)
         min_coeff(float, optional): minimum value of the lamb coefficient (default: 0.01)
         amsgrad (boolean, optional): NOT SUPPORTED in FusedLamb!
     """
+
     def __init__(self,
                  params,
                  lr=1e-3,
                  bias_correction=True,
-                 betas=(0.9,
-                        0.999),
+                 betas=(0.9, 0.999),
                  eps=1e-8,
                  eps_inside_sqrt=False,
                  weight_decay=0.,
                  max_grad_norm=0.,
                  max_coeff=10.0,
                  min_coeff=0.01,
                  amsgrad=False):
@@ -60,20 +62,15 @@
                         max_grad_norm=max_grad_norm,
                         max_coeff=max_coeff,
                         min_coeff=min_coeff)
         super(FusedLamb, self).__init__(params, defaults)
         self.eps_mode = 0 if eps_inside_sqrt else 1
         self.lamb_coeffs = []
 
-    def step(self,
-             closure=None,
-             grads=None,
-             output_params=None,
-             scale=1.,
-             grad_norms=None):
+    def step(self, closure=None, grads=None, output_params=None, scale=1., grad_norms=None):
         """Performs a single optimization step.
 
         Arguments:
             closure (callable, optional): A closure that reevaluates the model
                 and returns the loss.
             grads (list of tensors, optional): weight gradient to use for the
                 optimizer update. If gradients have type torch.half, parameters
@@ -110,28 +107,30 @@
 
         if grad_norms is None:
             grad_norms = [None] * len(self.param_groups)
 
         #remove the previous coeffs
         del self.lamb_coeffs[:]
 
-        for group, grads_this_group, output_params_this_group, grad_norm_group in zip(self.param_groups, grads_group, output_params_group, grad_norms):
+        for group, grads_this_group, output_params_this_group, grad_norm_group in zip(
+                self.param_groups, grads_group, output_params_group, grad_norms):
             if grads_this_group is None:
                 grads_this_group = [None] * len(group['params'])
             if output_params_this_group is None:
                 output_params_this_group = [None] * len(group['params'])
 
             if grad_norm_group is None:
                 grad_norm_group = [None] * len(group['params'])
             elif not isinstance(grad_norm_group, list):
                 grad_norm_group = [grad_norm_group]
 
             bias_correction = 1 if group['bias_correction'] else 0
 
-            for p, grad, output_param, grad_norm in zip(group['params'], grads_this_group, output_params_this_group, grad_norm_group):
+            for p, grad, output_param, grad_norm in zip(group['params'], grads_this_group, output_params_this_group,
+                                                        grad_norm_group):
 
                 # compute combined scale factor for this group
                 combined_scale = scale
                 if group['max_grad_norm'] > 0:
                     # norm is in fact norm*scale
                     clip = ((grad_norm / scale) + 1e-6) / group['max_grad_norm']
                     if clip > 1:
@@ -158,32 +157,18 @@
                 exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']
                 beta1, beta2 = group['betas']
                 max_coeff = group['max_coeff']
                 min_coeff = group['min_coeff']
 
                 state['step'] += 1
 
-                out_p = torch.tensor(
-                    [],
-                    dtype=torch.float) if output_param is None else output_param
-                lamb_coeff = self.fused_lamb_cuda.lamb(p.data,
-                                                       out_p,
-                                                       exp_avg,
-                                                       exp_avg_sq,
-                                                       grad,
-                                                       group['lr'],
-                                                       beta1,
-                                                       beta2,
-                                                       max_coeff,
-                                                       min_coeff,
-                                                       group['eps'],
-                                                       combined_scale,
-                                                       state['step'],
-                                                       self.eps_mode,
-                                                       bias_correction,
+                out_p = torch.tensor([], dtype=torch.float) if output_param is None else output_param
+                lamb_coeff = self.fused_lamb_cuda.lamb(p.data, out_p, exp_avg, exp_avg_sq, grad, group['lr'], beta1,
+                                                       beta2, max_coeff, min_coeff, group['eps'], combined_scale,
+                                                       state['step'], self.eps_mode, bias_correction,
                                                        group['weight_decay'])
                 self.lamb_coeffs.append(lamb_coeff)
         return loss
 
     def get_lamb_coeffs(self):
         lamb_coeffs = [lamb_coeff.item() for lamb_coeff in self.lamb_coeffs]
         return lamb_coeffs
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/op_builder/__init__.py` & `deepspeed-0.9.0/deepspeed/ops/op_builder/__init__.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,12 @@
-"""
-Copyright 2020 The Microsoft DeepSpeed Team
-"""
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+
 import sys
 import os
 import pkgutil
 import importlib
 
 from .builder import get_default_compute_capabilities, OpBuilder
 
@@ -41,13 +43,11 @@
 
 
 # reflect builder names and add builder closure, such as 'TransformerBuilder()' creates op builder wrt current accelerator
 for _, module_name, _ in pkgutil.iter_modules([os.path.dirname(this_module.__file__)]):
     if module_name != 'all_ops' and module_name != 'builder':
         module = importlib.import_module(f".{module_name}", package=op_builder_dir)
         for member_name in module.__dir__():
-            if member_name.endswith(
-                    'Builder'
-            ) and member_name != "OpBuilder" and member_name != "CUDAOpBuilder":
+            if member_name.endswith('Builder') and member_name != "OpBuilder" and member_name != "CUDAOpBuilder":
                 # assign builder name to variable with same name
                 # the following is equivalent to i.e. TransformerBuilder = "TransformerBuilder"
                 this_module.__dict__[member_name] = builder_closure(member_name)
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/op_builder/all_ops.py` & `deepspeed-0.9.0/deepspeed/ops/op_builder/all_ops.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,10 +1,12 @@
-"""
-Copyright 2020 The Microsoft DeepSpeed Team
-"""
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+
 import os
 import pkgutil
 import importlib
 try:
     # during installation time accelerator is visible, otherwise return deepspeed.accelerator
     from accelerator import get_accelerator
 except ImportError:
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/op_builder/async_io.py` & `deepspeed-0.9.0/deepspeed/ops/op_builder/async_io.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,10 +1,12 @@
-"""
-Copyright 2020 The Microsoft DeepSpeed Team
-"""
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+
 import distutils.spawn
 import subprocess
 
 from .builder import OpBuilder
 
 
 class AsyncIOBuilder(OpBuilder):
@@ -15,22 +17,18 @@
         super().__init__(name=self.NAME)
 
     def absolute_name(self):
         return f'deepspeed.ops.aio.{self.NAME}_op'
 
     def sources(self):
         return [
-            'csrc/aio/py_lib/deepspeed_py_copy.cpp',
-            'csrc/aio/py_lib/py_ds_aio.cpp',
-            'csrc/aio/py_lib/deepspeed_py_aio.cpp',
-            'csrc/aio/py_lib/deepspeed_py_aio_handle.cpp',
-            'csrc/aio/py_lib/deepspeed_aio_thread.cpp',
-            'csrc/aio/common/deepspeed_aio_utils.cpp',
-            'csrc/aio/common/deepspeed_aio_common.cpp',
-            'csrc/aio/common/deepspeed_aio_types.cpp',
+            'csrc/aio/py_lib/deepspeed_py_copy.cpp', 'csrc/aio/py_lib/py_ds_aio.cpp',
+            'csrc/aio/py_lib/deepspeed_py_aio.cpp', 'csrc/aio/py_lib/deepspeed_py_aio_handle.cpp',
+            'csrc/aio/py_lib/deepspeed_aio_thread.cpp', 'csrc/aio/common/deepspeed_aio_utils.cpp',
+            'csrc/aio/common/deepspeed_aio_common.cpp', 'csrc/aio/common/deepspeed_aio_types.cpp',
             'csrc/aio/py_lib/deepspeed_pin_tensor.cpp'
         ]
 
     def include_paths(self):
         return ['csrc/aio/py_lib', 'csrc/aio/common']
 
     def cxx_args(self):
@@ -52,54 +50,42 @@
         ]
 
     def extra_ldflags(self):
         return ['-laio']
 
     def check_for_libaio_pkg(self):
         libs = dict(
-            dpkg=["-l",
-                  "libaio-dev",
-                  "apt"],
-            pacman=["-Q",
-                    "libaio",
-                    "pacman"],
-            rpm=["-q",
-                 "libaio-devel",
-                 "yum"],
+            dpkg=["-l", "libaio-dev", "apt"],
+            pacman=["-Q", "libaio", "pacman"],
+            rpm=["-q", "libaio-devel", "yum"],
         )
 
         found = False
         for pkgmgr, data in libs.items():
             flag, lib, tool = data
             path = distutils.spawn.find_executable(pkgmgr)
             if path is not None:
                 cmd = f"{pkgmgr} {flag} {lib}"
-                result = subprocess.Popen(cmd,
-                                          stdout=subprocess.PIPE,
-                                          stderr=subprocess.PIPE,
-                                          shell=True)
+                result = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)
                 if result.wait() == 0:
                     found = True
                 else:
-                    self.warning(
-                        f"{self.NAME}: please install the {lib} package with {tool}")
+                    self.warning(f"{self.NAME}: please install the {lib} package with {tool}")
                 break
         return found
 
     def is_compatible(self, verbose=True):
         # Check for the existence of libaio by using distutils
         # to compile and link a test program that calls io_submit,
         # which is a function provided by libaio that is used in the async_io op.
         # If needed, one can define -I and -L entries in CFLAGS and LDFLAGS
         # respectively to specify the directories for libaio.h and libaio.so.
         aio_compatible = self.has_function('io_submit', ('aio', ))
         if verbose and not aio_compatible:
-            self.warning(
-                f"{self.NAME} requires the dev libaio .so object and headers but these were not found."
-            )
+            self.warning(f"{self.NAME} requires the dev libaio .so object and headers but these were not found.")
 
             # Check for the libaio package via known package managers
             # to print suggestions on which package to install.
             self.check_for_libaio_pkg()
 
             self.warning(
                 "If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found."
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/op_builder/builder.py` & `deepspeed-0.9.0/deepspeed/ops/op_builder/builder.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,12 @@
-"""
-Copyright 2020 The Microsoft DeepSpeed Team
-"""
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+
 import os
 import sys
 import time
 import importlib
 from pathlib import Path
 import subprocess
 import shlex
@@ -23,46 +25,41 @@
 
 DEFAULT_TORCH_EXTENSION_PATH = "/tmp/torch_extensions"
 DEFAULT_COMPUTE_CAPABILITIES = "6.0;6.1;7.0"
 
 try:
     import torch
 except ImportError:
-    print(
-        f"{WARNING} unable to import torch, please install it if you want to pre-compile any deepspeed ops."
-    )
+    print(f"{WARNING} unable to import torch, please install it if you want to pre-compile any deepspeed ops.")
 else:
     TORCH_MAJOR = int(torch.__version__.split('.')[0])
     TORCH_MINOR = int(torch.__version__.split('.')[1])
 
 
 def installed_cuda_version(name=""):
     import torch.cuda
     if not torch.cuda.is_available():
         return 0, 0
     import torch.utils.cpp_extension
     cuda_home = torch.utils.cpp_extension.CUDA_HOME
     assert cuda_home is not None, "CUDA_HOME does not exist, unable to compile CUDA op(s)"
     # Ensure there is not a cuda version mismatch between torch and nvcc compiler
-    output = subprocess.check_output([cuda_home + "/bin/nvcc",
-                                      "-V"],
-                                     universal_newlines=True)
+    output = subprocess.check_output([cuda_home + "/bin/nvcc", "-V"], universal_newlines=True)
     output_split = output.split()
     release_idx = output_split.index("release")
     release = output_split[release_idx + 1].replace(',', '').split(".")
     # Ignore patch versions, only look at major + minor
     cuda_major, cuda_minor = release[:2]
     return int(cuda_major), int(cuda_minor)
 
 
 def get_default_compute_capabilities():
     compute_caps = DEFAULT_COMPUTE_CAPABILITIES
     import torch.utils.cpp_extension
-    if torch.utils.cpp_extension.CUDA_HOME is not None and installed_cuda_version(
-    )[0] >= 11:
+    if torch.utils.cpp_extension.CUDA_HOME is not None and installed_cuda_version()[0] >= 11:
         if installed_cuda_version()[0] == 11 and installed_cuda_version()[1] == 0:
             # Special treatment of CUDA 11.0 because compute_86 is not supported.
             compute_caps += ";8.0"
         else:
             compute_caps += ";8.0;8.6"
     return compute_caps
 
@@ -71,45 +68,35 @@
 # to build deepspeed and system-wide installed cuda 11.2
 cuda_minor_mismatch_ok = {
     10: [
         "10.0",
         "10.1",
         "10.2",
     ],
-    11: ["11.0",
-         "11.1",
-         "11.2",
-         "11.3",
-         "11.4",
-         "11.5",
-         "11.6",
-         "11.7",
-         "11.8"],
+    11: ["11.0", "11.1", "11.2", "11.3", "11.4", "11.5", "11.6", "11.7", "11.8"],
 }
 
 
 def assert_no_cuda_mismatch(name=""):
     cuda_major, cuda_minor = installed_cuda_version(name)
     if cuda_minor == 0 and cuda_major == 0:
         return False
     sys_cuda_version = f'{cuda_major}.{cuda_minor}'
     torch_cuda_version = ".".join(torch.version.cuda.split('.')[:2])
     # This is a show-stopping error, should probably not proceed past this
     if sys_cuda_version != torch_cuda_version:
-        if (cuda_major in cuda_minor_mismatch_ok
-                and sys_cuda_version in cuda_minor_mismatch_ok[cuda_major]
+        if (cuda_major in cuda_minor_mismatch_ok and sys_cuda_version in cuda_minor_mismatch_ok[cuda_major]
                 and torch_cuda_version in cuda_minor_mismatch_ok[cuda_major]):
             print(f"Installed CUDA version {sys_cuda_version} does not match the "
                   f"version torch was compiled with {torch.version.cuda} "
                   "but since the APIs are compatible, accepting this combination")
             return True
-        raise Exception(
-            f">- DeepSpeed Op Builder: Installed CUDA version {sys_cuda_version} does not match the "
-            f"version torch was compiled with {torch.version.cuda}, unable to compile "
-            "cuda/cpp extensions without a matching cuda version.")
+        raise Exception(f">- DeepSpeed Op Builder: Installed CUDA version {sys_cuda_version} does not match the "
+                        f"version torch was compiled with {torch.version.cuda}, unable to compile "
+                        "cuda/cpp extensions without a matching cuda version.")
     return True
 
 
 class OpBuilder(ABC):
     _rocm_version = None
     _is_rocm_pytorch = None
 
@@ -138,58 +125,54 @@
         pass
 
     @staticmethod
     def validate_torch_version(torch_info):
         install_torch_version = torch_info['version']
         current_torch_version = ".".join(torch.__version__.split('.')[:2])
         if install_torch_version != current_torch_version:
-            raise RuntimeError(
-                "PyTorch version mismatch! DeepSpeed ops were compiled and installed "
-                "with a different version than what is being used at runtime. "
-                f"Please re-install DeepSpeed or switch torch versions. "
-                f"Install torch version={install_torch_version}, "
-                f"Runtime torch version={current_torch_version}")
+            raise RuntimeError("PyTorch version mismatch! DeepSpeed ops were compiled and installed "
+                               "with a different version than what is being used at runtime. "
+                               f"Please re-install DeepSpeed or switch torch versions. "
+                               f"Install torch version={install_torch_version}, "
+                               f"Runtime torch version={current_torch_version}")
 
     @staticmethod
     def validate_torch_op_version(torch_info):
         if not OpBuilder.is_rocm_pytorch():
             current_cuda_version = ".".join(torch.version.cuda.split('.')[:2])
             install_cuda_version = torch_info['cuda_version']
             if install_cuda_version != current_cuda_version:
-                raise RuntimeError(
-                    "CUDA version mismatch! DeepSpeed ops were compiled and installed "
-                    "with a different version than what is being used at runtime. "
-                    f"Please re-install DeepSpeed or switch torch versions. "
-                    f"Install CUDA version={install_cuda_version}, "
-                    f"Runtime CUDA version={current_cuda_version}")
+                raise RuntimeError("CUDA version mismatch! DeepSpeed ops were compiled and installed "
+                                   "with a different version than what is being used at runtime. "
+                                   f"Please re-install DeepSpeed or switch torch versions. "
+                                   f"Install CUDA version={install_cuda_version}, "
+                                   f"Runtime CUDA version={current_cuda_version}")
         else:
             current_hip_version = ".".join(torch.version.hip.split('.')[:2])
             install_hip_version = torch_info['hip_version']
             if install_hip_version != current_hip_version:
-                raise RuntimeError(
-                    "HIP version mismatch! DeepSpeed ops were compiled and installed "
-                    "with a different version than what is being used at runtime. "
-                    f"Please re-install DeepSpeed or switch torch versions. "
-                    f"Install HIP version={install_hip_version}, "
-                    f"Runtime HIP version={current_hip_version}")
+                raise RuntimeError("HIP version mismatch! DeepSpeed ops were compiled and installed "
+                                   "with a different version than what is being used at runtime. "
+                                   f"Please re-install DeepSpeed or switch torch versions. "
+                                   f"Install HIP version={install_hip_version}, "
+                                   f"Runtime HIP version={current_hip_version}")
 
     @staticmethod
     def is_rocm_pytorch():
         if OpBuilder._is_rocm_pytorch is not None:
             return OpBuilder._is_rocm_pytorch
 
         _is_rocm_pytorch = False
         try:
             import torch
         except ImportError:
             pass
         else:
             if TORCH_MAJOR > 1 or (TORCH_MAJOR == 1 and TORCH_MINOR >= 5):
-                _is_rocm_pytorch = hasattr(torch.version,
-                                           'hip') and torch.version.hip is not None
+                _is_rocm_pytorch = hasattr(torch.version, 'hip') and torch.version.hip is not None
                 if _is_rocm_pytorch:
                     from torch.utils.cpp_extension import ROCM_HOME
                     _is_rocm_pytorch = ROCM_HOME is not None
         OpBuilder._is_rocm_pytorch = _is_rocm_pytorch
         return OpBuilder._is_rocm_pytorch
 
     @staticmethod
@@ -242,18 +225,15 @@
     def extra_ldflags(self):
         return []
 
     def libraries_installed(self, libraries):
         valid = False
         check_cmd = 'dpkg -l'
         for lib in libraries:
-            result = subprocess.Popen(f'dpkg -l {lib}',
-                                      stdout=subprocess.PIPE,
-                                      stderr=subprocess.PIPE,
-                                      shell=True)
+            result = subprocess.Popen(f'dpkg -l {lib}', stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)
             valid = valid or result.wait() == 0
         return valid
 
     def has_function(self, funcname, libraries, verbose=False):
         '''
         Test for existence of a function within a tuple of libraries.
 
@@ -276,17 +256,15 @@
             # Configure compiler and linker to build according to Python install.
             distutils.sysconfig.customize_compiler(compiler)
 
             # Create a temporary directory to hold test files.
             tempdir = tempfile.mkdtemp()
 
             # Define a simple C program that calls the function in question
-            prog = "void %s(void); int main(int argc, char** argv) { %s(); return 0; }" % (
-                funcname,
-                funcname)
+            prog = "void %s(void); int main(int argc, char** argv) { %s(); return 0; }" % (funcname, funcname)
 
             # Write the test program to a file.
             filename = os.path.join(tempdir, 'test.c')
             with open(filename, 'w') as f:
                 f.write(prog)
 
             # Redirect stderr file descriptor to a file to silence compile/link warnings.
@@ -299,24 +277,21 @@
             # Otherwise, a local directory will be used instead of tempdir
             drive, driveless_filename = os.path.splitdrive(filename)
             root_dir = driveless_filename[0] if os.path.isabs(driveless_filename) else ''
             output_dir = os.path.join(drive, root_dir)
 
             # Attempt to compile the C program into an object file.
             cflags = shlex.split(os.environ.get('CFLAGS', ""))
-            objs = compiler.compile([filename],
-                                    output_dir=output_dir,
-                                    extra_preargs=self.strip_empty_entries(cflags))
+            objs = compiler.compile([filename], output_dir=output_dir, extra_preargs=self.strip_empty_entries(cflags))
 
             # Attempt to link the object file into an executable.
             # Be sure to tack on any libraries that have been specified.
             ldflags = shlex.split(os.environ.get('LDFLAGS', ""))
             compiler.link_executable(objs,
-                                     os.path.join(tempdir,
-                                                  'a.out'),
+                                     os.path.join(tempdir, 'a.out'),
                                      extra_preargs=self.strip_empty_entries(ldflags),
                                      libraries=libraries)
 
             # Compile and link succeeded
             return True
 
         except CompileError:
@@ -352,45 +327,40 @@
             cpu_info = self._backup_cpuinfo()
             if cpu_info is None:
                 return "-march=native"
 
         try:
             cpu_info = get_cpu_info()
         except Exception as e:
-            self.warning(
-                f"{self.name} attempted to use `py-cpuinfo` but failed (exception type: {type(e)}, {e}), "
-                "falling back to `lscpu` to get this information.")
+            self.warning(f"{self.name} attempted to use `py-cpuinfo` but failed (exception type: {type(e)}, {e}), "
+                         "falling back to `lscpu` to get this information.")
             cpu_info = self._backup_cpuinfo()
             if cpu_info is None:
                 return "-march=native"
 
         if cpu_info['arch'].startswith('PPC_'):
             # gcc does not provide -march on PowerPC, use -mcpu instead
             return '-mcpu=native'
         return '-march=native'
 
     def is_cuda_enable(self):
         try:
             if torch.cuda.is_available():
                 return '-D__ENABLE_CUDA__'
         except:
-            print(
-                f"{WARNING} {self.name} torch.cuda is missing, only cpu ops can be compiled!"
-            )
+            print(f"{WARNING} {self.name} torch.cuda is missing, only cpu ops can be compiled!")
             return '-D__DISABLE_CUDA__'
         return '-D__DISABLE_CUDA__'
 
     def _backup_cpuinfo(self):
         # Construct cpu_info dict from lscpu that is similar to what py-cpuinfo provides
         if not self.command_exists('lscpu'):
-            self.warning(
-                f"{self.name} attempted to query 'lscpu' after failing to use py-cpuinfo "
-                "to detect the CPU architecture. 'lscpu' does not appear to exist on "
-                "your system, will fall back to use -march=native and non-vectorized execution."
-            )
+            self.warning(f"{self.name} attempted to query 'lscpu' after failing to use py-cpuinfo "
+                         "to detect the CPU architecture. 'lscpu' does not appear to exist on "
+                         "your system, will fall back to use -march=native and non-vectorized execution.")
             return None
         result = subprocess.check_output('lscpu', shell=True)
         result = result.decode('utf-8').strip().lower()
 
         cpu_info = {}
         cpu_info['arch'] = None
         cpu_info['flags'] = ""
@@ -414,17 +384,16 @@
             cpu_info = self._backup_cpuinfo()
             if cpu_info is None:
                 return '-D__SCALAR__'
 
         try:
             cpu_info = get_cpu_info()
         except Exception as e:
-            self.warning(
-                f"{self.name} attempted to use `py-cpuinfo` but failed (exception type: {type(e)}, {e}), "
-                "falling back to `lscpu` to get this information.")
+            self.warning(f"{self.name} attempted to use `py-cpuinfo` but failed (exception type: {type(e)}, {e}), "
+                         "falling back to `lscpu` to get this information.")
             cpu_info = self._backup_cpuinfo()
             if cpu_info is None:
                 return '-D__SCALAR__'
 
         if cpu_info['arch'] == 'X86_64':
             if 'avx512' in cpu_info['flags'] or 'avx512f' in cpu_info['flags']:
                 return '-D__AVX512__'
@@ -439,41 +408,36 @@
             cmds = [cmd]
         valid = False
         for cmd in cmds:
             result = subprocess.Popen(f'type {cmd}', stdout=subprocess.PIPE, shell=True)
             valid = valid or result.wait() == 0
 
         if not valid and len(cmds) > 1:
-            print(
-                f"{WARNING} {self.name} requires one of the following commands '{cmds}', but it does not exist!"
-            )
+            print(f"{WARNING} {self.name} requires one of the following commands '{cmds}', but it does not exist!")
         elif not valid and len(cmds) == 1:
-            print(
-                f"{WARNING} {self.name} requires the '{cmd}' command, but it does not exist!"
-            )
+            print(f"{WARNING} {self.name} requires the '{cmd}' command, but it does not exist!")
         return valid
 
     def warning(self, msg):
         self.error_log = f"{msg}"
         print(f"{WARNING} {msg}")
 
     def deepspeed_src_path(self, code_path):
         if os.path.isabs(code_path):
             return code_path
         else:
             return os.path.join(Path(__file__).parent.parent.absolute(), code_path)
 
     def builder(self):
         from torch.utils.cpp_extension import CppExtension
-        return CppExtension(
-            name=self.absolute_name(),
-            sources=self.strip_empty_entries(self.sources()),
-            include_dirs=self.strip_empty_entries(self.include_paths()),
-            extra_compile_args={'cxx': self.strip_empty_entries(self.cxx_args())},
-            extra_link_args=self.strip_empty_entries(self.extra_ldflags()))
+        return CppExtension(name=self.absolute_name(),
+                            sources=self.strip_empty_entries(self.sources()),
+                            include_dirs=self.strip_empty_entries(self.include_paths()),
+                            extra_compile_args={'cxx': self.strip_empty_entries(self.cxx_args())},
+                            extra_link_args=self.strip_empty_entries(self.extra_ldflags()))
 
     def load(self, verbose=True):
         from deepspeed.git_version_info import installed_ops, torch_info
         if installed_ops[self.name]:
             # Ensure the op we're about to load was compiled with the same
             # torch/cuda versions we are currently using at runtime.
             self.validate_torch_version(torch_info)
@@ -488,60 +452,56 @@
         if not self.is_compatible(verbose):
             raise RuntimeError(
                 f"Unable to JIT load the {self.name} op due to it not being compatible due to hardware/software issue. {self.error_log}"
             )
         try:
             import ninja  # noqa: F401
         except ImportError:
-            raise RuntimeError(
-                f"Unable to JIT load the {self.name} op due to ninja not being installed."
-            )
+            raise RuntimeError(f"Unable to JIT load the {self.name} op due to ninja not being installed.")
 
         if isinstance(self, CUDAOpBuilder) and not self.is_rocm_pytorch():
             self.build_for_cpu = not assert_no_cuda_mismatch(self.name)
 
         self.jit_mode = True
         from torch.utils.cpp_extension import load
 
         start_build = time.time()
         sources = [self.deepspeed_src_path(path) for path in self.sources()]
-        extra_include_paths = [
-            self.deepspeed_src_path(path) for path in self.include_paths()
-        ]
+        extra_include_paths = [self.deepspeed_src_path(path) for path in self.include_paths()]
 
         # Torch will try and apply whatever CCs are in the arch list at compile time,
         # we have already set the intended targets ourselves we know that will be
         # needed at runtime. This prevents CC collisions such as multiple __half
         # implementations. Stash arch list to reset after build.
         torch_arch_list = None
         if "TORCH_CUDA_ARCH_LIST" in os.environ:
             torch_arch_list = os.environ.get("TORCH_CUDA_ARCH_LIST")
             os.environ["TORCH_CUDA_ARCH_LIST"] = ""
 
-        op_module = load(
-            name=self.name,
-            sources=self.strip_empty_entries(sources),
-            extra_include_paths=self.strip_empty_entries(extra_include_paths),
-            extra_cflags=self.strip_empty_entries(self.cxx_args()),
-            extra_cuda_cflags=self.strip_empty_entries(self.nvcc_args()),
-            extra_ldflags=self.strip_empty_entries(self.extra_ldflags()),
-            verbose=verbose)
+        op_module = load(name=self.name,
+                         sources=self.strip_empty_entries(sources),
+                         extra_include_paths=self.strip_empty_entries(extra_include_paths),
+                         extra_cflags=self.strip_empty_entries(self.cxx_args()),
+                         extra_cuda_cflags=self.strip_empty_entries(self.nvcc_args()),
+                         extra_ldflags=self.strip_empty_entries(self.extra_ldflags()),
+                         verbose=verbose)
 
         build_duration = time.time() - start_build
         if verbose:
             print(f"Time to load {self.name} op: {build_duration} seconds")
 
         # Reset arch list so we are not silently removing it for other possible use cases
         if torch_arch_list:
             os.environ["TORCH_CUDA_ARCH_LIST"] = torch_arch_list
 
         return op_module
 
 
 class CUDAOpBuilder(OpBuilder):
+
     def compute_capability_args(self, cross_compile_archs=None):
         """
         Returns nvcc compute capability compile flags.
 
         1. `TORCH_CUDA_ARCH_LIST` takes priority over `cross_compile_archs`.
         2. If neither is set default compute capabilities will be used
         3. Under `jit_mode` compute capabilities of all visible cards will be used plus PTX
@@ -580,16 +540,15 @@
                 if cross_compile_archs is None:
                     cross_compile_archs = get_default_compute_capabilities()
             ccs = cross_compile_archs.split(';')
 
         ccs = self.filter_ccs(ccs)
         if len(ccs) == 0:
             raise RuntimeError(
-                f"Unable to load {self.name} op due to no compute capabilities remaining after filtering"
-            )
+                f"Unable to load {self.name} op due to no compute capabilities remaining after filtering")
 
         args = []
         for cc in ccs:
             num = cc[0] + cc[2]
             args.append(f'-gencode=arch=compute_{num},code=sm_{num}')
             if cc.endswith('+PTX'):
                 args.append(f'-gencode=arch=compute_{num},code=compute_{num}')
@@ -626,40 +585,41 @@
         else:
             from torch.utils.cpp_extension import CUDAExtension as ExtensionBuilder
 
         compile_args = {'cxx': self.strip_empty_entries(self.cxx_args())} if self.build_for_cpu else \
                        {'cxx': self.strip_empty_entries(self.cxx_args()), \
                            'nvcc': self.strip_empty_entries(self.nvcc_args())}
 
-        cuda_ext = ExtensionBuilder(
-            name=self.absolute_name(),
-            sources=self.strip_empty_entries(self.sources()),
-            include_dirs=self.strip_empty_entries(self.include_paths()),
-            libraries=self.strip_empty_entries(self.libraries_args()),
-            extra_compile_args=compile_args)
+        cuda_ext = ExtensionBuilder(name=self.absolute_name(),
+                                    sources=self.strip_empty_entries(self.sources()),
+                                    include_dirs=self.strip_empty_entries(self.include_paths()),
+                                    libraries=self.strip_empty_entries(self.libraries_args()),
+                                    extra_compile_args=compile_args)
 
         if self.is_rocm_pytorch():
             # hip converts paths to absolute, this converts back to relative
             sources = cuda_ext.sources
             curr_file = Path(__file__).parent.parent  # ds root
             for i in range(len(sources)):
                 src = Path(sources[i])
-                sources[i] = str(src.relative_to(curr_file))
+                if src.is_absolute():
+                    sources[i] = str(src.relative_to(curr_file))
+                else:
+                    sources[i] = str(src)
             cuda_ext.sources = sources
         return cuda_ext
 
     def hipify_extension(self):
         if self.is_rocm_pytorch():
             from torch.utils.hipify import hipify_python
             hipify_python.hipify(
                 project_directory=os.getcwd(),
                 output_directory=os.getcwd(),
                 header_include_dirs=self.include_paths(),
-                includes=[os.path.join(os.getcwd(),
-                                       '*')],
+                includes=[os.path.join(os.getcwd(), '*')],
                 extra_files=[os.path.abspath(s) for s in self.sources()],
                 show_detailed=True,
                 is_pytorch_extension=True,
                 hipify_extra_files_only=True,
             )
 
     def cxx_args(self):
@@ -671,31 +631,25 @@
     def nvcc_args(self):
         if self.build_for_cpu:
             return []
         args = ['-O3']
         if self.is_rocm_pytorch():
             ROCM_MAJOR, ROCM_MINOR = self.installed_rocm_version()
             args += [
-                '-std=c++14',
-                '-U__HIP_NO_HALF_OPERATORS__',
-                '-U__HIP_NO_HALF_CONVERSIONS__',
+                '-std=c++14', '-U__HIP_NO_HALF_OPERATORS__', '-U__HIP_NO_HALF_CONVERSIONS__',
                 '-U__HIP_NO_HALF2_OPERATORS__',
                 '-DROCM_VERSION_MAJOR=%s' % ROCM_MAJOR,
                 '-DROCM_VERSION_MINOR=%s' % ROCM_MINOR
             ]
         else:
             cuda_major, _ = installed_cuda_version()
             args += [
-                '-allow-unsupported-compiler' if sys.platform == "win32" else '',
-                '--use_fast_math',
-                '-std=c++17'
-                if sys.platform == "win32" and cuda_major > 10 else '-std=c++14',
-                '-U__CUDA_NO_HALF_OPERATORS__',
-                '-U__CUDA_NO_HALF_CONVERSIONS__',
-                '-U__CUDA_NO_HALF2_OPERATORS__'
+                '-allow-unsupported-compiler' if sys.platform == "win32" else '', '--use_fast_math',
+                '-std=c++17' if sys.platform == "win32" and cuda_major > 10 else '-std=c++14',
+                '-U__CUDA_NO_HALF_OPERATORS__', '-U__CUDA_NO_HALF_CONVERSIONS__', '-U__CUDA_NO_HALF2_OPERATORS__'
             ]
             if os.environ.get('DS_DEBUG_CUDA_BUILD', '0') == '1':
                 args.append('--ptxas-options=-v')
             args += self.compute_capability_args()
         return args
 
     def libraries_args(self):
@@ -705,14 +659,15 @@
         if sys.platform == "win32":
             return ['cublas', 'curand']
         else:
             return []
 
 
 class TorchCPUOpBuilder(CUDAOpBuilder):
+
     def extra_ldflags(self):
         if self.build_for_cpu:
             return ['-fopenmp']
 
         if not self.is_rocm_pytorch():
             return ['-lcurand']
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/op_builder/cpu_adagrad.py` & `deepspeed-0.9.0/deepspeed/ops/op_builder/cpu_adagrad.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,10 +1,12 @@
-"""
-Copyright 2020 The Microsoft DeepSpeed Team
-"""
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+
 import os
 from .builder import TorchCPUOpBuilder
 
 
 class CPUAdagradBuilder(TorchCPUOpBuilder):
     BUILD_VAR = "DS_BUILD_CPU_ADAGRAD"
     NAME = "cpu_adagrad"
@@ -34,17 +36,12 @@
         import torch
         if self.build_for_cpu:
             CUDA_INCLUDE = []
         elif not self.is_rocm_pytorch():
             CUDA_INCLUDE = [os.path.join(torch.utils.cpp_extension.CUDA_HOME, "include")]
         else:
             CUDA_INCLUDE = [
-                os.path.join(torch.utils.cpp_extension.ROCM_HOME,
-                             "include"),
-                os.path.join(torch.utils.cpp_extension.ROCM_HOME,
-                             "include",
-                             "rocrand"),
-                os.path.join(torch.utils.cpp_extension.ROCM_HOME,
-                             "include",
-                             "hiprand"),
+                os.path.join(torch.utils.cpp_extension.ROCM_HOME, "include"),
+                os.path.join(torch.utils.cpp_extension.ROCM_HOME, "include", "rocrand"),
+                os.path.join(torch.utils.cpp_extension.ROCM_HOME, "include", "hiprand"),
             ]
         return ['csrc/includes'] + CUDA_INCLUDE
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/op_builder/cpu_adam.py` & `deepspeed-0.9.0/deepspeed/ops/op_builder/cpu_adam.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,12 @@
-"""
-Copyright 2020 The Microsoft DeepSpeed Team
-"""
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+
 import os
 from .builder import TorchCPUOpBuilder
 
 
 class CPUAdamBuilder(TorchCPUOpBuilder):
     BUILD_VAR = "DS_BUILD_CPU_ADAM"
     NAME = "cpu_adam"
@@ -35,17 +37,12 @@
         import torch
         if self.build_for_cpu:
             CUDA_INCLUDE = []
         elif not self.is_rocm_pytorch():
             CUDA_INCLUDE = [os.path.join(torch.utils.cpp_extension.CUDA_HOME, "include")]
         else:
             CUDA_INCLUDE = [
-                os.path.join(torch.utils.cpp_extension.ROCM_HOME,
-                             "include"),
-                os.path.join(torch.utils.cpp_extension.ROCM_HOME,
-                             "include",
-                             "rocrand"),
-                os.path.join(torch.utils.cpp_extension.ROCM_HOME,
-                             "include",
-                             "hiprand"),
+                os.path.join(torch.utils.cpp_extension.ROCM_HOME, "include"),
+                os.path.join(torch.utils.cpp_extension.ROCM_HOME, "include", "rocrand"),
+                os.path.join(torch.utils.cpp_extension.ROCM_HOME, "include", "hiprand"),
             ]
         return ['csrc/includes'] + CUDA_INCLUDE
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/op_builder/fused_adam.py` & `deepspeed-0.9.0/deepspeed/ops/op_builder/fused_adam.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,10 +1,12 @@
-"""
-Copyright 2020 The Microsoft DeepSpeed Team
-"""
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+
 from .builder import CUDAOpBuilder
 
 import sys
 
 
 class FusedAdamBuilder(CUDAOpBuilder):
     BUILD_VAR = "DS_BUILD_FUSED_ADAM"
@@ -25,13 +27,11 @@
     def cxx_args(self):
         args = super().cxx_args()
         return args + self.version_dependent_macros()
 
     def nvcc_args(self):
         nvcc_flags = ['-O3'] + self.version_dependent_macros()
         if not self.is_rocm_pytorch():
-            nvcc_flags.extend([
-                '-allow-unsupported-compiler' if sys.platform == "win32" else '',
-                '-lineinfo',
-                '--use_fast_math'
-            ] + self.compute_capability_args())
+            nvcc_flags.extend(
+                ['-allow-unsupported-compiler' if sys.platform == "win32" else '', '-lineinfo', '--use_fast_math'] +
+                self.compute_capability_args())
         return nvcc_flags
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/op_builder/fused_lamb.py` & `deepspeed-0.9.0/deepspeed/ops/op_builder/fused_lamb.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,10 +1,12 @@
-"""
-Copyright 2020 The Microsoft DeepSpeed Team
-"""
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+
 from .builder import CUDAOpBuilder
 
 import sys
 
 
 class FusedLambBuilder(CUDAOpBuilder):
     BUILD_VAR = 'DS_BUILD_FUSED_LAMB'
@@ -26,18 +28,13 @@
         args = super().cxx_args()
         return args + self.version_dependent_macros()
 
     def nvcc_args(self):
         nvcc_flags = ['-O3'] + self.version_dependent_macros()
         if self.is_rocm_pytorch():
             ROCM_MAJOR, ROCM_MINOR = self.installed_rocm_version()
-            nvcc_flags += [
-                '-DROCM_VERSION_MAJOR=%s' % ROCM_MAJOR,
-                '-DROCM_VERSION_MINOR=%s' % ROCM_MINOR
-            ]
+            nvcc_flags += ['-DROCM_VERSION_MAJOR=%s' % ROCM_MAJOR, '-DROCM_VERSION_MINOR=%s' % ROCM_MINOR]
         else:
-            nvcc_flags.extend([
-                '-allow-unsupported-compiler' if sys.platform == "win32" else '',
-                '-lineinfo',
-                '--use_fast_math'
-            ] + self.compute_capability_args())
+            nvcc_flags.extend(
+                ['-allow-unsupported-compiler' if sys.platform == "win32" else '', '-lineinfo', '--use_fast_math'] +
+                self.compute_capability_args())
         return nvcc_flags
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/op_builder/quantizer.py` & `deepspeed-0.9.0/deepspeed/ops/op_builder/quantizer.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,8 +1,11 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 from .builder import CUDAOpBuilder
 
 
 class QuantizerBuilder(CUDAOpBuilder):
     BUILD_VAR = "DS_BUILD_QUANTIZER"
     NAME = "quantizer"
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/op_builder/random_ltd.py` & `deepspeed-0.9.0/deepspeed/ops/op_builder/random_ltd.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,10 +1,12 @@
-"""
-Copyright 2022 The Microsoft DeepSpeed Team
-"""
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+
 from .builder import CUDAOpBuilder
 
 
 class RandomLTDBuilder(CUDAOpBuilder):
     BUILD_VAR = "DS_BUILD_RANDOM_LTD"
     NAME = "random_ltd"
 
@@ -19,22 +21,17 @@
         if not self.is_rocm_pytorch():
             return ['-lcurand']
         else:
             return []
 
     def sources(self):
         return [
-            'csrc/random_ltd/pt_binding.cpp',
-            'csrc/random_ltd/gather_scatter.cu',
-            'csrc/random_ltd/slice_attn_masks.cu',
-            'csrc/random_ltd/token_sort.cu'
+            'csrc/random_ltd/pt_binding.cpp', 'csrc/random_ltd/gather_scatter.cu',
+            'csrc/random_ltd/slice_attn_masks.cu', 'csrc/random_ltd/token_sort.cu'
         ]
 
     def include_paths(self):
         includes = ['csrc/includes']
         if self.is_rocm_pytorch():
             from torch.utils.cpp_extension import ROCM_HOME
-            includes += [
-                '{}/hiprand/include'.format(ROCM_HOME),
-                '{}/rocrand/include'.format(ROCM_HOME)
-            ]
+            includes += ['{}/hiprand/include'.format(ROCM_HOME), '{}/rocrand/include'.format(ROCM_HOME)]
         return includes
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/op_builder/sparse_attn.py` & `deepspeed-0.9.0/deepspeed/ops/op_builder/sparse_attn.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,10 +1,12 @@
-"""
-Copyright 2020 The Microsoft DeepSpeed Team
-"""
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+
 from .builder import OpBuilder
 
 try:
     from packaging import version as pkg_version
 except ImportError:
     pkg_version = None
 
@@ -43,43 +45,37 @@
 
         # torch-cpu will not have a cuda version
         if torch.version.cuda is None:
             cuda_compatible = False
             self.warning(f"{self.NAME} cuda is not available from torch")
         else:
             major, minor = torch.version.cuda.split('.')[:2]
-            cuda_compatible = (int(major) == 10
-                               and int(minor) >= 1) or (int(major) >= 11)
+            cuda_compatible = (int(major) == 10 and int(minor) >= 1) or (int(major) >= 11)
             if not cuda_compatible:
                 self.warning(f"{self.NAME} requires CUDA version 10.1+")
 
         TORCH_MAJOR = int(torch.__version__.split('.')[0])
         TORCH_MINOR = int(torch.__version__.split('.')[1])
         torch_compatible = TORCH_MAJOR == 1 and TORCH_MINOR >= 5
         if not torch_compatible:
-            self.warning(
-                f'{self.NAME} requires a torch version >= 1.5 but detected {TORCH_MAJOR}.{TORCH_MINOR}'
-            )
+            self.warning(f'{self.NAME} requires a torch version >= 1.5 but detected {TORCH_MAJOR}.{TORCH_MINOR}')
 
         try:
             import triton
         except ImportError:
             # auto-install of triton is broken on some systems, reverting to manual install for now
             # see this issue: https://github.com/microsoft/DeepSpeed/issues/1710
-            self.warning(
-                f"please install triton==1.0.0 if you want to use sparse attention")
+            self.warning(f"please install triton==1.0.0 if you want to use sparse attention")
             return False
 
         if pkg_version:
             installed_triton = pkg_version.parse(triton.__version__)
             triton_mismatch = installed_triton != pkg_version.parse("1.0.0")
         else:
             installed_triton = triton.__version__
             triton_mismatch = installed_triton != "1.0.0"
 
         if triton_mismatch:
-            self.warning(
-                f"using untested triton version ({installed_triton}), only 1.0.0 is known to be compatible"
-            )
+            self.warning(f"using untested triton version ({installed_triton}), only 1.0.0 is known to be compatible")
             return False
 
         return super().is_compatible(verbose) and torch_compatible and cuda_compatible
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/op_builder/spatial_inference.py` & `deepspeed-0.9.0/deepspeed/ops/op_builder/spatial_inference.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,10 +1,12 @@
-'''
-Copyright 2022 The Microsoft DeepSpeed Team
-'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+
 from .builder import CUDAOpBuilder, installed_cuda_version
 
 
 class SpatialInferenceBuilder(CUDAOpBuilder):
     BUILD_VAR = "DS_BUILD_SPATIAL_INFERENCE"
     NAME = "spatial_inference"
 
@@ -15,27 +17,25 @@
     def absolute_name(self):
         return f'deepspeed.ops.spatial.{self.NAME}_op'
 
     def is_compatible(self, verbose=True):
         try:
             import torch
         except ImportError:
-            self.warning(
-                "Please install torch if trying to pre-compile inference kernels")
+            self.warning("Please install torch if trying to pre-compile inference kernels")
             return False
 
         cuda_okay = True
         if not self.is_rocm_pytorch() and torch.cuda.is_available():
             sys_cuda_major, _ = installed_cuda_version()
             torch_cuda_major = int(torch.version.cuda.split('.')[0])
             cuda_capability = torch.cuda.get_device_properties(0).major
             if cuda_capability >= 8:
                 if torch_cuda_major < 11 or sys_cuda_major < 11:
-                    self.warning(
-                        "On Ampere and higher architectures please use CUDA 11+")
+                    self.warning("On Ampere and higher architectures please use CUDA 11+")
                     cuda_okay = False
         return super().is_compatible(verbose) and cuda_okay
 
     def sources(self):
         return [
             'csrc/spatial/csrc/opt_bias_add.cu',
             'csrc/spatial/csrc/pt_binding.cpp',
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/op_builder/stochastic_transformer.py` & `deepspeed-0.9.0/deepspeed/ops/op_builder/stochastic_transformer.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,10 +1,12 @@
-"""
-Copyright 2020 The Microsoft DeepSpeed Team
-"""
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+
 from .transformer import TransformerBuilder
 
 
 class StochasticTransformerBuilder(TransformerBuilder):
     BUILD_VAR = "DS_BUILD_STOCHASTIC_TRANSFORMER"
     NAME = "stochastic_transformer"
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/op_builder/transformer.py` & `deepspeed-0.9.0/deepspeed/ops/op_builder/transformer.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,10 +1,12 @@
-"""
-Copyright 2020 The Microsoft DeepSpeed Team
-"""
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+
 from .builder import CUDAOpBuilder
 
 
 class TransformerBuilder(CUDAOpBuilder):
     BUILD_VAR = "DS_BUILD_TRANSFORMER"
     NAME = "transformer"
 
@@ -19,26 +21,19 @@
         if not self.is_rocm_pytorch():
             return ['-lcurand']
         else:
             return []
 
     def sources(self):
         return [
-            'csrc/transformer/ds_transformer_cuda.cpp',
-            'csrc/transformer/cublas_wrappers.cu',
-            'csrc/transformer/transform_kernels.cu',
-            'csrc/transformer/gelu_kernels.cu',
-            'csrc/transformer/dropout_kernels.cu',
-            'csrc/transformer/normalize_kernels.cu',
-            'csrc/transformer/softmax_kernels.cu',
-            'csrc/transformer/general_kernels.cu'
+            'csrc/transformer/ds_transformer_cuda.cpp', 'csrc/transformer/cublas_wrappers.cu',
+            'csrc/transformer/transform_kernels.cu', 'csrc/transformer/gelu_kernels.cu',
+            'csrc/transformer/dropout_kernels.cu', 'csrc/transformer/normalize_kernels.cu',
+            'csrc/transformer/softmax_kernels.cu', 'csrc/transformer/general_kernels.cu'
         ]
 
     def include_paths(self):
         includes = ['csrc/includes']
         if self.is_rocm_pytorch():
             from torch.utils.cpp_extension import ROCM_HOME
-            includes += [
-                '{}/hiprand/include'.format(ROCM_HOME),
-                '{}/rocrand/include'.format(ROCM_HOME)
-            ]
+            includes += ['{}/hiprand/include'.format(ROCM_HOME), '{}/rocrand/include'.format(ROCM_HOME)]
         return includes
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/op_builder/transformer_inference.py` & `deepspeed-0.9.0/deepspeed/ops/op_builder/transformer_inference.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,8 +1,11 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 from .builder import CUDAOpBuilder, installed_cuda_version
 
 
 class InferenceBuilder(CUDAOpBuilder):
     BUILD_VAR = "DS_BUILD_TRANSFORMER_INFERENCE"
     NAME = "transformer_inference"
@@ -14,32 +17,28 @@
     def absolute_name(self):
         return f'deepspeed.ops.transformer.inference.{self.NAME}_op'
 
     def is_compatible(self, verbose=True):
         try:
             import torch
         except ImportError:
-            self.warning(
-                "Please install torch if trying to pre-compile inference kernels")
+            self.warning("Please install torch if trying to pre-compile inference kernels")
             return False
 
         cuda_okay = True
         if not self.is_rocm_pytorch() and torch.cuda.is_available():
             sys_cuda_major, _ = installed_cuda_version()
             torch_cuda_major = int(torch.version.cuda.split('.')[0])
             cuda_capability = torch.cuda.get_device_properties(0).major
             if cuda_capability < 6:
-                self.warning(
-                    "NVIDIA Inference is only supported on Pascal and newer architectures"
-                )
+                self.warning("NVIDIA Inference is only supported on Pascal and newer architectures")
                 cuda_okay = False
             if cuda_capability >= 8:
                 if torch_cuda_major < 11 or sys_cuda_major < 11:
-                    self.warning(
-                        "On Ampere and higher architectures please use CUDA 11+")
+                    self.warning("On Ampere and higher architectures please use CUDA 11+")
                     cuda_okay = False
         return super().is_compatible(verbose) and cuda_okay
 
     def filter_ccs(self, ccs):
         ccs_retained = []
         ccs_pruned = []
         for cc in ccs:
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/quantizer/quantizer.py` & `deepspeed-0.9.0/deepspeed/ops/quantizer/quantizer.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,12 @@
-'''
-Copyright 2020 The Microsoft DeepSpeed Team
-'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+
 import torch
 
 from deepspeed.ops.op_builder import QuantizerBuilder
 
 # Cuda modules will be imported if needed
 quantizer_cuda_module = None
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/random_ltd/dropping_utils.py` & `deepspeed-0.9.0/deepspeed/ops/random_ltd/dropping_utils.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,12 @@
-"""
-Copyright 2022 The Microsoft DeepSpeed Team
-"""
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+
 import torch
 
 from deepspeed.ops.op_builder import RandomLTDBuilder
 """
 Returns:
     sampled_indices: [layers, batch_size, reserved_length]
     new_mask: [batch_size, 1, reserved_length, reserved_length]
@@ -19,17 +21,15 @@
                       layers: int = 1,
                       device: str = 'cpu',
                       attn_mask: torch.Tensor = None):
 
     prob_dist = torch.ones((layers * batch_size, seq_length), device=device)
     sampled_indices = torch.multinomial(prob_dist, reserved_length)
 
-    sampled_indices = sampled_indices.reshape(layers,
-                                              batch_size,
-                                              reserved_length).to(torch.int32)
+    sampled_indices = sampled_indices.reshape(layers, batch_size, reserved_length).to(torch.int32)
     global random_ltd_module
     if random_ltd_module is None:
         random_ltd_module = RandomLTDBuilder().load()
     sampled_indices = random_ltd_module.token_sort_(sampled_indices, seq_length)
 
     # Not certain the optimized kernel is actually better here, cause it kind of screws
     # with alignment right if the sequence length is not divisble by like 16
@@ -55,17 +55,15 @@
                        layers: int = 1,
                        device: str = 'cpu',
                        attn_mask: torch.Tensor = None):
     assert attn_mask is not None
     prob_dist = torch.ones((layers * batch_size, seq_length), device=device)
     sampled_indices = torch.multinomial(prob_dist, reserved_length)
 
-    sampled_indices = sampled_indices.reshape(layers,
-                                              batch_size,
-                                              reserved_length).to(torch.int32)
+    sampled_indices = sampled_indices.reshape(layers, batch_size, reserved_length).to(torch.int32)
     global random_ltd_module
     if random_ltd_module is None:
         random_ltd_module = RandomLTDBuilder().load()
 
     sampled_indices = random_ltd_module.token_sort_(sampled_indices, seq_length)
     dtype = sampled_indices.dtype
 
@@ -78,19 +76,17 @@
             tmp_mask_list.append(mask_tmp[:, :, :, sampled_indices[l][i]])
         new_mask.append(torch.cat(tmp_mask_list, dim=0))
 
     return sampled_indices.to(dtype), new_mask
 
 
 class GatherTokens(torch.autograd.Function):
+
     @staticmethod
-    def forward(ctx,
-                activations: torch.Tensor,
-                sorted_indices: torch.Tensor,
-                batch_first: bool):
+    def forward(ctx, activations: torch.Tensor, sorted_indices: torch.Tensor, batch_first: bool):
         global random_ltd_module
         if random_ltd_module is None:
             random_ltd_module = RandomLTDBuilder().load()
         ctx.save_for_backward(activations, sorted_indices)
         ctx.batch_first = batch_first
         return activations, random_ltd_module.token_gather(activations, sorted_indices, batch_first)
 
@@ -100,33 +96,26 @@
         g_gradients = g_gradients.contiguous()
         global random_ltd_module
         if random_ltd_module is None:
             random_ltd_module = RandomLTDBuilder().load()
         activations, sorted_indices = ctx.saved_tensors
         batch_first = ctx.batch_first
 
-        return random_ltd_module.token_scatter_(a_gradients,
-                                                g_gradients,
-                                                sorted_indices,
-                                                batch_first), None, None
+        return random_ltd_module.token_scatter_(a_gradients, g_gradients, sorted_indices, batch_first), None, None
 
 
 class ScatterTokens(torch.autograd.Function):
+
     @staticmethod
-    def forward(ctx,
-                all_activations: torch.Tensor,
-                layer_activations: torch.Tensor,
-                sorted_indices: torch.Tensor,
+    def forward(ctx, all_activations: torch.Tensor, layer_activations: torch.Tensor, sorted_indices: torch.Tensor,
                 batch_first: bool):
         global random_ltd_module
         if random_ltd_module is None:
             random_ltd_module = RandomLTDBuilder().load()
-        scatter_results = random_ltd_module.token_scatter_(all_activations.clone(),
-                                                           layer_activations,
-                                                           sorted_indices,
+        scatter_results = random_ltd_module.token_scatter_(all_activations.clone(), layer_activations, sorted_indices,
                                                            batch_first)
 
         ctx.save_for_backward(sorted_indices)
         ctx.batch_first = batch_first
         return scatter_results
 
     @staticmethod
@@ -135,11 +124,9 @@
         out_gradients = out_gradients.contiguous()
         global random_ltd_module
         if random_ltd_module is None:
             random_ltd_module = RandomLTDBuilder().load()
         sorted_indices, = ctx.saved_tensors
         batch_first = ctx.batch_first
 
-        ret_val = random_ltd_module.token_gather(out_gradients,
-                                                 sorted_indices,
-                                                 batch_first)
+        ret_val = random_ltd_module.token_gather(out_gradients, sorted_indices, batch_first)
         return out_gradients, ret_val, None, None
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/sparse_attention/bert_sparse_self_attention.py` & `deepspeed-0.9.0/deepspeed/ops/sparse_attention/bert_sparse_self_attention.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,22 +1,24 @@
-"""
-Copyright 2020 The Microsoft DeepSpeed Team
-"""
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 from torch import nn
 from deepspeed.ops.sparse_attention import SparseSelfAttention, FixedSparsityConfig
 
 
 class BertSparseSelfAttention(nn.Module):
     """Implements Sparse Self Attention layer of Bert model based on https://github.com/microsoft/DeepSpeedExamples/blob/master/bing_bert/nvidia/modelingpreln.py#L373
 
     For more information please see, TODO DeepSpeed Sparse Transformer.
 
     For usage example please see, TODO DeepSpeed Sparse Transformer Tutorial.
     """
+
     def __init__(
         self,
         config,
         # SparsityConfig parameters needs to be set accordingly
         sparsity_config=FixedSparsityConfig(num_heads=4)):
         """Initialize the bert sparse self attention layer.
 
@@ -25,31 +27,28 @@
         Arguments:
             config: required: Bert model config
             sparsity_config: optional: this parameter determines sparsity pattern configuration; it is based on FixedSparsityConfig class.
         """
 
         super(BertSparseSelfAttention, self).__init__()
         if config.hidden_size % config.num_attention_heads != 0:
-            raise ValueError(
-                "The hidden size (%d) is not a multiple of the number of attention "
-                "heads (%d)" % (config.hidden_size,
-                                config.num_attention_heads))
+            raise ValueError("The hidden size (%d) is not a multiple of the number of attention "
+                             "heads (%d)" % (config.hidden_size, config.num_attention_heads))
         self.num_attention_heads = config.num_attention_heads
         self.attention_head_size = int(config.hidden_size / config.num_attention_heads)
         self.all_head_size = self.num_attention_heads * self.attention_head_size
 
         self.query = nn.Linear(config.hidden_size, self.all_head_size)
         self.key = nn.Linear(config.hidden_size, self.all_head_size)
         self.value = nn.Linear(config.hidden_size, self.all_head_size)
 
         self.sparse_self_attention = SparseSelfAttention(sparsity_config)
 
     def transpose_for_scores(self, x):
-        new_x_shape = x.size()[:-1] + (self.num_attention_heads,
-                                       self.attention_head_size)
+        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
         x = x.view(*new_x_shape)
         return x.permute(0, 2, 1, 3)
 
     def forward(self, hidden_states, attention_mask):
         """Applies forward phase of bert sparse self attention
 
         Arguments:
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/sparse_attention/matmul.py` & `deepspeed-0.9.0/deepspeed/ops/sparse_attention/matmul.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,44 +1,26 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 # DeepSpeed note, code taken & adapted from commit 9aa94789f13ada713af36cfd8cca2fc9a7f6b79a
 # https://github.com/ptillet/torch-blocksparse/blob/master/torch_blocksparse/matmul.py
 import importlib
 import torch
 
 import triton
 import triton.language as tl
 import triton._C.libtriton as libtriton
 from deepspeed.accelerator import get_accelerator
 
 
 @triton.jit
-def _kernel(A,
-            B,
-            C,
-            stride_za,
-            stride_ha,
-            stride_ma,
-            stride_ka,
-            stride_zb,
-            stride_hb,
-            stride_kb,
-            stride_nb,
-            stride_zc,
-            stride_hc,
-            stride_mc,
-            stride_nc,
-            DS0,
-            DS1,
-            SDD_K,
-            SDD_off_width,
-            lut,
-            locks,
-            nlocks,
-            **meta):
+def _kernel(A, B, C, stride_za, stride_ha, stride_ma, stride_ka, stride_zb, stride_hb, stride_kb, stride_nb, stride_zc,
+            stride_hc, stride_mc, stride_nc, DS0, DS1, SDD_K, SDD_off_width, lut, locks, nlocks, **meta):
     TM = meta['TM']
     TN = meta['TN']
     TK = meta['TK']
     TZ = meta['TZ']
     BLOCK = meta['BLOCK']
     #------------#
     #- Prologue -#
@@ -190,16 +172,15 @@
 
     pc = C + offpc + offhc * stride_hc + pidz * stride_zc + rcm[:, None] * stride_mc + rcn[None, :] * stride_nc
     # write-back directly
     if lockid == 0:
         tl.store(pc, c, mask=checkc)
     # accumulate partial results using spin-locks
     else:
-        plock = locks + tl.program_id(2) * nlocks * tl.num_programs(1) + tl.program_id(
-            1) * nlocks + lockid - 1
+        plock = locks + tl.program_id(2) * nlocks * tl.num_programs(1) + tl.program_id(1) * nlocks + lockid - 1
         pcount = plock + tl.num_programs(2) * tl.num_programs(1) * nlocks
         while tl.atomic_cas(plock, 0, 1) == 1:
             pass
         count = tl.load(pcount)
         if count == 0:
             tl.store(pc, c, mask=checkc)
         else:
@@ -288,18 +269,15 @@
     @staticmethod
     def make_sdd_lut(layout, block, dtype, device):
         #_sparse_matmul._load_utils()
         #start_width = 64 // block
         #segmented = _sparse_matmul.sdd_segment(layout.type(torch.int32), start_width)
         start_width = (128 if block > 16 else 32) // block
         layout = layout.type(torch.int32)
-        segmented = libtriton.superblock(layout.data_ptr(),
-                                         layout.shape[0],
-                                         layout.shape[1],
-                                         layout.shape[2],
+        segmented = libtriton.superblock(layout.data_ptr(), layout.shape[0], layout.shape[1], layout.shape[2],
                                          start_width)
         luts, widths, packs = [], [], []
         for size, nnz in segmented:
             """ width = nnz.shape[0] // (size * size)
             h = nnz[:, 0]
             i = nnz[:, 1]
             j = nnz[:, 2]
@@ -313,59 +291,41 @@
             luts.append(torch.from_numpy(nnz).type(torch.int32).to(device))
             widths.append(width)
             packs.append(size)
         # create locks
         return luts, None, widths, packs
 
     @staticmethod
-    def _sdd_matmul(a,
-                    b,
-                    trans_a,
-                    trans_b,
-                    trans_c,
-                    spdims,
-                    block,
-                    luts,
-                    num_locks,
-                    widths,
-                    packs,
-                    bench,
-                    time):
+    def _sdd_matmul(a, b, trans_a, trans_b, trans_c, spdims, block, luts, num_locks, widths, packs, bench, time):
         if trans_c:
             a, b = b, a
             trans_a, trans_b = not trans_b, not trans_a
         AS0 = a.size(0)
         # Shape check
         a_dim = -2 if trans_a else -1
         b_dim = -1 if trans_b else -2
         a_inner, b_inner = a.shape[a_dim], b.shape[b_dim]
         if a_inner != b_inner:
-            raise ValueError(
-                f"Size of tensor A along the {a_dim} dim ({a_inner}) must match size "
-                f"of tensor B along the {b_dim} dim ({b_inner})")
+            raise ValueError(f"Size of tensor A along the {a_dim} dim ({a_inner}) must match size "
+                             f"of tensor B along the {b_dim} dim ({b_inner})")
         if a_inner % 16 != 0:
             raise ValueError('Reduction size for SDD must be a multiple of 16')
 
         batch_size = a.size(0)
         a_outer = a.size(3 if trans_a else 2)
         dtype = a.dtype
         is_16_multiple = a_inner % 16 == 0
         is_32_multiple = a_inner % 32 == 0
         is_64_multiple = a_inner % 64 == 0
         if not is_16_multiple:
             raise ValueError('Reduction size for SDD must be a multiple of 16')
         device = a.device
         # create kernel
         total_width = sum([width * pack * pack for width, pack in zip(widths, packs)])
-        c = torch.empty((batch_size,
-                         total_width,
-                         block,
-                         block),
-                        dtype=dtype,
-                        device=a.device)
+        c = torch.empty((batch_size, total_width, block, block), dtype=dtype, device=a.device)
         for lut, width, pack in zip(luts, widths, packs):
             F32TK = [8, 16]
             F16TK = [16]
             F16TK += [32] if is_32_multiple else []
             F16TK += [64] if is_64_multiple else []
             TK = {torch.float32: F32TK, torch.float16: F16TK}[dtype]
             num_lock = 1
@@ -383,20 +343,15 @@
             locks = _sparse_matmul.get_locks(2 * width * AS0 * num_lock, a.device)
             # maximum grid size is 65535
             # so operation might be decomposed into multiple
             # kernel calls
             max_width = 49152
             total = 0 if bench else None
             for off_width in range(0, width, max_width):
-                grid = lambda meta: [
-                    meta['TZ'],
-                    min(max_width,
-                        width - off_width),
-                    batch_size
-                ]
+                grid = lambda meta: [meta['TZ'], min(max_width, width - off_width), batch_size]
                 _kernel[grid](a,
                               b,
                               c,
                               a.stride(0),
                               a.stride(1),
                               a.stride(3 if trans_a else 2),
                               a.stride(2 if trans_a else 3),
@@ -500,68 +455,41 @@
         wincs = wincs.view(-1)
         # adjust offset and segment size
         offsets *= 2 * div
         segments *= div
         # create header
         width = column.size(0)
         offsets += 6 * width
-        header = torch.stack((offsets,
-                              segments,
-                              column,
-                              depth,
-                              lockid,
-                              maxid),
-                             dim=1).view(-1).contiguous()
+        header = torch.stack((offsets, segments, column, depth, lockid, maxid), dim=1).view(-1).contiguous()
         incs = torch.stack((xincs, wincs), dim=1).view(-1).contiguous()
         incs = torch.cat((incs, torch.zeros(2, device=incs.device, dtype=incs.dtype)))
         # create lut
         lut = torch.cat((header, incs))
         lut = lut.type(torch.int32).to(device)
         # create locks
         num_locks = max(1, lockid.max())
         return lut, num_locks, width, None
 
     @staticmethod
-    def _dds_matmul(a,
-                    b,
-                    trans_a,
-                    trans_b,
-                    trans_c,
-                    spdims,
-                    block,
-                    lut,
-                    num_locks,
-                    width,
-                    packs,
-                    bench,
-                    time):
+    def _dds_matmul(a, b, trans_a, trans_b, trans_c, spdims, block, lut, num_locks, width, packs, bench, time):
         global triton
         if triton is None:
             triton = importlib.import_module('triton')
 
         # shapes / dtypes
         AS0 = a.size(0)
         AS1 = a.size(1)
         AS2 = a.size(3 if trans_a else 2)
         AS3 = a.size(2 if trans_a else 3)
         BS0 = spdims[0]
         BS1 = block * spdims[2 if trans_b else 1]
         BS2 = block * spdims[1 if trans_b else 2]
         dtype = a.dtype
         # kernel
-        meta = {
-            'TN': block,
-            'TM': 128,
-            'TK': 16,
-            'BLOCK': block,
-            'TZ': 1,
-            'SDD': False,
-            'DSD': False,
-            'DDS': True
-        }
+        meta = {'TN': block, 'TM': 128, 'TK': 16, 'BLOCK': block, 'TZ': 1, 'SDD': False, 'DSD': False, 'DDS': True}
         # output
         CS0 = AS0
         CS1 = AS1
         CS2 = BS2 if trans_c else AS2
         CS3 = AS2 if trans_c else BS2
         locks = _sparse_matmul.get_locks(2 * AS0 * AS2 // 32 * num_locks, a.device)
         c = torch.empty((CS0, CS1, CS2, CS3), dtype=dtype, device=a.device)
@@ -589,27 +517,15 @@
                       locks,
                       num_locks,
                       num_warps=4,
                       **meta)
         return c
 
     @staticmethod
-    def _dsd_matmul(a,
-                    b,
-                    trans_a,
-                    trans_b,
-                    trans_c,
-                    spdims,
-                    block,
-                    lut,
-                    num_locks,
-                    width,
-                    packs,
-                    bench,
-                    time):
+    def _dsd_matmul(a, b, trans_a, trans_b, trans_c, spdims, block, lut, num_locks, width, packs, bench, time):
         global triton
         if triton is None:
             triton = importlib.import_module('triton')
 
         # shapes / dtypes
         AS0 = spdims[0]
         AS1 = block * spdims[2 if trans_a else 1]
@@ -617,24 +533,15 @@
         BS0 = b.size(0)
         BS1 = b.size(1)
         BS2 = b.size(3 if trans_b else 2)
         BS3 = b.size(2 if trans_b else 3)
         dtype = a.dtype
         # kernel
 
-        meta = {
-            'TM': block,
-            'TN': 128,
-            'TK': 16,
-            'BLOCK': block,
-            'TZ': 1,
-            'SDD': False,
-            'DSD': True,
-            'DDS': False
-        }
+        meta = {'TM': block, 'TN': 128, 'TK': 16, 'BLOCK': block, 'TZ': 1, 'SDD': False, 'DSD': True, 'DDS': False}
         # output
         CS0 = BS0
         CS1 = BS1
         CS2 = BS3 if trans_c else AS1
         CS3 = AS1 if trans_c else BS3
         locks = _sparse_matmul.get_locks(2 * BS0 * BS3 // 32 * num_locks, a.device)
         c = torch.empty((CS0, CS1, CS2, CS3), dtype=dtype, device=a.device)
@@ -661,61 +568,22 @@
                       lut,
                       locks,
                       num_locks,
                       num_warps=4,
                       **meta)
         return c
 
-    fn = {
-        'sdd': _sdd_matmul.__get__(object),
-        'dsd': _dsd_matmul.__get__(object),
-        'dds': _dds_matmul.__get__(object)
-    }
+    fn = {'sdd': _sdd_matmul.__get__(object), 'dsd': _dsd_matmul.__get__(object), 'dds': _dds_matmul.__get__(object)}
 
     @staticmethod
-    def forward(ctx,
-                a,
-                b,
-                trans_a,
-                trans_b,
-                trans_c,
-                mode,
-                spdims,
-                block,
-                c_lut,
-                c_num_locks,
-                c_width,
-                c_packs,
-                c_bench,
-                c_time,
-                da_lut,
-                da_num_locks,
-                da_width,
-                da_packs,
-                da_bench,
-                da_time,
-                db_lut,
-                db_num_locks,
-                db_width,
-                db_packs,
-                db_bench,
-                db_time):
-        c = _sparse_matmul.fn[mode](a,
-                                    b,
-                                    trans_a,
-                                    trans_b,
-                                    trans_c,
-                                    spdims,
-                                    block,
-                                    c_lut,
-                                    c_num_locks,
-                                    c_width,
-                                    c_packs,
-                                    c_bench,
-                                    c_time)
+    def forward(ctx, a, b, trans_a, trans_b, trans_c, mode, spdims, block, c_lut, c_num_locks, c_width, c_packs,
+                c_bench, c_time, da_lut, da_num_locks, da_width, da_packs, da_bench, da_time, db_lut, db_num_locks,
+                db_width, db_packs, db_bench, db_time):
+        c = _sparse_matmul.fn[mode](a, b, trans_a, trans_b, trans_c, spdims, block, c_lut, c_num_locks, c_width,
+                                    c_packs, c_bench, c_time)
         # save for backward
         ctx.save_for_backward(a, b)
         ctx.da_num_locks = da_num_locks
         ctx.da_lut = da_lut
         ctx.da_width = da_width
         ctx.da_packs = da_packs
         ctx.da_bench = da_bench
@@ -737,42 +605,22 @@
     def backward(ctx, dc):
         # saved for backward
         a, b = ctx.saved_tensors
         mode = ctx.mode
         # gradients w.r.t. a
         if ctx.needs_input_grad[0]:
             mode_da = mode[1] + mode[0] + mode[2]
-            da = _sparse_matmul.fn[mode_da](dc,
-                                            b,
-                                            False,
-                                            not ctx.trans_b,
-                                            ctx.trans_a,
-                                            ctx.spdims,
-                                            ctx.block,
-                                            ctx.da_lut,
-                                            ctx.da_num_locks,
-                                            ctx.da_width,
-                                            ctx.da_packs,
-                                            ctx.da_bench,
+            da = _sparse_matmul.fn[mode_da](dc, b, False, not ctx.trans_b, ctx.trans_a, ctx.spdims, ctx.block,
+                                            ctx.da_lut, ctx.da_num_locks, ctx.da_width, ctx.da_packs, ctx.da_bench,
                                             ctx.da_time)
         # gradients w.r.t. b
         if ctx.needs_input_grad[1]:
             mode_db = mode[2] + mode[1] + mode[0]
-            db = _sparse_matmul.fn[mode_db](a,
-                                            dc,
-                                            not ctx.trans_a,
-                                            False,
-                                            ctx.trans_b,
-                                            ctx.spdims,
-                                            ctx.block,
-                                            ctx.db_lut,
-                                            ctx.db_num_locks,
-                                            ctx.db_width,
-                                            ctx.db_packs,
-                                            ctx.db_bench,
+            db = _sparse_matmul.fn[mode_db](a, dc, not ctx.trans_a, False, ctx.trans_b, ctx.spdims, ctx.block,
+                                            ctx.db_lut, ctx.db_num_locks, ctx.db_width, ctx.db_packs, ctx.db_bench,
                                             ctx.db_time)
         return da, db, None, None, None,\
                None, None, None, None,\
                None, None, None, None, None, None,\
                None, None, None, None, None, None,\
                None, None, None, None, None, None
 
@@ -781,41 +629,46 @@
     """Block-Sparse MatMul class; this class handles three types of matrix-multiplication:
        - sparse = dense X dense
        - dense = sparse X dense
        - dense = dense X sparse
 
     For more details about sparsity config, please see `Generative Modeling with Sparse Transformers`: https://arxiv.org/abs/1904.10509
     """
+
     def make_lut(self, dtype, device):
         """Generates the sparsity layout/s used in block-sparse matmul
         """
         key = (dtype, device)
         if key in self.lut_cache:
             return self.lut_cache[key]
         # C look-up table
         layout, block = self.layout, self.block
         step = 16
         if self.mode == 'sdd':
             c_lut, c_num_locks, c_width, c_packs = _sparse_matmul.make_sdd_lut(layout, block, dtype, device)
         elif self.mode == 'dsd':
-            c_lut, c_num_locks, c_width, c_packs = _sparse_matmul.make_dxx_lut(layout, block, step, not self.trans_a, device)
+            c_lut, c_num_locks, c_width, c_packs = _sparse_matmul.make_dxx_lut(layout, block, step, not self.trans_a,
+                                                                               device)
         elif self.mode == 'dds':
-            c_lut, c_num_locks, c_width, c_packs = _sparse_matmul.make_dxx_lut(layout, block, step, self.trans_b, device)
+            c_lut, c_num_locks, c_width, c_packs = _sparse_matmul.make_dxx_lut(layout, block, step, self.trans_b,
+                                                                               device)
         # DA look-up table
         if self.mode == 'sdd':
             da_lut, da_num_locks, da_width, da_packs = _sparse_matmul.make_dxx_lut(layout, block, step, True, device)
         elif self.mode == 'dsd':
             da_lut, da_num_locks, da_width, da_packs = _sparse_matmul.make_sdd_lut(layout, block, dtype, device)
         elif self.mode == 'dds':
-            da_lut, da_num_locks, da_width, da_packs = _sparse_matmul.make_dxx_lut(layout, block, step, not self.trans_b, device)
+            da_lut, da_num_locks, da_width, da_packs = _sparse_matmul.make_dxx_lut(layout, block, step,
+                                                                                   not self.trans_b, device)
         # DB look-up table
         if self.mode == 'sdd':
             db_lut, db_num_locks, db_width, db_packs = _sparse_matmul.make_dxx_lut(layout, block, step, False, device)
         elif self.mode == 'dsd':
-            db_lut, db_num_locks, db_width, db_packs = _sparse_matmul.make_dxx_lut(layout, block, step, self.trans_a, device)
+            db_lut, db_num_locks, db_width, db_packs = _sparse_matmul.make_dxx_lut(layout, block, step, self.trans_a,
+                                                                                   device)
         elif self.mode == 'dds':
             db_lut, db_num_locks, db_width, db_packs = _sparse_matmul.make_sdd_lut(layout, block, dtype, device)
         self.lut_cache[key] = (c_lut, c_num_locks, c_width, c_packs,\
                                da_lut, da_num_locks, da_width, da_packs,\
                                db_lut, db_num_locks, db_width, db_packs)
         return self.lut_cache[key]
 
@@ -841,31 +694,29 @@
         self.mode = mode
         self.block = block
         self.layout = layout
         layout_dim = layout.ndim
         assert layout_dim in (2, 3), "Layout should be a 2 or 3 dimensional tensor of 0s and 1s"
         if not mode == 'sdd':
             # Dims to be reduced on the 'inside' of the matmul, either -1 or -2
-            trans_dense, trans_sparse, sparse_inner = (trans_b, trans_a, -1) if mode == 'dsd' else (trans_a, trans_b, -2)
-            self.dense_inner_dim = -(
-                (sparse_inner % 2) + 1) if not trans_dense else sparse_inner
-            sparse_inner = sparse_inner if not trans_sparse else -(
-                (sparse_inner % 2) + 1)
+            trans_dense, trans_sparse, sparse_inner = (trans_b, trans_a, -1) if mode == 'dsd' else (trans_a, trans_b,
+                                                                                                    -2)
+            self.dense_inner_dim = -((sparse_inner % 2) + 1) if not trans_dense else sparse_inner
+            sparse_inner = sparse_inner if not trans_sparse else -((sparse_inner % 2) + 1)
 
             # Inner dim of the dense input should be equal to the inner dim of the sparse input
             self.dense_inner_size = layout.shape[sparse_inner] * block
             # Expected shape for sparse inputs
             self.sparse_shape = (layout.sum().item(), block, block)
 
         # Support using the same layout across attention heads etc.
         if layout_dim == 2:
             layout = layout.unsqueeze(0)
 
-        layout = layout.long(
-        )  # Above code assumes the layout tensor is an integral type
+        layout = layout.long()  # Above code assumes the layout tensor is an integral type
 
         self.spdims = layout.shape
         # timings
         self.bench = bench
         self.time_c = None
         self.time_da = None
         self.time_db = None
@@ -905,91 +756,63 @@
         a, b = self._validate_inputs(a, b)
 
         # pad shapes with ones
         a = MatMul._pad_shape(a, self.mode == 'dsd')
         b = MatMul._pad_shape(b, self.mode == 'dds')
         # execute
 
-        c = _sparse_matmul.apply(a,
-                                 b,
-                                 self.trans_a,
-                                 self.trans_b,
-                                 False,
-                                 self.mode,
-                                 self.spdims,
-                                 self.block,
-                                 c_lut,
-                                 c_num_locks,
-                                 c_width,
-                                 c_packs,
-                                 self.bench,
-                                 time_c,
-                                 da_lut,
-                                 da_num_locks,
-                                 da_width,
-                                 da_packs,
-                                 self.bench,
-                                 time_da,
-                                 db_lut,
-                                 db_num_locks,
-                                 db_width,
-                                 db_packs,
-                                 self.bench,
+        c = _sparse_matmul.apply(a, b, self.trans_a, self.trans_b, False, self.mode, self.spdims, self.block, c_lut,
+                                 c_num_locks, c_width, c_packs, self.bench, time_c, da_lut, da_num_locks, da_width,
+                                 da_packs, self.bench, time_da, db_lut, db_num_locks, db_width, db_packs, self.bench,
                                  time_db)
 
         # This removes any leading singleton dimensions we may have added to the tensor that weren't in the input
         dims_to_trim = c.ndim - original_dims
         for _ in range(dims_to_trim):
             c = c.squeeze(0)
 
         self.time_c = time_c[0]
         self.time_da = time_da[0]
         self.time_db = time_db[0]
         return c
 
     def _validate_inputs(self, a, b):
         if a.device != b.device:
-            raise ValueError(
-                f"Inputs must be on the same device; got {a.device} for tensor A "
-                f"and {b.device} for tensor B")
+            raise ValueError(f"Inputs must be on the same device; got {a.device} for tensor A "
+                             f"and {b.device} for tensor B")
         if not get_accelerator().on_accelerator(a):
             raise ValueError("Only GPU devices are supported for now")
 
         # When autocast is enabled, torch.matmul autocasts to float16, so we do the same here
         if torch.is_autocast_enabled():
             a, b = a.half(), b.half()
         elif a.dtype != b.dtype:
-            raise ValueError(
-                f"Inputs must be the same dtype; got {a.dtype} for A and {b.dtype} for B"
-            )
+            raise ValueError(f"Inputs must be the same dtype; got {a.dtype} for A and {b.dtype} for B")
 
         mode, trans_a, trans_b = self.mode, self.trans_a, self.trans_b
         if mode != 'sdd':
             # One input is sparse
             dense, dense_name, sparse, sparse_name = (a, 'A', b, 'B') if mode == 'dds' else (b, 'B', a, 'A')
             dense_inner = dense.shape[self.dense_inner_dim]
             if dense_inner != self.dense_inner_size:
-                raise ValueError(
-                    f"Expected tensor {dense_name} to have size {self.dense_inner_size} at dim "
-                    f"{self.dense_inner_dim % dense.ndim}, got {dense_inner}.")
+                raise ValueError(f"Expected tensor {dense_name} to have size {self.dense_inner_size} at dim "
+                                 f"{self.dense_inner_dim % dense.ndim}, got {dense_inner}.")
 
             if sparse.shape[-len(self.sparse_shape):] != self.sparse_shape:
-                raise ValueError(
-                    f"Expected tensor with trailing dimensions of shape {self.sparse_shape} for argument "
-                    f"{sparse_name}, got {sparse.shape}")
+                raise ValueError(f"Expected tensor with trailing dimensions of shape {self.sparse_shape} for argument "
+                                 f"{sparse_name}, got {sparse.shape}")
 
         def add_extra_dims(x):
             # Add extra leading singleton dimensions if needed
             dims_needed = 4 - x.ndim
             if dims_needed > 0:
                 singletons = [1] * dims_needed
                 x = x.view(*singletons, *x.shape)
             elif dims_needed < 0:
-                raise ValueError(
-                    "Tensors with more than 4 dimensions are not currently supported")
+                raise ValueError("Tensors with more than 4 dimensions are not currently supported")
 
             return x
 
         # Pad shapes with leading singleton dimensions
         a = add_extra_dims(a)
         b = add_extra_dims(b)
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/sparse_attention/softmax.py` & `deepspeed-0.9.0/deepspeed/ops/sparse_attention/softmax.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,8 +1,11 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 # DeepSpeed note, code taken & adapted from commit 9aa94789f13ada713af36cfd8cca2fc9a7f6b79a
 # https://github.com/ptillet/torch-blocksparse/blob/master/torch_blocksparse/matmul.py
 
 import torch
 
 import triton
@@ -24,37 +27,19 @@
     if n < 512:
         return 4
     if n < 2048:
         return 8
     return 16
 
 
-@triton.heuristics({
-    'num_warps': lambda *args,
-    **meta: num_warps(args[6] * meta['BLOCK'])
-})
-@triton.heuristics({
-    'TN': lambda *args,
-    **meta: next_power_of_2(args[6] * meta['BLOCK'])
-})
+@triton.heuristics({'num_warps': lambda *args, **meta: num_warps(args[6] * meta['BLOCK'])})
+@triton.heuristics({'TN': lambda *args, **meta: next_power_of_2(args[6] * meta['BLOCK'])})
 @triton.jit
-def _forward(X,
-             scale,
-             LUT,
-             RPE,
-             KP_M,
-             ATTN_M,
-             sizemax,
-             stride_zx,
-             stride_zrpe,
-             stride_hrpe,
-             stride_srpe,
-             stride_zkpm,
-             stride_zattnm,
-             **meta):
+def _forward(X, scale, LUT, RPE, KP_M, ATTN_M, sizemax, stride_zx, stride_zrpe, stride_hrpe, stride_srpe, stride_zkpm,
+             stride_zattnm, **meta):
     TN = meta['TN']
     BLOCK = meta['BLOCK']
     pidhm = tl.program_id(0)
     pidz = tl.program_id(1)
     # create index ranges
     rxm = pidhm % BLOCK
     rbm = pidhm // BLOCK
@@ -98,22 +83,16 @@
             attn_m = tl.where(attn_m == 0, -float('inf'), 0.)
         x = x + attn_m
     # computation
     x = tl.softmax(x)
     tl.store(px, x, mask=check)
 
 
-@triton.heuristics({
-    'num_warps': lambda *args,
-    **meta: num_warps(args[4] * meta['BLOCK'])
-})
-@triton.heuristics({
-    'TN': lambda *args,
-    **meta: next_power_of_2(args[4]) * meta['BLOCK']
-})
+@triton.heuristics({'num_warps': lambda *args, **meta: num_warps(args[4] * meta['BLOCK'])})
+@triton.heuristics({'TN': lambda *args, **meta: next_power_of_2(args[4]) * meta['BLOCK']})
 @triton.jit
 def _backward(X, scale, DX, LUT, sizemax, stride_zx, stride_zdx, **meta):
     pidhm = tl.program_id(0)
     pidz = tl.program_id(1)
     TN = meta['TN']
     BLOCK = meta['BLOCK']
     # create index ranges
@@ -164,29 +143,16 @@
         # construct look-up table
         offsets = offsets * 4 + 2 * sizes.numel()
         header = torch.stack((sizes, offsets), dim=1).view(-1)
         lut = torch.cat((header, core)).type(torch.int32).to(device)
         return lut, int(sizes.max())
 
     @staticmethod
-    def forward(ctx,
-                x,
-                scale,
-                rpe,
-                key_padding_mask,
-                attn_mask,
-                kp_mask_mode,
-                attn_mask_mode,
-                spdims,
-                block,
-                lut,
-                num_blocks,
-                maxlut,
-                bench,
-                time):
+    def forward(ctx, x, scale, rpe, key_padding_mask, attn_mask, kp_mask_mode, attn_mask_mode, spdims, block, lut,
+                num_blocks, maxlut, bench, time):
 
         apply_scale = False if scale == 1.0 else True
 
         # handle None rpe
         if rpe is None:
             apply_rpe = False
             stride_zrpe, stride_hrpe, stride_srpe = 0, 0, 0
@@ -247,44 +213,36 @@
     def backward(ctx, dx):
 
         # retrieve from context
         x, lut = ctx.saved_tensors
         # run kernel
         M = x.shape[0]
         grid = lambda opt: [ctx.spdims[0] * ctx.spdims[1] * ctx.block, M]
-        _backward[grid](x,
-                        ctx.scale,
-                        dx,
-                        lut,
-                        ctx.maxlut,
-                        x.stride(0),
-                        dx.stride(0),
-                        BLOCK=ctx.block)
+        _backward[grid](x, ctx.scale, dx, lut, ctx.maxlut, x.stride(0), dx.stride(0), BLOCK=ctx.block)
         return dx, None, None, None, None, None, None, None, None, None, None, None, None, None, None
 
 
 class Softmax:
     """Block-Sparse Softmax class; this class computes softmax on a block sparse matrix. It is also able to apply either/all of the following masks:
        - relative position embedding
        - key padding mask
        - attention mask
 
     For more details about sparsity config, please see `Generative Modeling with Sparse Transformers`: https://arxiv.org/abs/1904.10509
     """
+
     def sparse_softmax(*args, **kwargs):
         return _sparse_softmax.apply(*args, **kwargs)
 
     def make_lut(self, device):
         """Generates the sparsity layout used in block-sparse softmax
         """
         key = (device, )
         if key not in self.lut_cache:
-            self.lut_cache[key] = _sparse_softmax.make_lut(self.layout,
-                                                           self.block,
-                                                           device)
+            self.lut_cache[key] = _sparse_softmax.make_lut(self.layout, self.block, device)
         return self.lut_cache[key]
 
     def __init__(self, layout, block, bench=False):
         """Initialize the Block-Sparse Softmax class.
 
         Arguments:
              layout: required: sparsity layout tensor
@@ -328,23 +286,11 @@
         if rpe is not None and rpe.dtype != x.dtype:
             raise ValueError('relative position embedding must be %s' % x.dtype)
         if attn_mask is not None and attn_mask.dtype != x.dtype:
             raise ValueError('Attention mask must be %s' % x.dtype)
         if key_padding_mask is not None and key_padding_mask.dtype != x.dtype:
             raise ValueError('Key padding mask must be %s' % x.dtype)
         lut, maxlut = self.make_lut(x.device)
-        x = Softmax.sparse_softmax(x,
-                                   scale,
-                                   rpe,
-                                   key_padding_mask,
-                                   attn_mask,
-                                   key_padding_mask_mode,
-                                   attn_mask_mode,
-                                   self.spdims,
-                                   self.block,
-                                   lut,
-                                   self.num_blocks,
-                                   maxlut,
-                                   self.bench,
-                                   time_y)
+        x = Softmax.sparse_softmax(x, scale, rpe, key_padding_mask, attn_mask, key_padding_mask_mode, attn_mask_mode,
+                                   self.spdims, self.block, lut, self.num_blocks, maxlut, self.bench, time_y)
         self.time_y = time_y[0]
         return x
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/sparse_attention/sparse_attention_utils.py` & `deepspeed-0.9.0/deepspeed/ops/sparse_attention/sparse_attention_utils.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,70 +1,66 @@
-"""
-Copyright 2020 The Microsoft DeepSpeed Team
-"""
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import torch
 from torch.nn import functional as F
 from deepspeed.ops.sparse_attention import BertSparseSelfAttention, SparsityConfig
 '''
 This file contains few utility functions to handle adapting pretrained model with sparse self-attention module.
 '''
 
 
 class SparseAttentionUtils:
     """This class provides some utility functions that are use integrating sparse attention into transformer models.
     Such utilities include extending position embeddings, replacing current self-attention layer with sparse attention, padding sequences to multiple of block size, etc.
 
     """
+
     @staticmethod
     def extend_position_embedding(model, max_position):
         """This function extends the position embedding weights of a model loaded from a checkpoint.
         It assumes the new max position is bigger than the original max length.
 
         Arguments:
             model: required: a transformer model
             max_position: required: an integer determining new position embedding size
         Return:
             model: updated model; in which position embedding weights have been extended based on new size
         """
 
         if hasattr(model, 'bert'):
-            original_max_position = model.bert.embeddings.position_embeddings.weight.size(
-                0)
+            original_max_position = model.bert.embeddings.position_embeddings.weight.size(0)
             assert max_position > original_max_position
             extend_multiples = max(1, max_position // original_max_position)
             model.bert.embeddings.position_embeddings.weight.data = model.bert.embeddings.position_embeddings.weight.repeat(
-                extend_multiples,
-                1)
+                extend_multiples, 1)
         elif hasattr(model, 'roberta'):
             # RoBERTa has positions 0 & 1 reserved, so embedding size is max position + 2
             original_max_position, embed_size = model.roberta.embeddings.position_embeddings.weight.shape
             original_max_position -= 2
             extend_multiples = max(1, max_position // original_max_position)
             assert max_position > original_max_position
             max_position += 2
             extended_position_embedding = model.roberta.embeddings.position_embeddings.weight.new_empty(
-                max_position,
-                embed_size)
+                max_position, embed_size)
             k = 2
             for i in range(extend_multiples):
                 extended_position_embedding[k:(
-                    k + original_max_position
-                )] = model.roberta.embeddings.position_embeddings.weight[2:]
+                    k + original_max_position)] = model.roberta.embeddings.position_embeddings.weight[2:]
                 k += original_max_position
             model.roberta.embeddings.position_embeddings.weight.data = extended_position_embedding
         else:
             raise ValueError(
                 'Please extend \"extend_position_embedding\" function to support your model type. It currently only supports \"bert\" & \"roberta\"!'
             )
 
         model.config.max_position_embeddings = max_position
-        print(
-            f'Extended position embeddings to {original_max_position * extend_multiples}'
-        )
+        print(f'Extended position embeddings to {original_max_position * extend_multiples}')
 
         return model
 
     @staticmethod
     def update_tokenizer_model_max_length(tokenizer, max_position):
         """This function updates the position embedding length of a tokenizer to a new max position.
 
@@ -98,29 +94,25 @@
 
         Return:
             model: updated model; in which self attention layer has been replaced with DeepSpeed Sparse Self Attention layer.
         """
 
         if hasattr(model, 'bert'):
             model.config.max_position_embeddings = max_position
-            model.replace_self_attention_layer_with_sparse_self_attention_layer(
-                model.config,
-                model.bert.encoder.layer,
-                sparsity_config)
+            model.replace_self_attention_layer_with_sparse_self_attention_layer(model.config, model.bert.encoder.layer,
+                                                                                sparsity_config)
         elif hasattr(model, 'roberta'):
             model.config.max_position_embeddings = max_position + 2
-            model.replace_self_attention_layer_with_sparse_self_attention_layer(
-                model.config,
-                model.roberta.encoder.layer,
-                sparsity_config)
+            model.replace_self_attention_layer_with_sparse_self_attention_layer(model.config,
+                                                                                model.roberta.encoder.layer,
+                                                                                sparsity_config)
         else:
             raise ValueError(
                 'Please extend \"update_model_self_attention_to_sparse_self_attention\" function to support \
-                                     your model type. It currently only supports \"bert\" & \"roberta\"!'
-            )
+                                     your model type. It currently only supports \"bert\" & \"roberta\"!')
         return model
 
     @staticmethod
     def replace_self_attention_layer_with_sparse_self_attention_layer(
         config,
         layers,
         # SparsityConfig parameters needs to be set accordingly
@@ -144,22 +136,16 @@
             deepspeed_sparse_self_attn.value = layer.attention.self.value
 
             layer.attention.self = deepspeed_sparse_self_attn
 
         return layers
 
     @staticmethod
-    def pad_to_block_size(block_size,
-                          input_ids,
-                          attention_mask,
-                          token_type_ids,
-                          position_ids,
-                          inputs_embeds,
-                          pad_token_id,
-                          model_embeddings):
+    def pad_to_block_size(block_size, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds,
+                          pad_token_id, model_embeddings):
         """This function pads input tokens and attention mask on sequence length dimension to be multiple of block size.
             This is a requirement for Sparse Transformer in which the self attention layer works on sequences of length multiple of block size.
             It needs to be called in your model, such as BertModel, right before you calculate the embedding outputs.
             Note)
             1- instead of passing your embedding layer to this function, you can simply add this function to your model. It can be more simplified if given attention_mask and/or token_type_ids are none.
             2- you need to call unpad function before returning your model output to unpad the encoder sequence output.
 
@@ -183,18 +169,15 @@
         """
 
         batch_size, seq_len = input_ids.shape if input_ids is not None else inputs_embeds.shape[:-1]
 
         pad_len = (block_size - seq_len % block_size) % block_size
         if pad_len > 0:
             if inputs_embeds is not None:
-                pad_input_ids = inputs_embeds.new_full((batch_size,
-                                                        pad_len),
-                                                       pad_token_id,
-                                                       dtype=torch.long)
+                pad_input_ids = inputs_embeds.new_full((batch_size, pad_len), pad_token_id, dtype=torch.long)
                 pad_inputs_embeds = model_embeddings(pad_input_ids)
                 inputs_embeds = torch.cat([inputs_embeds, pad_inputs_embeds], dim=-2)
             # may not be needed as input_ids are not used if inputs_embeds are given
             if input_ids is not None:
                 input_ids = F.pad(input_ids, (0, pad_len), value=pad_token_id)
             if position_ids is not None:
                 # pad position_id with pad_token_id
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/sparse_attention/sparse_self_attention.py` & `deepspeed-0.9.0/deepspeed/ops/sparse_attention/sparse_self_attention.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,24 +1,26 @@
-"""
-Copyright 2020 The Microsoft DeepSpeed Team
-"""
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import torch.nn as nn
 import torch
 from torch import distributed as dist
 from deepspeed.ops.sparse_attention import SparsityConfig
 
 
 class SparseSelfAttention(nn.Module):
     """Implements an efficient Sparse Self Attention of Transformer layer based on `Generative Modeling with Sparse Transformers`: https://arxiv.org/abs/1904.10509
 
     For more information please see, TODO DeepSpeed Sparse Transformer.
 
     For usage example please see, TODO DeepSpeed Sparse Transformer Tutorial.
     """
+
     def __init__(
             self,
             # SparsityConfig parameters needs to be set accordingly
             sparsity_config=SparsityConfig(num_heads=4),
             key_padding_mask_mode='add',
             attn_mask_mode='mul',
             max_seq_length=2048):
@@ -49,43 +51,36 @@
         # if layout is never synchronized across GPUs, broadcast the layout from global rank 0
         if self._need_layout_synchronization and dist.is_initialized():
             dist.broadcast(self.master_layout, src=0)
             self._need_layout_synchronization = False
 
         if (L % self.sparsity_config.block != 0):
             raise ValueError(
-                f'Sequence Length, {L}, needs to be dividable by Block size {self.sparsity_config.block}!'
-            )
+                f'Sequence Length, {L}, needs to be dividable by Block size {self.sparsity_config.block}!')
 
         num_blocks = L // self.sparsity_config.block
         return self.master_layout[..., :num_blocks, :num_blocks].cpu()  # layout needs to be a CPU tensor
 
     # add to cache
     def get_ops(self, H, L):
         from deepspeed.ops.sparse_attention.matmul import MatMul
         from deepspeed.ops.sparse_attention.softmax import Softmax
         if L not in SparseSelfAttention.ops:
             sparsity_layout = self.get_layout(L)
-            sparse_dot_sdd_nt = MatMul(sparsity_layout,
-                                       self.sparsity_config.block,
-                                       'sdd',
-                                       trans_a=False,
-                                       trans_b=True)
+            sparse_dot_sdd_nt = MatMul(sparsity_layout, self.sparsity_config.block, 'sdd', trans_a=False, trans_b=True)
 
             sparse_dot_dsd_nn = MatMul(sparsity_layout,
                                        self.sparsity_config.block,
                                        'dsd',
                                        trans_a=False,
                                        trans_b=False)
 
             sparse_softmax = Softmax(sparsity_layout, self.sparsity_config.block)
 
-            SparseSelfAttention.ops[L] = (sparse_dot_sdd_nt,
-                                          sparse_dot_dsd_nn,
-                                          sparse_softmax)
+            SparseSelfAttention.ops[L] = (sparse_dot_sdd_nt, sparse_dot_dsd_nn, sparse_softmax)
         return SparseSelfAttention.ops[L]
 
     def transpose_key_for_scores(self, x, L):
         bsz, num_heads, seq_len, head_dim = x.size()
         if seq_len != L:
             return x.permute(0, 1, 3, 2)
         return x
@@ -96,21 +91,15 @@
             xdim = x.dim()
             for d in range(xdim - 1, 0, -1):
                 x = x.squeeze(dim=d)
             return x
         return x.squeeze()
 
     # forward pass
-    def forward(self,
-                query,
-                key,
-                value,
-                rpe=None,
-                key_padding_mask=None,
-                attn_mask=None):
+    def forward(self, query, key, value, rpe=None, key_padding_mask=None, attn_mask=None):
         """Applies forward phase of sparse self attention
 
         Arguments:
             query: required: query tensor
             key: required: key tensor
             value: required: value tensor
             rpe: optional: a tensor same dimension as x that is used as relative position embedding
@@ -130,34 +119,31 @@
 
         # check that operation is supported
         if query.shape != key.shape or key.shape != value.shape:
             raise NotImplementedError('only self-attention is supported for now')
 
         # squeeze key_padding_mask if it is given
         if key_padding_mask is not None:
-            key_padding_mask = self.transpose_mask_for_sparse(query.dtype,
-                                                              key_padding_mask,
-                                                              is_key_padding_mask=True)
+            key_padding_mask = self.transpose_mask_for_sparse(query.dtype, key_padding_mask, is_key_padding_mask=True)
 
         # squeeze attn_mask if it is given
         if attn_mask is not None:
             attn_mask = self.transpose_mask_for_sparse(query.dtype, attn_mask)
 
         # cache look-up table computations etc
         sparse_dot_sdd_nt, sparse_dot_dsd_nn, sparse_softmax = self.get_ops(num_heads, tgt_len)
 
         scaling = float(head_dim)**-0.5
 
         # attention scores
         attn_output_weights = sparse_dot_sdd_nt(query, key)
-        attn_output_weights = sparse_softmax(
-            attn_output_weights,
-            scale=scaling,
-            rpe=rpe,
-            key_padding_mask=key_padding_mask,
-            attn_mask=attn_mask,
-            key_padding_mask_mode=self.key_padding_mask_mode,
-            attn_mask_mode=self.attn_mask_mode)
+        attn_output_weights = sparse_softmax(attn_output_weights,
+                                             scale=scaling,
+                                             rpe=rpe,
+                                             key_padding_mask=key_padding_mask,
+                                             attn_mask=attn_mask,
+                                             key_padding_mask_mode=self.key_padding_mask_mode,
+                                             attn_mask_mode=self.attn_mask_mode)
 
         # outputs
         attn_output = sparse_dot_dsd_nn(attn_output_weights, value)
         return attn_output
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/sparse_attention/sparsity_config.py` & `deepspeed-0.9.0/deepspeed/ops/sparse_attention/sparsity_config.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,19 +1,21 @@
-"""
-Copyright 2020 The Microsoft DeepSpeed Team
-"""
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import torch
 import random
 
 
 class SparsityConfig:
     """Abstract Configuration class to store `sparsity configuration of a self attention layer`.
     It contains shared property of different block-sparse sparsity patterns. However, each class needs to extend it based on required property and functionality.
     """
+
     def __init__(self, num_heads, block=16, different_layout_per_head=False):
         """Initialize the Sparsity Pattern Config.
 
         For usage example please see, TODO DeepSpeed Sparse Transformer Tutorial
 
         Arguments:
              num_heads: required: an integer determining number of attention heads of the layer.
@@ -33,17 +35,15 @@
              seq_len: required: an integer determining number of attention heads of the layer.
 
         Return:
              layout: a tensor of dimension (num_heads, num_blocks, num_blocks) for sparsity layout of all head; initialized with zero
         """
 
         if (seq_len % self.block != 0):
-            raise ValueError(
-                f'Sequence Length, {seq_len}, needs to be dividable by Block size {self.block}!'
-            )
+            raise ValueError(f'Sequence Length, {seq_len}, needs to be dividable by Block size {self.block}!')
         num_blocks = seq_len // self.block
         # TODO Currently we allocate layout per head; needs to be updated if heads share a single layout.
         layout = torch.zeros((self.num_heads, num_blocks, num_blocks), dtype=torch.int64)
         return layout
 
     def check_and_propagate_first_head_layout(self, layout):
         """If all heads require same sparsity layout, it propagate first head layout to all heads
@@ -60,14 +60,15 @@
         return layout
 
 
 class DenseSparsityConfig(SparsityConfig):
     """Configuration class to store `Dense` configuration.
     In reality, this is not sparse and all blocks are used. We keep it for the sake of comparison and comprehension.
     """
+
     def __init__(self, num_heads, block=16, different_layout_per_head=False):
         """Initialize the Dense Sparsity Pattern Config.
         In reality, this is not sparse and all blocks are used. We keep it for the sake of comparison and comprehension.
 
         Arguments:
              num_heads: required: an integer determining number of attention heads of the layer.
              seq_len: required: an integer determining number of attention heads of the layer.
@@ -92,14 +93,15 @@
 
 
 class FixedSparsityConfig(SparsityConfig):
     """Configuration class to store `Fixed` sparsity configuration.
     For more details about this sparsity config, please see `Generative Modeling with Sparse Transformers`: https://arxiv.org/abs/1904.10509; this has been customized.
     This class extends parent class of `SparsityConfig` and customizes it for `Fixed` sparsity.
     """
+
     def __init__(self,
                  num_heads,
                  block=16,
                  different_layout_per_head=False,
                  num_local_blocks=4,
                  num_global_blocks=1,
                  attention='bidirectional',
@@ -127,22 +129,19 @@
         if (num_local_blocks % num_global_blocks != 0):
             raise ValueError(
                 f'Number of blocks in a local window, {num_local_blocks}, must be dividable by number of global blocks, {num_global_blocks}!'
             )
         self.num_global_blocks = num_global_blocks
 
         if (attention != 'unidirectional' and attention != 'bidirectional'):
-            raise NotImplementedError(
-                'only \"uni/bi-directional\" attentions are supported for now!')
+            raise NotImplementedError('only \"uni/bi-directional\" attentions are supported for now!')
         self.attention = attention
 
         if (attention != 'bidirectional' and horizontal_global_attention):
-            raise ValueError(
-                'only \"bi-directional\" attentions can support horizontal global attention!'
-            )
+            raise ValueError('only \"bi-directional\" attentions can support horizontal global attention!')
         self.horizontal_global_attention = horizontal_global_attention
 
         if (num_different_global_patterns > 1 and not different_layout_per_head):
             raise ValueError(
                 f'Number of different layouts cannot be more than one when you have set a single layout for all heads! Set different_layout_per_head to True.'
             )
         if (num_different_global_patterns > (num_local_blocks // num_global_blocks)):
@@ -162,17 +161,15 @@
              layout: a tensor of dimension (num_heads, num_blocks, num_blocks) containing sparsity layout of all head in which local layout is set
         """
 
         num_blocks = layout.shape[1]
         for i in range(0, num_blocks, self.num_local_blocks):
             end = min(i + self.num_local_blocks, num_blocks)
             for row in range(i, end):
-                for col in range(
-                        i,
-                    (row + 1 if self.attention == 'unidirectional' else end)):
+                for col in range(i, (row + 1 if self.attention == 'unidirectional' else end)):
                     layout[h, row, col] = 1
         return layout
 
     def set_global_layout(self, h, layout):
         """Sets global attention layout used by the given head in the sparse attention.
 
         Currently we set global blocks starting from the last block of a local window to the first one. That means if a local window consists of 4 blocks and global attention size is one block, we use block #4 in each local window as global. If we have different layout per head, then other heads will get #3, #2, and #1. And if we have more heads (and different layout has set) than num of global attentions, multiple head may have same global attentions.
@@ -202,16 +199,15 @@
 
             # horizontal global attention; only in bidirectional attention
             if (self.horizontal_global_attention):
                 layout[h, i:i + self.num_global_blocks, :] = 1
 
         # set last global blocks; handle possible short last local window
         if (end < num_blocks):
-            start = min(end + first_global_block_idx,
-                        num_blocks - self.num_global_blocks)
+            start = min(end + first_global_block_idx, num_blocks - self.num_global_blocks)
             end = start + self.num_global_blocks
 
             # vertical global attention
             first_row = 0 if self.attention == 'bidirectional' else start
             #(((start // self.num_local_blocks) + 1) * self.num_local_blocks)
             #if (first_row < num_blocks):
             layout[h, first_row:, start:end] = 1
@@ -246,14 +242,15 @@
       - user can set random layout; default value is zero means no random block
       - user can provide a list of local block sizes
       - user can provide a list of global block indices.
 
     For more details about `Fixed` sparsity config, please see `Generative Modeling with Sparse Transformers`: https://arxiv.org/abs/1904.10509; this has been customized.
     This class extends parent class of `SparsityConfig` and customizes it for `Fixed` sparsity.
     """
+
     def __init__(self,
                  num_heads,
                  block=16,
                  different_layout_per_head=False,
                  num_random_blocks=0,
                  local_window_blocks=[4],
                  global_block_indices=[0],
@@ -292,22 +289,19 @@
                 if start_idx >= end_idx:
                     raise ValueError(
                         f'Global block start index, {start_idx}, must be smaller than global block end index, {end_idx}!'
                     )
         self.global_block_end_indices = global_block_end_indices
 
         if (attention != 'unidirectional' and attention != 'bidirectional'):
-            raise NotImplementedError(
-                'only \"uni/bi-directional\" attentions are supported for now!')
+            raise NotImplementedError('only \"uni/bi-directional\" attentions are supported for now!')
         self.attention = attention
 
         if (attention != 'bidirectional' and horizontal_global_attention):
-            raise ValueError(
-                'only \"bi-directional\" attentions can support horizontal global attention!'
-            )
+            raise ValueError('only \"bi-directional\" attentions can support horizontal global attention!')
         self.horizontal_global_attention = horizontal_global_attention
 
     def set_random_layout(self, h, layout):
         """Sets random attention layout used by the given head in the sparse attention.
         Note) By default, it assumes there will be a unique random block layout for all heads; unless `different_layout_per_head` parameter is set in which each head can have a different random layout.
 
         Arguments:
@@ -341,27 +335,23 @@
         num_blocks = layout.shape[1]
         start_block_idx = 0
         end_block_idx = 0
         for block_size in self.local_window_blocks:
             end_block_idx += block_size
             end_block_idx = min(end_block_idx, num_blocks)
             for row in range(start_block_idx, end_block_idx):
-                for col in range(
-                        start_block_idx,
-                    (row + 1 if self.attention == 'unidirectional' else end_block_idx)):
+                for col in range(start_block_idx, (row + 1 if self.attention == 'unidirectional' else end_block_idx)):
                     layout[h, row, col] = 1
             start_block_idx += block_size
 
         # if there is any remaining not attended part, use the lats local window block size as local window for the remaining applicable local windows
         for i in range(start_block_idx, num_blocks, block_size):
             end_block_idx = min(i + block_size, num_blocks)
             for row in range(i, end_block_idx):
-                for col in range(
-                        i,
-                    (row + 1 if self.attention == 'unidirectional' else end_block_idx)):
+                for col in range(i, (row + 1 if self.attention == 'unidirectional' else end_block_idx)):
                     layout[h, row, col] = 1
         return layout
 
     def set_global_layout(self, h, layout):
         """Sets global attention layout used by the given head in the sparse attention.
 
         Arguments:
@@ -419,14 +409,15 @@
 
 
 class BigBirdSparsityConfig(SparsityConfig):
     """Configuration class to store `BigBird` sparsity configuration.
     For more details about this sparsity config, please see `Big Bird: Transformers for Longer Sequences`: https://arxiv.org/pdf/2007.14062.pdf
     This class extends parent class of `SparsityConfig` and customizes it for `BigBird` sparsity.
     """
+
     def __init__(self,
                  num_heads,
                  block=16,
                  different_layout_per_head=False,
                  num_random_blocks=1,
                  num_sliding_window_blocks=3,
                  num_global_blocks=1,
@@ -448,16 +439,15 @@
         super().__init__(num_heads, block, different_layout_per_head)
 
         self.num_random_blocks = num_random_blocks
         self.num_sliding_window_blocks = num_sliding_window_blocks
         self.num_global_blocks = num_global_blocks
 
         if (attention != 'unidirectional' and attention != 'bidirectional'):
-            raise NotImplementedError(
-                'only \"uni/bi-directional\" attentions are supported for now!')
+            raise NotImplementedError('only \"uni/bi-directional\" attentions are supported for now!')
         self.attention = attention
 
     def set_random_layout(self, h, layout):
         """Sets random attention layout used by the given head in the sparse attention.
         Note) By default, it assumes there will be a unique random block layout for all heads; unless `different_layout_per_head` parameter is set in which each head can have a different random layout.
 
         Arguments:
@@ -471,18 +461,15 @@
         num_blocks = layout.shape[1]
         if (num_blocks < self.num_random_blocks):
             raise ValueError(
                 f'Number of random blocks, {self.num_random_blocks}, must be smaller than overall number of blocks in a row, {num_blocks}!'
             )
 
         for row in range(0, num_blocks):
-            sample_range = range(
-                0,
-                num_blocks) if self.attention == 'bidirectional' else range(0,
-                                                                            row + 1)
+            sample_range = range(0, num_blocks) if self.attention == 'bidirectional' else range(0, row + 1)
             rnd_cols = random.sample(sample_range, self.num_random_blocks)
             layout[h, row, rnd_cols] = 1
         return layout
 
     def set_sliding_window_layout(self, h, layout):
         """Sets sliding local attention layout used by the given head in the sparse attention.
 
@@ -560,14 +547,15 @@
     """Configuration class to store edited `Longformer` sparsity configuration.
 
     Note) this is a block-sparse version of the Longformer which is slightly different than original Longformer; which is element-wise sparsity.
 
     For more details about this sparsity config, please see `Longformer: The Long-Document Transformer`: https://arxiv.org/pdf/2004.05150.pdf
     This class extends parent class of `SparsityConfig` and customizes it for `Longformer` sparsity.
     """
+
     def __init__(self,
                  num_heads,
                  block=16,
                  different_layout_per_head=False,
                  num_sliding_window_blocks=3,
                  global_block_indices=[0],
                  global_block_end_indices=None,
@@ -683,19 +671,16 @@
         return layout
 
 
 class LocalSlidingWindowSparsityConfig(SparsityConfig):
     """Configuration class to store `Local Sliding Window` sparsity configuration - a purely-local sliding window attention.
     This class extends parent class of `SparsityConfig` and customizes it for `Local` sparsity.
     """
-    def __init__(self,
-                 num_heads,
-                 block=16,
-                 num_sliding_window_blocks=3,
-                 attention='unidirectional'):
+
+    def __init__(self, num_heads, block=16, num_sliding_window_blocks=3, attention='unidirectional'):
         """Initialize the Local Sliding Window Sparsity Pattern Config.
         For usage example please see, TODO DeepSpeed Sparse Transformer Tutorial
         Arguments:
              num_heads: required: an integer determining number of attention heads of the layer.
              block: optional: an integer determining the block size. Current implementation of sparse self-attention is based on blocked sparse matrices. In which this parameter defines size of such blocks, `Block X Block`.
              num_sliding_window_blocks: optional: an integer determining the number of blocks in sliding local attention window.
 	     attention: optional: a string determining attention type. Attention can be `unidirectional`, such as autoregressive models, in which tokens attend only to tokens appear before them in the context. Considering that, the upper triangular of attention matrix is empty as above figure. Or it can be `bidirectional`, such as BERT, in which tokens can attend to any other tokens before or after them. Then, the upper triangular part of the attention matrix is mirror of the lower triangular in the above figure.
@@ -719,16 +704,15 @@
             raise ValueError(
                 f'Number of sliding window blocks, {self.num_sliding_window_blocks}, must be smaller than overal number of blocks in a row, {num_blocks}!'
             )
 
         w = self.num_sliding_window_blocks // 2
         for row in range(0, num_blocks):
             start = max(0, row - w)
-            end = min(row + w + 1,
-                      num_blocks) if self.attention == "bidirectional" else row + 1
+            end = min(row + w + 1, num_blocks) if self.attention == "bidirectional" else row + 1
             layout[h, row, start:end] = 1
         return layout
 
     def make_layout(self, seq_len):
         """Generates `Local Sliding Window` sparsity layout used by each head in the sparse attention.
         Arguments:
              seq_len: required: an integer determining number of attention heads of the layer.
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/sparse_attention/trsrc/__init__.py` & `deepspeed-0.9.0/deepspeed/ops/sparse_attention/trsrc/__init__.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,8 +1,11 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import sys
 import os
 
 
 def _build_file_index(directory, suffix='.tr'):
     """Build an index of source files and their basenames in a given directory.
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/sparse_attention/trsrc/matmul.tr` & `deepspeed-0.9.0/deepspeed/ops/sparse_attention/trsrc/matmul.tr`

 * *Files 3% similar despite different names*

```diff
@@ -1,9 +1,16 @@
-// DeepSpeed note, code taken & adapted from commit 9aa94789f13ada713af36cfd8cca2fc9a7f6b79a
-// https://github.com/ptillet/torch-blocksparse/blob/master/torch_blocksparse/matmul.py
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
+/*
+DeepSpeed note, code taken & adapted from commit 9aa94789f13ada713af36cfd8cca2fc9a7f6b79a
+ https:github.com/ptillet/torch-blocksparse/blob/master/torch_blocksparse/matmul.py
+*/
 
 __global__ void NAME (TYPE* A __readonly  __noalias __aligned(16),
                         TYPE* B __readonly  __noalias __aligned(16),
                         TYPE* C __noalias __aligned(16),
                         int lda __multipleof(8),
                         int ldb __multipleof(8),
                         int ldc __multipleof(8),
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/sparse_attention/trsrc/softmax_bwd.tr` & `deepspeed-0.9.0/deepspeed/ops/sparse_attention/trsrc/softmax_bwd.tr`

 * *Files 13% similar despite different names*

```diff
@@ -1,9 +1,16 @@
-// DeepSpeed note, code taken & adapted from commit 9aa94789f13ada713af36cfd8cca2fc9a7f6b79a
-// https://github.com/ptillet/torch-blocksparse/blob/master/torch_blocksparse/softmax.py
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
+/*
+DeepSpeed note, code taken & adapted from commit 9aa94789f13ada713af36cfd8cca2fc9a7f6b79a
+ https:github.com/ptillet/torch-blocksparse/blob/master/torch_blocksparse/softmax.py
+*/
 
 __global__ void softmax_bwd(TYPE * X __readonly __noalias __aligned(16),
                             float scale,
                             TYPE* DX __readonly __noalias __aligned(16),
                             int* LUT,
                             int sizemax,
                             long stride_zx __multipleof(BLOCK),
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/sparse_attention/trsrc/softmax_fwd.tr` & `deepspeed-0.9.0/deepspeed/ops/sparse_attention/trsrc/softmax_fwd.tr`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,16 @@
-// DeepSpeed note, code taken & adapted from commit 9aa94789f13ada713af36cfd8cca2fc9a7f6b79a
-// https://github.com/ptillet/torch-blocksparse/blob/master/torch_blocksparse/softmax.py
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
+/*
+DeepSpeed note, code taken & adapted from commit 9aa94789f13ada713af36cfd8cca2fc9a7f6b79a
+ https:github.com/ptillet/torch-blocksparse/blob/master/torch_blocksparse/softmax.py
+*/
 
 __global__ void softmax_fwd(TYPE *X __readonly __noalias __aligned(16),
                             float scale,
                             int *LUT __readonly __noalias __aligned(16),
                             TYPE *RPE __readonly __noalias __aligned(16),
                             TYPE *KP_M __readonly __noalias __aligned(16),
                             TYPE *ATTN_M __readonly __noalias __aligned(16),
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/transformer/inference/bias_add.py` & `deepspeed-0.9.0/deepspeed/ops/transformer/inference/bias_add.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-'''
-Copyright 2022 The Microsoft DeepSpeed Team
-'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 from typing import Optional
 import torch
 from deepspeed.ops.op_builder import SpatialInferenceBuilder
 
 spatial_cuda_module = None
 
@@ -18,11 +19,8 @@
         spatial_cuda_module = SpatialInferenceBuilder().load()
 
     if other is None:
         return spatial_cuda_module.nhwc_bias_add(activation, bias)
     elif other_bias is None:
         return spatial_cuda_module.nhwc_bias_add_add(activation, bias, other)
     else:
-        return spatial_cuda_module.nhwc_bias_add_bias_add(activation,
-                                                          bias,
-                                                          other,
-                                                          other_bias)
+        return spatial_cuda_module.nhwc_bias_add_bias_add(activation, bias, other, other_bias)
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/transformer/inference/config.py` & `deepspeed-0.9.0/deepspeed/ops/transformer/inference/config.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,15 +1,18 @@
-'''
-Copyright 2022 The Microsoft DeepSpeed Team
-'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+
 import json
 from deepspeed.utils.types import ActivationFuncType
 
 
 class TransformerConfig():
+
     def __init__(self, hidden_size, intermediate_size, heads, num_hidden_layers):
         self.layer_id = -1
         self.hidden_size = hidden_size
         self.intermediate_size = intermediate_size
         self.heads = heads
         self.num_hidden_layers = num_hidden_layers
 
@@ -36,14 +39,15 @@
                 a high accuracy level. On the other hand, for the downstream tasks, such as fine-tuning, we recommend
                 to turn it off in order to be able to reproduce the same result through the regular kernel execution.
 
             scale_attention: If true, both q and k are scaled by 1/sqrt(attention_heads) before attention computation.
             return_tuple: if True, returns the transformer output as a tuple, otherwise returns as a tensor
             bigscience_bloom: This flag is added temporarily for supporting the BLOOM-176B model architecture.
     """
+
     def __init__(self,
                  hidden_size=-1,
                  intermediate_size=-1,
                  heads=-1,
                  num_hidden_layers=-1,
                  layer_norm_eps=1e-12,
                  local_rank=-1,
@@ -61,24 +65,24 @@
                  rotate_every_two=True,
                  return_tuple=True,
                  mlp_after_attn=True,
                  mlp_act_func_type=ActivationFuncType.GELU,
                  training_mp_size=1,
                  bigscience_bloom=False,
                  max_out_tokens=1024,
+                 min_out_tokens=1,
                  enable_qkv_quantization=False,
                  use_mup=False,
                  scale_attn_by_inverse_layer_idx=False,
-                 return_single_tuple=False):
+                 return_single_tuple=False,
+                 set_empty_params=False,
+                 transposed_mode=False):
         super(DeepSpeedInferenceConfig,
-              self).__init__(
-                  hidden_size,
-                  (intermediate_size if intermediate_size > 0 else 4 * hidden_size),
-                  heads,
-                  num_hidden_layers)
+              self).__init__(hidden_size, (intermediate_size if intermediate_size > 0 else 4 * hidden_size), heads,
+                             num_hidden_layers)
         self.fp16 = fp16
         self.pre_layer_norm = pre_layer_norm
         self.local_rank = local_rank
         self.stochastic_mode = stochastic_mode
         self.epsilon = layer_norm_eps
         self.mp_size = mp_size
         self.q_int8 = q_int8
@@ -92,18 +96,21 @@
         self.return_tuple = return_tuple
         self.mlp_after_attn = mlp_after_attn
         self.mlp_act_func_type = mlp_act_func_type
         self.specialized_mode = False
         self.training_mp_size = training_mp_size
         self.bigscience_bloom = bigscience_bloom
         self.max_out_tokens = max_out_tokens
+        self.min_out_tokens = min_out_tokens
         self.scale_attn_by_inverse_layer_idx = scale_attn_by_inverse_layer_idx
         self.enable_qkv_quantization = enable_qkv_quantization
         self.use_mup = use_mup
         self.return_single_tuple = return_single_tuple
+        self.set_empty_params = set_empty_params
+        self.transposed_mode = transposed_mode
 
     @classmethod
     def from_dict(cls, json_object):
         config = DeepSpeedInferenceConfig()
         for key, value in json_object.items():
             config.__dict__[key] = value
         return config
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/transformer/inference/diffusers_transformer_block.py` & `deepspeed-0.9.0/deepspeed/ops/transformer/inference/diffusers_transformer_block.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-'''
-Copyright 2022 The Microsoft DeepSpeed Team
-'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import torch
 import torch.nn as nn
 
 from deepspeed import module_inject
 from .diffusers_attention import DeepSpeedDiffusersAttention
 from .bias_add import nhwc_bias_add
@@ -27,100 +28,76 @@
     global spatial_cuda_module
     if spatial_cuda_module is None:
         spatial_cuda_module = SpatialInferenceBuilder().load()
     return spatial_cuda_module
 
 
 class DeepSpeedDiffusersTransformerBlock(nn.Module):
-    def __init__(self,
-                 equivalent_module: nn.Module,
-                 config: Diffusers2DTransformerConfig):
+
+    def __init__(self, equivalent_module: nn.Module, config: Diffusers2DTransformerConfig):
         super(DeepSpeedDiffusersTransformerBlock, self).__init__()
         self.quantizer = module_inject.GroupQuantizer(q_int8=config.int8_quantization)
         # Ensure ops are built by the time we start running
         self.config = config
 
         self.ff1_w = self.quantizer.quantize(
-            nn.Parameter(equivalent_module.ff.net[0].proj.weight.data,
-                         requires_grad=False))
-        self.ff1_b = nn.Parameter(equivalent_module.ff.net[0].proj.bias.data,
-                                  requires_grad=False)
-        self.ff2_w = self.quantizer.quantize(
-            nn.Parameter(equivalent_module.ff.net[2].weight.data,
-                         requires_grad=False))
-        self.ff2_b = nn.Parameter(equivalent_module.ff.net[2].bias.data,
-                                  requires_grad=False)
-
-        self.norm1_g = nn.Parameter(equivalent_module.norm1.weight.data,
-                                    requires_grad=False)
-        self.norm1_b = nn.Parameter(equivalent_module.norm1.bias.data,
-                                    requires_grad=False)
+            nn.Parameter(equivalent_module.ff.net[0].proj.weight.data, requires_grad=False))
+        self.ff1_b = nn.Parameter(equivalent_module.ff.net[0].proj.bias.data, requires_grad=False)
+        self.ff2_w = self.quantizer.quantize(nn.Parameter(equivalent_module.ff.net[2].weight.data,
+                                                          requires_grad=False))
+        self.ff2_b = nn.Parameter(equivalent_module.ff.net[2].bias.data, requires_grad=False)
+
+        self.norm1_g = nn.Parameter(equivalent_module.norm1.weight.data, requires_grad=False)
+        self.norm1_b = nn.Parameter(equivalent_module.norm1.bias.data, requires_grad=False)
         self.norm1_eps = equivalent_module.norm1.eps
 
-        self.norm2_g = nn.Parameter(equivalent_module.norm2.weight.data,
-                                    requires_grad=False)
-        self.norm2_b = nn.Parameter(equivalent_module.norm2.bias.data,
-                                    requires_grad=False)
+        self.norm2_g = nn.Parameter(equivalent_module.norm2.weight.data, requires_grad=False)
+        self.norm2_b = nn.Parameter(equivalent_module.norm2.bias.data, requires_grad=False)
         self.norm2_eps = equivalent_module.norm2.eps
 
-        self.norm3_g = nn.Parameter(equivalent_module.norm3.weight.data,
-                                    requires_grad=False)
-        self.norm3_b = nn.Parameter(equivalent_module.norm3.bias.data,
-                                    requires_grad=False)
+        self.norm3_g = nn.Parameter(equivalent_module.norm3.weight.data, requires_grad=False)
+        self.norm3_b = nn.Parameter(equivalent_module.norm3.bias.data, requires_grad=False)
         self.norm3_eps = equivalent_module.norm3.eps
 
         self.attn_1 = equivalent_module.attn1
         self.attn_2 = equivalent_module.attn2
 
         # Pull the bias in if we can
         if isinstance(self.attn_1, DeepSpeedDiffusersAttention):
             self.attn_1.do_out_bias = False
             self.attn_1_bias = self.attn_1.attn_ob
         else:
-            self.attn_1_bias = nn.Parameter(torch.zeros_like(self.norm2_g),
-                                            requires_grad=False)
+            self.attn_1_bias = nn.Parameter(torch.zeros_like(self.norm2_g), requires_grad=False)
 
         # Pull the bias in if we can
         if isinstance(self.attn_2, DeepSpeedDiffusersAttention):
             self.attn_2.do_out_bias = False
             self.attn_2_bias = self.attn_2.attn_ob
         else:
-            self.attn_2_bias = nn.Paramaeter(torch.zeros_like(self.norm3_g),
-                                             requires_grad=False)
+            self.attn_2_bias = nn.Paramaeter(torch.zeros_like(self.norm3_g), requires_grad=False)
 
         self.transformer_cuda_module = load_transformer_module()
         load_spatial_module()
 
     def forward(self, hidden_states, context=None, timestep=None, **kwargs):
         # In v0.12.0 of diffuser, several new kwargs were added. Capturing
         # those with kwargs to maintain backward compatibility
 
         # In v0.11.0 of diffusers, the kwarg was changed from 'context' to 'encoder_hidden_states'
         # This is so we can support older and newer versions of diffusers
         if "encoder_hidden_states" in kwargs and kwargs["encoder_hidden_states"] != None:
             context = kwargs["encoder_hidden_states"]
 
-        out_norm_1 = self.transformer_cuda_module.layer_norm(hidden_states,
-                                                             self.norm1_g,
-                                                             self.norm1_b,
-                                                             self.norm1_eps)
+        out_norm_1 = self.transformer_cuda_module.layer_norm(hidden_states, self.norm1_g, self.norm1_b, self.norm1_eps)
         out_attn_1 = self.attn_1(out_norm_1)
 
-        out_norm_2, out_attn_1 = self.transformer_cuda_module.layer_norm_residual_store_pre_ln_res(out_attn_1,
-                                                                 self.attn_1_bias,
-                                                                 hidden_states,
-                                                                 self.norm2_g,
-                                                                 self.norm2_b,
-                                                                 self.norm2_eps)
+        out_norm_2, out_attn_1 = self.transformer_cuda_module.layer_norm_residual_store_pre_ln_res(
+            out_attn_1, self.attn_1_bias, hidden_states, self.norm2_g, self.norm2_b, self.norm2_eps)
         out_attn_2 = self.attn_2(out_norm_2, context=context)
-        out_norm_3, out_attn_2 = self.transformer_cuda_module.layer_norm_residual_store_pre_ln_res(out_attn_2,
-                                                                 self.attn_2_bias,
-                                                                 out_attn_1,
-                                                                 self.norm3_g,
-                                                                 self.norm3_b,
-                                                                 self.norm3_eps)
+        out_norm_3, out_attn_2 = self.transformer_cuda_module.layer_norm_residual_store_pre_ln_res(
+            out_attn_2, self.attn_2_bias, out_attn_1, self.norm3_g, self.norm3_b, self.norm3_eps)
 
         out_ff1 = nn.functional.linear(out_norm_3, self.ff1_w)
         out_geglu = self.transformer_cuda_module.bias_geglu(out_ff1, self.ff1_b)
 
         out_ff2 = nn.functional.linear(out_geglu, self.ff2_w)
         return nhwc_bias_add(out_ff2, self.ff2_b, other=out_attn_2)
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/transformer/inference/ds_attention.py` & `deepspeed-0.9.0/deepspeed/ops/transformer/inference/ds_attention.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,54 +1,63 @@
-'''
-Copyright 2022 The Microsoft DeepSpeed Team
-'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import math
 import torch
 import torch.nn as nn
 from deepspeed import comm as dist
 from deepspeed.accelerator import get_accelerator
 from .op_binding import LinearOp, VectorMatMulOp, SoftmaxContextOp, QKVGemmOp, SoftmaxOp
 
 minus_inf = -10000.0
 
 
 class DeepSpeedSelfAttention(nn.Module):
     num_layers = 0
+    _qkv_buffers = []
 
     def __init__(self, config, mp_group=None, q_scales=None, q_groups=1, merge_count=1):
         super(DeepSpeedSelfAttention, self).__init__()
         self.config = config
         data_type = torch.int8 if config.q_int8 else torch.half if config.fp16 else torch.float
         data_type_fp = torch.half if config.fp16 else torch.float
         self.config.layer_id = DeepSpeedSelfAttention.num_layers
         DeepSpeedSelfAttention.num_layers = DeepSpeedSelfAttention.num_layers + 1
-        device = get_accelerator().current_device_name(
-        )  #if config.bigscience_bloom else 'cpu'
-        qkv_size_per_partition = (self.config.hidden_size // self.config.mp_size) * 3
-        self.attn_qkvw = nn.Parameter(torch.empty(self.config.hidden_size,
-                                                  qkv_size_per_partition,
-                                                  dtype=data_type,
-                                                  device=device),
-                                      requires_grad=False)
-        self.attn_qkvb = nn.Parameter(torch.empty(qkv_size_per_partition,
-                                                  dtype=data_type_fp,
-                                                  device=device),
-                                      requires_grad=False)
-        out_size_per_partition = self.config.hidden_size // self.config.mp_size
-        self.attn_ow = nn.Parameter(torch.empty(out_size_per_partition,
-                                                self.config.hidden_size,
-                                                dtype=data_type,
-                                                device=device),
-                                    requires_grad=False)
-
-        self.attn_ob = nn.Parameter(torch.empty(self.config.hidden_size,
-                                                dtype=data_type_fp,
-                                                device=device),
-                                    requires_grad=False)
+        device = get_accelerator().current_device_name()  #if config.bigscience_bloom else 'cpu'
+        if self.config.set_empty_params:
+            self.attn_qw = None
+            self.attn_qb = None
+            self.attn_kw = None
+            self.attn_kb = None
+            self.attn_vw = None
+            self.attn_vb = None
+            self.attn_qkvw = None
+            self.attn_qkvb = None
+            self.attn_ow = None
+            self.attn_ob = None
+        else:
+            qkv_size_per_partition = (self.config.hidden_size // self.config.mp_size) * 3
+            self.attn_qkvw = nn.Parameter(torch.empty(self.config.hidden_size,
+                                                      qkv_size_per_partition,
+                                                      dtype=data_type,
+                                                      device=device),
+                                          requires_grad=False)
+            self.attn_qkvb = nn.Parameter(torch.empty(qkv_size_per_partition, dtype=data_type_fp, device=device),
+                                          requires_grad=False)
+            out_size_per_partition = self.config.hidden_size // self.config.mp_size
+            self.attn_ow = nn.Parameter(torch.empty(out_size_per_partition,
+                                                    self.config.hidden_size,
+                                                    dtype=data_type,
+                                                    device=device),
+                                        requires_grad=False)
+
+            self.attn_ob = nn.Parameter(torch.empty(self.config.hidden_size, dtype=data_type_fp, device=device),
+                                        requires_grad=False)
 
         self.num_attention_heads_per_partition = self.config.heads // self.config.mp_size
         self.hidden_size_per_partition = self.config.hidden_size // self.config.mp_size
         self.hidden_size_per_attention_head = self.config.hidden_size // self.config.heads
 
         self.mp_group = mp_group
 
@@ -65,14 +74,22 @@
             self.norm_factor *= math.sqrt(self.config.layer_id + 1)
             # https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/gpt2/modeling_gpt2.py#L191
 
         self.qkv_func = QKVGemmOp(config)
         self.score_context_func = SoftmaxContextOp(config)
         self.linear_func = LinearOp(config)
         self.vector_matmul_func = VectorMatMulOp(config)
+        if len(DeepSpeedSelfAttention._qkv_buffers) == 0:
+            DeepSpeedSelfAttention._qkv_buffers = [
+                torch.empty(self.hidden_size_per_partition * 3,
+                            self.config.hidden_size,
+                            dtype=data_type_fp,
+                            device=device),
+                torch.empty(self.hidden_size_per_partition * 3, dtype=data_type_fp, device=device)
+            ]
 
     def compute_attention(self, qkv_out, input_mask, layer_past, alibi):
         if isinstance(qkv_out, list):
             qkv_out = qkv_out[0]
 
         no_masking = input_mask is None
 
@@ -89,81 +106,91 @@
             layer_id=self.config.layer_id,
             num_layers=DeepSpeedSelfAttention.num_layers,
             alibi=alibi)
 
         context_layer, key_layer, value_layer = attn_key_value
         return context_layer, key_layer, value_layer
 
+    def _merge_qkv(self):
+        qvkw = DeepSpeedSelfAttention._qkv_buffers[0]
+        qvkw[:self.hidden_size_per_partition, :] = self.attn_qw
+        qvkw[self.hidden_size_per_partition:2 * self.hidden_size_per_partition, :] = self.attn_kw
+        qvkw[2 * self.hidden_size_per_partition:, :] = self.attn_vw
+        if self.attn_qb is not None:
+            qvkb = DeepSpeedSelfAttention._qkv_buffers[1]
+            qvkb[:self.hidden_size_per_partition] = self.attn_qb
+            qvkb[self.hidden_size_per_partition:2 * self.hidden_size_per_partition] = self.attn_kb
+            qvkb[2 * self.hidden_size_per_partition:] = self.attn_vb
+        return DeepSpeedSelfAttention._qkv_buffers
+
     def forward(self,
                 input,
                 input_mask,
                 head_mask=None,
                 layer_past=None,
                 get_present=False,
                 encoder_hidden_states=None,
                 encoder_attention_mask=None,
                 output_attentions=False,
                 norm_w=None,
                 norm_b=None,
                 alibi=None):
+        if self.attn_qkvw is None:
+            self._attn_qkvw, self._attn_qkvb = self._merge_qkv()
+        else:
+            self._attn_qkvw = self.attn_qkvw
+            self._attn_qkvb = self.attn_qkvb
 
         if not self.config.pre_layer_norm:
             qkv_out = self.linear_func(input=input,
-                                       weight=self.attn_qkvw,
-                                       bias=self.attn_qkvb,
+                                       weight=self._attn_qkvw,
+                                       bias=self._attn_qkvb,
                                        add_bias=self.attn_qkvb is not None,
                                        do_flash_attn=False,
                                        num_heads=self.num_attention_heads_per_partition,
                                        num_layers=DeepSpeedSelfAttention.num_layers)
         else:
-            qkv_out = self.qkv_func(
-                input=input,
-                weight=self.attn_qkvw,
-                bias=(self.attn_qkvb if self.attn_qkvb is not None else norm_b),
-                gamma=norm_w,
-                beta=norm_b,
-                add_bias=(self.attn_qkvb is not None),
-                num_layers=DeepSpeedSelfAttention.num_layers,
-                num_heads=self.num_attention_heads_per_partition)
-
-        context_layer, key_layer, value_layer = self.compute_attention(
-            qkv_out=qkv_out,
-            input_mask=input_mask,
-            layer_past=layer_past,
-            alibi=alibi)
-
+            qkv_out = self.qkv_func(input=input,
+                                    weight=self._attn_qkvw,
+                                    bias=(self._attn_qkvb if self._attn_qkvb is not None else norm_b),
+                                    gamma=norm_w,
+                                    beta=norm_b,
+                                    add_bias=(self.attn_qkvb is not None),
+                                    num_layers=DeepSpeedSelfAttention.num_layers,
+                                    num_heads=self.num_attention_heads_per_partition)
+        context_layer, key_layer, value_layer = self.compute_attention(qkv_out=qkv_out,
+                                                                       input_mask=input_mask,
+                                                                       layer_past=layer_past,
+                                                                       alibi=alibi)
         output = self.vector_matmul_func(input=context_layer, weight=self.attn_ow)
 
         inp_norm = qkv_out[-1]
 
-        if self.config.mlp_after_attn and self.mp_group is not None and dist.get_world_size(
-                group=self.mp_group) > 1:
+        if self.config.mlp_after_attn and self.mp_group is not None and dist.get_world_size(group=self.mp_group) > 1:
             dist.all_reduce(output, group=self.mp_group)
 
         return (output, key_layer, value_layer, context_layer, inp_norm)
 
 
 class BloomSelfAttention(DeepSpeedSelfAttention):
+
     def __init__(self, *args, **kwargs):
         super(BloomSelfAttention, self).__init__(*args, **kwargs)
         self.softmax_func = SoftmaxOp(self.config)
 
     ########### This part is taken/modified form the HF modeling_bloom.py ################
     # Reference: https://github.com/huggingface/transformers/blob/main/src/transformers/models/bloom/modeling_bloom.py
 
     def _transpose_for_context(self, x):
         x = x.permute(0, 2, 1, 3).contiguous()
         new_x_layer_shape = x.size()[:-2] + \
                                     (self.hidden_size_per_partition,)
         return x.view(*new_x_layer_shape).contiguous()
 
-    def _split_tensor_along_last_dim(self,
-                                     tensor,
-                                     num_partitions,
-                                     contiguous_split_chunks=True):
+    def _split_tensor_along_last_dim(self, tensor, num_partitions, contiguous_split_chunks=True):
         """Split a tensor along its last dimension.
 
         Args:
             tensor: ([`torch.tensor`], *required*):
                 input tensor to split
             num_partitions ([`int`], *required*):
                 number of partitions to split the tensor
@@ -192,85 +219,62 @@
 
         if no_masking:
             input_mask = torch.empty(1)
 
         mixed_x_layer = qkv_out
         alibi = alibi.to(get_accelerator().current_device_name())
         head_dim = self.hidden_size_per_partition // self.num_attention_heads_per_partition
-        new_tensor_shape = mixed_x_layer.size()[:-1] + (
-            self.num_attention_heads_per_partition,
-            3 * head_dim)
+        new_tensor_shape = mixed_x_layer.size()[:-1] + (self.num_attention_heads_per_partition, 3 * head_dim)
         mixed_x_layer = mixed_x_layer.view(*new_tensor_shape)
 
         query_layer, key_layer, value_layer = self._split_tensor_along_last_dim(mixed_x_layer, 3)
 
         # [batch_size, head_dim, q_length, k_length]
-        output_size = (query_layer.size(0),
-                       query_layer.size(2),
-                       query_layer.size(1),
-                       key_layer.size(1))
+        output_size = (query_layer.size(0), query_layer.size(2), query_layer.size(1), key_layer.size(1))
         # [batch_size, q_length, num_heads, head_dim] -> [q_length, batch_size * num_heads, head_dim]
-        query_layer = query_layer.transpose(1,
-                                            2).reshape(output_size[0] * output_size[1],
-                                                       output_size[2],
-                                                       -1)
+        query_layer = query_layer.transpose(1, 2).reshape(output_size[0] * output_size[1], output_size[2], -1)
         # [batch_size, k_length, num_heads, head_dim] -> [k_length, batch_size * num_heads, head_dim]
-        key_layer = key_layer.transpose(1,
-                                        2).reshape(output_size[0] * output_size[1],
-                                                   output_size[3],
-                                                   -1).transpose(-1,
-                                                                 -2)
-        value_layer = value_layer.transpose(1,
-                                            2).reshape(output_size[0] * output_size[1],
-                                                       output_size[3],
-                                                       -1)
+        key_layer = key_layer.transpose(1, 2).reshape(output_size[0] * output_size[1], output_size[3],
+                                                      -1).transpose(-1, -2)
+        value_layer = value_layer.transpose(1, 2).reshape(output_size[0] * output_size[1], output_size[3], -1)
         if layer_past is not None:
             past_key, past_value = layer_past
             # concatenate along seq_length dimension -> [batch_size, qk_length, num_heads, head_dim]
             key_layer = torch.cat((past_key.type_as(key_layer), key_layer), dim=-1)
-            value_layer = torch.cat((past_value.type_as(value_layer),
-                                     value_layer),
-                                    dim=-2)
+            value_layer = torch.cat((past_value.type_as(value_layer), value_layer), dim=-2)
 
         presents = (key_layer, value_layer)
         # Raw attention scores. [batch_size * num_heads, q_length, k_length]
         matmul_result = torch.matmul(query_layer, key_layer)
         # change view to [batch_size, num_heads, q_length, k_length]
-        attention_scores = matmul_result.view(output_size[0],
-                                              output_size[1],
-                                              output_size[2],
-                                              -1)
-
-        offset = dist.get_rank(
-        ) * self.num_attention_heads_per_partition if dist.is_initialized() else 0
-        attention_probs = self.softmax_func(
-            attn_scores=attention_scores,
-            attn_mask=((1 - input_mask).half() * minus_inf),
-            alibi=alibi,
-            triangular=(self.config.triangular_masking
-                        and (attention_scores.shape[-2] > 1)),
-            recompute=False,
-            local_attention=False,
-            window_size=1,
-            async_op=False,
-            layer_scale=1 / (self.norm_factor * self.norm_factor),
-            head_offset=offset)
+        attention_scores = matmul_result.view(output_size[0], output_size[1], output_size[2], -1)
+
+        offset = dist.get_rank() * self.num_attention_heads_per_partition if dist.is_initialized() else 0
+        attention_probs = self.softmax_func(attn_scores=attention_scores,
+                                            attn_mask=((1 - input_mask).half() * minus_inf),
+                                            alibi=alibi,
+                                            triangular=(self.config.triangular_masking
+                                                        and (attention_scores.shape[-2] > 1)),
+                                            recompute=False,
+                                            local_attention=False,
+                                            window_size=1,
+                                            async_op=False,
+                                            layer_scale=1 / (self.norm_factor * self.norm_factor),
+                                            head_offset=offset)
 
         # change view [batch_size x num_heads, q_length, k_length]
         attention_probs_reshaped = attention_probs.view(*matmul_result.shape)
 
         # matmul: [batch_size * num_heads, q_length, head_dim]
         context_layer = torch.bmm(attention_probs_reshaped, value_layer)
 
         # change view [batch_size, num_heads, q_length, head_dim]
         context_layer = context_layer.view(
-            context_layer.size(0) // self.num_attention_heads_per_partition,
-            self.num_attention_heads_per_partition,
-            context_layer.size(1),
-            context_layer.shape[-1])
+            context_layer.size(0) // self.num_attention_heads_per_partition, self.num_attention_heads_per_partition,
+            context_layer.size(1), context_layer.shape[-1])
 
         context_layer = self._transpose_for_context(context_layer)
         key_layer = presents[0]
         value_layer = presents[1]
 
         return context_layer, key_layer, value_layer
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/transformer/inference/ds_mlp.py` & `deepspeed-0.9.0/deepspeed/ops/transformer/inference/ds_mlp.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,60 +1,56 @@
-'''
-Copyright 2022 The Microsoft DeepSpeed Team
-'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import math
 import torch
 import torch.nn as nn
 from deepspeed import comm as dist
 from deepspeed.accelerator import get_accelerator
 from .op_binding import MLPGemmOp, VectorMatMulOp, GELUGemmOp, ResidualAddOp
 
 
 class DeepSpeedMLP(nn.Module):
-    def __init__(self,
-                 config,
-                 mp_group=None,
-                 q_scales=None,
-                 q_groups=1,
-                 merge_count=1,
-                 mlp_extra_grouping=False):
+
+    def __init__(self, config, mp_group=None, q_scales=None, q_groups=1, merge_count=1, mlp_extra_grouping=False):
         super(DeepSpeedMLP, self).__init__()
 
         self.config = config
         data_type = torch.int8 if config.q_int8 else torch.half if config.fp16 else torch.float
         data_type_fp = torch.half if config.fp16 else torch.float
         device = get_accelerator().current_device_name()
-        self.attn_nw = nn.Parameter(torch.empty(self.config.hidden_size,
-                                                dtype=data_type_fp,
-                                                device=device),
-                                    requires_grad=False)
-        self.attn_nb = nn.Parameter(torch.empty(self.config.hidden_size,
-                                                dtype=data_type_fp,
-                                                device=device),
-                                    requires_grad=False)
-        intm_size_per_partition = self.config.intermediate_size // self.config.mp_size
-        self.inter_w = nn.Parameter(torch.empty(self.config.hidden_size,
-                                                intm_size_per_partition,
-                                                dtype=data_type,
-                                                device=device),
-                                    requires_grad=False)
-        self.inter_b = nn.Parameter(torch.empty(intm_size_per_partition,
-                                                dtype=data_type_fp,
-                                                device=device),
-                                    requires_grad=False)
-        self.output_w = nn.Parameter(torch.empty(intm_size_per_partition,
-                                                 self.config.hidden_size,
-                                                 dtype=data_type,
-                                                 device=device),
-                                     requires_grad=False)
-        self.output_b = nn.Parameter(torch.empty(self.config.hidden_size,
-                                                 dtype=data_type_fp,
-                                                 device=device),
-                                     requires_grad=False)
+        if self.config.set_empty_params:
+            self.attn_nw = None
+            self.attn_nb = None
+            self.inter_w = None
+            self.inter_b = None
+            self.output_w = None
+            self.output_b = None
+        else:
+            self.attn_nw = nn.Parameter(torch.empty(self.config.hidden_size, dtype=data_type_fp, device=device),
+                                        requires_grad=False)
+            self.attn_nb = nn.Parameter(torch.empty(self.config.hidden_size, dtype=data_type_fp, device=device),
+                                        requires_grad=False)
+            intm_size_per_partition = self.config.intermediate_size // self.config.mp_size
+            self.inter_w = nn.Parameter(torch.empty(self.config.hidden_size,
+                                                    intm_size_per_partition,
+                                                    dtype=data_type,
+                                                    device=device),
+                                        requires_grad=False)
+            self.inter_b = nn.Parameter(torch.empty(intm_size_per_partition, dtype=data_type_fp, device=device),
+                                        requires_grad=False)
+            self.output_w = nn.Parameter(torch.empty(intm_size_per_partition,
+                                                     self.config.hidden_size,
+                                                     dtype=data_type,
+                                                     device=device),
+                                         requires_grad=False)
+            self.output_b = nn.Parameter(torch.empty(self.config.hidden_size, dtype=data_type_fp, device=device),
+                                         requires_grad=False)
 
         # used for quantization
         self.q_scales = q_scales
         self.q_groups = q_groups * 2 if mlp_extra_grouping else q_groups
         self.merge_count = int(math.log2(merge_count))
         self.mp_group = mp_group
 
@@ -75,20 +71,17 @@
                                                       residual=residual,
                                                       input_bias=bias,
                                                       weight_interm=self.inter_w,
                                                       weight_out=self.output_w,
                                                       bias=self.inter_b,
                                                       gamma=self.attn_nw,
                                                       beta=self.attn_nb)
-        residual = self.residual_add_func(
-            hidden_state=output,
-            residual=residual,
-            attention_output=input,
-            attention_bias=bias if bias is not None else self.output_b,
-            final_bias=self.output_b,
-            add_bias=bias is not None,
-            residual_add=residual_add)
-
+        residual = self.residual_add_func(hidden_state=output,
+                                          residual=residual,
+                                          attention_output=input,
+                                          attention_bias=bias if bias is not None else self.output_b,
+                                          final_bias=self.output_b,
+                                          add_bias=bias is not None,
+                                          residual_add=residual_add)
         if self.mp_group is not None and dist.get_world_size(group=self.mp_group) > 1:
             dist.all_reduce(residual, group=self.mp_group)
-
         return residual
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/transformer/inference/moe_inference.py` & `deepspeed-0.9.0/deepspeed/ops/transformer/inference/moe_inference.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,10 +1,12 @@
-'''
-Copyright 2020 The Microsoft DeepSpeed Team
-'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+
 import json
 import math
 import torch
 from torch.autograd import Function
 #from ...inference.engine import inference_cuda_module, specialized_mode
 # Cuda modules will be imported if needed
 inference_cuda_module = None
@@ -39,14 +41,15 @@
                 that by enabling it, the pretraining tasks such as BERT are not affected and can obtain
                 a high accuracy level. On the other hand, for the downstream tasks, such as fine-tuning, we recommend
                 to turn it off in order to be able to reproduce the same result through the regular kernel execution.
 
             scale_attention: If true, both q and k are scaled by 1/sqrt(attention_heads) before attention computation.
             return_tuple: if True, returns the transformer output as a tuple, otherwise returns as a tensor
     """
+
     def __init__(self,
                  hidden_size=-1,
                  intermediate_size=-1,
                  heads=-1,
                  num_hidden_layers=-1,
                  layer_norm_eps=1e-12,
                  local_rank=-1,
@@ -68,31 +71,18 @@
                  min_capacity=1,
                  noisy_gate_policy=None,
                  drop_tokens=True,
                  use_rts=False,
                  mlp_type='standard',
                  scale_attn_by_inverse_layer_idx=False):
         super(DeepSpeedMoEInferenceConfig,
-              self).__init__(
-                  hidden_size,
-                  (intermediate_size if intermediate_size > 0 else 4 * hidden_size),
-                  heads,
-                  num_hidden_layers,
-                  layer_norm_eps,
-                  local_rank,
-                  mp_size,
-                  fp16,
-                  q_int8,
-                  pre_layer_norm,
-                  stochastic_mode,
-                  scale_attention,
-                  triangular_masking,
-                  local_attention,
-                  window_size,
-                  return_tuple)
+              self).__init__(hidden_size, (intermediate_size if intermediate_size > 0 else 4 * hidden_size), heads,
+                             num_hidden_layers, layer_norm_eps, local_rank, mp_size, fp16, q_int8, pre_layer_norm,
+                             stochastic_mode, scale_attention, triangular_masking, local_attention, window_size,
+                             return_tuple)
         self.moe_experts = moe_experts
         self.k = k
         self.capacity_factor = capacity_factor
         self.eval_capacity_factor = eval_capacity_factor
         self.min_capacity = min_capacity
         self.noisy_gate_policy = noisy_gate_policy
         self.drop_tokens = drop_tokens
@@ -112,102 +102,63 @@
     def from_json_file(cls, json_file):
         with open(json_file, "r", encoding='utf-8') as reader:
             text = reader.read()
         return cls.from_dict(json.loads(text))
 
 
 class DeepSpeedMLPFunction(Function):
+
     @staticmethod
-    def forward(ctx,
-                input,
-                inter_w,
-                inter_b,
-                config,
-                output_b,
-                output_w,
-                q_scales,
-                q_groups,
-                merge_count,
-                mp_group,
+    def forward(ctx, input, inter_w, inter_b, config, output_b, output_w, q_scales, q_groups, merge_count, mp_group,
                 async_op):
         if config.q_int8:
-            intermediate = inference_cuda_module.fused_gemm_gelu_int8(
-                input,
-                inter_w,
-                inter_b,
-                config.epsilon,
-                q_scales[2],
-                (q_groups * (2**merge_count)),
-                config.pre_layer_norm)
-            output = inference_cuda_module.vector_matmul_int8(intermediate,
-                                                              output_w,
-                                                              q_scales[3],
-                                                              q_groups,
+            intermediate = inference_cuda_module.fused_gemm_gelu_int8(input, inter_w, inter_b, config.epsilon,
+                                                                      q_scales[2], (q_groups * (2**merge_count)),
+                                                                      config.pre_layer_norm)
+            output = inference_cuda_module.vector_matmul_int8(intermediate, output_w, q_scales[3], q_groups,
                                                               (merge_count))
         else:
             mlp_gemm_func = inference_cuda_module.fused_gemm_gelu_fp16 if config.fp16 else \
                                     inference_cuda_module.fused_gemm_gelu_fp32
 
-            output = mlp_gemm_func(input,
-                                   inter_w,
-                                   inter_b,
-                                   output_w,
-                                   config.epsilon,
-                                   config.pre_layer_norm,
-                                   async_op)
+            output = mlp_gemm_func(input, inter_w, inter_b, output_w, config.epsilon, config.pre_layer_norm, async_op)
         if mp_group is not None and dist.get_world_size(group=mp_group) > 1:
             dist.all_reduce(output, group=mp_group, async_op=async_op)
 
         return output + output_b
 
     @staticmethod
     def backward(ctx, grad_output):
         raise RuntimeError('You are running with DeepSpeed Inference mode. \
                             Please switch to Training mode for running backward!')
 
 
 class DeepSpeedMoEMLP(nn.Module):
-    def __init__(self,
-                 config,
-                 q_scales=None,
-                 q_groups=1,
-                 merge_count=1,
-                 mlp_extra_grouping=False,
-                 mp_group=None):
+
+    def __init__(self, config, q_scales=None, q_groups=1, merge_count=1, mlp_extra_grouping=False, mp_group=None):
         super(DeepSpeedMoEMLP, self).__init__()
 
         self.config = config
         self.attn_nw = nn.Parameter(torch.Tensor(self.config.hidden_size))
         self.attn_nb = nn.Parameter(torch.Tensor(self.config.hidden_size))
-        interm_size = self.config.intermediate_size // (
-            1 if mp_group is None else dist.get_world_size(group=mp_group))
+        interm_size = self.config.intermediate_size // (1 if mp_group is None else dist.get_world_size(group=mp_group))
         self.inter_w = nn.Parameter(torch.Tensor(self.config.hidden_size, interm_size))
         self.inter_b = nn.Parameter(torch.Tensor(interm_size))
-        self.output_w = nn.Parameter(torch.Tensor((interm_size),
-                                                  self.config.hidden_size))
+        self.output_w = nn.Parameter(torch.Tensor((interm_size), self.config.hidden_size))
         self.output_b = nn.Parameter(torch.Tensor(self.config.hidden_size))
 
         # used for quantization
         self.q_scales = q_scales
         self.q_groups = q_groups * 2 if mlp_extra_grouping else q_groups
         self.merge_count = int(math.log2(merge_count))
         self.mp_group = mp_group
 
     def forward(self, input, async_op=False):
-        return DeepSpeedMLPFunction.apply(input,
-                                          self.inter_w,
-                                          self.inter_b,
-                                          self.config,
-                                          self.output_b,
-                                          self.output_w,
-                                          self.q_scales,
-                                          self.q_groups,
-                                          self.merge_count,
-                                          self.mp_group,
-                                          async_op)
+        return DeepSpeedMLPFunction.apply(input, self.inter_w, self.inter_b, self.config, self.output_b, self.output_w,
+                                          self.q_scales, self.q_groups, self.merge_count, self.mp_group, async_op)
 
 
 class DeepSpeedMoEInference(nn.Module):
     """Initialize the DeepSpeed MoE Transformer Layer.
         Arguments:
             layer_id: The layer index starting from 0, e.g. if model has 24 transformer layers,
                 layer_id will be 0,1,2...23 when each layer object is instantiated
@@ -247,55 +198,38 @@
                 inference_cuda_module = builder.load()
                 specialized_mode = True
             else:
                 inference_cuda_module = InferenceBuilder().load()
         self.config.specialized_mode = specialized_mode
 
         DeepSpeedMoEInference.layer_id += 1
-        self.attention = DeepSpeedSelfAttention(self.config,
-                                                mp_group,
-                                                quantize_scales,
-                                                quantize_groups,
-                                                merge_count)
+        self.attention = DeepSpeedSelfAttention(self.config, mp_group, quantize_scales, quantize_groups, merge_count)
         self.attn_nw = nn.Parameter(torch.Tensor(self.config.hidden_size))
         self.attn_nb = nn.Parameter(torch.Tensor(self.config.hidden_size))
 
         self.norm_w = nn.Parameter(torch.Tensor(self.config.hidden_size))
         self.norm_b = nn.Parameter(torch.Tensor(self.config.hidden_size))
 
         if config.mlp_type == 'residual':
-            self.res_mlp = DeepSpeedMoEMLP(config,
-                                           quantize_scales,
-                                           quantize_groups,
-                                           merge_count,
-                                           mlp_extra_grouping,
+            self.res_mlp = DeepSpeedMoEMLP(config, quantize_scales, quantize_groups, merge_count, mlp_extra_grouping,
                                            mp_group)
             self.res_coef = nn.Parameter(torch.Tensor(self.config.hidden_size, 2))
             self.coef_func = inference_cuda_module.softmax_fp16 if self.config.fp16 or self.config.q_int8 else \
                                         inference_cuda_module.softmax_fp32
             self.vector_matmul_func = inference_cuda_module.vector_matmul_fp16 if config.fp16 else \
                                     inference_cuda_module.vector_matmul_fp32
 
         config.mp_size = 1
         self.mlp = nn.ModuleList(
-            DeepSpeedMoEMLP(config,
-                            quantize_scales,
-                            quantize_groups,
-                            merge_count,
-                            mlp_extra_grouping,
-                            expert_mp_group) for i in range(self.config.moe_experts))
-
-        self.moe_gate = TopKGate(self.config.hidden_size,
-                                 self.config.global_experts,
-                                 self.config.k,
-                                 self.config.capacity_factor,
-                                 self.config.eval_capacity_factor,
-                                 self.config.min_capacity,
-                                 self.config.noisy_gate_policy,
-                                 self.config.drop_tokens,
+            DeepSpeedMoEMLP(config, quantize_scales, quantize_groups, merge_count, mlp_extra_grouping, expert_mp_group)
+            for i in range(self.config.moe_experts))
+
+        self.moe_gate = TopKGate(self.config.hidden_size, self.config.global_experts, self.config.k,
+                                 self.config.capacity_factor, self.config.eval_capacity_factor,
+                                 self.config.min_capacity, self.config.noisy_gate_policy, self.config.drop_tokens,
                                  self.config.use_rts)
 
         self.ep_group = ep_group
         self.mp_group = mp_group
         self.expert_mp_group = expert_mp_group
 
         print("DeepSpeed MoE Transformer Inference config is ", self.config.__dict__)
@@ -311,59 +245,47 @@
         inp = self.vector_matmul_func(inp, self.res_coef, async_op)
         return self.coef_func(inp, torch.empty(1), False, False, False, 256, async_op)
 
     def moe_gate_einsum(self, attention_output):
         _, combined_weights, dispatch_mask, _ = self.moe_gate(
             attention_output.view(-1, self.config.hidden_size),
             None,
-            )
-        dispatched_attention = self.einsum_sec_sm_ecm(
-            dispatch_mask.type_as(attention_output),
-            attention_output.view(-1,
-                                  self.config.hidden_size))
+        )
+        dispatched_attention = self.einsum_sec_sm_ecm(dispatch_mask.type_as(attention_output),
+                                                      attention_output.view(-1, self.config.hidden_size))
         return dispatched_attention, combined_weights
 
     def expert_exec(self, dispatched_input):
-        dispatched_input = dispatched_input.reshape(
-            self.config.global_experts // self.config.moe_experts,
-            self.config.moe_experts,
-            -1,
-            self.config.hidden_size)
+        dispatched_input = dispatched_input.reshape(self.config.global_experts // self.config.moe_experts,
+                                                    self.config.moe_experts, -1, self.config.hidden_size)
 
         chunks = dispatched_input.chunk(self.config.moe_experts, dim=1)
         expert_outputs = torch.empty((
             self.config.moe_experts,
             chunks[0].shape[0],
         ) + chunks[0].shape[2:],
                                      dtype=dispatched_input.dtype,
                                      device=dispatched_input.device)
         for chunk, expert in zip(chunks, range(len(self.mlp))):
-            expert_outputs[expert] = self.mlp[expert](chunk.view(
-                -1,
-                dispatched_input.shape[-2],
-                dispatched_input.shape[-1]))
+            expert_outputs[expert] = self.mlp[expert](chunk.view(-1, dispatched_input.shape[-2],
+                                                                 dispatched_input.shape[-1]))
         return expert_outputs
 
     def _alltoall(self, dispatched_attention):
         if dist.get_world_size(group=self.ep_group) > 1:
             dispatched_input = torch.empty_like(dispatched_attention)
-            dist.all_to_all_single(dispatched_input,
-                                   dispatched_attention,
-                                   group=self.ep_group)
+            dist.all_to_all_single(dispatched_input, dispatched_attention, group=self.ep_group)
             return dispatched_input
         else:
             return dispatched_attention
 
     def scale_expert_output(self, attention_output, expert_output, combined_weights):
         combined_output = torch.matmul(
-            combined_weights.type_as(attention_output).reshape(
-                combined_weights.shape[0],
-                -1),
-            expert_output.reshape(-1,
-                                  expert_output.shape[-1]))
+            combined_weights.type_as(attention_output).reshape(combined_weights.shape[0], -1),
+            expert_output.reshape(-1, expert_output.shape[-1]))
         return combined_output.reshape(attention_output.shape)
 
     def forward(self,
                 input,
                 input_mask=None,
                 attention_mask=None,
                 head_mask=None,
@@ -381,79 +303,60 @@
         input_type = input.dtype
 
         if (self.config.fp16 or self.config.q_int8) \
             and input.dtype == torch.float:
             input = input.half()
 
         with torch.no_grad():
-            attention_output = self.attention(input,
-                                              input_mask,
-                                              head_mask,
-                                              layer_past,
-                                              get_present,
-                                              encoder_hidden_states,
-                                              encoder_attention_mask,
-                                              output_attentions,
-                                              self.norm_w,
-                                              self.norm_b)
+            attention_output = self.attention(input, input_mask, head_mask, layer_past, get_present,
+                                              encoder_hidden_states, encoder_attention_mask, output_attentions,
+                                              self.norm_w, self.norm_b)
 
             if get_present:
                 attention_output, p_key, p_value = attention_output[0:3]
                 presents = (p_key, p_value)
             elif output_attentions:
                 attention_output, _, _, context_output = attention_output[0:4]
             else:
                 attention_output = attention_output[0]
 
             residual_add = attention_output + self.attention.attn_ob
-            attention_output = self.ds_layernorm(residual_add,
-                                                 self.attn_nw,
-                                                 self.attn_nb,
-                                                 self.config.epsilon)
+            attention_output = self.ds_layernorm(residual_add, self.attn_nw, self.attn_nb, self.config.epsilon)
 
             if self.config.mlp_type == 'residual':
                 res_mlp_out = self.res_mlp(attention_output, async_op=True)
                 res_coef_out = self.res_coef_func(attention_output, async_op=True)
 
             if self.expert_mp_group is not None:
                 tensor_list = [
-                    torch.empty_like(attention_output)
-                    for _ in range(dist.get_world_size(group=self.expert_mp_group))
+                    torch.empty_like(attention_output) for _ in range(dist.get_world_size(group=self.expert_mp_group))
                 ]
                 tensor_list[dist.get_rank(group=self.expert_mp_group)] = attention_output
-                dist.all_gather(tensor_list,
-                                attention_output,
-                                group=self.expert_mp_group)
+                dist.all_gather(tensor_list, attention_output, group=self.expert_mp_group)
                 attention_output = torch.cat(tensor_list).contiguous()
 
             ############## MoE Gating + Experts ###############
             dispatched_attention, combined_weights = self.moe_gate_einsum(attention_output)
             dispatched_input = self._alltoall(dispatched_attention)
             expert_outputs = self.expert_exec(dispatched_input)
             expert_output = self._alltoall(expert_outputs)
-            output = self.scale_expert_output(attention_output,
-                                              expert_output,
-                                              combined_weights)
+            output = self.scale_expert_output(attention_output, expert_output, combined_weights)
             ################################################
 
             if self.expert_mp_group is not None:
-                output = output.split(output.shape[0] //
-                                      dist.get_world_size(group=self.expert_mp_group),
+                output = output.split(output.shape[0] // dist.get_world_size(group=self.expert_mp_group),
                                       dim=0)[dist.get_rank(group=self.expert_mp_group)]
 
             if self.config.mlp_type == 'residual':
                 inference_cuda_module.moe_res_matmul(res_mlp_out, res_coef_out, output)
 
             output = self.bias_residual_func(output, residual_add, torch.empty(1))
 
             if not self.config.pre_layer_norm:
-                output = self.ds_layernorm(output,
-                                           self.norm_w,
-                                           self.norm_b,
-                                           self.config.epsilon)
+                output = self.ds_layernorm(output, self.norm_w, self.norm_b, self.config.epsilon)
 
             if input_type != output.dtype:
                 output = output.to(input_type)
 
         if get_present:
             output = (output, presents)
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/transformer/inference/op_binding/gelu_gemm.py` & `deepspeed-0.9.0/deepspeed/ops/transformer/inference/op_binding/gelu_gemm.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,32 +1,29 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import torch
 from ..config import DeepSpeedInferenceConfig
 from .base import BaseOp
 
 
 class GELUGemmOp(BaseOp):
+
     def __init__(self, config: DeepSpeedInferenceConfig):
         super(GELUGemmOp, self).__init__(config)
         if self.config.fp16:
             self.fused_gemm_gelu = self.inference_cuda_module.fused_gemm_gelu_fp16
         else:
             self.fused_gemm_gelu = self.inference_cuda_module.fused_gemm_gelu_fp32
 
     def forward(self,
                 input: torch.Tensor,
                 weight: torch.Tensor,
                 bias: torch.Tensor,
                 weight_out: torch.Tensor,
                 async_op: bool = False):
-        output = self.fused_gemm_gelu(input,
-                                      weight,
-                                      weight.scale,
-                                      bias,
-                                      weight_out,
-                                      weight_out.scale,
-                                      self.config.epsilon,
-                                      self.config.pre_layer_norm,
-                                      self.config.q_int8,
-                                      async_op)
+        output = self.fused_gemm_gelu(input, weight, weight.scale, bias, weight_out, weight_out.scale,
+                                      self.config.epsilon, self.config.pre_layer_norm, self.config.q_int8, async_op,
+                                      self.config.transposed_mode)
         return output
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/transformer/inference/op_binding/linear.py` & `deepspeed-0.9.0/deepspeed/ops/transformer/inference/op_binding/linear.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,15 +1,19 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import torch
 from ..config import DeepSpeedInferenceConfig
 from .base import BaseOp
 
 
 class LinearOp(BaseOp):
+
     def __init__(self, config: DeepSpeedInferenceConfig):
         super(LinearOp, self).__init__(config)
         if self.config.fp16:
             self.linear_func = self.inference_cuda_module.linear_layer_fp16
         else:
             self.linear_func = self.inference_cuda_module.linear_layer_fp32
 
@@ -18,14 +22,10 @@
                 weight: torch.Tensor,
                 bias: torch.Tensor,
                 add_bias: bool,
                 do_flash_attn: bool,
                 num_heads: int,
                 external_cache: bool = None,
                 num_layers: int = None):
-        qkv_out = self.linear_func(input,
-                                   weight,
-                                   bias,
-                                   add_bias,
-                                   do_flash_attn,
-                                   num_heads)
+        qkv_out = self.linear_func(input, weight, bias, add_bias, do_flash_attn, num_heads,
+                                   self.config.transposed_mode)
         return qkv_out
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/transformer/inference/op_binding/qkv_gemm.py` & `deepspeed-0.9.0/deepspeed/ops/transformer/inference/op_binding/qkv_gemm.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,16 +1,20 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import torch
 from ..config import DeepSpeedInferenceConfig
 from .base import BaseOp
 from deepspeed import comm as dist
 
 
 class QKVGemmOp(BaseOp):
+
     def __init__(self, config: DeepSpeedInferenceConfig):
         super(QKVGemmOp, self).__init__(config)
         if self.config.fp16:
             self.qkv_gemm_func = self.inference_cuda_module.qkv_gemm_fp16
         else:
             self.qkv_gemm_func = self.inference_cuda_module.qkv_gemm_fp32
 
@@ -20,25 +24,15 @@
                 bias: torch.Tensor,
                 gamma: torch.Tensor,
                 beta: torch.Tensor,
                 add_bias: bool,
                 num_layers: int,
                 num_heads: int = None,
                 max_out_tokens: int = None):
-        q_scale = weight.scale
+        q_scale = weight.scale if hasattr(weight, 'scale') else torch.empty(1)
         external_cache = self.config.bigscience_bloom
         rank = dist.get_rank() if dist.is_initialized() else 0
         q_int8 = self.config.q_int8
-        output = self.qkv_gemm_func(input,
-                                    weight,
-                                    q_scale,
-                                    bias,
-                                    gamma,
-                                    beta,
-                                    self.config.epsilon,
-                                    add_bias,
-                                    num_layers,
-                                    external_cache,
-                                    self.config.mp_size,
-                                    rank,
-                                    q_int8)
+        output = self.qkv_gemm_func(input, weight, q_scale, bias, gamma, beta, self.config.epsilon, add_bias,
+                                    num_layers, external_cache, self.config.mp_size, rank, q_int8,
+                                    self.config.transposed_mode)
         return output
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/transformer/inference/op_binding/softmax.py` & `deepspeed-0.9.0/deepspeed/ops/transformer/inference/op_binding/softmax_context.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,41 +1,35 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import torch
+from deepspeed import comm as dist
 from ..config import DeepSpeedInferenceConfig
 from .base import BaseOp
 
 
-class SoftmaxOp(BaseOp):
+class SoftmaxContextOp(BaseOp):
+
     def __init__(self, config: DeepSpeedInferenceConfig):
-        super(SoftmaxOp, self).__init__(config)
+        super(SoftmaxContextOp, self).__init__(config)
         if self.config.fp16:
-            self.softmax_func = self.inference_cuda_module.softmax_fp16
+            self.softmax_context_func = self.inference_cuda_module.softmax_context_fp16
         else:
-            self.softmax_func = self._not_implemented
+            self.softmax_context_func = self.inference_cuda_module.softmax_context_fp32
 
-    def _not_implemented(self, *args, **kwargs):
-        raise NotImplementedError
+    def forward(self, query_key_value: torch.Tensor, attn_mask: torch.Tensor, heads: int, norm_factor: float,
+                no_masking: bool, layer_id: int, num_layers: int, alibi: torch.Tensor):
+
+        if alibi is not None:
+            batch_heads = query_key_value.shape[0] * heads
+            offset = dist.get_rank() * batch_heads if dist.is_initialized() else 0
+            alibi = alibi[offset:batch_heads + offset, :, :]
+        else:
+            alibi = torch.empty(1)
 
-    def forward(self,
-                attn_scores: torch.Tensor,
-                attn_mask: torch.Tensor,
-                alibi: torch.Tensor,
-                triangular: bool,
-                recompute: bool,
-                local_attention: bool,
-                window_size: int,
-                async_op: bool,
-                layer_scale: float,
-                head_offset: int):
-        output = self.softmax_func(attn_scores,
-                                   attn_mask,
-                                   alibi,
-                                   triangular,
-                                   recompute,
-                                   local_attention,
-                                   window_size,
-                                   async_op,
-                                   layer_scale,
-                                   head_offset,
-                                   self.config.mp_size)
+        output = self.softmax_context_func(query_key_value, attn_mask, self.config.rotary_dim, self.config.rotate_half,
+                                           self.config.rotate_every_two, heads, norm_factor,
+                                           self.config.triangular_masking, self.config.local_attention,
+                                           self.config.window_size, no_masking, layer_id, num_layers, alibi)
         return output
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/transformer/inference/op_binding/vector_matmul.py` & `deepspeed-0.9.0/deepspeed/ops/transformer/inference/op_binding/vector_matmul.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,20 +1,24 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import torch
 from ..config import DeepSpeedInferenceConfig
 from .base import BaseOp
 
 
 class VectorMatMulOp(BaseOp):
+
     def __init__(self, config: DeepSpeedInferenceConfig):
         super(VectorMatMulOp, self).__init__(config)
         if self.config.fp16:
             self.vector_matmul_func = self.inference_cuda_module.vector_matmul_fp16
         else:
             self.vector_matmul_func = self.inference_cuda_module.vector_matmul_fp32
 
     def forward(self, input: torch.Tensor, weight: torch.Tensor, async_op: bool = False):
-        q_scale = weight.scale
+        q_scale = weight.scale if hasattr(weight, 'scale') else torch.empty(1)
         q_int8 = self.config.q_int8
-        output = self.vector_matmul_func(input, weight, async_op, q_scale, q_int8)
+        output = self.vector_matmul_func(input, weight, async_op, q_scale, q_int8, self.config.transposed_mode)
         return output
```

### Comparing `deepspeed-0.8.3/deepspeed/ops/transformer/inference/triton_ops.py` & `deepspeed-0.9.0/deepspeed/ops/transformer/inference/triton_ops.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,8 +1,11 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 """
 Inspired by original Triton implementation:
 https://github.com/openai/triton/blob/b244db06da24a87453a40ad35b085ee37dac3705/python/tutorials/06-fused-attention.py
 """
 
 import torch
 import triton
@@ -98,27 +101,25 @@
     offs_n = tl.arange(0, BLOCK_DMODEL)
     off_o = off_hz * stride_oh + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on
     out_ptrs = Out + off_o
     tl.store(out_ptrs, acc)
 
 
 class triton_flash_attn(torch.nn.Module):
+
     def __init__(self, ):
         super(triton_flash_attn, self).__init__()
 
     def forward(self, q, k, v, sm_scale, block_128=True):
         BLOCK = 128 if block_128 else 64
         # shape constraints
         Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]
         o = torch.empty_like(q)
         grid = (triton.cdiv(q.shape[2], BLOCK), q.shape[0] * q.shape[1])
-        tmp = torch.empty((q.shape[0] * q.shape[1],
-                           q.shape[2]),
-                          device=q.device,
-                          dtype=torch.float32)
+        tmp = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)
         num_warps = 4 if Lk <= 64 else 8
 
         _fwd_kernel[grid](
             q,
             k,
             v,
             sm_scale,
```

### Comparing `deepspeed-0.8.3/deepspeed/profiling/config.py` & `deepspeed-0.9.0/deepspeed/profiling/config.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,18 +1,18 @@
-'''Copyright The Microsoft DeepSpeed Team'''
-"""
-Copyright (c) Microsoft Corporation
-Licensed under the MIT license.
-"""
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 from deepspeed.runtime.config_utils import get_scalar_param, DeepSpeedConfigObject
 from deepspeed.profiling.constants import *
 
 
 class DeepSpeedFlopsProfilerConfig(DeepSpeedConfigObject):
+
     def __init__(self, param_dict):
         super(DeepSpeedFlopsProfilerConfig, self).__init__()
 
         self.enabled = None
         self.profile_step = None
         self.module_depth = None
         self.top_modules = None
@@ -21,30 +21,22 @@
             flops_profiler_dict = param_dict[FLOPS_PROFILER]
         else:
             flops_profiler_dict = {}
 
         self._initialize(flops_profiler_dict)
 
     def _initialize(self, flops_profiler_dict):
-        self.enabled = get_scalar_param(flops_profiler_dict,
-                                        FLOPS_PROFILER_ENABLED,
-                                        FLOPS_PROFILER_ENABLED_DEFAULT)
+        self.enabled = get_scalar_param(flops_profiler_dict, FLOPS_PROFILER_ENABLED, FLOPS_PROFILER_ENABLED_DEFAULT)
 
-        self.profile_step = get_scalar_param(flops_profiler_dict,
-                                             FLOPS_PROFILER_PROFILE_STEP,
+        self.profile_step = get_scalar_param(flops_profiler_dict, FLOPS_PROFILER_PROFILE_STEP,
                                              FLOPS_PROFILER_PROFILE_STEP_DEFAULT)
 
-        self.module_depth = get_scalar_param(flops_profiler_dict,
-                                             FLOPS_PROFILER_MODULE_DEPTH,
+        self.module_depth = get_scalar_param(flops_profiler_dict, FLOPS_PROFILER_MODULE_DEPTH,
                                              FLOPS_PROFILER_MODULE_DEPTH_DEFAULT)
 
-        self.top_modules = get_scalar_param(flops_profiler_dict,
-                                            FLOPS_PROFILER_TOP_MODULES,
+        self.top_modules = get_scalar_param(flops_profiler_dict, FLOPS_PROFILER_TOP_MODULES,
                                             FLOPS_PROFILER_TOP_MODULES_DEFAULT)
 
-        self.detailed = get_scalar_param(flops_profiler_dict,
-                                         FLOPS_PROFILER_DETAILED,
-                                         FLOPS_PROFILER_DETAILED_DEFAULT)
+        self.detailed = get_scalar_param(flops_profiler_dict, FLOPS_PROFILER_DETAILED, FLOPS_PROFILER_DETAILED_DEFAULT)
 
-        self.output_file = get_scalar_param(flops_profiler_dict,
-                                            FLOPS_PROFILER_OUTPUT_FILE,
+        self.output_file = get_scalar_param(flops_profiler_dict, FLOPS_PROFILER_OUTPUT_FILE,
                                             FLOPS_PROFILER_OUTPUT_FILE_DEFAULT)
```

### Comparing `deepspeed-0.8.3/deepspeed/profiling/constants.py` & `deepspeed-0.9.0/deepspeed/profiling/constants.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,12 +1,11 @@
-'''Copyright The Microsoft DeepSpeed Team'''
-"""
-Copyright (c) Microsoft Corporation
-Licensed under the MIT license.
-"""
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 #########################################
 # flops profiler
 #########################################
 # Flops profiler. By default, this feature is not enabled.
 # Users can configure in ds_config.json as below example:
 FLOPS_PROFILER_FORMAT = '''
```

### Comparing `deepspeed-0.8.3/deepspeed/profiling/flops_profiler/profiler.py` & `deepspeed-0.9.0/deepspeed/profiling/flops_profiler/profiler.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,8 +1,11 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import time
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
 from functools import partial
 from typing import List, Optional
@@ -49,14 +52,15 @@
                 optimizer.step()
 
     To profile a trained model in inference, use the `get_model_profile` API.
 
     Args:
         object (torch.nn.Module): The PyTorch model to profile.
     """
+
     def __init__(self, model, ds_engine=None):
         self.model = model
         self.ds_engine = ds_engine
         self.started = False
         self.func_patched = False
 
     def start_profile(self, ignore_list=None):
@@ -74,16 +78,15 @@
         def register_module_hooks(module, ignore_list):
             if ignore_list and type(module) in ignore_list:
                 return
 
             # if computing the flops of a module directly
             if type(module) in MODULE_HOOK_MAPPING:
                 if not hasattr(module, "__flops_handle__"):
-                    module.__flops_handle__ = module.register_forward_hook(
-                        MODULE_HOOK_MAPPING[type(module)])
+                    module.__flops_handle__ = module.register_forward_hook(MODULE_HOOK_MAPPING[type(module)])
                 return
 
             # if computing the flops of the functionals in a module
             def pre_hook(module, input):
                 module_flop_count.append([])
                 module_mac_count.append([])
 
@@ -101,24 +104,22 @@
                 module.__post_hook_handle__ = module.register_forward_hook(post_hook)
 
             def start_time_hook(module, input):
                 get_accelerator().synchronize()
                 module.__start_time__ = time.time()
 
             if not hasattr(module, "__start_time_hook_handle"):
-                module.__start_time_hook_handle__ = module.register_forward_pre_hook(
-                    start_time_hook)
+                module.__start_time_hook_handle__ = module.register_forward_pre_hook(start_time_hook)
 
             def end_time_hook(module, input, output):
                 get_accelerator().synchronize()
                 module.__duration__ += time.time() - module.__start_time__
 
             if not hasattr(module, "__end_time_hook_handle__"):
-                module.__end_time_hook_handle__ = module.register_forward_hook(
-                    end_time_hook)
+                module.__end_time_hook_handle__ = module.register_forward_hook(end_time_hook)
 
         self.model.apply(partial(register_module_hooks, ignore_list=ignore_list))
         self.started = True
         self.func_patched = True
 
     def stop_profile(self):
         """Stop profiling.
@@ -150,14 +151,15 @@
         self.model.apply(remove_profile_attrs)
 
     def reset_profile(self):
         """Resets the profiling.
 
         Adds or resets the extra attributes.
         """
+
         def add_or_reset_attrs(module):
             module.__flops__ = 0
             module.__macs__ = 0
             module.__params__ = sum(p.numel() for p in module.parameters())
             module.__start_time__ = 0
             module.__duration__ = 0
 
@@ -228,23 +230,17 @@
 
         Args:
             as_string (bool, optional): whether to output the parameters as string. Defaults to False.
 
         Returns:
             The number of parameters in the model.
         """
-        return params_to_string(
-            self.model.__params__) if as_string else self.model.__params__
+        return params_to_string(self.model.__params__) if as_string else self.model.__params__
 
-    def print_model_profile(self,
-                            profile_step=1,
-                            module_depth=-1,
-                            top_modules=1,
-                            detailed=True,
-                            output_file=None):
+    def print_model_profile(self, profile_step=1, module_depth=-1, top_modules=1, detailed=True, output_file=None):
         """Prints the model graph with the measured profile attached to each module.
 
         Args:
             profile_step (int, optional): The global training step at which to profile. Note that warm up steps are needed for accurate time measurement.
             module_depth (int, optional): The depth of the model to which to print the aggregated module information. When set to -1, it prints information from the top to the innermost modules (the maximum depth).
             top_modules (int, optional): Limits the aggregated profile output to the number of top modules specified.
             detailed (bool, optional): Whether to print the detailed model profile.
@@ -269,97 +265,78 @@
         total_duration = self.get_total_duration()
         total_params = self.get_total_params()
 
         self.flops = total_flops
         self.macs = total_macs
         self.params = total_params
 
-        print(
-            "\n-------------------------- DeepSpeed Flops Profiler --------------------------"
-        )
+        print("\n-------------------------- DeepSpeed Flops Profiler --------------------------")
         print(f'Profile Summary at step {profile_step}:')
         print(
             "Notations:\ndata parallel size (dp_size), model parallel size(mp_size),\nnumber of parameters (params), number of multiply-accumulate operations(MACs),\nnumber of floating-point operations (flops), floating-point operations per second (FLOPS),\nfwd latency (forward propagation latency), bwd latency (backward propagation latency),\nstep (weights update latency), iter latency (sum of fwd, bwd and step latency)\n"
         )
         if self.ds_engine:
             print('{:<60}  {:<8}'.format('world size: ', self.ds_engine.world_size))
-            print('{:<60}  {:<8}'.format('data parallel size: ',
-                                         self.ds_engine.dp_world_size))
-            print('{:<60}  {:<8}'.format('model parallel size: ',
-                                         self.ds_engine.mp_world_size))
-            print('{:<60}  {:<8}'.format(
-                'batch size per GPU: ',
-                self.ds_engine.train_micro_batch_size_per_gpu()))
+            print('{:<60}  {:<8}'.format('data parallel size: ', self.ds_engine.dp_world_size))
+            print('{:<60}  {:<8}'.format('model parallel size: ', self.ds_engine.mp_world_size))
+            print('{:<60}  {:<8}'.format('batch size per GPU: ', self.ds_engine.train_micro_batch_size_per_gpu()))
 
         print('{:<60}  {:<8}'.format('params per gpu: ', params_to_string(total_params)))
         print('{:<60}  {:<8}'.format(
             'params of model = params per GPU * mp_size: ',
-            params_to_string(total_params *
-                             ((self.ds_engine.mp_world_size) if self.ds_engine else 1))))
+            params_to_string(total_params * ((self.ds_engine.mp_world_size) if self.ds_engine else 1))))
 
         print('{:<60}  {:<8}'.format('fwd MACs per GPU: ', macs_to_string(total_macs)))
 
         print('{:<60}  {:<8}'.format('fwd flops per GPU: ', num_to_string(total_flops)))
 
         print('{:<60}  {:<8}'.format(
             'fwd flops of model = fwd flops per GPU * mp_size: ',
-            num_to_string(total_flops *
-                          ((self.ds_engine.mp_world_size) if self.ds_engine else 1))))
+            num_to_string(total_flops * ((self.ds_engine.mp_world_size) if self.ds_engine else 1))))
 
         fwd_latency = self.get_total_duration()
         if self.ds_engine and self.ds_engine.wall_clock_breakdown():
             fwd_latency = self.ds_engine.timers('forward').elapsed(False) / 1000.0
         print('{:<60}  {:<8}'.format('fwd latency: ', duration_to_string(fwd_latency)))
-        print('{:<60}  {:<8}'.format(
-            'fwd FLOPS per GPU = fwd flops per GPU / fwd latency: ',
-            flops_to_string(total_flops / fwd_latency)))
+        print('{:<60}  {:<8}'.format('fwd FLOPS per GPU = fwd flops per GPU / fwd latency: ',
+                                     flops_to_string(total_flops / fwd_latency)))
 
         if self.ds_engine and self.ds_engine.wall_clock_breakdown():
             bwd_latency = self.ds_engine.timers('backward').elapsed(False) / 1000.0
             step_latency = self.ds_engine.timers('step').elapsed(False) / 1000.0
-            print('{:<60}  {:<8}'.format('bwd latency: ',
-                                         duration_to_string(bwd_latency)))
-            print('{:<60}  {:<8}'.format(
-                'bwd FLOPS per GPU = 2 * fwd flops per GPU / bwd latency: ',
-                flops_to_string(2 * total_flops / bwd_latency)))
-            print('{:<60}  {:<8}'.format(
-                'fwd+bwd FLOPS per GPU = 3 * fwd flops per GPU / (fwd+bwd latency): ',
-                flops_to_string(3 * total_flops / (fwd_latency + bwd_latency))))
+            print('{:<60}  {:<8}'.format('bwd latency: ', duration_to_string(bwd_latency)))
+            print('{:<60}  {:<8}'.format('bwd FLOPS per GPU = 2 * fwd flops per GPU / bwd latency: ',
+                                         flops_to_string(2 * total_flops / bwd_latency)))
+            print('{:<60}  {:<8}'.format('fwd+bwd FLOPS per GPU = 3 * fwd flops per GPU / (fwd+bwd latency): ',
+                                         flops_to_string(3 * total_flops / (fwd_latency + bwd_latency))))
 
-            print('{:<60}  {:<8}'.format('step latency: ',
-                                         duration_to_string(step_latency)))
+            print('{:<60}  {:<8}'.format('step latency: ', duration_to_string(step_latency)))
 
             iter_latency = fwd_latency + bwd_latency + step_latency
-            print('{:<60}  {:<8}'.format('iter latency: ',
-                                         duration_to_string(iter_latency)))
-            print('{:<60}  {:<8}'.format(
-                'FLOPS per GPU = 3 * fwd flops per GPU / iter latency: ',
-                flops_to_string(3 * total_flops / iter_latency)))
-
-            samples_per_iter = self.ds_engine.train_micro_batch_size_per_gpu(
-            ) * self.ds_engine.world_size
-            print('{:<60}  {:<8.2f}'.format('samples/second: ',
-                                            samples_per_iter / iter_latency))
+            print('{:<60}  {:<8}'.format('iter latency: ', duration_to_string(iter_latency)))
+            print('{:<60}  {:<8}'.format('FLOPS per GPU = 3 * fwd flops per GPU / iter latency: ',
+                                         flops_to_string(3 * total_flops / iter_latency)))
+
+            samples_per_iter = self.ds_engine.train_micro_batch_size_per_gpu() * self.ds_engine.world_size
+            print('{:<60}  {:<8.2f}'.format('samples/second: ', samples_per_iter / iter_latency))
 
         def flops_repr(module):
             params = module.__params__
             flops = get_module_flops(module)
             macs = get_module_macs(module)
             items = [
                 params_to_string(params),
                 "{:.2%} Params".format(params / total_params if total_params else 0),
                 macs_to_string(macs),
                 "{:.2%} MACs".format(0.0 if total_macs == 0 else macs / total_macs),
             ]
             duration = get_module_duration(module)
 
             items.append(duration_to_string(duration))
-            items.append(
-                "{:.2%} latency".format(0.0 if total_duration == 0 else duration /
-                                        total_duration))
+            items.append("{:.2%} latency".format(0.0 if total_duration == 0 else duration / total_duration))
             items.append(flops_to_string(0.0 if duration == 0 else flops / duration))
             items.append(module.original_extra_repr())
             return ", ".join(items)
 
         def add_extra_repr(module):
             flops_extra_repr = flops_repr.__get__(module)
             if module.extra_repr != flops_extra_repr:
@@ -370,54 +347,45 @@
         def del_extra_repr(module):
             if hasattr(module, "original_extra_repr"):
                 module.extra_repr = module.original_extra_repr
                 del module.original_extra_repr
 
         self.model.apply(add_extra_repr)
 
-        print(
-            "\n----------------------------- Aggregated Profile per GPU -----------------------------"
-        )
-        self.print_model_aggregated_profile(module_depth=module_depth,
-                                            top_modules=top_modules)
+        print("\n----------------------------- Aggregated Profile per GPU -----------------------------")
+        self.print_model_aggregated_profile(module_depth=module_depth, top_modules=top_modules)
 
         if detailed:
-            print(
-                "\n------------------------------ Detailed Profile per GPU ------------------------------"
-            )
+            print("\n------------------------------ Detailed Profile per GPU ------------------------------")
             print(
                 "Each module profile is listed after its name in the following order: \nparams, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS"
             )
             print(
                 "\nNote: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.\n2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.\n"
             )
             print(self.model)
 
         self.model.apply(del_extra_repr)
 
-        print(
-            "------------------------------------------------------------------------------"
-        )
+        print("------------------------------------------------------------------------------")
 
         if output_file:
             sys.stdout = original_stdout
             f.close()
 
     def print_model_aggregated_profile(self, module_depth=-1, top_modules=1):
         """Prints the names of the top top_modules modules in terms of aggregated time, flops, and parameters at depth module_depth.
 
         Args:
             module_depth (int, optional): the depth of the modules to show. Defaults to -1 (the innermost modules).
             top_modules (int, optional): the number of top modules to show. Defaults to 1.
         """
         info = {}
         if not hasattr(self.model, "__flops__"):
-            print(
-                "no __flops__ attribute in the model, call this function after start_profile and before end_profile"
-            )
+            print("no __flops__ attribute in the model, call this function after start_profile and before end_profile")
             return
 
         def walk_module(module, curr_depth, info):
             if curr_depth not in info:
                 info[curr_depth] = {}
             if module.__class__.__name__ not in info[curr_depth]:
                 info[curr_depth][module.__class__.__name__] = [
@@ -435,41 +403,30 @@
 
         walk_module(self.model, 0, info)
 
         depth = module_depth
         if module_depth == -1:
             depth = len(info) - 1
 
-        print(
-            f'Top {top_modules} modules in terms of params, MACs or fwd latency at different model depths:'
-        )
+        print(f'Top {top_modules} modules in terms of params, MACs or fwd latency at different model depths:')
 
         for d in range(depth):
             num_items = min(top_modules, len(info[d]))
 
             sort_macs = {
                 k: macs_to_string(v[0])
-                for k,
-                v in sorted(info[d].items(),
-                            key=lambda item: item[1][0],
-                            reverse=True)[:num_items]
+                for k, v in sorted(info[d].items(), key=lambda item: item[1][0], reverse=True)[:num_items]
             }
             sort_params = {
                 k: params_to_string(v[1])
-                for k,
-                v in sorted(info[d].items(),
-                            key=lambda item: item[1][1],
-                            reverse=True)[:num_items]
+                for k, v in sorted(info[d].items(), key=lambda item: item[1][1], reverse=True)[:num_items]
             }
             sort_time = {
                 k: duration_to_string(v[2])
-                for k,
-                v in sorted(info[d].items(),
-                            key=lambda item: item[1][2],
-                            reverse=True)[:num_items]
+                for k, v in sorted(info[d].items(), key=lambda item: item[1][2], reverse=True)[:num_items]
             }
 
             print(f"depth {d}:")
             print(f"    params      - {sort_params}")
             print(f"    MACs        - {sort_macs}")
             print(f"    fwd latency - {sort_time}")
 
@@ -495,17 +452,15 @@
     return input.numel(), 0
 
 
 def _elu_flops_compute(input: Tensor, alpha: float = 1.0, inplace: bool = False):
     return input.numel(), 0
 
 
-def _leaky_relu_flops_compute(input: Tensor,
-                              negative_slope: float = 0.01,
-                              inplace: bool = False):
+def _leaky_relu_flops_compute(input: Tensor, negative_slope: float = 0.01, inplace: bool = False):
     return input.numel(), 0
 
 
 def _relu6_flops_compute(input: Tensor, inplace: bool = False):
     return input.numel(), 0
 
 
@@ -525,21 +480,15 @@
                         ceil_mode=False,
                         count_include_pad=True,
                         divisor_override=None,
                         return_indices=None):
     return input.numel(), 0
 
 
-def _conv_flops_compute(input,
-                        weight,
-                        bias=None,
-                        stride=1,
-                        padding=0,
-                        dilation=1,
-                        groups=1):
+def _conv_flops_compute(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1):
     assert weight.shape[1] * groups == input.shape[1]
 
     batch_size = input.shape[0]
     in_channels = input.shape[1]
     out_channels = weight.shape[0]
     kernel_dims = list(weight.shape[2:])
     input_dims = list(input.shape[2:])
@@ -548,16 +497,16 @@
 
     paddings = padding if type(padding) is tuple else (padding, ) * length
     strides = stride if type(stride) is tuple else (stride, ) * length
     dilations = dilation if type(dilation) is tuple else (dilation, ) * length
 
     output_dims = []
     for idx, input_dim in enumerate(input_dims):
-        output_dim = (input_dim + 2 * paddings[idx] -
-                      (dilations[idx] * (kernel_dims[idx] - 1) + 1)) // strides[idx] + 1
+        output_dim = (input_dim + 2 * paddings[idx] - (dilations[idx] *
+                                                       (kernel_dims[idx] - 1) + 1)) // strides[idx] + 1
         output_dims.append(output_dim)
 
     filters_per_channel = out_channels // groups
     conv_per_position_macs = int(_prod(kernel_dims)) * in_channels * filters_per_channel
     active_elements_count = batch_size * int(_prod(output_dims))
     overall_conv_macs = conv_per_position_macs * active_elements_count
     overall_conv_flops = 2 * overall_conv_macs
@@ -590,16 +539,16 @@
     paddings = padding if type(padding) is tuple else (padding, ) * length
     strides = stride if type(stride) is tuple else (stride, ) * length
     dilations = dilation if type(dilation) is tuple else (dilation, ) * length
 
     output_dims = []
     for idx, input_dim in enumerate(input_dims):
 
-        output_dim = (input_dim + 2 * paddings[idx] -
-                      (dilations[idx] * (kernel_dims[idx] - 1) + 1)) // strides[idx] + 1
+        output_dim = (input_dim + 2 * paddings[idx] - (dilations[idx] *
+                                                       (kernel_dims[idx] - 1) + 1)) // strides[idx] + 1
         output_dims.append(output_dim)
 
     paddings = padding if type(padding) is tuple else (padding, padding)
     strides = stride if type(stride) is tuple else (stride, stride)
     dilations = dilation if type(dilation) is tuple else (dilation, dilation)
 
     filters_per_channel = out_channels // groups
@@ -1206,16 +1155,15 @@
     model.eval()
 
     if input_shape is not None:
         assert type(input_shape) is tuple, "input_shape must be a tuple"
         assert len(input_shape) >= 1, "input_shape must have at least one element"
         try:
             input = torch.ones(()).new_empty(
-                (*input_shape,
-                 ),
+                (*input_shape, ),
                 dtype=next(model.parameters()).dtype,
                 device=next(model.parameters()).device,
             )
         except StopIteration:
             input = torch.ones(()).new_empty((*input_shape, ))
 
         args = [input]
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/activation_checkpointing/checkpointing.py` & `deepspeed-0.9.0/deepspeed/runtime/activation_checkpointing/checkpointing.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,20 +1,20 @@
-'''Copyright The Microsoft DeepSpeed Team'''
-'''
-Copyright (c) Microsoft Corporation
-Licensed under the MIT license.
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
 
+# DeepSpeed Team
+"""
 Use to partition the activations stored for backward propagation
 Therefore reduces the memory consumption
 Also implements CPU checkpointing and contiguous memory checkpointing
 Reduces memory consumption and memory fragmentation
 
 Code for rng checkpointing taken from NVIDIA Megatron-LM mpu/random.py
 b886b7bb972afe72bac0f5de4f42a4a7bae8ebef
-'''
+"""
 
 # Parts of the code here are adapted from PyTorch
 # repo: https://github.com/pytorch/pytorch
 import copy
 import torch
 import contextlib
 from deepspeed import comm as dist
@@ -78,25 +78,23 @@
                 x = inp
 
             x = x.detach()
             x.requires_grad = requires_grad
             out.append(x)
         return tuple(out)
     else:
-        raise RuntimeError(
-            "Only tuple of tensors is supported. Got Unsupported input type: ",
-            type(inputs).__name__)
+        raise RuntimeError("Only tuple of tensors is supported. Got Unsupported input type: ", type(inputs).__name__)
 
 
 def _set_cuda_rng_state(new_state, device=-1):
     """Sets the random number generator state of the current GPU.
 
     Arguments:
         new_state (torch.ByteTensor): The desired state
-    This function is adapted from PyTorch repo (torch.cuda.set_rng_state)
+    This function is adapted from PyTorch repo (torch.cuda.set_rng_state) #ignore-cuda
     with a single change: the input state is not cloned. Cloning caused
     major performance issues for +4 GPU cases.
     """
     if hasattr(_C, '_cuda_setRNGState') and callable(_C._cuda_setRNGState):
         # older PyTorch
         def cb():
             with get_accelerator().device(device):
@@ -124,14 +122,15 @@
     """Tracker for the cuda RNG states.
 
     Using the `add` method, a cuda rng state is initialized based on
     the input `seed` and is assigned to `name`. Later, by forking the
     rng state, we can perform operations and return to our starting
     cuda state.
     """
+
     def __init__(self):
         # Map from a string name to the cuda rng state.
         self.states_ = {}
         # Seeds are just for book keeping and ensure no seed is set twice.
         self.seeds_ = set()
 
     def reset(self):
@@ -223,21 +222,17 @@
     # Data parallel gets the original seed.
     data_parallel_seed = seed
 
     if dist.get_rank() == 0:
         logger.info(
             '> initializing model parallel cuda seeds on global rank {}, '
             'model parallel rank {}, and data parallel rank {} with '
-            'model parallel seed: {} and data parallel seed: {}'.format(
-                dist.get_rank(),
-                tp_rank,
-                mpu.get_data_parallel_rank(),
-                model_parallel_seed,
-                data_parallel_seed),
-        )
+            'model parallel seed: {} and data parallel seed: {}'.format(dist.get_rank(), tp_rank,
+                                                                        mpu.get_data_parallel_rank(),
+                                                                        model_parallel_seed, data_parallel_seed), )
     _CUDA_RNG_STATE_TRACKER.reset()
     # Set the default state.
     get_accelerator().manual_seed(data_parallel_seed)
     # and model parallel state.
     _CUDA_RNG_STATE_TRACKER.add(_MODEL_PARALLEL_RNG_TRACKER_NAME, model_parallel_seed)
 
 
@@ -278,17 +273,15 @@
             continue
 
         partition_size = item.numel()
         tensor_size = partition_size * mp_size
         if device is not None:
             flat_tensor = torch.zeros([tensor_size], dtype=item.dtype, device=device)
         else:
-            flat_tensor = torch.zeros([tensor_size],
-                                      dtype=item.dtype,
-                                      device=item.device)
+            flat_tensor = torch.zeros([tensor_size], dtype=item.dtype, device=item.device)
         partitions = []
         for i in range(mp_size):
             part_i = flat_tensor.narrow(0, partition_size * i, partition_size)
             if i == mp_rank:
                 part_i.copy_(item)
             partitions.append(part_i)
         dist.all_gather(partitions, partitions[mp_rank], group=mp_group)
@@ -380,57 +373,46 @@
         if not is_activation_to_checkpoint(item):
             inputs.append(item)
             num_non_fp_tensors += 1
             continue
 
         i = arg_index - num_non_fp_tensors
         partition_size = get_partition_size(item)
-        partition = item.detach().contiguous().view(-1).narrow(
-            0,
-            get_partition_start(item),
-            partition_size).clone()
+        partition = item.detach().contiguous().view(-1).narrow(0, get_partition_start(item), partition_size).clone()
 
         buffer_device = torch.device('cpu') if cpu_checkpoint else partition.device
 
         if contiguous_checkpoint:
             if i >= len(contiguous_data_buffers):
                 tensor_list = [
-                    torch.tensor(()).new_empty([partition_size],
-                                               dtype=partition.dtype,
-                                               device=buffer_device)
+                    torch.tensor(()).new_empty([partition_size], dtype=partition.dtype, device=buffer_device)
                     for _ in range(num_layers)
                 ]
                 contiguous_data_buffers.append(tensor_list)
                 data_offsets.append(0)
             elif contiguous_data_buffers[i] is None:
                 tensor_list = [
-                    torch.tensor(()).new_empty([partition_size],
-                                               dtype=partition.dtype,
-                                               device=buffer_device)
+                    torch.tensor(()).new_empty([partition_size], dtype=partition.dtype, device=buffer_device)
                     for _ in range(num_layers)
                 ]
                 contiguous_data_buffers[i] = tensor_list
                 data_offsets[i] = 0
 
             # Because the 'new_empty' returns uninitialized pages,
             # the pages need to be populated during the cudaMemcpy time
             # which increases the data copy time. To avoid this, we
             # pre-populate these pages by simply writing 0 ahead of
             # the actual cudaMemcpy operation time. Due to the
             # previously launched GPU kernels, there is a small
             # window of time here for CPUs to populate pages asynchronously.
             contiguous_data_buffers[i][data_offsets[i]].data[range(
-                0,
-                contiguous_data_buffers[i][data_offsets[i]].data.shape[0],
-                int(mmap.PAGESIZE /
-                    contiguous_data_buffers[i][data_offsets[i]].data.element_size())
-            )] = 0
+                0, contiguous_data_buffers[i][data_offsets[i]].data.shape[0],
+                int(mmap.PAGESIZE / contiguous_data_buffers[i][data_offsets[i]].data.element_size()))] = 0
 
-            contiguous_partition = contiguous_data_buffers[i][
-                data_offsets[i]].data.copy_(partition.data)
+            contiguous_partition = contiguous_data_buffers[i][data_offsets[i]].data.copy_(partition.data)
             data_offsets[i] = data_offsets[i] + 1
             inputs.append(contiguous_partition)
         else:
             partition = partition.cpu() if CPU_CHECKPOINT else partition
             inputs.append(partition)
 
     return inputs
@@ -455,29 +437,22 @@
         i = arg_index - num_non_fp_tensors
 
         if contiguous_checkpoint:
             numel = size.numel()
             if i >= len(contiguous_size_buffers):
                 tmp = torch.tensor(())
                 contiguous_size_buffers.append(
-                    tmp.new_empty([numel * num_layers],
-                                  dtype=size.dtype,
-                                  device=size.device))
+                    tmp.new_empty([numel * num_layers], dtype=size.dtype, device=size.device))
                 size_offsets.append(0)
             elif contiguous_size_buffers[i] is None:
                 tmp = torch.tensor(())
-                contiguous_size_buffers[i] = tmp.new_empty([numel * num_layers],
-                                                           dtype=size.dtype,
-                                                           device=size.device)
+                contiguous_size_buffers[i] = tmp.new_empty([numel * num_layers], dtype=size.dtype, device=size.device)
                 size_offsets[i] = 0
 
-            contiguous_size = contiguous_size_buffers[i].narrow(
-                0,
-                size_offsets[i],
-                numel).data.copy_(size.data)
+            contiguous_size = contiguous_size_buffers[i].narrow(0, size_offsets[i], numel).data.copy_(size.data)
             contiguous_size = contiguous_size.view_as(size)
             size_offsets[i] = size_offsets[i] + numel
             new_args.append(contiguous_size)
         else:
             new_args.append(size)
 
     return new_args
@@ -495,21 +470,22 @@
 
     return new_args
 
 
 class CheckpointFunction(torch.autograd.Function):
     """This function is adapted from torch.utils.checkpoint with
        two main changes:
-           1) torch.cuda.set_rng_state is replaced with `_set_cuda_rng_state`
+           1) torch.cuda.set_rng_state is replaced with `_set_cuda_rng_state`  #ignore-cuda
            2) the states in the model parallel tracker are also properly
               tracked/set/reset.
            3) Performance activation partitioning, contiguous memory optimization
            4) CPU Checkpointing
            5) Profile forward and backward functions
     """
+
     @staticmethod
     def forward(ctx, run_function, all_outputs, *args):
         global mpu, timers, SYNCHRONIZE, PROFILE_TIME
 
         def save_args_for_backward(*all_args):
             tensor_args, non_tensor_args, tensor_flags = extract_tensors(all_objects=all_args)
             ctx.deepspeed_saved_tensors = tensor_args
@@ -547,39 +523,30 @@
 
         global cuda_device, transport_stream, PARTITION_ACTIVATIONS, buffer_0, buffer_1, buffer_0_offset, buffer_1_offset
 
         if cuda_device is None:
             see_memory_usage("First Forward Beginning", force=False)
             if dist.get_rank() == 0:
                 logger.info(f"Activation Checkpointing Information")
+                logger.info(f"----Partition Activations {PARTITION_ACTIVATIONS}, CPU CHECKPOINTING {CPU_CHECKPOINT}")
                 logger.info(
-                    f"----Partition Activations {PARTITION_ACTIVATIONS}, CPU CHECKPOINTING {CPU_CHECKPOINT}"
-                )
-                logger.info(
-                    f"----contiguous Memory Checkpointing {CONTIGUOUS_CHECKPOINTING} with {num_layers} total layers"
-                )
+                    f"----contiguous Memory Checkpointing {CONTIGUOUS_CHECKPOINTING} with {num_layers} total layers")
                 logger.info(f"----Synchronization {SYNCHRONIZE}")
                 logger.info(f"----Profiling time in checkpointing {PROFILE_TIME}")
 
             cuda_device = get_accelerator().current_device_name()
             transport_stream = get_accelerator().Stream(device=cuda_device)
 
         if PARTITION_ACTIVATIONS:
-            inputs = partition_activations(args,
-                                           CPU_CHECKPOINT,
-                                           CONTIGUOUS_CHECKPOINTING)
+            inputs = partition_activations(args, CPU_CHECKPOINT, CONTIGUOUS_CHECKPOINTING)
         elif CPU_CHECKPOINT:
-            inputs = copy_to_device(args,
-                                    device=torch.device('cpu'),
-                                    criterion_func=is_activation_to_checkpoint)
+            inputs = copy_to_device(args, device=torch.device('cpu'), criterion_func=is_activation_to_checkpoint)
 
         # just in case something funky is happening such as reuse of inputs
-        inputs_cuda = copy_to_device(args,
-                                     device=cuda_device,
-                                     criterion_func=is_activation_to_checkpoint)
+        inputs_cuda = copy_to_device(args, device=cuda_device, criterion_func=is_activation_to_checkpoint)
 
         # Copy the rng states.
         ctx.fwd_cpu_rng_state = torch.get_rng_state()
         ctx.fwd_cuda_rng_state = get_accelerator().get_rng_state()
         ctx.fwd_cuda_rng_state_tracker = get_cuda_rng_tracker().get_states()
 
         see_memory_usage("Before running forward on the layer", force=False)
@@ -587,18 +554,15 @@
         with torch.no_grad():
             outputs = run_function(*inputs_cuda)
 
         see_memory_usage("After running forward on the layer", force=False)
         del inputs_cuda
 
         if PARTITION_ACTIVATIONS:
-            new_args = get_partitioned_activations_for_backward(
-                args,
-                inputs,
-                CONTIGUOUS_CHECKPOINTING)
+            new_args = get_partitioned_activations_for_backward(args, inputs, CONTIGUOUS_CHECKPOINTING)
             assert len(new_args) % 2 == 0, f'save_for_backward called with odd number of args, {len(new_args)}'
             save_args_for_backward(*new_args)
         elif CPU_CHECKPOINT:
             new_args = get_cpu_activations_for_backward(args, inputs)
             save_args_for_backward(*new_args)
         else:
             save_args_for_backward(*args)
@@ -609,17 +573,15 @@
         if SYNCHRONIZE:
             get_accelerator().synchronize()
 
         # Tensors returned from forward() may not be differentiable.
         if torch.is_tensor(outputs):
             non_grad_outputs = [outputs] if not outputs.is_floating_point() else []
         else:
-            non_grad_outputs = [
-                o for o in outputs if torch.is_tensor(o) and not o.is_floating_point()
-            ]
+            non_grad_outputs = [o for o in outputs if torch.is_tensor(o) and not o.is_floating_point()]
         ctx.mark_non_differentiable(*non_grad_outputs)
 
         if torch.is_tensor(outputs):
             all_outputs += [outputs]
             return outputs
         else:
             all_outputs += outputs
@@ -657,22 +619,19 @@
             raise RuntimeError("Checkpointing is not compatible with .grad(), "
                                "please use .backward() if possible")
 
         global cuda_device, transport_stream, PARTITION_ACTIVATIONS
 
         if PARTITION_ACTIVATIONS:
             # with get_accelerator().stream(transport_stream):
-            inputs = gather_partitioned_activations(
-                ctx.deepspeed_saved_tensors,
-                device=cuda_device if CPU_CHECKPOINT else None)
+            inputs = gather_partitioned_activations(ctx.deepspeed_saved_tensors,
+                                                    device=cuda_device if CPU_CHECKPOINT else None)
             detached_inputs = detach_variable(inputs)
         elif CPU_CHECKPOINT:
-            inputs = move_to_device(ctx.deepspeed_saved_tensors,
-                                    cuda_device,
-                                    is_activation_to_checkpoint)
+            inputs = move_to_device(ctx.deepspeed_saved_tensors, cuda_device, is_activation_to_checkpoint)
             detached_inputs = detach_variable(inputs)
         else:
             inputs = ctx.deepspeed_saved_tensors
             detached_inputs = detach_variable(inputs)
 
         # Add non tensor input args
         detached_inputs = merge_tensors(tensor_objects=detached_inputs,
@@ -758,16 +717,15 @@
         return tuple(all_outputs)
 
 
 def partition_activations_in_checkpoint(partition_activation):
     global PARTITION_ACTIVATIONS
     PARTITION_ACTIVATIONS = partition_activation
     if dist.get_rank() == 0:
-        logger.info(
-            f"**************Partition Activations {PARTITION_ACTIVATIONS}************")
+        logger.info(f"**************Partition Activations {PARTITION_ACTIVATIONS}************")
 
 
 def set_num_layers(nlayers):
     global num_layers
     num_layers = nlayers
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/activation_checkpointing/config.py` & `deepspeed-0.9.0/deepspeed/runtime/activation_checkpointing/config.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,12 +1,11 @@
-'''Copyright The Microsoft DeepSpeed Team'''
-"""
-Copyright (c) Microsoft Corporation
-Licensed under the MIT license.
-"""
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 from deepspeed.runtime.config_utils import get_scalar_param, DeepSpeedConfigObject
 
 #########################################
 #  DeepSpeed Activation Checkpointing
 #########################################
 # Activation Checkpointing Allows to save memory by only keeping a select few
@@ -44,24 +43,23 @@
 ACT_CHKPT_CPU_CHECKPOINTING_DEFAULT = False
 
 ACT_CHKPT = 'activation_checkpointing'
 
 ACT_CHKPT_DEFAULT = {
     ACT_CHKPT_PARTITION_ACTIVATIONS: ACT_CHKPT_PARTITION_ACTIVATIONS_DEFAULT,
     ACT_CHKPT_NUMBER_CHECKPOINTS: ACT_CHKPT_NUMBER_CHECKPOINTS_DEFAULT,
-    ACT_CHKPT_CONTIGUOUS_MEMORY_OPTIMIZATION:
-    ACT_CHKPT_CONTIGUOUS_MEMORY_OPTIMIZATION_DEFAULT,
-    ACT_CHKPT_SYNCHRONIZE_CHECKPOINT_BOUNDARY:
-    ACT_CHKPT_SYNCHRONIZE_CHECKPOINT_BOUNDARY_DEFAULT,
+    ACT_CHKPT_CONTIGUOUS_MEMORY_OPTIMIZATION: ACT_CHKPT_CONTIGUOUS_MEMORY_OPTIMIZATION_DEFAULT,
+    ACT_CHKPT_SYNCHRONIZE_CHECKPOINT_BOUNDARY: ACT_CHKPT_SYNCHRONIZE_CHECKPOINT_BOUNDARY_DEFAULT,
     ACT_CHKPT_PROFILE: ACT_CHKPT_PROFILE_DEFAULT,
     ACT_CHKPT_CPU_CHECKPOINTING: ACT_CHKPT_CPU_CHECKPOINTING_DEFAULT
 }
 
 
 class DeepSpeedActivationCheckpointingConfig(DeepSpeedConfigObject):
+
     def __init__(self, param_dict):
         super(DeepSpeedActivationCheckpointingConfig, self).__init__()
 
         self.partition_activations = None
         self.contiguous_memory_optimization = None
         self.cpu_checkpointing = None
         self.number_checkpoints = None
@@ -72,33 +70,25 @@
             act_chkpt_config_dict = param_dict[ACT_CHKPT]
         else:
             act_chkpt_config_dict = ACT_CHKPT_DEFAULT
 
         self._initialize(act_chkpt_config_dict)
 
     def _initialize(self, act_chkpt_config_dict):
-        self.partition_activations = get_scalar_param(
-            act_chkpt_config_dict,
-            ACT_CHKPT_PARTITION_ACTIVATIONS,
-            ACT_CHKPT_PARTITION_ACTIVATIONS_DEFAULT)
-
-        self.contiguous_memory_optimization = get_scalar_param(
-            act_chkpt_config_dict,
-            ACT_CHKPT_CONTIGUOUS_MEMORY_OPTIMIZATION,
-            ACT_CHKPT_CONTIGUOUS_MEMORY_OPTIMIZATION_DEFAULT)
+        self.partition_activations = get_scalar_param(act_chkpt_config_dict, ACT_CHKPT_PARTITION_ACTIVATIONS,
+                                                      ACT_CHKPT_PARTITION_ACTIVATIONS_DEFAULT)
 
-        self.cpu_checkpointing = get_scalar_param(act_chkpt_config_dict,
-                                                  ACT_CHKPT_CPU_CHECKPOINTING,
+        self.contiguous_memory_optimization = get_scalar_param(act_chkpt_config_dict,
+                                                               ACT_CHKPT_CONTIGUOUS_MEMORY_OPTIMIZATION,
+                                                               ACT_CHKPT_CONTIGUOUS_MEMORY_OPTIMIZATION_DEFAULT)
+
+        self.cpu_checkpointing = get_scalar_param(act_chkpt_config_dict, ACT_CHKPT_CPU_CHECKPOINTING,
                                                   ACT_CHKPT_CPU_CHECKPOINTING_DEFAULT)
 
-        self.number_checkpoints = get_scalar_param(act_chkpt_config_dict,
-                                                   ACT_CHKPT_NUMBER_CHECKPOINTS,
+        self.number_checkpoints = get_scalar_param(act_chkpt_config_dict, ACT_CHKPT_NUMBER_CHECKPOINTS,
                                                    ACT_CHKPT_NUMBER_CHECKPOINTS_DEFAULT)
 
-        self.profile = get_scalar_param(act_chkpt_config_dict,
-                                        ACT_CHKPT_PROFILE,
-                                        ACT_CHKPT_PROFILE_DEFAULT)
-
-        self.synchronize_checkpoint_boundary = get_scalar_param(
-            act_chkpt_config_dict,
-            ACT_CHKPT_SYNCHRONIZE_CHECKPOINT_BOUNDARY,
-            ACT_CHKPT_SYNCHRONIZE_CHECKPOINT_BOUNDARY_DEFAULT)
+        self.profile = get_scalar_param(act_chkpt_config_dict, ACT_CHKPT_PROFILE, ACT_CHKPT_PROFILE_DEFAULT)
+
+        self.synchronize_checkpoint_boundary = get_scalar_param(act_chkpt_config_dict,
+                                                                ACT_CHKPT_SYNCHRONIZE_CHECKPOINT_BOUNDARY,
+                                                                ACT_CHKPT_SYNCHRONIZE_CHECKPOINT_BOUNDARY_DEFAULT)
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/bf16_optimizer.py` & `deepspeed-0.9.0/deepspeed/runtime/bf16_optimizer.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,45 +1,38 @@
-"""
-Copyright 2022 The Microsoft DeepSpeed Team
-"""
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 from collections import OrderedDict
 import torch
 import sys
 import os
 from deepspeed import comm as dist
 from deepspeed.runtime.constants import PIPE_REPLICATED
 from deepspeed.ops.op_builder import UtilsBuilder
 from deepspeed.runtime import ZeROOptimizer
 from packaging import version as pkg_version
 
 from deepspeed.git_version_info import version
-from deepspeed.runtime.utils import (get_global_norm_of_tensors,
-                                     clip_tensors_by_global_norm,
-                                     DummyOptim,
-                                     align_dense_tensors,
-                                     all_gather_dp_groups,
-                                     bwc_tensor_model_parallel_rank,
-                                     is_model_parallel_parameter,
-                                     see_memory_usage)
+from deepspeed.runtime.utils import (get_global_norm_of_tensors, clip_tensors_by_global_norm, DummyOptim,
+                                     align_dense_tensors, all_gather_dp_groups, bwc_tensor_model_parallel_rank,
+                                     is_model_parallel_parameter, see_memory_usage)
 
 from deepspeed.utils import link_hp_params, fragment_address
 from deepspeed.checkpoint import enable_universal_checkpoint
-from deepspeed.checkpoint.constants import (DS_VERSION,
-                                            PARTITION_COUNT,
-                                            BASE_OPTIMIZER_STATE,
-                                            SINGLE_PARTITION_OF_FP32_GROUPS,
-                                            CLIP_GRAD,
-                                            GROUP_PADDINGS,
+from deepspeed.checkpoint.constants import (DS_VERSION, PARTITION_COUNT, BASE_OPTIMIZER_STATE,
+                                            SINGLE_PARTITION_OF_FP32_GROUPS, CLIP_GRAD, GROUP_PADDINGS,
                                             PARAM_SLICE_MAPPINGS)
 
 setattr(sys.modules[__name__], 'fragment_address', fragment_address)
 
 
 class BF16_Optimizer(ZeROOptimizer):
+
     def __init__(self,
                  init_optimizer,
                  param_names,
                  mpu=None,
                  clip_grad=0.0,
                  norm_type=2,
                  allgather_bucket_size=5000000000,
@@ -54,17 +47,15 @@
 
         self.clip_grad = clip_grad
         self.norm_type = norm_type
         self.mpu = mpu
         self.allgather_bucket_size = int(allgather_bucket_size)
         self.dp_process_group = dp_process_group
         self.dp_rank = dist.get_rank(group=self.dp_process_group)
-        self.real_dp_process_group = [
-            dp_process_group for i in range(len(self.optimizer.param_groups))
-        ]
+        self.real_dp_process_group = [dp_process_group for i in range(len(self.optimizer.param_groups))]
 
         # Load pre-built or JIT compile (un)flatten ops
         util_ops = UtilsBuilder().load()
         self.flatten = util_ops.flatten
         self.unflatten = util_ops.unflatten
 
         #align nccl all-gather send buffers to 4-bye boundary
@@ -91,87 +82,70 @@
         if self.using_real_optimizer:
             self._setup_for_real_optimizer()
 
         see_memory_usage('end bf16_optimizer', force=True)
 
     def _setup_for_real_optimizer(self):
         dp_world_size = dist.get_world_size(group=self.dp_process_group)
-        self.partition_count = [
-            dp_world_size for i in range(len(self.optimizer.param_groups))
-        ]
+        self.partition_count = [dp_world_size for i in range(len(self.optimizer.param_groups))]
 
         for i, param_group in enumerate(self.optimizer.param_groups):
             see_memory_usage(f'before initializing group {i}', force=True)
 
             partition_id = dist.get_rank(group=self.real_dp_process_group[i])
 
             # grab the original list
             self.bf16_groups.append(param_group['params'])
 
             # create flat bf16 params
             self.bf16_groups_flat.append(
-                self._flatten_dense_tensors_aligned(
-                    self.bf16_groups[i],
-                    self.nccl_start_alignment_factor * dp_world_size))
+                self._flatten_dense_tensors_aligned(self.bf16_groups[i],
+                                                    self.nccl_start_alignment_factor * dp_world_size))
 
             # Make bf16 params point to flat tensor storage
-            self._update_storage_to_flattened_tensor(
-                tensor_list=self.bf16_groups[i],
-                flat_tensor=self.bf16_groups_flat[i])
+            self._update_storage_to_flattened_tensor(tensor_list=self.bf16_groups[i],
+                                                     flat_tensor=self.bf16_groups_flat[i])
 
             # divide flat weights into equal sized partitions
             partition_size = self.bf16_groups_flat[i].numel() // dp_world_size
             bf16_dp_partitions = [
-                self.bf16_groups_flat[i].narrow(0,
-                                                dp_index * partition_size,
-                                                partition_size)
+                self.bf16_groups_flat[i].narrow(0, dp_index * partition_size, partition_size)
                 for dp_index in range(dp_world_size)
             ]
             self.bf16_partitioned_groups.append(bf16_dp_partitions)
 
             # create fp32 params partition
-            self.fp32_groups_flat_partition.append(
-                bf16_dp_partitions[partition_id].clone().float().detach())
+            self.fp32_groups_flat_partition.append(bf16_dp_partitions[partition_id].clone().float().detach())
             self.fp32_groups_flat_partition[i].requires_grad = True
 
             num_elem_list = [t.numel() for t in self.bf16_groups[i]]
 
             # create fp32 gradients
-            self.fp32_groups_gradients_flat.append(
-                torch.zeros_like(self.bf16_groups_flat[i],
-                                 dtype=torch.float32))
+            self.fp32_groups_gradients_flat.append(torch.zeros_like(self.bf16_groups_flat[i], dtype=torch.float32))
 
             # track individual fp32 gradients for entire model
-            fp32_gradients = self._split_flat_tensor(
-                flat_tensor=self.fp32_groups_gradients_flat[i],
-                num_elem_list=num_elem_list)
+            fp32_gradients = self._split_flat_tensor(flat_tensor=self.fp32_groups_gradients_flat[i],
+                                                     num_elem_list=num_elem_list)
             self.fp32_groups_gradients.append(fp32_gradients)
             self.fp32_groups_gradient_dict[i] = fp32_gradients
 
             # flat tensor corresponding to actual fp32 gradients (i.e., minus alignment padding)
             length_without_padding = sum(num_elem_list)
             self.fp32_groups_actual_gradients_flat.append(
-                torch.narrow(self.fp32_groups_gradients_flat[i],
-                             0,
-                             0,
-                             length_without_padding))
+                torch.narrow(self.fp32_groups_gradients_flat[i], 0, 0, length_without_padding))
 
             # flat tensor corresponding to gradient partition
             self.fp32_groups_gradient_flat_partition.append(
-                torch.narrow(self.fp32_groups_gradients_flat[i],
-                             0,
-                             partition_id * partition_size,
-                             partition_size))
+                torch.narrow(self.fp32_groups_gradients_flat[i], 0, partition_id * partition_size, partition_size))
 
             # track fp32 gradient updates
             self.fp32_groups_has_gradients.append([False] * len(self.bf16_groups[i]))
 
             # Record padding required for alignment
-            if partition_id == dist.get_world_size(
-                    group=self.real_dp_process_group[i]) - 1:
+            if partition_id == dist.get_world_size(group=self.real_dp_process_group[i]) - 1:
                 padding = self.bf16_groups_flat[i].numel() - length_without_padding
             else:
                 padding = 0
 
             self.group_paddings.append(padding)
 
             # update optimizer param groups to reference fp32 params partition
@@ -195,47 +169,46 @@
     def _create_param_mapping(self):
         param_mapping = []
         for i, _ in enumerate(self.optimizer.param_groups):
             param_mapping_per_group = OrderedDict()
             for lp in self.bf16_groups[i]:
                 if lp._hp_mapping is not None:
                     lp_name = self.param_names[lp]
-                    param_mapping_per_group[
-                        lp_name] = lp._hp_mapping.get_hp_fragment_address()
+                    param_mapping_per_group[lp_name] = lp._hp_mapping.get_hp_fragment_address()
             param_mapping.append(param_mapping_per_group)
 
         return param_mapping
 
     def _link_all_hp_params(self):
         dp_world_size = dist.get_world_size(group=self.dp_process_group)
         for i, _ in enumerate(self.optimizer.param_groups):
             # Link bf16 and fp32 params in partition
             partition_id = dist.get_rank(group=self.real_dp_process_group[i])
             partition_size = self.bf16_groups_flat[i].numel() // dp_world_size
             flat_hp_partition = self.fp32_groups_flat_partition[i]
-            link_hp_params(
-                lp_param_list=self.bf16_groups[i],
-                flat_hp_partition=flat_hp_partition,
-                gradient_dict=self.fp32_groups_gradient_dict,
-                offload_gradient_dict=None,
-                use_offload=False,
-                param_group_index=i,
-                partition_start=partition_id * partition_size,
-                partition_size=partition_size,
-                partition_optimizer_state=self.optimizer.state[flat_hp_partition],
-                dp_group=self.real_dp_process_group[i])
+            link_hp_params(lp_param_list=self.bf16_groups[i],
+                           flat_hp_partition=flat_hp_partition,
+                           gradient_dict=self.fp32_groups_gradient_dict,
+                           offload_gradient_dict=None,
+                           use_offload=False,
+                           param_group_index=i,
+                           partition_start=partition_id * partition_size,
+                           partition_size=partition_size,
+                           partition_optimizer_state=self.optimizer.state[flat_hp_partition],
+                           dp_group=self.real_dp_process_group[i])
 
     def initialize_optimizer_states(self):
         """Take an optimizer step with zero-valued gradients to allocate internal
         optimizer state.
 
         This helps prevent memory fragmentation by allocating optimizer state at the
         beginning of training instead of after activations have been allocated.
         """
-        for param_partition, grad_partition in zip(self.fp32_groups_flat_partition, self.fp32_groups_gradient_flat_partition):
+        for param_partition, grad_partition in zip(self.fp32_groups_flat_partition,
+                                                   self.fp32_groups_gradient_flat_partition):
             param_partition.grad = grad_partition
 
         self.optimizer.step()
 
         self.clear_hp_grads()
 
     def _split_flat_tensor(self, flat_tensor, num_elem_list):
@@ -258,27 +231,25 @@
         return self.flatten(align_dense_tensors(tensor_list, alignment))
 
     @torch.no_grad()
     def step(self, closure=None):
         if closure is not None:
             raise NotImplementedError(f'{self.__class__} does not support closure.')
 
-        all_groups_norm = get_global_norm_of_tensors(
-            input_tensors=self.get_grads_for_norm(),
-            mpu=self.mpu,
-            norm_type=self.norm_type)
+        all_groups_norm = get_global_norm_of_tensors(input_tensors=self.get_grads_for_norm(),
+                                                     mpu=self.mpu,
+                                                     norm_type=self.norm_type)
         self._global_grad_norm = all_groups_norm
 
         assert all_groups_norm > 0.
         if self.clip_grad > 0.:
-            clip_tensors_by_global_norm(
-                input_tensors=self.get_grads_for_norm(for_clipping=True),
-                max_norm=self.clip_grad,
-                global_norm=all_groups_norm,
-                mpu=self.mpu)
+            clip_tensors_by_global_norm(input_tensors=self.get_grads_for_norm(for_clipping=True),
+                                        max_norm=self.clip_grad,
+                                        global_norm=all_groups_norm,
+                                        mpu=self.mpu)
 
         self.optimizer.step()
 
         self.update_lp_params()
 
         self.clear_hp_grads()
         self.step_count += 1
@@ -339,15 +310,16 @@
 
                 grads.append(self.fp32_groups_gradients[i][j])
 
         return grads
 
     @torch.no_grad()
     def update_lp_params(self):
-        for i, (bf16_partitions, fp32_partition) in enumerate(zip(self.bf16_partitioned_groups, self.fp32_groups_flat_partition)):
+        for i, (bf16_partitions,
+                fp32_partition) in enumerate(zip(self.bf16_partitioned_groups, self.fp32_groups_flat_partition)):
             partition_id = dist.get_rank(group=self.real_dp_process_group[i])
             bf16_partitions[partition_id].data.copy_(fp32_partition.data)
             # print_rank_0(f'update_lp_params {i=} {partition_id=}', force=True)
             # if i == 0:
             #     print_rank_0(f'{fp32_partition[:10]=}', force=True)
 
         all_gather_dp_groups(partitioned_param_groups=self.bf16_partitioned_groups,
@@ -391,51 +363,42 @@
 
     def load_state_dict(self,
                         state_dict_list,
                         checkpoint_folder,
                         load_optimizer_states=True,
                         load_from_fp32_weights=False):
         if checkpoint_folder:
-            self._load_universal_checkpoint(checkpoint_folder,
-                                            load_optimizer_states,
-                                            load_from_fp32_weights)
+            self._load_universal_checkpoint(checkpoint_folder, load_optimizer_states, load_from_fp32_weights)
         else:
-            self._load_legacy_checkpoint(state_dict_list,
-                                         load_optimizer_states,
-                                         load_from_fp32_weights)
-
-    def _load_legacy_checkpoint(self,
-                                state_dict_list,
-                                load_optimizer_states=True,
-                                load_from_fp32_weights=False):
+            self._load_legacy_checkpoint(state_dict_list, load_optimizer_states, load_from_fp32_weights)
+
+    def _load_legacy_checkpoint(self, state_dict_list, load_optimizer_states=True, load_from_fp32_weights=False):
 
         dp_rank = dist.get_rank(group=self.dp_process_group)
         current_rank_sd = state_dict_list[dp_rank]
 
         ckpt_version = current_rank_sd.get(DS_VERSION, False)
         assert ckpt_version, f"Empty ds_version in checkpoint, not clear how to proceed"
         ckpt_version = pkg_version.parse(ckpt_version)
 
         self.clip_grad = current_rank_sd.get(CLIP_GRAD, self.clip_grad)
 
         if load_optimizer_states:
             self.optimizer.load_state_dict(current_rank_sd[BASE_OPTIMIZER_STATE])
 
         if load_from_fp32_weights:
-            for current, saved in zip(self.fp32_groups_flat_partition, current_rank_sd[SINGLE_PARTITION_OF_FP32_GROUPS]):
+            for current, saved in zip(self.fp32_groups_flat_partition,
+                                      current_rank_sd[SINGLE_PARTITION_OF_FP32_GROUPS]):
                 src_tensor = _get_padded_tensor(saved, current.numel())
                 current.data.copy_(src_tensor.data)
 
         if load_optimizer_states:
             self._link_all_hp_params()
 
-    def _load_universal_checkpoint(self,
-                                   checkpoint_folder,
-                                   load_optimizer_states,
-                                   load_from_fp32_weights):
+    def _load_universal_checkpoint(self, checkpoint_folder, load_optimizer_states, load_from_fp32_weights):
         self._load_hp_checkpoint_state(checkpoint_folder)
 
     @property
     def param_groups(self):
         """Forward the wrapped optimizer's parameters."""
         return self.optimizer.param_groups
 
@@ -444,19 +407,16 @@
         tp_rank = bwc_tensor_model_parallel_rank(mpu=self.mpu)
         tp_world_size = self.mpu.get_slice_parallel_world_size()
 
         for i, _ in enumerate(self.optimizer.param_groups):
             for lp in self.bf16_groups[i]:
                 if lp._hp_mapping is not None:
                     #print(f"Loading {self.param_names[lp]} {tp_rank=} {tp_world_size=}")
-                    lp.load_hp_checkpoint_state(
-                        os.path.join(checkpoint_dir,
-                                     self.param_names[lp]),
-                        tp_rank,
-                        tp_world_size)
+                    lp.load_hp_checkpoint_state(os.path.join(checkpoint_dir, self.param_names[lp]), tp_rank,
+                                                tp_world_size)
 
 
 def _get_padded_tensor(src_tensor, size):
     if src_tensor.numel() >= size:
         return src_tensor
     padded_tensor = torch.zeros(size, dtype=src_tensor.dtype, device=src_tensor.device)
     slice_tensor = torch.narrow(padded_tensor, 0, 0, src_tensor.numel())
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/checkpoint_engine/checkpoint_engine.py` & `deepspeed-0.9.0/deepspeed/runtime/checkpoint_engine/checkpoint_engine.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,8 +1,11 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import os
 
 
 class CheckpointEngine(object):
 
     # init checkpoint engine for save/load
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/checkpoint_engine/nebula_checkpoint_engine.py` & `deepspeed-0.9.0/deepspeed/runtime/checkpoint_engine/nebula_checkpoint_engine.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,8 +1,11 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import os
 import torch
 import torch_nebula
 
 from deepspeed.runtime.checkpoint_engine.checkpoint_engine import \
     CheckpointEngine
@@ -11,28 +14,28 @@
 
 
 def _get_tag_from_path(path):
     return os.path.basename(os.path.dirname(path))
 
 
 class NebulaCheckpointEngine(CheckpointEngine):
+
     def __init__(self, config_params=None):
         super().__init__(config_params)
         self.checkpoint = None
         self.tag_flag = None
         self.enable_nebula_load = config_params.enable_nebula_load
         self.nebula_load_path = config_params.load_path
         if self.nebula_load_path is None:
             self.nebula_load_path = config_params.persistent_storage_path
 
         nebula_config_params = {
             NEBULA_PERSISTENT_STORAGE_PATH: config_params.persistent_storage_path,
             NEBULA_PERSISTENT_TIME_INTERVAL: config_params.persistent_time_interval,
-            NEBULA_NUM_OF_VERSION_IN_RETENTION:
-            config_params.num_of_version_in_retention,
+            NEBULA_NUM_OF_VERSION_IN_RETENTION: config_params.num_of_version_in_retention,
         }
         torch_nebula.init(**nebula_config_params)
 
     def create(self, tag):
         log_dist(f"[Nebula] Start Checkpoint for tag:{tag}", ranks=[0])
         # -2 means: customer needs to  explicitly tell nebula
         # current checkpoint is complete by commit methond.
@@ -50,66 +53,55 @@
         return None
 
     def load(self, path: str, map_location=None):
         tag = _get_tag_from_path(path)
         first_load_flag = self.tag_flag is None or self.tag_flag == tag
         if not self.enable_nebula_load and first_load_flag:
             self.tag_flag = tag
-            logger.info(
-                f"[Nebula] Disable nebula load. Loading checkpoint from {path} ...")
+            logger.info(f"[Nebula] Disable nebula load. Loading checkpoint from {path} ...")
             partition = torch.load(path, map_location=map_location)
             logger.info(f"[Nebula] Disable nebula load. Loaded checkpoint from {path} .")
             return partition
 
         partititon_name = os.path.basename(path)
-        logger.info(
-            f"[Nebula] Loading {path} under tag {tag} from nebula path {self.nebula_load_path}..."
-        )
+        logger.info(f"[Nebula] Loading {path} under tag {tag} from nebula path {self.nebula_load_path}...")
 
         checkpoint = None
         if tag in (None, 'latest', 'latest_universal'):
             # In some cases, there is the inconsistent tag between deepspeed metadata (latest file)
             # and nebula metadata, will lead to the failure on loading with deepspeed tag. Then we
             # will try to load the valid latest checkpoint from nebula(tier3 > tier1). So, in summary
             # when met failure loading for given tag, the loading priority would be like:
             #               nebula tier3 latest > nebula tier1 latest.
-            checkpoint = torch_nebula.get_latest_checkpoint(
-                persist_path=self.nebula_load_path)
+            checkpoint = torch_nebula.get_latest_checkpoint(persist_path=self.nebula_load_path)
         else:
-            checkpoint = torch_nebula.get_checkpoint(tag=tag,
-                                                     persist_path=self.nebula_load_path)
+            checkpoint = torch_nebula.get_checkpoint(tag=tag, persist_path=self.nebula_load_path)
 
         if checkpoint is None or (checkpoint is not None and checkpoint.tag == ''):
             logger.info(
                 f"Unable to find valid checkpoint tag:{tag} from Nebula, try to get latest checkpoint again from nebula {self.nebula_load_path} path!"
             )
             # nebula tier3 latest
-            checkpoint = torch_nebula.get_latest_checkpoint(
-                persist_path=self.nebula_load_path)
+            checkpoint = torch_nebula.get_latest_checkpoint(persist_path=self.nebula_load_path)
             if checkpoint is None or (checkpoint is not None and checkpoint.tag == ''):
                 logger.info(
                     f"Unable to find latest checkpoint from Nebula tier3, try to get latest checkpoint again from nebula tier1 path!"
                 )
                 # nebula tier1 latest
                 checkpoint = torch_nebula.get_latest_checkpoint()
-                logger.warning(
-                    f"Unable to find valid checkpoint from Nebula under tag:{tag}.")
+                logger.warning(f"Unable to find valid checkpoint from Nebula under tag:{tag}.")
                 return None
 
         tag = checkpoint.tag
         self.tag_flag = -1
         partition = checkpoint.load(partititon_name, map_location=map_location)
-        logger.info(
-            f"[Nebula] Loaded {path} under tag {tag} from {self.nebula_load_path}.")
+        logger.info(f"[Nebula] Loaded {path} under tag {tag} from {self.nebula_load_path}.")
         return partition
 
     def commit(self, tag):
         # nebula commit will be call when all files under give tag are ready to be persisted in the async way.
-        logger.info(
-            f"[Nebula] all files for {tag} are saved in tier1. It is ready to start persisting"
-        )
+        logger.info(f"[Nebula] all files for {tag} are saved in tier1. It is ready to start persisting")
         commit_rls = self.checkpoint.commit()
         if not commit_rls:
-            logger.error(
-                f"[Nebula] failed to commit the checkpoint, please check the log.")
+            logger.error(f"[Nebula] failed to commit the checkpoint, please check the log.")
             return False
         return commit_rls
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/checkpoint_engine/torch_checkpoint_engine.py` & `deepspeed-0.9.0/deepspeed/runtime/checkpoint_engine/torch_checkpoint_engine.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,16 +1,20 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import torch
 from deepspeed.utils import logger, log_dist
 from deepspeed.runtime.checkpoint_engine.checkpoint_engine import \
     CheckpointEngine
 
 
 class TorchCheckpointEngine(CheckpointEngine):
+
     def __init__(self, config_params=None):
         super().__init__(config_params)
 
     def create(self, tag):
         log_dist(f"[Torch] Checkpoint {tag} is about to be saved!", ranks=[0])
 
     def save(self, state_dict, path: str):
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/comm/coalesced_collectives.py` & `deepspeed-0.9.0/deepspeed/runtime/comm/coalesced_collectives.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,33 +1,31 @@
-'''Copyright The Microsoft DeepSpeed Team'''
-"""batched collective operations for overhead amortization and better
-bandwidth utilization"""
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+"""
+batched collective operations for overhead amortization and better
+bandwidth utilization
+"""
 
 import math
 from typing import List
 
 import torch
 from torch import Tensor
 from deepspeed import comm as dist
 # NOTE: Use torch.distributed's ProcessGroup class until we have our own.
 from torch.distributed import ProcessGroup
 import torch.nn.functional
 
 from deepspeed.utils import instrument_w_nvtx
 
 
-def _torch_reduce_scatter_fn(input_tensor: Tensor,
-                             output_tensor: Tensor,
-                             group=None,
-                             async_op=False,
-                             prof=False):
-    return instrument_w_nvtx(dist.reduce_scatter_fn)(output_tensor,
-                                                     input_tensor,
-                                                     group=group,
-                                                     async_op=async_op)
+def _torch_reduce_scatter_fn(input_tensor: Tensor, output_tensor: Tensor, group=None, async_op=False, prof=False):
+    return instrument_w_nvtx(dist.reduce_scatter_fn)(output_tensor, input_tensor, group=group, async_op=async_op)
 
 
 @instrument_w_nvtx
 @torch.no_grad()
 def reduce_scatter_coalesced(
     tensors: List[Tensor],
     group: ProcessGroup = None,
@@ -41,21 +39,18 @@
     world_sz = dist.get_world_size(group)
 
     partition_lst_for_each_tensor = [None] * len(tensors)
     for tensor_idx, tensor in enumerate(tensors):
         flattened_tensor = tensor.view(-1)
         chunk_sz = math.ceil(tensor.numel() / world_sz)
         partition_lst_for_each_tensor[tensor_idx] = [
-            flattened_tensor[rank * chunk_sz:rank * chunk_sz + chunk_sz]
-            for rank in range(0,
-                              world_sz)
+            flattened_tensor[rank * chunk_sz:rank * chunk_sz + chunk_sz] for rank in range(0, world_sz)
         ]
 
-    padded_partition_sz_for_each_tensor = tuple(
-        math.ceil(t.numel() / world_sz) for t in tensors)
+    padded_partition_sz_for_each_tensor = tuple(math.ceil(t.numel() / world_sz) for t in tensors)
 
     if len(tensors) == 1 and tensors[0].numel() % world_sz == 0:
         # if there's only one tensor being reduced and we don't need to pad
         # we have an opportunity to avoid a memory allocation
         tensor_partition_flat_buffer = tensors[0].view(-1)
     else:
         # interleave tensor partitions such that the correct reduced partitions of each tensor
@@ -64,41 +59,33 @@
         for rank in range(world_sz):
             for tensor_idx in range(len(tensors)):
                 # add tensor content
                 tensor_chunk = partition_lst_for_each_tensor[tensor_idx][rank]
                 tensor_partitions_lst_with_padding.append(tensor_chunk)
 
                 # add padding if necessary
-                padding_sz = padded_partition_sz_for_each_tensor[
-                    tensor_idx] - tensor_chunk.numel()
+                padding_sz = padded_partition_sz_for_each_tensor[tensor_idx] - tensor_chunk.numel()
                 if padding_sz > 0:
                     tensor_partitions_lst_with_padding.append(
-                        torch.empty(padding_sz,
-                                    dtype=tensor_chunk.dtype,
-                                    device=tensor_chunk.device))
+                        torch.empty(padding_sz, dtype=tensor_chunk.dtype, device=tensor_chunk.device))
 
-        tensor_partition_flat_buffer = instrument_w_nvtx(
-            torch.cat)(tensor_partitions_lst_with_padding)
+        tensor_partition_flat_buffer = instrument_w_nvtx(torch.cat)(tensor_partitions_lst_with_padding)
 
     tensor_partition_flat_buffer.div_(world_sz)  # pre-divide
-    tensor_partition_buffer_for_each_rank: List[Tensor] = torch.chunk(
-        tensor_partition_flat_buffer,
-        world_sz)
+    tensor_partition_buffer_for_each_rank: List[Tensor] = torch.chunk(tensor_partition_flat_buffer, world_sz)
 
     # batched reduce-scatter call
     _torch_reduce_scatter_fn(tensor_partition_flat_buffer,
                              tensor_partition_buffer_for_each_rank[this_rank],
                              group=group)
 
     # reverse procedure of the interleaving done previously, done on the
     # result of the batched reduce-scatter
     output_lst: List[Tensor] = [None] * len(tensors)
     offset = 0
     for tensor_idx in range(len(tensors)):
         output_lst[tensor_idx] = tensor_partition_buffer_for_each_rank[this_rank].narrow(
-            0,
-            offset,
-            partition_lst_for_each_tensor[tensor_idx][this_rank].numel())
+            0, offset, partition_lst_for_each_tensor[tensor_idx][this_rank].numel())
 
         offset += padded_partition_sz_for_each_tensor[tensor_idx]
 
     return output_lst
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/comm/mpi.py` & `deepspeed-0.9.0/deepspeed/runtime/comm/mpi.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,21 +1,23 @@
-'''
-Copyright 2020 The Microsoft DeepSpeed Team
-'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import torch
 import cupy
 import time
 import numpy as np
 from mpi4py import MPI
 
 from deepspeed.runtime.compression.cupy import CupyBackend
 
 
 class MpiBackend(object):
+
     def __init__(self, cuda_aware):
         self.comm = MPI.COMM_WORLD
         self.rank = self.comm.Get_rank()
         self.size = self.comm.Get_size()
         self.cuda_aware = cuda_aware
         self.compression_backend = CupyBackend()
 
@@ -27,57 +29,34 @@
                     req.append(comm.Irecv(recbuf[idx], source=idx))
                 else:
                     recbuf[rank] = sendbuf
         else:
             req.append(comm.Isend(sendbuf, dest=root))
         return req
 
-    def gather_cuda(self,
-                    rank,
-                    world_size,
-                    comm,
-                    cupy_sign_list_packed,
-                    cupy_recvbuf_sign,
-                    cupy_worker_scale,
+    def gather_cuda(self, rank, world_size, comm, cupy_sign_list_packed, cupy_recvbuf_sign, cupy_worker_scale,
                     cupy_recvbuf_scale):
         # We do in-place operations on cupy buffers so we do not return any buffers
         requests = []
         for idx in range(world_size):
-            req_sign = self.my_igather(rank,
-                                       world_size,
-                                       comm,
-                                       cupy_sign_list_packed[idx],
-                                       cupy_recvbuf_sign,
-                                       root=idx)
+            req_sign = self.my_igather(rank, world_size, comm, cupy_sign_list_packed[idx], cupy_recvbuf_sign, root=idx)
             requests += req_sign
 
         for idx in range(world_size):
-            req_scale = self.my_igather(rank,
-                                        world_size,
-                                        comm,
-                                        cupy_worker_scale,
-                                        cupy_recvbuf_scale,
-                                        root=idx)
+            req_scale = self.my_igather(rank, world_size, comm, cupy_worker_scale, cupy_recvbuf_scale, root=idx)
             requests += req_scale
 
         MPI.Request.Waitall(requests)
 
-    def gather_host(self,
-                    rank,
-                    world_size,
-                    comm,
-                    cupy_sign_list_packed,
-                    cupy_recvbuf_sign,
-                    cupy_worker_scale,
+    def gather_host(self, rank, world_size, comm, cupy_sign_list_packed, cupy_recvbuf_sign, cupy_worker_scale,
                     cupy_recvbuf_scale):
 
         # In-place operations are not possible for newly created cupy arrays
         # so we need to return the new buffers
-        numpy_recvbuf_sign = np.zeros([world_size,
-                                       cupy_sign_list_packed[rank].size],
+        numpy_recvbuf_sign = np.zeros([world_size, cupy_sign_list_packed[rank].size],
                                       dtype=cupy_sign_list_packed[0].dtype)
         numpy_recvbuf_scale = np.zeros([world_size, 1], dtype=cupy_worker_scale.dtype)
 
         # 1. convert from cupy to numpy
         numpy_sign_list_packed = cupy_sign_list_packed
 
         for idx in range(world_size):
@@ -97,20 +76,15 @@
                                        comm,
                                        numpy_sign_list_packed[idx],
                                        numpy_recvbuf_sign,
                                        root=idx)
             requests += req_sign
 
         for idx in range(world_size):
-            req_scale = self.my_igather(rank,
-                                        world_size,
-                                        comm,
-                                        numpy_worker_scale,
-                                        numpy_recvbuf_scale,
-                                        root=idx)
+            req_scale = self.my_igather(rank, world_size, comm, numpy_worker_scale, numpy_recvbuf_scale, root=idx)
             requests += req_scale
 
         MPI.Request.Waitall(requests)
 
         # 3. Convert back from numpy to cupy
         cupy_recvbuf_sign = cupy.asarray(numpy_recvbuf_sign)
         for idx in range(world_size):
@@ -118,38 +92,26 @@
 
         cupy_worker_scale = cupy.asarray(numpy_worker_scale)
         cupy_recvbuf_scale = cupy.asarray(numpy_recvbuf_scale)
         cupy.cuda.get_current_stream().synchronize()
 
         return cupy_sign_list_packed, cupy_recvbuf_sign, cupy_worker_scale, cupy_recvbuf_scale
 
-    def allgather_cuda(self,
-                       comm,
-                       cupy_server_sign_packed,
-                       cupy_recvbuf_sign_server,
-                       cupy_server_scale,
+    def allgather_cuda(self, comm, cupy_server_sign_packed, cupy_recvbuf_sign_server, cupy_server_scale,
                        cupy_recvbuf_scale_server):
         comm.Allgather(cupy_server_sign_packed, cupy_recvbuf_sign_server)
         comm.Allgather(cupy_server_scale, cupy_recvbuf_scale_server)
 
-    def allgather_host(self,
-                       comm,
-                       cupy_server_sign_packed,
-                       cupy_recvbuf_sign_server,
-                       cupy_server_scale,
+    def allgather_host(self, comm, cupy_server_sign_packed, cupy_recvbuf_sign_server, cupy_server_scale,
                        cupy_recvbuf_scale_server):
 
         # 1. Convert cupy to numpy
-        numpy_recvbuf_sign_server = np.zeros(
-            [comm.Get_size(),
-             cupy_server_sign_packed.size],
-            dtype=cupy_server_sign_packed.dtype)
-        numpy_recvbuf_scale_server = np.zeros([comm.Get_size(),
-                                               1],
-                                              dtype=cupy_server_scale.dtype)
+        numpy_recvbuf_sign_server = np.zeros([comm.Get_size(), cupy_server_sign_packed.size],
+                                             dtype=cupy_server_sign_packed.dtype)
+        numpy_recvbuf_scale_server = np.zeros([comm.Get_size(), 1], dtype=cupy_server_scale.dtype)
 
         numpy_server_sign_packed = cupy.asnumpy(cupy_server_sign_packed)
         numpy_recvbuf_sign_server = cupy.asnumpy(cupy_recvbuf_sign_server)
         numpy_server_scale = cupy.asnumpy(cupy_server_scale)
         numpy_recvbuf_scale_server = cupy.asnumpy(cupy_recvbuf_scale_server)
         cupy.cuda.get_current_stream().synchronize()
 
@@ -163,127 +125,90 @@
         cupy_recvbuf_sign_server = cupy.asarray(numpy_recvbuf_sign_server)
         cupy_server_scale = cupy.asarray(numpy_server_scale)
         cupy_recvbuf_scale_server = cupy.asarray(numpy_recvbuf_scale_server)
         cupy.cuda.get_current_stream().synchronize()
 
         return cupy_server_sign_packed, cupy_recvbuf_sign_server, cupy_server_scale, cupy_recvbuf_scale_server
 
-    def compressed_allreduce(self,
-                             buffer_m: torch.tensor,
-                             worker_error,
-                             server_error,
-                             local_rank):
+    def compressed_allreduce(self, buffer_m: torch.tensor, worker_error, server_error, local_rank):
 
         all_start_time = time.time()
         original_shape = buffer_m.size()
         if len(original_shape) > 1:
             buffer_m = torch.flatten(buffer_m)
         original_size = buffer_m.numel()
         worker_error_size = worker_error.numel()
         cupy.cuda.Device(local_rank).use()
 
         if original_size != worker_error_size:
-            empty_tensor = torch.zeros(worker_error_size - original_size,
-                                       device=buffer_m.device)
+            empty_tensor = torch.zeros(worker_error_size - original_size, device=buffer_m.device)
             buffer_m = torch.cat([buffer_m, empty_tensor])
 
         buffer_m.add_(worker_error)
         worker_scale = torch.norm(buffer_m) / np.sqrt(torch.numel(buffer_m))
-        worker_error.set_(buffer_m - worker_scale *
-                          buffer_m.sign().add_(1).bool().float().add_(-0.5).mul_(2.0))
+        worker_error.set_(buffer_m - worker_scale * buffer_m.sign().add_(1).bool().float().add_(-0.5).mul_(2.0))
 
         cupy_sign_list_packed = self.compression_backend.compress_by_chunk(
-            self.compression_backend.torch2cupy(buffer_m.sign_().add_(1).bool()),
-            self.size)
+            self.compression_backend.torch2cupy(buffer_m.sign_().add_(1).bool()), self.size)
         cupy_worker_scale = self.compression_backend.torch2cupy(worker_scale)
 
-        cupy_recvbuf_sign = cupy.zeros(
-            [self.size,
-             cupy_sign_list_packed[self.rank].size],
-            dtype=cupy_sign_list_packed[0].dtype)
+        cupy_recvbuf_sign = cupy.zeros([self.size, cupy_sign_list_packed[self.rank].size],
+                                       dtype=cupy_sign_list_packed[0].dtype)
         cupy_recvbuf_scale = cupy.zeros([self.size, 1], dtype=cupy_worker_scale.dtype)
 
         # Communication Phase 1
         gather_start = time.time()
         if self.cuda_aware:
-            self.gather_cuda(self.rank,
-                             self.size,
-                             self.comm,
-                             cupy_sign_list_packed,
-                             cupy_recvbuf_sign,
-                             cupy_worker_scale,
-                             cupy_recvbuf_scale)
+            self.gather_cuda(self.rank, self.size, self.comm, cupy_sign_list_packed, cupy_recvbuf_sign,
+                             cupy_worker_scale, cupy_recvbuf_scale)
         else:
-            _, cupy_recvbuf_sign, _, cupy_recvbuf_scale = self.gather_host(self.rank,
-               self.size,
-               self.comm,
-               cupy_sign_list_packed,
-               cupy_recvbuf_sign,
-               cupy_worker_scale,
-               cupy_recvbuf_scale)
+            _, cupy_recvbuf_sign, _, cupy_recvbuf_scale = self.gather_host(self.rank, self.size, self.comm,
+                                                                           cupy_sign_list_packed, cupy_recvbuf_sign,
+                                                                           cupy_worker_scale, cupy_recvbuf_scale)
         gather_end = time.time()
 
         # cupy_sign_list_packed, cupy_worker_scale, worker_scale = None, None, None
         cupy_sign_list_packed = None
 
         compensated_server_m = self.compression_backend.cupy2torch(
-            (cupy.unpackbits(cupy_recvbuf_sign.flatten())).reshape(
-                self.size,
-                -1)).float().add_(-0.5).mul_(2.0).mul_(
-                    self.compression_backend.cupy2torch(cupy_recvbuf_scale).mul_(
-                        1 / self.size)).sum(0)
+            (cupy.unpackbits(cupy_recvbuf_sign.flatten())).reshape(self.size, -1)).float().add_(-0.5).mul_(2.0).mul_(
+                self.compression_backend.cupy2torch(cupy_recvbuf_scale).mul_(1 / self.size)).sum(0)
         compensated_server_m.add_(server_error)
-        server_scale = torch.norm(compensated_server_m) / np.sqrt(
-            compensated_server_m.numel())
-        server_error.set_(
-            compensated_server_m - server_scale *
-            compensated_server_m.sign().add_(1).bool().float().add_(-0.5).mul_(2.0))
+        server_scale = torch.norm(compensated_server_m) / np.sqrt(compensated_server_m.numel())
+        server_error.set_(compensated_server_m -
+                          server_scale * compensated_server_m.sign().add_(1).bool().float().add_(-0.5).mul_(2.0))
 
         cupy_server_scale = self.compression_backend.torch2cupy(server_scale)
 
         cupy_server_sign_packed = self.compression_backend.compress_by_chunk(
-            self.compression_backend.torch2cupy(
-                compensated_server_m.sign_().add_(1).bool()),
-            1)
+            self.compression_backend.torch2cupy(compensated_server_m.sign_().add_(1).bool()), 1)
         compensated_server_m = None
 
-        cupy_recvbuf_sign_server = cupy.zeros(
-            [self.size,
-             cupy_server_sign_packed[0].size],
-            dtype=cupy_recvbuf_sign.dtype)
-        cupy_recvbuf_scale_server = cupy.zeros([self.size,
-                                                1],
-                                               dtype=cupy_recvbuf_scale.dtype)
+        cupy_recvbuf_sign_server = cupy.zeros([self.size, cupy_server_sign_packed[0].size],
+                                              dtype=cupy_recvbuf_sign.dtype)
+        cupy_recvbuf_scale_server = cupy.zeros([self.size, 1], dtype=cupy_recvbuf_scale.dtype)
         # cupy_recvbuf_sign, cupy_recvbuf_scale = None, None
         cupy_recvbuf_sign = None
 
         # Communication Phase 2
         if self.cuda_aware:
-            self.allgather_cuda(self.comm,
-                                cupy_server_sign_packed[0],
-                                cupy_recvbuf_sign_server,
-                                cupy_server_scale,
+            self.allgather_cuda(self.comm, cupy_server_sign_packed[0], cupy_recvbuf_sign_server, cupy_server_scale,
                                 cupy_recvbuf_scale_server)
         else:
-            _, cupy_recvbuf_sign_server, _, cupy_recvbuf_scale_server = self.allgather_host(self.comm,
-                  cupy_server_sign_packed[0],
-                  cupy_recvbuf_sign_server,
-                  cupy_server_scale,
-                  cupy_recvbuf_scale_server)
+            _, cupy_recvbuf_sign_server, _, cupy_recvbuf_scale_server = self.allgather_host(
+                self.comm, cupy_server_sign_packed[0], cupy_recvbuf_sign_server, cupy_server_scale,
+                cupy_recvbuf_scale_server)
 
         # cupy_server_sign_packed, cupy_server_scale, server_scale = None, None, None
         cupy_server_sign_packed = None
 
         buffer_m.data.copy_(
-            self.compression_backend.cupy2torch(
-                (cupy.unpackbits(cupy_recvbuf_sign_server.flatten())).reshape(
-                    self.size,
-                    -1)).float().add_(-0.5).mul_(2.0).mul_(
-                        self.compression_backend.cupy2torch(
-                            cupy_recvbuf_scale_server)).flatten().data)
+            self.compression_backend.cupy2torch((cupy.unpackbits(cupy_recvbuf_sign_server.flatten())).reshape(
+                self.size, -1)).float().add_(-0.5).mul_(2.0).mul_(
+                    self.compression_backend.cupy2torch(cupy_recvbuf_scale_server)).flatten().data)
         if original_size != worker_error_size:
             buffer_m = buffer_m[0:original_size]
         if len(original_shape) > 1:
             buffer_m = buffer_m.reshape(original_shape)
 
         # cupy_recvbuf_sign_server, cupy_recvbuf_scale_server = None, None
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/comm/nccl.py` & `deepspeed-0.9.0/deepspeed/runtime/comm/nccl.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,34 +1,36 @@
-'''
-Copyright 2020 The Microsoft DeepSpeed Team
-'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import torch
 from deepspeed import comm as dist
 import cupy
 import numpy as np
 
 from deepspeed.runtime.compression.cupy import CupyBackend
 from deepspeed.accelerator import get_accelerator
 
 
 class NcclBackend(object):
+
     def __init__(self, mpu=None):
         if mpu is None:
             self.world_group = dist.new_group(ranks=range(dist.get_world_size()))
         else:
             self.mpu = mpu
             self.world_group = self.mpu.get_data_parallel_group()
         self.rank = dist.get_rank(group=self.world_group)
         self.size = dist.get_world_size(group=self.world_group)
         self.compression_backend = CupyBackend()
         self.bool_not_supported = False
         TORCH_MAJOR = int(torch.__version__.split('.')[0])
         TORCH_MINOR = int(torch.__version__.split('.')[1])
-        if TORCH_MAJOR >= 1 and TORCH_MINOR >= 10:
+        if (TORCH_MAJOR == 1 and TORCH_MINOR >= 10) or TORCH_MAJOR == 2:
             self.bool_not_supported = True
 
     def my_igather(self, rank, size, group, sendbuf, recvbuf, root):
         req = []
         if rank == root:
             for idx in range(size):
                 if idx != rank:
@@ -45,160 +47,123 @@
                 if idx != rank:
                     dist.recv(recvbuf[idx], src=idx, group=group)
                 else:
                     recvbuf[rank] = sendbuf
         else:
             dist.send(sendbuf, group=group, dst=root)
 
-    def compressed_allreduce(self,
-                             buffer_m: torch.tensor,
-                             worker_error,
-                             server_error,
-                             local_rank):
+    def compressed_allreduce(self, buffer_m: torch.tensor, worker_error, server_error, local_rank):
 
         # all_start_time = time.time()
         original_shape = buffer_m.size()
         if len(original_shape) > 1:
             buffer_m = torch.flatten(buffer_m)
         original_size = buffer_m.numel()
         worker_error_size = worker_error.numel()
         cupy.cuda.Device(local_rank).use()
 
         if original_size != worker_error_size:
-            empty_tensor = torch.zeros(worker_error_size - original_size,
-                                       device=buffer_m.device)
+            empty_tensor = torch.zeros(worker_error_size - original_size, device=buffer_m.device)
             buffer_m = torch.cat([buffer_m, empty_tensor])
 
         buffer_m.add_(worker_error)
         worker_scale = torch.norm(buffer_m) / np.sqrt(buffer_m.numel())
-        worker_error.set_(buffer_m - worker_scale *
-                          buffer_m.sign().add_(1).bool().float().add_(-0.5).mul_(2.0))
+        worker_error.set_(buffer_m - worker_scale * buffer_m.sign().add_(1).bool().float().add_(-0.5).mul_(2.0))
 
         if self.bool_not_supported:
             cupy_sign_list_packed = self.compression_backend.compress_by_chunk(
-                self.compression_backend.torch2cupy(
-                    buffer_m.sign_().add_(1).bool().to(dtype=torch.uint8)),
-                self.size)
+                self.compression_backend.torch2cupy(buffer_m.sign_().add_(1).bool().to(dtype=torch.uint8)), self.size)
         else:
             cupy_sign_list_packed = self.compression_backend.compress_by_chunk(
-                self.compression_backend.torch2cupy(buffer_m.sign_().add_(1).bool()),
-                self.size)
+                self.compression_backend.torch2cupy(buffer_m.sign_().add_(1).bool()), self.size)
         cupy_worker_scale = self.compression_backend.torch2cupy(worker_scale)
 
-        cupy_recvbuf_sign = cupy.zeros(
-            [self.size,
-             cupy_sign_list_packed[self.rank].size],
-            dtype=cupy_sign_list_packed[0].dtype)
+        cupy_recvbuf_sign = cupy.zeros([self.size, cupy_sign_list_packed[self.rank].size],
+                                       dtype=cupy_sign_list_packed[0].dtype)
         # cupy_recvbuf_scale = cupy.zeros([self.size, 1], dtype=cupy_worker_scale.dtype)
 
         sign_list_packed = [
-            self.compression_backend.cupy2torch(cupy_sign_list_packed[idx])
-            for idx in range(self.size)
+            self.compression_backend.cupy2torch(cupy_sign_list_packed[idx]) for idx in range(self.size)
         ]
 
         # worker_scale = self.compression_backend.cupy2torch(cupy_worker_scale)
         recvbuf_sign = self.compression_backend.cupy2torch(cupy_recvbuf_sign)
         #recvbuf_scale = self.compression_backend.cupy2torch(cupy_recvbuf_scale)
         recvbuf_scale = [
-            torch.zeros(1,
-                        dtype=worker_scale.dtype,
-                        device=torch.device(get_accelerator().device_name(local_rank)))
+            torch.zeros(1, dtype=worker_scale.dtype, device=torch.device(get_accelerator().device_name(local_rank)))
             for i in range(self.size)
         ]
 
         # communication phase 1
         # gather_start = time.time()
         # Alltoall for sign
-        dist.all_to_all_single(recvbuf_sign,
-                               torch.stack(sign_list_packed),
-                               group=self.world_group)
+        dist.all_to_all_single(recvbuf_sign, torch.stack(sign_list_packed), group=self.world_group)
         # Allgather for scale
         dist.all_gather(recvbuf_scale, worker_scale, group=self.world_group)
 
         # gather_end = time.time()
 
         # cupy_sign_list_packed, sign_list_packed, cupy_worker_scale, worker_scale = None, None, None, None
         cupy_sign_list_packed = None
 
         cupy_recvbuf_sign = self.compression_backend.torch2cupy(recvbuf_sign)
         #cupy_recvbuf_scale = self.compression_backend.torch2cupy(torch.stack(recvbuf_scale))
 
         compensated_server_m = self.compression_backend.cupy2torch(
-            (cupy.unpackbits(cupy_recvbuf_sign.flatten())).reshape(
-                self.size,
-                -1)).float().add_(-0.5).mul_(2.0).mul_(
-                    torch.stack(recvbuf_scale).mul_(1 / self.size)).sum(0)
+            (cupy.unpackbits(cupy_recvbuf_sign.flatten())).reshape(self.size, -1)).float().add_(-0.5).mul_(2.0).mul_(
+                torch.stack(recvbuf_scale).mul_(1 / self.size)).sum(0)
         compensated_server_m.add_(server_error)
-        server_scale = torch.norm(compensated_server_m) / np.sqrt(
-            compensated_server_m.numel())
-        server_error.set_(
-            compensated_server_m - server_scale *
-            compensated_server_m.sign().add_(1).bool().float().add_(-0.5).mul_(2.0))
+        server_scale = torch.norm(compensated_server_m) / np.sqrt(compensated_server_m.numel())
+        server_error.set_(compensated_server_m -
+                          server_scale * compensated_server_m.sign().add_(1).bool().float().add_(-0.5).mul_(2.0))
 
         # cupy_server_scale = self.compression_backend.torch2cupy(server_scale)
 
         if self.bool_not_supported:
             cupy_server_sign_packed = self.compression_backend.compress_by_chunk(
-                self.compression_backend.torch2cupy(
-                    compensated_server_m.sign_().add_(1).bool().to(dtype=torch.uint8)),
+                self.compression_backend.torch2cupy(compensated_server_m.sign_().add_(1).bool().to(dtype=torch.uint8)),
                 1)
         else:
             cupy_server_sign_packed = self.compression_backend.compress_by_chunk(
-                self.compression_backend.torch2cupy(
-                    compensated_server_m.sign_().add_(1).bool()),
-                1)
+                self.compression_backend.torch2cupy(compensated_server_m.sign_().add_(1).bool()), 1)
         compensated_server_m = None
 
-        cupy_recvbuf_sign_server = cupy.zeros(
-            [self.size,
-             cupy_server_sign_packed[0].size],
-            dtype=cupy_recvbuf_sign.dtype)
+        cupy_recvbuf_sign_server = cupy.zeros([self.size, cupy_server_sign_packed[0].size],
+                                              dtype=cupy_recvbuf_sign.dtype)
         # cupy_recvbuf_sign, recvbuf_sign = None, None
         cupy_recvbuf_sign = None
 
-        server_sign_packed = [
-            self.compression_backend.cupy2torch(cupy_server_sign_packed[0])
-        ]
+        server_sign_packed = [self.compression_backend.cupy2torch(cupy_server_sign_packed[0])]
         recvbuf_sign_server = [
-            self.compression_backend.cupy2torch(cupy_recvbuf_sign_server[idx])
-            for idx in range(self.size)
+            self.compression_backend.cupy2torch(cupy_recvbuf_sign_server[idx]) for idx in range(self.size)
         ]
 
         # server_scale = self.compression_backend.cupy2torch(cupy_server_scale)
-        cupy_recvbuf_scale_server = cupy.zeros([self.size,
-                                                1],
-                                               dtype=cupy_worker_scale.dtype)
+        cupy_recvbuf_scale_server = cupy.zeros([self.size, 1], dtype=cupy_worker_scale.dtype)
         # cupy_recvbuf_scale, recvbuf_scale = None, None
 
         recvbuf_scale_server = [
-            self.compression_backend.cupy2torch(cupy_recvbuf_scale_server[idx])
-            for idx in range(self.size)
+            self.compression_backend.cupy2torch(cupy_recvbuf_scale_server[idx]) for idx in range(self.size)
         ]
 
         # Communication Phase 2
-        dist.all_gather(recvbuf_sign_server,
-                        server_sign_packed[0],
-                        group=self.world_group)
+        dist.all_gather(recvbuf_sign_server, server_sign_packed[0], group=self.world_group)
         dist.all_gather(recvbuf_scale_server, server_scale, group=self.world_group)
 
         cupy_server_sign_packed = None
 
         # need to convert from a tensor list to a single tensor
         # dist.all_gather only provides a tensor list as the recv/output buffer
         recvbuf_sign_server = torch.stack(recvbuf_sign_server)
 
-        cupy_recvbuf_sign_server = self.compression_backend.torch2cupy(
-            recvbuf_sign_server)
+        cupy_recvbuf_sign_server = self.compression_backend.torch2cupy(recvbuf_sign_server)
 
         buffer_m.data.copy_(
-            self.compression_backend.cupy2torch(
-                (cupy.unpackbits(cupy_recvbuf_sign_server.flatten())).reshape(
-                    self.size,
-                    -1)).float().add_(-0.5).mul_(2.0).mul_(
-                        self.compression_backend.cupy2torch(
-                            cupy_recvbuf_scale_server)).flatten().data)
+            self.compression_backend.cupy2torch((cupy.unpackbits(cupy_recvbuf_sign_server.flatten())).reshape(
+                self.size, -1)).float().add_(-0.5).mul_(2.0).mul_(
+                    self.compression_backend.cupy2torch(cupy_recvbuf_scale_server)).flatten().data)
         if original_size != worker_error_size:
             buffer_m = buffer_m[0:original_size]
         if len(original_shape) > 1:
             buffer_m = buffer_m.reshape(original_shape)
 
         return buffer_m
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/compression/cupy.py` & `deepspeed-0.9.0/deepspeed/runtime/compression/cupy.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,17 +1,19 @@
-'''
-Copyright 2020 The Microsoft DeepSpeed Team
-'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import cupy
 from torch.utils.dlpack import to_dlpack
 from torch.utils.dlpack import from_dlpack
 
 
 class CupyBackend(object):
+
     def __init__(self):
         pass
 
     def torch2cupy(self, tensor):
         return cupy.fromDlpack(to_dlpack(tensor))
 
     def cupy2torch(self, cupy_tensor):
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/config.py` & `deepspeed-0.9.0/deepspeed/runtime/config.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,14 +1,15 @@
-'''Copyright The Microsoft DeepSpeed Team'''
-"""
-Copyright (c) Microsoft Corporation
-Licensed under the MIT license.
-"""
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+
 import os
 from typing import Union
+from enum import Enum
 
 import torch
 import json
 import hjson
 import copy
 import base64
 
@@ -26,14 +27,15 @@
 )
 from .zero.config import get_zero_config, ZeroStageEnum
 from .activation_checkpointing.config import DeepSpeedActivationCheckpointingConfig
 from ..comm.config import DeepSpeedCommsConfig
 from ..monitor.config import get_monitor_config
 
 from deepspeed import comm as dist
+from deepspeed.runtime.config_utils import DeepSpeedConfigModel
 
 from ..git_version_info import version as __version__
 from ..utils import logger
 
 from ..elasticity import (
     elasticity_enabled,
     compute_elastic_config,
@@ -67,20 +69,15 @@
 ADAM_OPTIMIZER = 'adam'
 ADAMW_OPTIMIZER = 'adamw'
 LAMB_OPTIMIZER = 'lamb'
 ONEBIT_ADAM_OPTIMIZER = 'onebitadam'
 ZERO_ONE_ADAM_OPTIMIZER = 'zerooneadam'
 ONEBIT_LAMB_OPTIMIZER = 'onebitlamb'
 DEEPSPEED_OPTIMIZERS = [
-    ADAGRAD_OPTIMIZER,
-    ADAM_OPTIMIZER,
-    ADAMW_OPTIMIZER,
-    LAMB_OPTIMIZER,
-    ONEBIT_ADAM_OPTIMIZER,
-    ONEBIT_LAMB_OPTIMIZER,
+    ADAGRAD_OPTIMIZER, ADAM_OPTIMIZER, ADAMW_OPTIMIZER, LAMB_OPTIMIZER, ONEBIT_ADAM_OPTIMIZER, ONEBIT_LAMB_OPTIMIZER,
     ZERO_ONE_ADAM_OPTIMIZER
 ]
 
 # extra optimizer parameters for adam/adamw
 TORCH_ADAM_PARAM = "torch_adam"
 
 # default to adamw logic for adam/adamw optimizers unless user explicitly opts out
@@ -88,19 +85,44 @@
 ADAM_W_MODE_DEFAULT = True
 
 
 class DeepSpeedConfigError(Exception):
     pass
 
 
+class DtypeEnum(Enum):
+    # The torch dtype must always be the first value (so we return torch.dtype)
+    fp16 = torch.float16, "torch.float16", "fp16", "float16", "half"
+    fp32 = torch.float32, "torch.float32", "fp32", "float32", "float"
+    int8 = torch.int8, "torch.int8", "int8"
+    bf16 = torch.bfloat16, "torch.bfloat16", "bf16", "bfloat16"
+
+    # Copied from https://stackoverflow.com/a/43210118
+    # Allows us to use multiple values for each Enum index and returns first
+    # listed value when Enum is called
+    def __new__(cls, *values):
+        obj = object.__new__(cls)
+        # first value is canonical value
+        obj._value_ = values[0]
+        for other_value in values[1:]:
+            cls._value2member_map_[other_value] = obj
+        obj._all_values = values
+        return obj
+
+    def __repr__(self):
+        return "<%s.%s: %s>" % (
+            self.__class__.__name__,
+            self._name_,
+            ", ".join([repr(v) for v in self._all_values]),
+        )
+
+
 def get_pld_enabled(param_dict):
     if PROGRESSIVE_LAYER_DROP in param_dict.keys():
-        return get_scalar_param(param_dict[PROGRESSIVE_LAYER_DROP],
-                                PLD_ENABLED,
-                                PLD_ENABLED_DEFAULT)
+        return get_scalar_param(param_dict[PROGRESSIVE_LAYER_DROP], PLD_ENABLED, PLD_ENABLED_DEFAULT)
     else:
         return False
 
 
 def get_pld_params(param_dict):
     if PROGRESSIVE_LAYER_DROP in param_dict.keys():
         pld_params = copy.copy(param_dict[PROGRESSIVE_LAYER_DROP])
@@ -132,49 +154,42 @@
     else:
         return False
 
 
 def get_bfloat16_enabled(param_dict):
     for key in [BFLOAT16, BFLOAT16_OLD]:
         if key in param_dict.keys():
-            return get_scalar_param(param_dict[key],
-                                    BFLOAT16_ENABLED,
-                                    BFLOAT16_ENABLED_DEFAULT)
+            return get_scalar_param(param_dict[key], BFLOAT16_ENABLED, BFLOAT16_ENABLED_DEFAULT)
     return False
 
 
 def get_fp16_master_weights_and_grads_enabled(param_dict):
     if get_fp16_enabled(param_dict):
-        return get_scalar_param(param_dict[FP16],
-                                FP16_MASTER_WEIGHTS_AND_GRADS,
-                                FP16_MASTER_WEIGHTS_AND_GRADS_DEFAULT)
+        return get_scalar_param(param_dict[FP16], FP16_MASTER_WEIGHTS_AND_GRADS, FP16_MASTER_WEIGHTS_AND_GRADS_DEFAULT)
     else:
         return False
 
 
 def get_fp16_auto_cast(param_dict):
     if get_fp16_enabled(param_dict):
         return get_scalar_param(param_dict[FP16], FP16_AUTO_CAST, FP16_AUTO_CAST_DEFAULT)
 
 
 def get_loss_scale(param_dict):
     if get_fp16_enabled(param_dict):
-        return get_scalar_param(param_dict[FP16],
-                                FP16_LOSS_SCALE,
-                                FP16_LOSS_SCALE_DEFAULT)
+        return get_scalar_param(param_dict[FP16], FP16_LOSS_SCALE, FP16_LOSS_SCALE_DEFAULT)
     elif get_bfloat16_enabled(param_dict):
         return 1.0
     else:
         return FP16_LOSS_SCALE_DEFAULT
 
 
 def get_initial_dynamic_scale(param_dict):
     if get_fp16_enabled(param_dict):
-        initial_scale_power = get_scalar_param(param_dict[FP16],
-                                               FP16_INITIAL_SCALE_POWER,
+        initial_scale_power = get_scalar_param(param_dict[FP16], FP16_INITIAL_SCALE_POWER,
                                                FP16_INITIAL_SCALE_POWER_DEFAULT)
     elif get_bfloat16_enabled(param_dict):
         initial_scale_power = 0
     else:
         initial_scale_power = FP16_INITIAL_SCALE_POWER_DEFAULT
 
     return 2**initial_scale_power
@@ -187,73 +202,57 @@
         dynamic_loss_args = [
             FP16_INITIAL_SCALE_POWER,
             FP16_LOSS_SCALE_WINDOW,
             FP16_MIN_LOSS_SCALE,
             FP16_HYSTERESIS,
         ]
         if any(arg in list(fp16_dict.keys()) for arg in dynamic_loss_args):
-            init_scale = get_scalar_param(fp16_dict,
-                                          FP16_INITIAL_SCALE_POWER,
-                                          FP16_INITIAL_SCALE_POWER_DEFAULT)
-            scale_window = get_scalar_param(fp16_dict,
-                                            FP16_LOSS_SCALE_WINDOW,
-                                            FP16_LOSS_SCALE_WINDOW_DEFAULT)
-            delayed_shift = get_scalar_param(fp16_dict,
-                                             FP16_HYSTERESIS,
-                                             FP16_HYSTERESIS_DEFAULT)
-            min_loss_scale = get_scalar_param(fp16_dict,
-                                              FP16_MIN_LOSS_SCALE,
-                                              FP16_MIN_LOSS_SCALE_DEFAULT)
+            init_scale = get_scalar_param(fp16_dict, FP16_INITIAL_SCALE_POWER, FP16_INITIAL_SCALE_POWER_DEFAULT)
+            scale_window = get_scalar_param(fp16_dict, FP16_LOSS_SCALE_WINDOW, FP16_LOSS_SCALE_WINDOW_DEFAULT)
+            delayed_shift = get_scalar_param(fp16_dict, FP16_HYSTERESIS, FP16_HYSTERESIS_DEFAULT)
+            min_loss_scale = get_scalar_param(fp16_dict, FP16_MIN_LOSS_SCALE, FP16_MIN_LOSS_SCALE_DEFAULT)
             loss_scale_args = {
                 INITIAL_LOSS_SCALE: 2**init_scale,
                 SCALE_WINDOW: scale_window,
                 DELAYED_SHIFT: delayed_shift,
                 MIN_LOSS_SCALE: min_loss_scale,
             }
 
     return loss_scale_args
 
 
 def get_gradient_accumulation_steps(param_dict):
-    return get_scalar_param(param_dict,
-                            GRADIENT_ACCUMULATION_STEPS,
-                            GRADIENT_ACCUMULATION_STEPS_DEFAULT)
+    return get_scalar_param(param_dict, GRADIENT_ACCUMULATION_STEPS, GRADIENT_ACCUMULATION_STEPS_DEFAULT)
 
 
 def get_sparse_gradients_enabled(param_dict):
     return get_scalar_param(param_dict, SPARSE_GRADIENTS, SPARSE_GRADIENTS_DEFAULT)
 
 
 def get_communication_data_type(param_dict):
-    val = get_scalar_param(param_dict,
-                           COMMUNICATION_DATA_TYPE,
-                           COMMUNICATION_DATA_TYPE_DEFAULT)
+    val = get_scalar_param(param_dict, COMMUNICATION_DATA_TYPE, COMMUNICATION_DATA_TYPE_DEFAULT)
     val = val.lower() if val is not None else val
     if val is None:
         return val  # we must determine it by other parameters
     elif val == "fp32":
         return torch.float32
     elif val == "fp16":
         return torch.float16
     elif val == "bfp16":
         return torch.bfloat16
 
-    raise ValueError(
-        f"Invalid communication_data_type. Supported data types: ['fp16', 'bfp16', 'fp32']. Got: {val}"
-    )
+    raise ValueError(f"Invalid communication_data_type. Supported data types: ['fp16', 'bfp16', 'fp32']. Got: {val}")
 
 
 def get_prescale_gradients(param_dict):
     return get_scalar_param(param_dict, PRESCALE_GRADIENTS, PRESCALE_GRADIENTS_DEFAULT)
 
 
 def get_gradient_predivide_factor(param_dict):
-    return get_scalar_param(param_dict,
-                            GRADIENT_PREDIVIDE_FACTOR,
-                            GRADIENT_PREDIVIDE_FACTOR_DEFAULT)
+    return get_scalar_param(param_dict, GRADIENT_PREDIVIDE_FACTOR, GRADIENT_PREDIVIDE_FACTOR_DEFAULT)
 
 
 def get_steps_per_print(param_dict):
     return get_scalar_param(param_dict, STEPS_PER_PRINT, STEPS_PER_PRINT_DEFAULT)
 
 
 def get_disable_allgather(param_dict):
@@ -280,16 +279,15 @@
         elif mode == SPARSE_VARIABLE_MODE:
             return get_sparse_variable_config(sparsity)
         elif mode == SPARSE_BIGBIRD_MODE:
             return get_sparse_bigbird_config(sparsity)
         elif mode == SPARSE_BSLONGFORMER_MODE:
             return get_sparse_bslongformer_config(sparsity)
         else:
-            raise NotImplementedError(
-                f"Given sparsity mode, {mode}, has not been implemented yet!")
+            raise NotImplementedError(f"Given sparsity mode, {mode}, has not been implemented yet!")
 
     else:
         return None
 
 
 def get_sparse_dense_config(sparsity):
     block = get_scalar_param(sparsity, SPARSE_BLOCK, SPARSE_BLOCK_DEFAULT)
@@ -299,23 +297,17 @@
 def get_sparse_fixed_config(sparsity):
     block = get_scalar_param(sparsity, SPARSE_BLOCK, SPARSE_BLOCK_DEFAULT)
     different_layout_per_head = get_scalar_param(
         sparsity,
         SPARSE_DIFFERENT_LAYOUT_PER_HEAD,
         SPARSE_DIFFERENT_LAYOUT_PER_HEAD_DEFAULT,
     )
-    num_local_blocks = get_scalar_param(sparsity,
-                                        SPARSE_NUM_LOCAL_BLOCKS,
-                                        SPARSE_NUM_LOCAL_BLOCKS_DEFAULT)
-    num_global_blocks = get_scalar_param(sparsity,
-                                         SPARSE_NUM_GLOBAL_BLOCKS,
-                                         SPARSE_NUM_GLOBAL_BLOCKS_DEFAULT)
-    attention = get_scalar_param(sparsity,
-                                 SPARSE_ATTENTION_TYPE,
-                                 SPARSE_ATTENTION_TYPE_DEFAULT)
+    num_local_blocks = get_scalar_param(sparsity, SPARSE_NUM_LOCAL_BLOCKS, SPARSE_NUM_LOCAL_BLOCKS_DEFAULT)
+    num_global_blocks = get_scalar_param(sparsity, SPARSE_NUM_GLOBAL_BLOCKS, SPARSE_NUM_GLOBAL_BLOCKS_DEFAULT)
+    attention = get_scalar_param(sparsity, SPARSE_ATTENTION_TYPE, SPARSE_ATTENTION_TYPE_DEFAULT)
     horizontal_global_attention = get_scalar_param(
         sparsity,
         SPARSE_HORIZONTAL_GLOBAL_ATTENTION,
         SPARSE_HORIZONTAL_GLOBAL_ATTENTION_DEFAULT,
     )
     num_different_global_patterns = get_scalar_param(
         sparsity,
@@ -338,31 +330,23 @@
 def get_sparse_variable_config(sparsity):
     block = get_scalar_param(sparsity, SPARSE_BLOCK, SPARSE_BLOCK_DEFAULT)
     different_layout_per_head = get_scalar_param(
         sparsity,
         SPARSE_DIFFERENT_LAYOUT_PER_HEAD,
         SPARSE_DIFFERENT_LAYOUT_PER_HEAD_DEFAULT,
     )
-    num_random_blocks = get_scalar_param(sparsity,
-                                         SPARSE_NUM_RANDOM_BLOCKS,
-                                         SPARSE_NUM_RANDOM_BLOCKS_DEFAULT)
-    local_window_blocks = get_scalar_param(sparsity,
-                                           SPARSE_LOCAL_WINDOW_BLOCKS,
-                                           SPARSE_LOCAL_WINDOW_BLOCKS_DEFAULT)
-    global_block_indices = get_scalar_param(sparsity,
-                                            SPARSE_GLOBAL_BLOCK_INDICES,
-                                            SPARSE_GLOBAL_BLOCK_INDICES_DEFAULT)
+    num_random_blocks = get_scalar_param(sparsity, SPARSE_NUM_RANDOM_BLOCKS, SPARSE_NUM_RANDOM_BLOCKS_DEFAULT)
+    local_window_blocks = get_scalar_param(sparsity, SPARSE_LOCAL_WINDOW_BLOCKS, SPARSE_LOCAL_WINDOW_BLOCKS_DEFAULT)
+    global_block_indices = get_scalar_param(sparsity, SPARSE_GLOBAL_BLOCK_INDICES, SPARSE_GLOBAL_BLOCK_INDICES_DEFAULT)
     global_block_end_indices = get_scalar_param(
         sparsity,
         SPARSE_GLOBAL_BLOCK_END_INDICES,
         SPARSE_GLOBAL_BLOCK_END_INDICES_DEFAULT,
     )
-    attention = get_scalar_param(sparsity,
-                                 SPARSE_ATTENTION_TYPE,
-                                 SPARSE_ATTENTION_TYPE_DEFAULT)
+    attention = get_scalar_param(sparsity, SPARSE_ATTENTION_TYPE, SPARSE_ATTENTION_TYPE_DEFAULT)
     horizontal_global_attention = get_scalar_param(
         sparsity,
         SPARSE_HORIZONTAL_GLOBAL_ATTENTION,
         SPARSE_HORIZONTAL_GLOBAL_ATTENTION_DEFAULT,
     )
 
     return {
@@ -381,25 +365,21 @@
 def get_sparse_bigbird_config(sparsity):
     block = get_scalar_param(sparsity, SPARSE_BLOCK, SPARSE_BLOCK_DEFAULT)
     different_layout_per_head = get_scalar_param(
         sparsity,
         SPARSE_DIFFERENT_LAYOUT_PER_HEAD,
         SPARSE_DIFFERENT_LAYOUT_PER_HEAD_DEFAULT,
     )
-    num_random_blocks = get_scalar_param(sparsity,
-                                         SPARSE_NUM_RANDOM_BLOCKS,
-                                         SPARSE_NUM_RANDOM_BLOCKS_DEFAULT)
+    num_random_blocks = get_scalar_param(sparsity, SPARSE_NUM_RANDOM_BLOCKS, SPARSE_NUM_RANDOM_BLOCKS_DEFAULT)
     num_sliding_window_blocks = get_scalar_param(
         sparsity,
         SPARSE_NUM_SLIDING_WINDOW_BLOCKS,
         SPARSE_NUM_SLIDING_WINDOW_BLOCKS_DEFAULT,
     )
-    num_global_blocks = get_scalar_param(sparsity,
-                                         SPARSE_NUM_GLOBAL_BLOCKS,
-                                         SPARSE_NUM_GLOBAL_BLOCKS_DEFAULT)
+    num_global_blocks = get_scalar_param(sparsity, SPARSE_NUM_GLOBAL_BLOCKS, SPARSE_NUM_GLOBAL_BLOCKS_DEFAULT)
 
     return {
         SPARSE_MODE: SPARSE_BIGBIRD_MODE,
         SPARSE_BLOCK: block,
         SPARSE_DIFFERENT_LAYOUT_PER_HEAD: different_layout_per_head,
         SPARSE_NUM_RANDOM_BLOCKS: num_random_blocks,
         SPARSE_NUM_SLIDING_WINDOW_BLOCKS: num_sliding_window_blocks,
@@ -415,17 +395,15 @@
         SPARSE_DIFFERENT_LAYOUT_PER_HEAD_DEFAULT,
     )
     num_sliding_window_blocks = get_scalar_param(
         sparsity,
         SPARSE_NUM_SLIDING_WINDOW_BLOCKS,
         SPARSE_NUM_SLIDING_WINDOW_BLOCKS_DEFAULT,
     )
-    global_block_indices = get_scalar_param(sparsity,
-                                            SPARSE_GLOBAL_BLOCK_INDICES,
-                                            SPARSE_GLOBAL_BLOCK_INDICES_DEFAULT)
+    global_block_indices = get_scalar_param(sparsity, SPARSE_GLOBAL_BLOCK_INDICES, SPARSE_GLOBAL_BLOCK_INDICES_DEFAULT)
     global_block_end_indices = get_scalar_param(
         sparsity,
         SPARSE_GLOBAL_BLOCK_END_INDICES,
         SPARSE_GLOBAL_BLOCK_END_INDICES_DEFAULT,
     )
 
     return {
@@ -470,16 +448,15 @@
     if OPTIMIZER in param_dict.keys() and TYPE in param_dict[OPTIMIZER].keys():
         return param_dict[OPTIMIZER][TYPE]
     else:
         return OPTIMIZER_TYPE_DEFAULT
 
 
 def get_optimizer_params(param_dict):
-    if (get_optimizer_name(param_dict) is not None
-            and OPTIMIZER_PARAMS in param_dict[OPTIMIZER].keys()):
+    if (get_optimizer_name(param_dict) is not None and OPTIMIZER_PARAMS in param_dict[OPTIMIZER].keys()):
         return param_dict[OPTIMIZER][OPTIMIZER_PARAMS]
     else:
         return None
 
 
 def get_optimizer_gradient_clipping(param_dict):
     optimizer_params = get_optimizer_params(param_dict)
@@ -493,35 +470,30 @@
     if OPTIMIZER in param_dict.keys() and LEGACY_FUSION in param_dict[OPTIMIZER].keys():
         return param_dict[OPTIMIZER][LEGACY_FUSION]
     else:
         return LEGACY_FUSION_DEFAULT
 
 
 def get_zero_allow_untested_optimizer(param_dict):
-    return get_scalar_param(param_dict,
-                            ZERO_ALLOW_UNTESTED_OPTIMIZER,
-                            ZERO_ALLOW_UNTESTED_OPTIMIZER_DEFAULT)
+    return get_scalar_param(param_dict, ZERO_ALLOW_UNTESTED_OPTIMIZER, ZERO_ALLOW_UNTESTED_OPTIMIZER_DEFAULT)
 
 
 def get_zero_force_ds_cpu_optimizer(param_dict):
-    return get_scalar_param(param_dict,
-                            ZERO_FORCE_DS_CPU_OPTIMIZER,
-                            ZERO_FORCE_DS_CPU_OPTIMIZER_DEFAULT)
+    return get_scalar_param(param_dict, ZERO_FORCE_DS_CPU_OPTIMIZER, ZERO_FORCE_DS_CPU_OPTIMIZER_DEFAULT)
 
 
 def get_scheduler_name(param_dict):
     if SCHEDULER in param_dict.keys() and TYPE in param_dict[SCHEDULER].keys():
         return param_dict[SCHEDULER][TYPE]
     else:
         return SCHEDULER_TYPE_DEFAULT
 
 
 def get_scheduler_params(param_dict):
-    if (get_scheduler_name(param_dict) is not None
-            and SCHEDULER_PARAMS in param_dict[SCHEDULER].keys()):
+    if (get_scheduler_name(param_dict) is not None and SCHEDULER_PARAMS in param_dict[SCHEDULER].keys()):
         return param_dict[SCHEDULER][SCHEDULER_PARAMS]
     else:
         return None
 
 
 def get_train_batch_size(param_dict):
     return get_scalar_param(param_dict, TRAIN_BATCH_SIZE, TRAIN_BATCH_SIZE_DEFAULT)
@@ -532,23 +504,36 @@
         param_dict,
         TRAIN_MICRO_BATCH_SIZE_PER_GPU,
         TRAIN_MICRO_BATCH_SIZE_PER_GPU_DEFAULT,
     )
 
 
 def get_wall_clock_breakdown(param_dict):
-    return get_scalar_param(param_dict,
-                            WALL_CLOCK_BREAKDOWN,
-                            WALL_CLOCK_BREAKDOWN_DEFAULT)
+    return get_scalar_param(param_dict, WALL_CLOCK_BREAKDOWN, WALL_CLOCK_BREAKDOWN_DEFAULT)
 
 
 def get_memory_breakdown(param_dict):
     return get_scalar_param(param_dict, MEMORY_BREAKDOWN, MEMORY_BREAKDOWN_DEFAULT)
 
 
+class HybridEngineConfig(DeepSpeedConfigModel):
+    enabled: bool = False
+    max_out_tokens: int = 512
+    inference_tp_size: int = 1
+    release_inference_cache: bool = False
+    pin_parameters: bool = True
+    tp_gather_partition_size: int = 8
+
+
+def get_hybrid_engine_config(param_dict):
+    hybrid_engine_config_dict = param_dict.get("hybrid_engine", {})
+    hybrid_engine_config = HybridEngineConfig(**hybrid_engine_config_dict)
+    return hybrid_engine_config
+
+
 def get_eigenvalue_config(param_dict):
     if get_quantize_enabled(param_dict):
         param_dict = param_dict[QUANTIZE_TRAINING]
         assert not get_eigenvalue_enabled(param_dict), "Eigenvalue based MoQ is temporarily disabled"
         return (
             get_eigenvalue_enabled(param_dict),
             get_eigenvalue_verbose(param_dict),
@@ -570,53 +555,43 @@
             EIGENVALUE_LAYER_NAME_DEFAULT,
             EIGENVALUE_LAYER_NUM_DEFAULT,
         )
 
 
 def get_eigenvalue_enabled(param_dict):
     if EIGENVALUE in param_dict.keys():
-        return get_scalar_param(param_dict[EIGENVALUE],
-                                EIGENVALUE_ENABLED,
-                                EIGENVALUE_ENABLED_DEFAULT)
+        return get_scalar_param(param_dict[EIGENVALUE], EIGENVALUE_ENABLED, EIGENVALUE_ENABLED_DEFAULT)
     else:
         return EIGENVALUE_ENABLED_DEFAULT
 
 
 def get_eigenvalue_verbose(param_dict):
     if EIGENVALUE in param_dict.keys():
-        return get_scalar_param(param_dict[EIGENVALUE],
-                                EIGENVALUE_VERBOSE,
-                                EIGENVALUE_VERBOSE_DEFAULT)
+        return get_scalar_param(param_dict[EIGENVALUE], EIGENVALUE_VERBOSE, EIGENVALUE_VERBOSE_DEFAULT)
     else:
         return EIGENVALUE_VERBOSE_DEFAULT
 
 
 def get_eigenvalue_max_iter(param_dict):
     if EIGENVALUE in param_dict.keys():
-        return get_scalar_param(param_dict[EIGENVALUE],
-                                EIGENVALUE_MAX_ITER,
-                                EIGENVALUE_MAX_ITER_DEFAULT)
+        return get_scalar_param(param_dict[EIGENVALUE], EIGENVALUE_MAX_ITER, EIGENVALUE_MAX_ITER_DEFAULT)
     else:
         return EIGENVALUE_MAX_ITER_DEFAULT
 
 
 def get_eigenvalue_tol(param_dict):
     if EIGENVALUE in param_dict.keys():
-        return get_scalar_param(param_dict[EIGENVALUE],
-                                EIGENVALUE_TOL,
-                                EIGENVALUE_TOL_DEFAULT)
+        return get_scalar_param(param_dict[EIGENVALUE], EIGENVALUE_TOL, EIGENVALUE_TOL_DEFAULT)
     else:
         return EIGENVALUE_TOL_DEFAULT
 
 
 def get_eigenvalue_stability(param_dict):
     if EIGENVALUE in param_dict.keys():
-        return get_scalar_param(param_dict[EIGENVALUE],
-                                EIGENVALUE_STABILITY,
-                                EIGENVALUE_STABILITY_DEFAULT)
+        return get_scalar_param(param_dict[EIGENVALUE], EIGENVALUE_STABILITY, EIGENVALUE_STABILITY_DEFAULT)
     else:
         return EIGENVALUE_STABILITY_DEFAULT
 
 
 def get_eigenvalue_gas_boundary_resolution(param_dict):
     if EIGENVALUE in param_dict.keys():
         return get_scalar_param(
@@ -626,101 +601,88 @@
         )
     else:
         return EIGENVALUE_GAS_BOUNDARY_RESOLUTION_DEFAULT
 
 
 def get_eigenvalue_layer_name(param_dict):
     if EIGENVALUE in param_dict.keys():
-        return get_scalar_param(param_dict[EIGENVALUE],
-                                EIGENVALUE_LAYER_NAME,
-                                EIGENVALUE_LAYER_NAME_DEFAULT)
+        return get_scalar_param(param_dict[EIGENVALUE], EIGENVALUE_LAYER_NAME, EIGENVALUE_LAYER_NAME_DEFAULT)
     else:
         return EIGENVALUE_LAYER_NAME_DEFAULT
 
 
 def get_eigenvalue_layer_num(param_dict):
     if EIGENVALUE in param_dict.keys():
-        return get_scalar_param(param_dict[EIGENVALUE],
-                                EIGENVALUE_LAYER_NUM,
-                                EIGENVALUE_LAYER_NUM_DEFAULT)
+        return get_scalar_param(param_dict[EIGENVALUE], EIGENVALUE_LAYER_NUM, EIGENVALUE_LAYER_NUM_DEFAULT)
     else:
         return EIGENVALUE_LAYER_NUM_DEFAULT
 
 
 def get_checkpoint_params(param_dict):
     return param_dict.get(CHECKPOINT, {})
 
 
 def get_data_types_params(param_dict):
     return param_dict.get(DATA_TYPES, {})
 
 
 def get_checkpoint_tag_validation_mode(checkpoint_params):
-    tag_validation_mode = checkpoint_params.get(CHECKPOINT_TAG_VALIDATION,
-                                                CHECKPOINT_TAG_VALIDATION_DEFAULT)
+    tag_validation_mode = checkpoint_params.get(CHECKPOINT_TAG_VALIDATION, CHECKPOINT_TAG_VALIDATION_DEFAULT)
     tag_validation_mode = tag_validation_mode.upper()
     if tag_validation_mode in CHECKPOINT_TAG_VALIDATION_MODES:
         return tag_validation_mode
     else:
         raise DeepSpeedConfigError(
             "Checkpoint config contains invalid tag_validation "
-            f"value of {tag_validation_mode}, expecting one of {CHECKPOINT_TAG_VALIDATION_MODES}"
-        )
+            f"value of {tag_validation_mode}, expecting one of {CHECKPOINT_TAG_VALIDATION_MODES}")
 
 
 def get_checkpoint_parallel_write_pipeline(checkpoint_params):
     par_write_params = checkpoint_params.get(CHECKPOINT_PARALLEL_WRITE, {})
-    par_write_pipeline = par_write_params.get(
-        CHECKPOINT_PARALLEL_WRITE_PIPELINE_STAGE,
-        CHECKPOINT_PARALLEL_WRITE_PIPELINE_STAGE_DEFAULT)
+    par_write_pipeline = par_write_params.get(CHECKPOINT_PARALLEL_WRITE_PIPELINE_STAGE,
+                                              CHECKPOINT_PARALLEL_WRITE_PIPELINE_STAGE_DEFAULT)
     if par_write_pipeline in [True, False]:
         return par_write_pipeline
     else:
-        raise DeepSpeedConfigError(
-            "checkpoint::parallel_write::pipeline_stage "
-            f"value of '{par_write_pipeline}' is invalid, expecting: true or false")
+        raise DeepSpeedConfigError("checkpoint::parallel_write::pipeline_stage "
+                                   f"value of '{par_write_pipeline}' is invalid, expecting: true or false")
 
 
 def get_dataloader_drop_last(param_dict):
-    return get_scalar_param(param_dict,
-                            DATALOADER_DROP_LAST,
-                            DATALOADER_DROP_LAST_DEFAULT)
+    return get_scalar_param(param_dict, DATALOADER_DROP_LAST, DATALOADER_DROP_LAST_DEFAULT)
 
 
 '''Write deepspeed config files by modifying basic templates.
 Can be used for quickly changing parameters via command line parameters.'''
 
 
 class DeepSpeedConfigWriter:
+
     def __init__(self, data=None):
         self.data = data if data is not None else {}
 
     def add_config(self, key, value):
         self.data[key] = value
 
     def load_config(self, filename):
-        self.data = json.load(open(filename,
-                                   "r"),
-                              object_pairs_hook=dict_raise_error_on_duplicate_keys)
+        self.data = json.load(open(filename, "r"), object_pairs_hook=dict_raise_error_on_duplicate_keys)
 
     def write_config(self, filename):
         with open(filename, "w") as outfile:
             json.dump(self.data, outfile)
 
 
 class DeepSpeedConfig(object):
+
     def __init__(self, config: Union[str, dict], mpu=None):
         super(DeepSpeedConfig, self).__init__()
         if isinstance(config, dict):
             self._param_dict = config
         elif os.path.exists(config):
-            self._param_dict = hjson.load(
-                open(config,
-                     "r"),
-                object_pairs_hook=dict_raise_error_on_duplicate_keys)
+            self._param_dict = hjson.load(open(config, "r"), object_pairs_hook=dict_raise_error_on_duplicate_keys)
         else:
             try:
                 config_decoded = base64.urlsafe_b64decode(config).decode('utf-8')
                 self._param_dict = hjson.loads(config_decoded)
             except (UnicodeDecodeError, AttributeError):
                 raise ValueError(
                     f"Expected a string path to an existing deepspeed config, or a dictionary or a valid base64. Received: {config}"
@@ -746,32 +708,26 @@
             )
 
             elastic_dict = self._param_dict[ELASTICITY]
 
             # Ensure the resource scheduler saw the same elastic config we are using at runtime
             ensure_immutable_elastic_config(runtime_elastic_config_dict=elastic_dict)
 
-            self.elastic_model_parallel_size = elastic_dict.get(
-                MODEL_PARLLEL_SIZE,
-                MODEL_PARLLEL_SIZE_DEFAULT)
+            self.elastic_model_parallel_size = elastic_dict.get(MODEL_PARLLEL_SIZE, MODEL_PARLLEL_SIZE_DEFAULT)
             if self.elastic_model_parallel_size < 1:
-                raise ElasticityConfigError(
-                    "Model-Parallel size cannot be less than 1, "
-                    f"given model-parallel size: {self.elastic_model_parallel_size}")
+                raise ElasticityConfigError("Model-Parallel size cannot be less than 1, "
+                                            f"given model-parallel size: {self.elastic_model_parallel_size}")
 
-            self.num_gpus_per_node = elastic_dict.get(NUM_GPUS_PER_NODE,
-                                                      NUM_GPUS_PER_NODE_DEFAULT)
+            self.num_gpus_per_node = elastic_dict.get(NUM_GPUS_PER_NODE, NUM_GPUS_PER_NODE_DEFAULT)
             if self.num_gpus_per_node < 1:
-                raise ElasticityConfigError(
-                    "NUmber of GPUs per node cannot be less than 1, "
-                    f"given number of GPUs per node: {self.num_gpus_per_node}")
-
-            ignore_non_elastic_batch_info = elastic_dict.get(
-                IGNORE_NON_ELASTIC_BATCH_INFO,
-                IGNORE_NON_ELASTIC_BATCH_INFO_DEFAULT)
+                raise ElasticityConfigError("NUmber of GPUs per node cannot be less than 1, "
+                                            f"given number of GPUs per node: {self.num_gpus_per_node}")
+
+            ignore_non_elastic_batch_info = elastic_dict.get(IGNORE_NON_ELASTIC_BATCH_INFO,
+                                                             IGNORE_NON_ELASTIC_BATCH_INFO_DEFAULT)
 
             if not ignore_non_elastic_batch_info:
                 batch_params = [
                     TRAIN_BATCH_SIZE,
                     TRAIN_MICRO_BATCH_SIZE_PER_GPU,
                     GRADIENT_ACCUMULATION_STEPS,
                 ]
@@ -781,31 +737,25 @@
                         f"{GRADIENT_ACCUMULATION_STEPS}). These parameters *will not be used* since " \
                         "elastic training is enabled, which takes control of these parameters. " \
                         "If you want to suppress this error (the parameters will be silently ignored) " \
                         f"please set {IGNORE_NON_ELASTIC_BATCH_INFO}':true in your elasticity config.")
 
             # micro_bsz * world_size * gas = total_batch_size
             # gas = total_batch_size // (micro_bsz * world_size)
-            gradient_accu_steps = final_batch_size // (micro_batch_size *
-                                                       self.world_size)
+            gradient_accu_steps = final_batch_size // (micro_batch_size * self.world_size)
 
             if TRAIN_BATCH_SIZE in self._param_dict:
-                logger.warning(
-                    "[Elasticity] overriding training_batch_size: "
-                    f"{self._param_dict[TRAIN_BATCH_SIZE]} -> {final_batch_size}")
+                logger.warning("[Elasticity] overriding training_batch_size: "
+                               f"{self._param_dict[TRAIN_BATCH_SIZE]} -> {final_batch_size}")
             if TRAIN_MICRO_BATCH_SIZE_PER_GPU in self._param_dict:
-                logger.warning(
-                    "[Elasticity] overriding train_micro_batch_size_per_gpu: "
-                    f"{self._param_dict[TRAIN_MICRO_BATCH_SIZE_PER_GPU]} -> {micro_batch_size}"
-                )
+                logger.warning("[Elasticity] overriding train_micro_batch_size_per_gpu: "
+                               f"{self._param_dict[TRAIN_MICRO_BATCH_SIZE_PER_GPU]} -> {micro_batch_size}")
             if GRADIENT_ACCUMULATION_STEPS in self._param_dict:
-                logger.warning(
-                    "[Elasticity] overriding gradient_accumulation_steps: "
-                    f"{self._param_dict[GRADIENT_ACCUMULATION_STEPS]} -> {gradient_accu_steps}"
-                )
+                logger.warning("[Elasticity] overriding gradient_accumulation_steps: "
+                               f"{self._param_dict[GRADIENT_ACCUMULATION_STEPS]} -> {gradient_accu_steps}")
 
             logger.info(f"[Elasticity] valid GPU counts: {valid_gpus}")
 
             self._param_dict[TRAIN_BATCH_SIZE] = final_batch_size
             self._param_dict[TRAIN_MICRO_BATCH_SIZE_PER_GPU] = micro_batch_size
             self._param_dict[GRADIENT_ACCUMULATION_STEPS] = gradient_accu_steps
 
@@ -813,112 +763,105 @@
         self._initialize_params(copy.copy(self._param_dict))
         self._configure_train_batch_size()
         self._do_sanity_check()
 
     def _initialize_params(self, param_dict):
         self.train_batch_size = get_train_batch_size(param_dict)
         #print(f"beginning get_train_batch_size = {get_train_batch_size}")
-        self.train_micro_batch_size_per_gpu = get_train_micro_batch_size_per_gpu(
-            param_dict)
+        self.train_micro_batch_size_per_gpu = get_train_micro_batch_size_per_gpu(param_dict)
         self.gradient_accumulation_steps = get_gradient_accumulation_steps(param_dict)
         self.steps_per_print = get_steps_per_print(param_dict)
         self.dump_state = get_dump_state(param_dict)
 
         self.disable_allgather = get_disable_allgather(param_dict)
         self.communication_data_type = get_communication_data_type(param_dict)
         self.prescale_gradients = get_prescale_gradients(param_dict)
         self.gradient_predivide_factor = get_gradient_predivide_factor(param_dict)
         self.sparse_gradients_enabled = get_sparse_gradients_enabled(param_dict)
 
         self.zero_config = get_zero_config(param_dict)
         self.zero_optimization_stage = self.zero_config.stage
         self.zero_enabled = self.zero_optimization_stage > 0
 
-        self.activation_checkpointing_config = DeepSpeedActivationCheckpointingConfig(
-            param_dict)
+        self.activation_checkpointing_config = DeepSpeedActivationCheckpointingConfig(param_dict)
 
         self.comms_config = DeepSpeedCommsConfig(param_dict)
         self.monitor_config = get_monitor_config(param_dict)
 
         self.gradient_clipping = get_gradient_clipping(param_dict)
         self.fp16_enabled = get_fp16_enabled(param_dict)
         self.fp16_auto_cast = get_fp16_auto_cast(param_dict)
         self.bfloat16_enabled = get_bfloat16_enabled(param_dict)
-        assert not (self.fp16_enabled and self.bfloat16_enabled), 'bfloat16 and fp16 modes cannot be simultaneously enabled'
-        self.fp16_master_weights_and_gradients = get_fp16_master_weights_and_grads_enabled(
-            param_dict)
+        assert not (self.fp16_enabled
+                    and self.bfloat16_enabled), 'bfloat16 and fp16 modes cannot be simultaneously enabled'
+        self.fp16_master_weights_and_gradients = get_fp16_master_weights_and_grads_enabled(param_dict)
         self.amp_enabled = get_amp_enabled(param_dict)
         self.amp_params = get_amp_params(param_dict)
         self.loss_scale = get_loss_scale(param_dict)
         self.initial_dynamic_scale = get_initial_dynamic_scale(param_dict)
         self.dynamic_loss_scale_args = get_dynamic_loss_scale_args(param_dict)
 
         self.compression_config = get_compression_config(param_dict)
 
         self.optimizer_name = get_optimizer_name(param_dict)
-        if (self.optimizer_name is not None
-                and self.optimizer_name.lower() in DEEPSPEED_OPTIMIZERS):
+        if (self.optimizer_name is not None and self.optimizer_name.lower() in DEEPSPEED_OPTIMIZERS):
             self.optimizer_name = self.optimizer_name.lower()
 
         self.optimizer_params = get_optimizer_params(param_dict)
         self.optimizer_legacy_fusion = get_optimizer_legacy_fusion(param_dict)
 
-        self.zero_allow_untested_optimizer = get_zero_allow_untested_optimizer(
-            param_dict)
+        self.zero_allow_untested_optimizer = get_zero_allow_untested_optimizer(param_dict)
 
         self.zero_force_ds_cpu_optimizer = get_zero_force_ds_cpu_optimizer(param_dict)
 
         self.scheduler_name = get_scheduler_name(param_dict)
         self.scheduler_params = get_scheduler_params(param_dict)
 
         self.flops_profiler_config = DeepSpeedFlopsProfilerConfig(param_dict)
-        self.wall_clock_breakdown = (get_wall_clock_breakdown(param_dict)
-                                     | self.flops_profiler_config.enabled)
+        self.wall_clock_breakdown = (get_wall_clock_breakdown(param_dict) | self.flops_profiler_config.enabled)
         self.memory_breakdown = get_memory_breakdown(param_dict)
         self.autotuning_config = DeepSpeedAutotuningConfig(param_dict)
 
         (
             self.eigenvalue_enabled,
             self.eigenvalue_verbose,
             self.eigenvalue_max_iter,
             self.eigenvalue_tol,
             self.eigenvalue_stability,
             self.eigenvalue_gas_boundary_resolution,
             self.eigenvalue_layer_name,
             self.eigenvalue_layer_num,
         ) = get_eigenvalue_config(param_dict)
 
+        self.hybrid_engine = get_hybrid_engine_config(param_dict)
+
         self.sparse_attention = get_sparse_attention(param_dict)
         self.pipeline = get_pipeline_config(param_dict)
 
         self.pld_enabled = get_pld_enabled(param_dict)
         self.pld_params = get_pld_params(param_dict)
 
         self.curriculum_enabled_legacy = get_curriculum_enabled_legacy(param_dict)
         self.curriculum_params_legacy = get_curriculum_params_legacy(param_dict)
 
         self.data_efficiency_enabled = get_data_efficiency_enabled(param_dict)
         self.data_efficiency_config = get_data_efficiency_config(param_dict)
 
         checkpoint_params = get_checkpoint_params(param_dict)
         validation_mode = get_checkpoint_tag_validation_mode(checkpoint_params)
-        self.checkpoint_tag_validation_enabled = (validation_mode !=
-                                                  ValidationMode.IGNORE)
+        self.checkpoint_tag_validation_enabled = (validation_mode != ValidationMode.IGNORE)
         self.checkpoint_tag_validation_fail = validation_mode == ValidationMode.FAIL
-        self.load_universal_checkpoint = checkpoint_params.get(
-            LOAD_UNIVERSAL_CHECKPOINT,
-            LOAD_UNIVERSAL_CHECKPOINT_DEFAULT)
-
-        self.use_node_local_storage = checkpoint_params.get(
-            USE_NODE_LOCAL_STORAGE_CHECKPOINT,
-            USE_NODE_LOCAL_STORAGE_CHECKPOINT_DEFAULT)
+        self.load_universal_checkpoint = checkpoint_params.get(LOAD_UNIVERSAL_CHECKPOINT,
+                                                               LOAD_UNIVERSAL_CHECKPOINT_DEFAULT)
+
+        self.use_node_local_storage = checkpoint_params.get(USE_NODE_LOCAL_STORAGE_CHECKPOINT,
+                                                            USE_NODE_LOCAL_STORAGE_CHECKPOINT_DEFAULT)
 
         data_types_params = get_data_types_params(param_dict)
-        self.grad_accum_dtype = data_types_params.get(GRAD_ACCUM_DTYPE,
-                                                      GRAD_ACCUM_DTYPE_DEFAULT)
+        self.grad_accum_dtype = data_types_params.get(GRAD_ACCUM_DTYPE, GRAD_ACCUM_DTYPE_DEFAULT)
 
         par_write_pipe = get_checkpoint_parallel_write_pipeline(checkpoint_params)
         self.checkpoint_parallel_write_pipeline = par_write_pipe
 
         self.aio_config = get_aio_config(param_dict)
 
         self.dataloader_drop_last = get_dataloader_drop_last(param_dict)
@@ -927,31 +870,24 @@
 
     def _batch_assertion(self):
 
         train_batch = self.train_batch_size
         micro_batch = self.train_micro_batch_size_per_gpu
         grad_acc = self.gradient_accumulation_steps
 
-        assert (
-            train_batch > 0
-        ), f"Train batch size: {train_batch} has to be greater than 0"
+        assert (train_batch > 0), f"Train batch size: {train_batch} has to be greater than 0"
 
-        assert (
-            micro_batch > 0
-        ), f"Micro batch size per gpu: {micro_batch} has to be greater than 0"
+        assert (micro_batch > 0), f"Micro batch size per gpu: {micro_batch} has to be greater than 0"
 
-        assert (
-            grad_acc > 0
-        ), f"Gradient accumulation steps: {grad_acc} has to be greater than 0"
+        assert (grad_acc > 0), f"Gradient accumulation steps: {grad_acc} has to be greater than 0"
 
         assert train_batch == micro_batch * grad_acc * self.world_size, (
             f"Check batch related parameters. train_batch_size is not equal "
             "to micro_batch_per_gpu * gradient_acc_step * world_size "
-            f"{train_batch} != {micro_batch} * {grad_acc} * {self.world_size}"
-        )
+            f"{train_batch} != {micro_batch} * {grad_acc} * {self.world_size}")
 
     def _set_batch_related_parameters(self):
 
         train_batch = self.train_batch_size
         micro_batch = self.train_micro_batch_size_per_gpu
         grad_acc = self.gradient_accumulation_steps
 
@@ -1006,64 +942,55 @@
     def print_user_config(self):
         logger.info("  json = {}".format(
             json.dumps(
                 self._param_dict,
                 sort_keys=True,
                 indent=4,
                 cls=ScientificNotationEncoder,
-                separators=(",",
-                            ":"),
+                separators=(",", ":"),
             )))
 
     def print(self, name):
         logger.info("{}:".format(name))
         for arg in sorted(vars(self)):
             if arg != "_param_dict":
                 dots = "." * (29 - len(arg))
                 logger.info("  {} {} {}".format(arg, dots, getattr(self, arg)))
 
         self.print_user_config()
 
     def _do_error_check(self):
-        assert (
-            self.train_micro_batch_size_per_gpu
-        ), "DeepSpeedConfig: {} is not defined".format(TRAIN_MICRO_BATCH_SIZE_PER_GPU)
+        assert (self.train_micro_batch_size_per_gpu
+                ), "DeepSpeedConfig: {} is not defined".format(TRAIN_MICRO_BATCH_SIZE_PER_GPU)
 
         assert (
-            self.gradient_accumulation_steps
-        ), "DeepSpeedConfig: {} is not defined".format(GRADIENT_ACCUMULATION_STEPS)
+            self.gradient_accumulation_steps), "DeepSpeedConfig: {} is not defined".format(GRADIENT_ACCUMULATION_STEPS)
 
         if self.zero_enabled:
-            assert (
-                self.zero_optimization_stage <= ZeroStageEnum.max_stage
-            ), "DeepSpeedConfig: Maximum supported ZeRO stage is {}".format(
-                ZeroStageEnum.max_stage
-            )
+            assert (self.zero_optimization_stage <=
+                    ZeroStageEnum.max_stage), "DeepSpeedConfig: Maximum supported ZeRO stage is {}".format(
+                        ZeroStageEnum.max_stage)
 
         if self.fp16_master_weights_and_gradients:
             assert self.zero_enabled and self.zero_optimization_stage == ZeroStageEnum.gradients, "Fp16_master_weights_and_grads is only supported with ZeRO Stage 2 for now."
 
     def _do_warning_check(self):
         fp16_enabled = self.fp16_enabled
 
         vocabulary_size = self._param_dict.get(VOCABULARY_SIZE, VOCABULARY_SIZE_DEFAULT)
         if vocabulary_size and vocabulary_size % TENSOR_CORE_ALIGN_SIZE != 0:
             logger.warning(
-                "DeepSpeedConfig: vocabulary size {} is not aligned to {}, may import tensor core utilization."
-                .format(vocabulary_size,
-                        TENSOR_CORE_ALIGN_SIZE))
+                "DeepSpeedConfig: vocabulary size {} is not aligned to {}, may import tensor core utilization.".format(
+                    vocabulary_size, TENSOR_CORE_ALIGN_SIZE))
 
-        if (self.optimizer_params is not None
-                and MAX_GRAD_NORM in self.optimizer_params.keys()
+        if (self.optimizer_params is not None and MAX_GRAD_NORM in self.optimizer_params.keys()
                 and self.optimizer_params[MAX_GRAD_NORM] > 0):
             if fp16_enabled:
                 if self.global_rank == 0:
-                    logger.warning(
-                        "DeepSpeedConfig: In FP16 mode, DeepSpeed will pass {}:{} to FP16 wrapper"
-                        .format(MAX_GRAD_NORM,
-                                self.optimizer_params[MAX_GRAD_NORM]))
+                    logger.warning("DeepSpeedConfig: In FP16 mode, DeepSpeed will pass {}:{} to FP16 wrapper".format(
+                        MAX_GRAD_NORM, self.optimizer_params[MAX_GRAD_NORM]))
             else:
                 if self.global_rank == 0:
                     logger.warning(
                         "DeepSpeedConfig: In FP32 mode, DeepSpeed does not permit MAX_GRAD_NORM ({}) > 0, setting to zero"
                         .format(self.optimizer_params[MAX_GRAD_NORM]))
                 self.optimizer_params[MAX_GRAD_NORM] = 0.0
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/config_utils.py` & `deepspeed-0.9.0/deepspeed/runtime/config_utils.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,12 +1,11 @@
-'''Copyright The Microsoft DeepSpeed Team'''
-"""
-Copyright (c) Microsoft Corporation
-Licensed under the MIT license.
-"""
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 """
 Collection of DeepSpeed configuration utilities
 """
 import json
 import collections
 import collections.abc
 from functools import reduce
@@ -46,68 +45,58 @@
         class MyExampleConfig(DeepSpeedConfigModel):
             my_new_field: int = 0
             my_old_field: str = Field('0',
                                       deprecated=True,
                                       new_param='my_new_field',
                                       new_param_fn=(lambda x: int(x)))
     """
+
     def __init__(self, strict=False, **data):
-        if (
-                not strict
-        ):  # This is temporary until we refactor all DS configs, allows HF to load models
-            data = {
-                k: v
-                for k,
-                v in data.items() if (v != "auto" or k == "replace_method")
-            }
+        if (not strict):  # This is temporary until we refactor all DS configs, allows HF to load models
+            data = {k: v for k, v in data.items() if (v != "auto" or k == "replace_method")}
         super().__init__(**data)
         self._deprecated_fields_check(self)
 
     def _process_deprecated_field(self, pydantic_config, field):
         # Get information about the deprecated field
         fields_set = pydantic_config.__fields_set__
         dep_param = field.name
         kwargs = field.field_info.extra
         new_param_fn = kwargs.get("new_param_fn", lambda x: x)
         param_value = new_param_fn(getattr(pydantic_config, dep_param))
         new_param = kwargs.get("new_param", "")
         dep_msg = kwargs.get("deprecated_msg", "")
         if dep_param in fields_set:
             logger.warning(f"Config parameter {dep_param} is deprecated" +
-                           (f" use {new_param} instead" if new_param else "") +
-                           (f". {dep_msg}" if dep_msg else ""))
+                           (f" use {new_param} instead" if new_param else "") + (f". {dep_msg}" if dep_msg else ""))
             # Check if there is a new param and if it should be set with a value
             if new_param and kwargs.get("set_new_param", True):
                 # Remove the deprecate field if there is a replacing field
                 try:
                     delattr(pydantic_config, dep_param)
                 except Exception as e:
                     logger.error(f"Tried removing deprecated '{dep_param}' from config")
                     raise e
 
                 # Set new param value
                 new_param_nested = new_param.split(".")
                 if len(new_param_nested) > 1:
                     # If the new param exists in a subconfig, we need to get
                     # the fields set for that subconfig
-                    pydantic_config = reduce(getattr,
-                                             new_param_nested[:-1],
-                                             pydantic_config)
+                    pydantic_config = reduce(getattr, new_param_nested[:-1], pydantic_config)
                     fields_set = pydantic_config.__fields_set__
                 new_param_name = new_param_nested[-1]
                 assert (
                     new_param_name not in fields_set
                 ), f"Cannot provide deprecated parameter '{dep_param}' and replacing parameter '{new_param}' together"
                 # A custom function for converting the old param value to new param value can be provided
                 try:
                     setattr(pydantic_config, new_param_name, param_value)
                 except Exception as e:
-                    logger.error(
-                        f"Tried setting value for '{new_param}' with value from deprecated '{dep_param}'"
-                    )
+                    logger.error(f"Tried setting value for '{new_param}' with value from deprecated '{dep_param}'")
                     raise e
 
     def _deprecated_fields_check(self, pydantic_config):
         fields = pydantic_config.__fields__
         for field in fields.values():
             if field.field_info.extra.get("deprecated", False):
                 self._process_deprecated_field(pydantic_config, field)
@@ -117,20 +106,28 @@
         validate_assignment = True
         use_enum_values = True
         allow_population_by_field_name = True
         extra = "forbid"
         arbitrary_types_allowed = True
 
 
+def get_config_default(config, field_name):
+    assert field_name in config.__fields__, f"'{field_name}' is not a field in {config}"
+    assert not config.__fields__.get(
+        field_name).required, f"'{field_name}' is a required field and does not have a default value"
+    return config.__fields__.get(field_name).default
+
+
 class pp_int(int):
     """
     A wrapper for integers that will return a custom string or comma-formatted
     string of the integer. For example, print(pp_int(1e5)) will return
     "10,000". This is useful mainly for auto-generated documentation purposes.
     """
+
     def __new__(cls, val, custom_print_str=None):
         inst = super().__new__(cls, val)
         inst.custom_print_str = custom_print_str
         return inst
 
     def __repr__(self):
         if self.custom_print_str:
@@ -144,41 +141,40 @@
     This class overrides ``json.dumps`` default formatter.
 
     This version keeps everything as normal except formats numbers bigger than 1e3 using scientific notation.
 
     Just pass ``cls=ScientificNotationEncoder`` to ``json.dumps`` to activate it
 
     """
+
     def iterencode(self, o, _one_shot=False, level=0):
         indent = self.indent if self.indent is not None else 4
         prefix_close = " " * level * indent
         level += 1
         prefix = " " * level * indent
         if isinstance(o, bool):
             return "true" if o else "false"
         elif isinstance(o, float) or isinstance(o, int):
             if o > 1e3:
                 return f"{o:e}"
             else:
                 return f"{o}"
         elif isinstance(o, collections.abc.Mapping):
-            x = [
-                f'\n{prefix}"{k}": {self.iterencode(v, level=level)}' for k,
-                v in o.items()
-            ]
+            x = [f'\n{prefix}"{k}": {self.iterencode(v, level=level)}' for k, v in o.items()]
             return "{" + ", ".join(x) + f"\n{prefix_close}" + "}"
         elif isinstance(o, collections.abc.Sequence) and not isinstance(o, str):
             return f"[{ f', '.join(map(self.iterencode, o)) }]"
         return "\n, ".join(super().iterencode(o, _one_shot))
 
 
 class DeepSpeedConfigObject(object):
     """
     For json serialization
     """
+
     def repr(self):
         return self.__dict__
 
     def __repr__(self):
         return json.dumps(
             self.__dict__,
             sort_keys=True,
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/constants.py` & `deepspeed-0.9.0/deepspeed/runtime/constants.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,12 +1,11 @@
-'''Copyright The Microsoft DeepSpeed Team'''
-"""
-Copyright (c) Microsoft Corporation
-Licensed under the MIT license.
-"""
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 #############################################
 # Routes
 #############################################
 ROUTE_TRAIN = "train"
 ROUTE_EVAL = "eval"
 ROUTE_PREDICT = "predict"
@@ -366,19 +365,15 @@
 #   parallel_write: {
 #     pipeline_stage: [True|False]
 #   }
 # }
 CHECKPOINT = "checkpoint"
 CHECKPOINT_TAG_VALIDATION = "tag_validation"
 CHECKPOINT_TAG_VALIDATION_DEFAULT = ValidationMode.WARN
-CHECKPOINT_TAG_VALIDATION_MODES = [
-    ValidationMode.WARN,
-    ValidationMode.IGNORE,
-    ValidationMode.FAIL
-]
+CHECKPOINT_TAG_VALIDATION_MODES = [ValidationMode.WARN, ValidationMode.IGNORE, ValidationMode.FAIL]
 
 LOAD_UNIVERSAL_CHECKPOINT = "load_universal"
 LOAD_UNIVERSAL_CHECKPOINT_DEFAULT = False
 
 USE_NODE_LOCAL_STORAGE_CHECKPOINT = "use_node_local_storage"
 USE_NODE_LOCAL_STORAGE_CHECKPOINT_DEFAULT = False
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/data_pipeline/config.py` & `deepspeed-0.9.0/deepspeed/runtime/data_pipeline/config.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,10 +1,12 @@
-'''
-Copyright 2022 The Microsoft DeepSpeed Team
-'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+
 from .constants import *
 import copy
 from ..config_utils import get_scalar_param
 
 
 # TODO: Reducing config verbosity by returning None or {} when disabled.
 # One challenge is that we still need to somehow include the default values,
@@ -20,26 +22,22 @@
     output[DATA_ROUTING] = get_data_routing(sub_param_dict)
 
     return output
 
 
 def get_data_efficiency_enabled(param_dict):
     if DATA_EFFICIENCY in param_dict.keys():
-        return get_scalar_param(param_dict[DATA_EFFICIENCY],
-                                DATA_EFFICIENCY_ENABLED,
-                                DATA_EFFICIENCY_ENABLED_DEFAULT)
+        return get_scalar_param(param_dict[DATA_EFFICIENCY], DATA_EFFICIENCY_ENABLED, DATA_EFFICIENCY_ENABLED_DEFAULT)
     else:
         return False
 
 
 def get_data_efficiency_seed(param_dict):
     if DATA_EFFICIENCY in param_dict.keys():
-        return get_scalar_param(param_dict[DATA_EFFICIENCY],
-                                DATA_EFFICIENCY_SEED,
-                                DATA_EFFICIENCY_SEED_DEFAULT)
+        return get_scalar_param(param_dict[DATA_EFFICIENCY], DATA_EFFICIENCY_SEED, DATA_EFFICIENCY_SEED_DEFAULT)
     else:
         return DATA_EFFICIENCY_SEED_DEFAULT
 
 
 def get_data_sampling(param_dict):
     output = {}
     output[DATA_SAMPLING_ENABLED] = get_data_sampling_enabled(param_dict)
@@ -51,56 +49,51 @@
     output[CURRICULUM_LEARNING] = get_curriculum_learning(sub_param_dict)
 
     return output
 
 
 def get_data_sampling_enabled(param_dict):
     if DATA_SAMPLING in param_dict.keys():
-        return get_scalar_param(param_dict[DATA_SAMPLING],
-                                DATA_SAMPLING_ENABLED,
-                                DATA_SAMPLING_ENABLED_DEFAULT)
+        return get_scalar_param(param_dict[DATA_SAMPLING], DATA_SAMPLING_ENABLED, DATA_SAMPLING_ENABLED_DEFAULT)
     else:
         return False
 
 
 def get_data_sampling_num_epochs(param_dict):
     if DATA_SAMPLING in param_dict.keys():
-        return get_scalar_param(param_dict[DATA_SAMPLING],
-                                DATA_SAMPLING_NUM_EPOCHS,
-                                DATA_SAMPLING_NUM_EPOCHS_DEFAULT)
+        return get_scalar_param(param_dict[DATA_SAMPLING], DATA_SAMPLING_NUM_EPOCHS, DATA_SAMPLING_NUM_EPOCHS_DEFAULT)
     else:
         return DATA_SAMPLING_NUM_EPOCHS_DEFAULT
 
 
 def get_data_sampling_num_workers(param_dict):
     if DATA_SAMPLING in param_dict.keys():
-        return get_scalar_param(param_dict[DATA_SAMPLING],
-                                DATA_SAMPLING_NUM_WORKERS,
+        return get_scalar_param(param_dict[DATA_SAMPLING], DATA_SAMPLING_NUM_WORKERS,
                                 DATA_SAMPLING_NUM_WORKERS_DEFAULT)
     else:
         return DATA_SAMPLING_NUM_WORKERS_DEFAULT
 
 
 def get_curriculum_learning(param_dict):
     output = {}
     output[CURRICULUM_LEARNING_ENABLED] = get_curriculum_learning_enabled(param_dict)
     if CURRICULUM_LEARNING not in param_dict.keys():
         param_dict[CURRICULUM_LEARNING] = {}
     sub_param_dict = param_dict[CURRICULUM_LEARNING]
     if output[CURRICULUM_LEARNING_ENABLED]:
-        assert CURRICULUM_LEARNING_METRICS in sub_param_dict.keys(), f"Curriculum learning is enabled, {CURRICULUM_LEARNING_METRICS} must be specified"
+        assert CURRICULUM_LEARNING_METRICS in sub_param_dict.keys(
+        ), f"Curriculum learning is enabled, {CURRICULUM_LEARNING_METRICS} must be specified"
         for key, val in get_curriculum_learning_params(param_dict).items():
             output[key] = val
     return output
 
 
 def get_curriculum_learning_enabled(param_dict):
     if CURRICULUM_LEARNING in param_dict.keys():
-        return get_scalar_param(param_dict[CURRICULUM_LEARNING],
-                                CURRICULUM_LEARNING_ENABLED,
+        return get_scalar_param(param_dict[CURRICULUM_LEARNING], CURRICULUM_LEARNING_ENABLED,
                                 CURRICULUM_LEARNING_ENABLED_DEFAULT)
     else:
         return False
 
 
 def get_curriculum_learning_params(param_dict):
     if CURRICULUM_LEARNING in param_dict.keys():
@@ -109,16 +102,15 @@
         return curriculum_learning_params
     else:
         return {}
 
 
 def get_curriculum_enabled_legacy(param_dict):
     if CURRICULUM_LEARNING_LEGACY in param_dict.keys():
-        return get_scalar_param(param_dict[CURRICULUM_LEARNING_LEGACY],
-                                CURRICULUM_ENABLED_LEGACY,
+        return get_scalar_param(param_dict[CURRICULUM_LEARNING_LEGACY], CURRICULUM_ENABLED_LEGACY,
                                 CURRICULUM_ENABLED_DEFAULT_LEGACY)
     else:
         return False
 
 
 def get_curriculum_params_legacy(param_dict):
     if CURRICULUM_LEARNING_LEGACY in param_dict.keys():
@@ -138,17 +130,15 @@
     output[RANDOM_LTD] = get_random_ltd(sub_param_dict)
 
     return output
 
 
 def get_data_routing_enabled(param_dict):
     if DATA_ROUTING in param_dict.keys():
-        return get_scalar_param(param_dict[DATA_ROUTING],
-                                DATA_ROUTING_ENABLED,
-                                DATA_ROUTING_ENABLED_DEFAULT)
+        return get_scalar_param(param_dict[DATA_ROUTING], DATA_ROUTING_ENABLED, DATA_ROUTING_ENABLED_DEFAULT)
     else:
         return False
 
 
 def get_random_ltd(param_dict):
     output = {}
     output[RANDOM_LTD_ENABLED] = RANDOM_LTD_ENABLED_DEFAULT
@@ -160,17 +150,15 @@
         for key, val in get_random_ltd_params(param_dict).items():
             output[key] = val
     return output
 
 
 def get_random_ltd_enabled(param_dict):
     if RANDOM_LTD in param_dict.keys():
-        return get_scalar_param(param_dict[RANDOM_LTD],
-                                RANDOM_LTD_ENABLED,
-                                RANDOM_LTD_ENABLED_DEFAULT)
+        return get_scalar_param(param_dict[RANDOM_LTD], RANDOM_LTD_ENABLED, RANDOM_LTD_ENABLED_DEFAULT)
     else:
         return False
 
 
 def get_random_ltd_params(param_dict):
     if RANDOM_LTD in param_dict.keys():
         random_ltd_params = copy.copy(param_dict[RANDOM_LTD])
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/data_pipeline/constants.py` & `deepspeed-0.9.0/deepspeed/runtime/data_pipeline/constants.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,14 +1,15 @@
-'''
-Copyright 2022 The Microsoft DeepSpeed Team
-'''
-#########################################
-# Data efficiency library
-# See sample config at https://www.deepspeed.ai/docs/config-json/#data-efficiency
-#########################################
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+"""
+Data efficiency library
+ See sample config at https://www.deepspeed.ai/docs/config-json/data-efficiency
+"""
 DATA_EFFICIENCY = "data_efficiency"
 DATA_EFFICIENCY_ENABLED = "enabled"
 DATA_EFFICIENCY_ENABLED_DEFAULT = False
 DATA_EFFICIENCY_SEED = "seed"
 DATA_EFFICIENCY_SEED_DEFAULT = 1234
 
 #########################################
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/data_pipeline/curriculum_scheduler.py` & `deepspeed-0.9.0/deepspeed/runtime/data_pipeline/curriculum_scheduler.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,36 +1,34 @@
-'''
-Copyright 2021 The Microsoft DeepSpeed Team
-'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+
 import math
 from deepspeed.utils import logger
 from .constants import *
 
 
 class CurriculumScheduler(object):
+
     def __init__(self, config):
         super().__init__()
         self.state = {}
         assert CURRICULUM_LEARNING_MIN_DIFFICULTY in config, \
             f"Curriculum learning requires the config '{CURRICULUM_LEARNING_MIN_DIFFICULTY}'"
         assert CURRICULUM_LEARNING_MAX_DIFFICULTY in config, \
             f"Curriculum learning requires the config '{CURRICULUM_LEARNING_MAX_DIFFICULTY}'"
         assert CURRICULUM_LEARNING_SCHEDULE_TYPE in config, \
             f"Curriculum learning requires the config '{CURRICULUM_LEARNING_SCHEDULE_TYPE}'"
-        self.state[CURRICULUM_LEARNING_MIN_DIFFICULTY] = config[
-            CURRICULUM_LEARNING_MIN_DIFFICULTY]
-        self.state[CURRICULUM_LEARNING_MAX_DIFFICULTY] = config[
-            CURRICULUM_LEARNING_MAX_DIFFICULTY]
-        self.state[CURRICULUM_LEARNING_CURRENT_DIFFICULTY] = config[
-            CURRICULUM_LEARNING_MIN_DIFFICULTY]
-        self.state[CURRICULUM_LEARNING_SCHEDULE_TYPE] = config[
-            CURRICULUM_LEARNING_SCHEDULE_TYPE]
+        self.state[CURRICULUM_LEARNING_MIN_DIFFICULTY] = config[CURRICULUM_LEARNING_MIN_DIFFICULTY]
+        self.state[CURRICULUM_LEARNING_MAX_DIFFICULTY] = config[CURRICULUM_LEARNING_MAX_DIFFICULTY]
+        self.state[CURRICULUM_LEARNING_CURRENT_DIFFICULTY] = config[CURRICULUM_LEARNING_MIN_DIFFICULTY]
+        self.state[CURRICULUM_LEARNING_SCHEDULE_TYPE] = config[CURRICULUM_LEARNING_SCHEDULE_TYPE]
         self.first_step = True
-        if config[
-                CURRICULUM_LEARNING_SCHEDULE_TYPE] == CURRICULUM_LEARNING_SCHEDULE_FIXED_DISCRETE:
+        if config[CURRICULUM_LEARNING_SCHEDULE_TYPE] == CURRICULUM_LEARNING_SCHEDULE_FIXED_DISCRETE:
             """
             The schedule_config is a list of difficulty and a list of max
             step belonging to each difficulty. Example json config:
             "schedule_config": {
               "difficulty": [1,2,3],
               "max_step": [5,10]
             }
@@ -39,26 +37,20 @@
             The self.state[CURRICULUM_LEARNING_SCHEDULE_CONFIG] is a dictionary of
             difficulty : [max step for this difficulty, next difficulty].
             """
             assert CURRICULUM_LEARNING_SCHEDULE_DIFFICULTY in config[CURRICULUM_LEARNING_SCHEDULE_CONFIG], \
                 f"Curriculum learning with fixed_discrete schedule requires the schedule_config '{CURRICULUM_LEARNING_SCHEDULE_DIFFICULTY}'"
             assert CURRICULUM_LEARNING_SCHEDULE_MAX_STEP in config[CURRICULUM_LEARNING_SCHEDULE_CONFIG], \
                 f"Curriculum learning with fixed_discrete schedule requires the schedule_config '{CURRICULUM_LEARNING_SCHEDULE_MAX_STEP}'"
-            assert len(config[CURRICULUM_LEARNING_SCHEDULE_CONFIG]
-                       [CURRICULUM_LEARNING_SCHEDULE_MAX_STEP]) > 0
-            assert len(config[CURRICULUM_LEARNING_SCHEDULE_CONFIG]
-                       [CURRICULUM_LEARNING_SCHEDULE_DIFFICULTY]) > 0
-            assert len(config[CURRICULUM_LEARNING_SCHEDULE_CONFIG]
-                       [CURRICULUM_LEARNING_SCHEDULE_DIFFICULTY]) == len(
-                           config[CURRICULUM_LEARNING_SCHEDULE_CONFIG]
-                           [CURRICULUM_LEARNING_SCHEDULE_MAX_STEP]) + 1
-            self.state[CURRICULUM_LEARNING_SCHEDULE_CONFIG] = config[
-                CURRICULUM_LEARNING_SCHEDULE_CONFIG]
-        elif config[
-                CURRICULUM_LEARNING_SCHEDULE_TYPE] == CURRICULUM_LEARNING_SCHEDULE_FIXED_ROOT:
+            assert len(config[CURRICULUM_LEARNING_SCHEDULE_CONFIG][CURRICULUM_LEARNING_SCHEDULE_MAX_STEP]) > 0
+            assert len(config[CURRICULUM_LEARNING_SCHEDULE_CONFIG][CURRICULUM_LEARNING_SCHEDULE_DIFFICULTY]) > 0
+            assert len(config[CURRICULUM_LEARNING_SCHEDULE_CONFIG][CURRICULUM_LEARNING_SCHEDULE_DIFFICULTY]) == len(
+                config[CURRICULUM_LEARNING_SCHEDULE_CONFIG][CURRICULUM_LEARNING_SCHEDULE_MAX_STEP]) + 1
+            self.state[CURRICULUM_LEARNING_SCHEDULE_CONFIG] = config[CURRICULUM_LEARNING_SCHEDULE_CONFIG]
+        elif config[CURRICULUM_LEARNING_SCHEDULE_TYPE] == CURRICULUM_LEARNING_SCHEDULE_FIXED_ROOT:
             """
             The schedule_config includes:
             total_curriculum_step: how many steps the curriculum learning takes to go
             from min difficulty to max difficulty.
             difficulty_step: the difficulty level determined every time must
             be a multiple of this difficulty_step. This is used to determine
             the step of difficulty increase, and to ensure the use of NVIDIA
@@ -75,44 +67,38 @@
             """
             assert CURRICULUM_LEARNING_SCHEDULE_TOTAL_STEP in config[CURRICULUM_LEARNING_SCHEDULE_CONFIG], \
                 f"Curriculum learning with fixed_root schedule requires the schedule_config '{CURRICULUM_LEARNING_SCHEDULE_TOTAL_STEP}'"
             assert CURRICULUM_LEARNING_SCHEDULE_DIFFICULTY_STEP in config[CURRICULUM_LEARNING_SCHEDULE_CONFIG], \
                 f"Curriculum learning with fixed_root schedule requires the schedule_config '{CURRICULUM_LEARNING_SCHEDULE_DIFFICULTY_STEP}'"
             assert CURRICULUM_LEARNING_SCHEDULE_ROOT_DEGREE in config[CURRICULUM_LEARNING_SCHEDULE_CONFIG], \
                 f"Curriculum learning with fixed_root schedule requires the schedule_config '{CURRICULUM_LEARNING_SCHEDULE_ROOT_DEGREE}'"
-            if config[CURRICULUM_LEARNING_SCHEDULE_CONFIG][
-                    CURRICULUM_LEARNING_SCHEDULE_DIFFICULTY_STEP] % 8 != 0:
+            if config[CURRICULUM_LEARNING_SCHEDULE_CONFIG][CURRICULUM_LEARNING_SCHEDULE_DIFFICULTY_STEP] % 8 != 0:
                 logger.warning(
                     f'When using seqlen metric, the difficulty_step for curriculum learning has to be multiple of 8 (for FP16 data) or 16 (for INT8 data) to enable NVIDIA Tensor Core acceleration. Disregard this warning if this is unrelated to your metric/hardware.'
                 )
-            self.state[CURRICULUM_LEARNING_SCHEDULE_CONFIG] = config[
-                CURRICULUM_LEARNING_SCHEDULE_CONFIG]
-        elif config[
-                CURRICULUM_LEARNING_SCHEDULE_TYPE] == CURRICULUM_LEARNING_SCHEDULE_FIXED_LINEAR:
+            self.state[CURRICULUM_LEARNING_SCHEDULE_CONFIG] = config[CURRICULUM_LEARNING_SCHEDULE_CONFIG]
+        elif config[CURRICULUM_LEARNING_SCHEDULE_TYPE] == CURRICULUM_LEARNING_SCHEDULE_FIXED_LINEAR:
             """
             The schedule_config is the same as CURRICULUM_LEARNING_SCHEDULE_FIXED_ROOT but without the
             root_degree.
             "schedule_config": {
               "total_curriculum_step": 30000,
               "difficulty_step": 8
             }
             """
             assert CURRICULUM_LEARNING_SCHEDULE_TOTAL_STEP in config[CURRICULUM_LEARNING_SCHEDULE_CONFIG], \
                 f"Curriculum learning with fixed_linear schedule requires the schedule_config '{CURRICULUM_LEARNING_SCHEDULE_TOTAL_STEP}'"
             assert CURRICULUM_LEARNING_SCHEDULE_DIFFICULTY_STEP in config[CURRICULUM_LEARNING_SCHEDULE_CONFIG], \
                 f"Curriculum learning with fixed_linear schedule requires the schedule_config '{CURRICULUM_LEARNING_SCHEDULE_DIFFICULTY_STEP}'"
-            if config[CURRICULUM_LEARNING_SCHEDULE_CONFIG][
-                    CURRICULUM_LEARNING_SCHEDULE_DIFFICULTY_STEP] % 8 != 0:
+            if config[CURRICULUM_LEARNING_SCHEDULE_CONFIG][CURRICULUM_LEARNING_SCHEDULE_DIFFICULTY_STEP] % 8 != 0:
                 logger.warning(
                     f'When using seqlen metric, the difficulty_step for curriculum learning has to be multiple of 8 (for FP16 data) or 16 (for INT8 data) to enable NVIDIA Tensor Core acceleration. Disregard this warning if this is unrelated to your metric/hardware.'
                 )
-            self.state[CURRICULUM_LEARNING_SCHEDULE_CONFIG] = config[
-                CURRICULUM_LEARNING_SCHEDULE_CONFIG]
-        elif config[
-                CURRICULUM_LEARNING_SCHEDULE_TYPE] == CURRICULUM_LEARNING_SCHEDULE_CUSTOM:
+            self.state[CURRICULUM_LEARNING_SCHEDULE_CONFIG] = config[CURRICULUM_LEARNING_SCHEDULE_CONFIG]
+        elif config[CURRICULUM_LEARNING_SCHEDULE_TYPE] == CURRICULUM_LEARNING_SCHEDULE_CUSTOM:
             """
             Fully customized schedule. User need to provide a custom schedule
             function by using the set_custom_curriculum_learning_schedule API
             in deepspeed/runtime/engine.py
             """
             self.custom_get_difficulty = None
         else:
@@ -141,42 +127,32 @@
             if global_steps <= s_state[CURRICULUM_LEARNING_SCHEDULE_MAX_STEP][i]:
                 return s_state[CURRICULUM_LEARNING_SCHEDULE_DIFFICULTY][i]
 
     def __fixed_root_get_difficulty(self, global_steps, root_degree=None):
         s_state = self.state[CURRICULUM_LEARNING_SCHEDULE_CONFIG]
         if root_degree is None:
             root_degree = s_state[CURRICULUM_LEARNING_SCHEDULE_ROOT_DEGREE]
-        next_difficulty = (float(global_steps) /
-                           s_state[CURRICULUM_LEARNING_SCHEDULE_TOTAL_STEP])**(
-                               1.0 / root_degree)
-        next_difficulty = math.floor(next_difficulty *
-                                     (self.state[CURRICULUM_LEARNING_MAX_DIFFICULTY] -
-                                      self.state[CURRICULUM_LEARNING_MIN_DIFFICULTY]) +
-                                     self.state[CURRICULUM_LEARNING_MIN_DIFFICULTY])
-        next_difficulty -= (next_difficulty %
-                            s_state[CURRICULUM_LEARNING_SCHEDULE_DIFFICULTY_STEP])
-        next_difficulty = min(next_difficulty,
-                              self.state[CURRICULUM_LEARNING_MAX_DIFFICULTY])
+        next_difficulty = (float(global_steps) / s_state[CURRICULUM_LEARNING_SCHEDULE_TOTAL_STEP])**(1.0 / root_degree)
+        next_difficulty = math.floor(
+            next_difficulty *
+            (self.state[CURRICULUM_LEARNING_MAX_DIFFICULTY] - self.state[CURRICULUM_LEARNING_MIN_DIFFICULTY]) +
+            self.state[CURRICULUM_LEARNING_MIN_DIFFICULTY])
+        next_difficulty -= (next_difficulty % s_state[CURRICULUM_LEARNING_SCHEDULE_DIFFICULTY_STEP])
+        next_difficulty = min(next_difficulty, self.state[CURRICULUM_LEARNING_MAX_DIFFICULTY])
         return next_difficulty
 
     def get_difficulty(self, global_steps):
-        if self.state[
-                CURRICULUM_LEARNING_SCHEDULE_TYPE] == CURRICULUM_LEARNING_SCHEDULE_FIXED_DISCRETE:
+        if self.state[CURRICULUM_LEARNING_SCHEDULE_TYPE] == CURRICULUM_LEARNING_SCHEDULE_FIXED_DISCRETE:
             return self.__fixed_discrete_get_difficulty(global_steps)
-        elif self.state[
-                CURRICULUM_LEARNING_SCHEDULE_TYPE] == CURRICULUM_LEARNING_SCHEDULE_FIXED_LINEAR:
+        elif self.state[CURRICULUM_LEARNING_SCHEDULE_TYPE] == CURRICULUM_LEARNING_SCHEDULE_FIXED_LINEAR:
             return self.__fixed_root_get_difficulty(global_steps, 1)
-        elif self.state[
-                CURRICULUM_LEARNING_SCHEDULE_TYPE] == CURRICULUM_LEARNING_SCHEDULE_FIXED_ROOT:
+        elif self.state[CURRICULUM_LEARNING_SCHEDULE_TYPE] == CURRICULUM_LEARNING_SCHEDULE_FIXED_ROOT:
             return self.__fixed_root_get_difficulty(global_steps)
-        elif self.state[
-                CURRICULUM_LEARNING_SCHEDULE_TYPE] == CURRICULUM_LEARNING_SCHEDULE_CUSTOM:
+        elif self.state[CURRICULUM_LEARNING_SCHEDULE_TYPE] == CURRICULUM_LEARNING_SCHEDULE_CUSTOM:
             return self.custom_get_difficulty(global_steps)
         else:
             raise RuntimeError('Unsupported curriculum schedule type')
 
     def update_difficulty(self, global_steps):
-        if self.state[CURRICULUM_LEARNING_CURRENT_DIFFICULTY] < self.state[
-                CURRICULUM_LEARNING_MAX_DIFFICULTY]:
-            self.state[CURRICULUM_LEARNING_CURRENT_DIFFICULTY] = self.get_difficulty(
-                global_steps)
+        if self.state[CURRICULUM_LEARNING_CURRENT_DIFFICULTY] < self.state[CURRICULUM_LEARNING_MAX_DIFFICULTY]:
+            self.state[CURRICULUM_LEARNING_CURRENT_DIFFICULTY] = self.get_difficulty(global_steps)
         return self.state[CURRICULUM_LEARNING_CURRENT_DIFFICULTY]
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/data_pipeline/data_routing/basic_layer.py` & `deepspeed-0.9.0/deepspeed/runtime/data_pipeline/data_routing/basic_layer.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,23 +1,25 @@
-'''
-Copyright 2022 The Microsoft DeepSpeed Team
-'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 from deepspeed.utils import logger
 from torch import Tensor
 from torch.nn import Module
 from ..constants import *
 from deepspeed.ops.random_ltd.dropping_utils import gpt_sample_tokens, bert_sample_tokens, GatherTokens, ScatterTokens
 
 
 #####based on the paper random-ltd: https://arxiv.org/abs/2211.11586
 class RandomLayerTokenDrop(Module):
     """
     A  layer wrapper for random LTD
     """
+
     def __init__(self, layer: Module):
         super(RandomLayerTokenDrop, self).__init__()
         self.random_ltd_layer = layer
         self.reserved_length = None  #config['max_value']
         self.random_ltd_scheduler = None
         self.max_length = None
         self.reserved_length = -1
@@ -48,17 +50,15 @@
             raise NotImplementedError
 
         if self.model_type == 'encoder':
             self.index_generator = bert_sample_tokens
         elif self.model_type == 'decoder':
             self.index_generator = gpt_sample_tokens
         else:
-            logger.warning(
-                "************For now, we only support encoder-only or decoder-only models************"
-            )
+            logger.warning("************For now, we only support encoder-only or decoder-only models************")
             raise NotImplementedError
 
     def get_bsh(self, hidden_stats):
         self.curr_seq, self.curr_micro_batch = hidden_stats.size()[1], hidden_stats.size()[0]
 
     def get_sbh(self, hidden_stats):
         self.curr_seq, self.curr_micro_batch = hidden_stats.size()[0], hidden_stats.size()[1]
@@ -74,44 +74,40 @@
                 mask = None
             if self.random_ltd_layer_id == 0:
                 sampled_indices, part_attention_mask = self.index_generator(self.reserved_length,\
                                                                                       self.curr_seq, \
                                                                                       self.curr_micro_batch, \
                                                                                       self.random_ltd_num_layer, \
                                                                                       hidden_states.device, mask)
-                self.random_ltd_scheduler.state[
-                    RANDOM_LTD_SAMPLE_INDEX] = sampled_indices
-                self.random_ltd_scheduler.state[
-                    RANDOM_LTD_ATTENTION_MASK] = part_attention_mask
+                self.random_ltd_scheduler.state[RANDOM_LTD_SAMPLE_INDEX] = sampled_indices
+                self.random_ltd_scheduler.state[RANDOM_LTD_ATTENTION_MASK] = part_attention_mask
             else:
-                sampled_indices = self.random_ltd_scheduler.state[
-                    RANDOM_LTD_SAMPLE_INDEX]
-                part_attention_mask = self.random_ltd_scheduler.state[
-                    RANDOM_LTD_ATTENTION_MASK]
-
+                sampled_indices = self.random_ltd_scheduler.state[RANDOM_LTD_SAMPLE_INDEX]
+                part_attention_mask = self.random_ltd_scheduler.state[RANDOM_LTD_ATTENTION_MASK]
 
-            hidden_states, part_hidden_states = GatherTokens.apply(hidden_states, sampled_indices[self.random_ltd_layer_id,:,:], self.batch_first)
+            hidden_states, part_hidden_states = GatherTokens.apply(hidden_states,
+                                                                   sampled_indices[self.random_ltd_layer_id, :, :],
+                                                                   self.batch_first)
             if self.mask_name is not None:
                 if self.model_type == 'encoder':
-                    kwargs[self.mask_name] = part_attention_mask[
-                        self.random_ltd_layer_id]
+                    kwargs[self.mask_name] = part_attention_mask[self.random_ltd_layer_id]
                 else:
                     kwargs[self.mask_name] = part_attention_mask
 
             outputs = self.random_ltd_layer(part_hidden_states, **kwargs)
 
             if isinstance(outputs, tuple):
-                hidden_states = ScatterTokens.apply(hidden_states, outputs[0], sampled_indices[self.random_ltd_layer_id,:,:], self.batch_first)
+                hidden_states = ScatterTokens.apply(hidden_states, outputs[0],
+                                                    sampled_indices[self.random_ltd_layer_id, :, :], self.batch_first)
                 my_list = list(outputs)
                 my_list[0] = hidden_states
                 return tuple(my_list)
             elif isinstance(outputs, Tensor):
-                hidden_states = ScatterTokens.apply(hidden_states, outputs, sampled_indices[self.random_ltd_layer_id,:,:], self.batch_first)
+                hidden_states = ScatterTokens.apply(hidden_states, outputs,
+                                                    sampled_indices[self.random_ltd_layer_id, :, :], self.batch_first)
                 return hidden_states
             else:
-                logger.warning(
-                    "************For now, we only support tuple and tensor output.  \
-                       You need to adjust the output according to the layer in your model************"
-                )
+                logger.warning("************For now, we only support tuple and tensor output.  \
+                       You need to adjust the output according to the layer in your model************")
                 raise NotImplementedError
         else:
             return self.random_ltd_layer(hidden_states, **kwargs)
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/data_pipeline/data_routing/helper.py` & `deepspeed-0.9.0/deepspeed/runtime/data_pipeline/data_routing/helper.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-'''
-Copyright 2022 The Microsoft DeepSpeed Team
-'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 from .basic_layer import RandomLayerTokenDrop
 from collections import OrderedDict
 from deepspeed.compression.helper import recursive_getattr, recursive_setattr
 
 
 def convert_to_random_ltd(model, convert_type):
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/data_pipeline/data_routing/scheduler.py` & `deepspeed-0.9.0/deepspeed/runtime/data_pipeline/data_routing/scheduler.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,46 +1,46 @@
-'''
-Copyright 2022 The Microsoft DeepSpeed Team
-'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import math
 
 from deepspeed.utils import logger
 # from deepspeed.runtime.lr_schedules import WarmupLR
 from ..constants import *
 
 #####based on the paper random-ltd: https://arxiv.org/abs/2211.11586
 
 
 class BaseScheduler(object):
+
     def __init__(self):
         self.state = {}
 
     def __fixed_root_get_value(self, global_steps, root_degree=None):
         s_state = self.state[RANDOM_LTD_SCHEDULE_CONFIG]
         if root_degree is None:
             root_degree = s_state['root_degree']
-        next_seq = (float(global_steps) /
-                    s_state[RANDOM_LTD_REQUIRE_STEP])**(1.0 / root_degree)
-        next_seq = math.floor(
-            next_seq *
-            (self.state[RANDOM_LTD_MAX_VALUE] - self.state[RANDOM_LTD_MIN_VALUE]) +
-            self.state[RANDOM_LTD_MIN_VALUE])
+        next_seq = (float(global_steps) / s_state[RANDOM_LTD_REQUIRE_STEP])**(1.0 / root_degree)
+        next_seq = math.floor(next_seq * (self.state[RANDOM_LTD_MAX_VALUE] - self.state[RANDOM_LTD_MIN_VALUE]) +
+                              self.state[RANDOM_LTD_MIN_VALUE])
         next_seq -= (next_seq % s_state[RANDOM_LTD_INCREASE_STEP])
         next_seq = min(next_seq, self.state[RANDOM_LTD_MAX_VALUE])
         return next_seq
 
     def get_value(self, global_steps):
         if self.state[RANDOM_LTD_SCHEDULER_TYPE] == 'fixed_linear':
             return self.__fixed_root_get_value(global_steps, 1)
         else:
             raise RuntimeError('Unsupported random LTD schedule type')
 
 
 class RandomLTDScheduler(BaseScheduler):
+
     def __init__(self, config):
         super().__init__()
         self.model_layer_num = config[RANDOM_LTD_TOTAL_LAYER_NUM]
         self.random_ltd_layer_num = config[RANDOM_LTD_LAYER_NUM]
         self.config_schedule = config[RANDOM_LTD_SCHEDULER]
         self.global_batch_size = config[RANDOM_LTD_GLOBAL_BATCH_SIZE]
         self.reset_to_init()
@@ -57,20 +57,17 @@
             self.update_seq(step)
         return self.state[RANDOM_LTD_CONSUMED_LAYER_TOKENS]
 
     def reset_to_init(self):
         if self.config_schedule is not None:
             self.state[RANDOM_LTD_MIN_VALUE] = self.config_schedule[RANDOM_LTD_MIN_VALUE]
             self.state[RANDOM_LTD_MAX_VALUE] = self.config_schedule[RANDOM_LTD_MAX_VALUE]
-            self.state[RANDOM_LTD_CURRENT_VALUE] = self.config_schedule[
-                RANDOM_LTD_MIN_VALUE]
-            self.state[RANDOM_LTD_SCHEDULE_CONFIG] = self.config_schedule[
-                RANDOM_LTD_SCHEDULE_CONFIG]
-            self.state[RANDOM_LTD_SCHEDULER_TYPE] = self.config_schedule[
-                RANDOM_LTD_SCHEDULER_TYPE]
+            self.state[RANDOM_LTD_CURRENT_VALUE] = self.config_schedule[RANDOM_LTD_MIN_VALUE]
+            self.state[RANDOM_LTD_SCHEDULE_CONFIG] = self.config_schedule[RANDOM_LTD_SCHEDULE_CONFIG]
+            self.state[RANDOM_LTD_SCHEDULER_TYPE] = self.config_schedule[RANDOM_LTD_SCHEDULER_TYPE]
         self.state[RANDOM_LTD_CONSUMED_LAYER_TOKENS] = 0
         self.state[RANDOM_LTD_CURR_STEP] = -1
 
     def get_current_seq(self):
         return self.state[RANDOM_LTD_CURRENT_VALUE]
 
     def set_current_seq(self, seq_length):
@@ -91,22 +88,20 @@
         if global_steps != self.state[RANDOM_LTD_CURR_STEP]:
             self.state[RANDOM_LTD_CONSUMED_LAYER_TOKENS] += self.global_batch_size*(self.state[RANDOM_LTD_CURRENT_VALUE] * self.random_ltd_layer_num \
                 + self.state[RANDOM_LTD_MAX_VALUE] * (self.model_layer_num - self.random_ltd_layer_num))
             self.state[RANDOM_LTD_CURR_STEP] = global_steps
 
     def state_dict(self):
         return {
-            RANDOM_LTD_CONSUMED_LAYER_TOKENS:
-            self.state[RANDOM_LTD_CONSUMED_LAYER_TOKENS],
+            RANDOM_LTD_CONSUMED_LAYER_TOKENS: self.state[RANDOM_LTD_CONSUMED_LAYER_TOKENS],
             RANDOM_LTD_CURR_STEP: self.state[RANDOM_LTD_CURR_STEP],
             RANDOM_LTD_CURRENT_VALUE: self.state[RANDOM_LTD_CURRENT_VALUE],
             RANDOM_LTD_MIN_VALUE: self.state[RANDOM_LTD_MIN_VALUE],
             RANDOM_LTD_MAX_VALUE: self.state[RANDOM_LTD_MAX_VALUE],
         }
 
     def load_state_dict(self, state_dict):
-        self.state[RANDOM_LTD_CONSUMED_LAYER_TOKENS] = state_dict[
-            RANDOM_LTD_CONSUMED_LAYER_TOKENS]
+        self.state[RANDOM_LTD_CONSUMED_LAYER_TOKENS] = state_dict[RANDOM_LTD_CONSUMED_LAYER_TOKENS]
         self.state[RANDOM_LTD_CURR_STEP] = state_dict[RANDOM_LTD_CURR_STEP]
         self.state[RANDOM_LTD_CURRENT_VALUE] = state_dict[RANDOM_LTD_CURRENT_VALUE]
         self.state[RANDOM_LTD_MIN_VALUE] = state_dict[RANDOM_LTD_MIN_VALUE]
         self.state[RANDOM_LTD_MAX_VALUE] = state_dict[RANDOM_LTD_MAX_VALUE]
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/data_pipeline/data_routing/utils.py` & `deepspeed-0.9.0/deepspeed/runtime/data_pipeline/data_routing/utils.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,21 +1,21 @@
-'''
-Copyright 2022 The Microsoft DeepSpeed Team
-'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import torch
 
 
 def bsh_decoder_gather(reserved_length, hidden_states, mask):
     # random-layer-token-drop
     rand_list = []
     part_hidden_states = []  #  batch, seq, hidden ## different from megatron
     for k in range(hidden_states.size(0)):
-        B_tmp = torch.randperm(hidden_states.size(1),
-                               device=hidden_states.device)[:reserved_length]
+        B_tmp = torch.randperm(hidden_states.size(1), device=hidden_states.device)[:reserved_length]
         B = B_tmp.sort()[0]
         rand_list.append(B)
         part_hidden_states.append(hidden_states[k:k + 1, B, :])
 
     part_hidden_states = torch.cat(part_hidden_states, dim=0)
     part_mask = mask[:, :, :reserved_length, :reserved_length]
     return part_hidden_states, rand_list, part_mask
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/data_pipeline/data_sampling/data_sampler.py` & `deepspeed-0.9.0/deepspeed/runtime/data_pipeline/data_sampling/data_sampler.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,25 +1,28 @@
-'''
-Copyright 2022 The Microsoft DeepSpeed Team
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+"""
+coding=utf-8
+ Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.
+
+ Licensed under the Apache License, Version 2.0 (the "License");
+ you may not use this file except in compliance with the License.
+ You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
 Part of this code was adopted from https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/data/data_samplers.py
-'''
-# coding=utf-8
-# Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
 
 import torch
 import os
 import numpy as np
 
 import deepspeed.comm as dist
 from deepspeed.utils import logger
@@ -27,87 +30,79 @@
 from ..constants import *
 from ..curriculum_scheduler import CurriculumScheduler
 from .indexed_dataset import MMapIndexedDataset
 from .utils import create_mmap_dataset_builder, close_mmap_dataset_builder, find_fit_int_dtype
 
 
 class DeepSpeedDataSampler(object):
+
     def __init__(self,
                  data_efficiency_config,
                  one_epoch_total_samples,
                  micro_batch_size,
                  data_parallel_rank,
                  data_parallel_size,
                  data_parallel_group,
                  gradient_accumulation_steps,
                  global_rank,
                  drop_last=True):
         # Keep a copy of input params for later use.
         self.data_efficiency_config = data_efficiency_config
         self.one_epoch_total_samples = one_epoch_total_samples
         self.index_dtype = find_fit_int_dtype(0, one_epoch_total_samples)
-        self.total_samples = one_epoch_total_samples * self.data_efficiency_config[
-            DATA_SAMPLING][DATA_SAMPLING_NUM_EPOCHS]
+        self.total_samples = one_epoch_total_samples * self.data_efficiency_config[DATA_SAMPLING][
+            DATA_SAMPLING_NUM_EPOCHS]
         self.micro_batch_size = micro_batch_size
         self.data_parallel_rank = data_parallel_rank
         self.data_parallel_group = data_parallel_group
         self.micro_batch_times_data_parallel_size = \
             self.micro_batch_size * data_parallel_size
         self.gradient_accumulation_steps = gradient_accumulation_steps
         self.global_batch_size = self.micro_batch_times_data_parallel_size * \
             self.gradient_accumulation_steps
         self.global_rank = global_rank
         self.drop_last = drop_last
-        self.np_rng = np.random.default_rng(
-            self.data_efficiency_config[DATA_EFFICIENCY_SEED])
+        self.np_rng = np.random.default_rng(self.data_efficiency_config[DATA_EFFICIENCY_SEED])
         self.state = {}
         self.batch = []
         self.consumed_samples = 0
-        if self.data_efficiency_config[DATA_SAMPLING][CURRICULUM_LEARNING][
-                CURRICULUM_LEARNING_ENABLED]:
+        if self.data_efficiency_config[DATA_SAMPLING][CURRICULUM_LEARNING][CURRICULUM_LEARNING_ENABLED]:
             self.curriculum_step = 0
             self.current_difficulties = {}
             self.data_cluster_paths = []
             self.data_cluster_current_position = []
             self.curriculum_schedulers = {}
             self.curriculum_index_to_sample = {}
             self.curriculum_index_to_metric = {}
             self.difficulty_type = {}
             self.clustering_type = {}
             self.data_1epoch_size = None
             if self.global_rank == 0:
                 self.data_clusters = []
                 self.data_cluster_sizes = []
-                cluster_path = self.data_efficiency_config[DATA_SAMPLING][
-                    CURRICULUM_LEARNING][CURRICULUM_LEARNING_CLUSTER_PATH]
+                cluster_path = self.data_efficiency_config[DATA_SAMPLING][CURRICULUM_LEARNING][
+                    CURRICULUM_LEARNING_CLUSTER_PATH]
                 if not os.path.exists(cluster_path):
                     os.makedirs(cluster_path)
-            for metric in self.data_efficiency_config[DATA_SAMPLING][
-                    CURRICULUM_LEARNING][CURRICULUM_LEARNING_METRICS]:
+            for metric in self.data_efficiency_config[DATA_SAMPLING][CURRICULUM_LEARNING][CURRICULUM_LEARNING_METRICS]:
                 self.curriculum_schedulers[metric] = CurriculumScheduler(
-                    data_efficiency_config[DATA_SAMPLING][CURRICULUM_LEARNING]
-                    [CURRICULUM_LEARNING_METRICS][metric])
-                self.difficulty_type[metric] = data_efficiency_config[DATA_SAMPLING][
-                    CURRICULUM_LEARNING][CURRICULUM_LEARNING_METRICS][metric][
-                        CURRICULUM_LEARNING_DIFFICULTY_TYPE]
-                self.clustering_type[metric] = data_efficiency_config[DATA_SAMPLING][
-                    CURRICULUM_LEARNING][CURRICULUM_LEARNING_METRICS][metric][
-                        CURRICULUM_LEARNING_CLUSTERING_TYPE]
+                    data_efficiency_config[DATA_SAMPLING][CURRICULUM_LEARNING][CURRICULUM_LEARNING_METRICS][metric])
+                self.difficulty_type[metric] = data_efficiency_config[DATA_SAMPLING][CURRICULUM_LEARNING][
+                    CURRICULUM_LEARNING_METRICS][metric][CURRICULUM_LEARNING_DIFFICULTY_TYPE]
+                self.clustering_type[metric] = data_efficiency_config[DATA_SAMPLING][CURRICULUM_LEARNING][
+                    CURRICULUM_LEARNING_METRICS][metric][CURRICULUM_LEARNING_CLUSTERING_TYPE]
                 if self.global_rank == 0:
                     if self.clustering_type[metric] != CURRICULUM_LEARNING_SINGLE_CLUSTER:
                         self.curriculum_index_to_sample[metric] = MMapIndexedDataset(
-                            data_efficiency_config[DATA_SAMPLING][CURRICULUM_LEARNING]
-                            [CURRICULUM_LEARNING_METRICS][metric]
-                            [CURRICULUM_LEARNING_SAMPLE_PATH],
+                            data_efficiency_config[DATA_SAMPLING][CURRICULUM_LEARNING][CURRICULUM_LEARNING_METRICS]
+                            [metric][CURRICULUM_LEARNING_SAMPLE_PATH],
                             skip_warmup=True)
-                        if self.difficulty_type[
-                                metric] == CURRICULUM_LEARNING_VALUE_BASED:
+                        if self.difficulty_type[metric] == CURRICULUM_LEARNING_VALUE_BASED:
                             self.curriculum_index_to_metric[metric] = MMapIndexedDataset(
-                                data_efficiency_config[DATA_SAMPLING]
-                                [CURRICULUM_LEARNING][CURRICULUM_LEARNING_METRICS]
+                                data_efficiency_config[DATA_SAMPLING][CURRICULUM_LEARNING][CURRICULUM_LEARNING_METRICS]
                                 [metric][CURRICULUM_LEARNING_METRIC_PATH],
                                 skip_warmup=True)
 
         # Sanity checks.
         assert self.total_samples > 0, \
             'no sample to consume: {}'.format(self.total_samples)
         assert self.micro_batch_size > 0
@@ -118,65 +113,54 @@
 
     def __len__(self):
         return self.total_samples
 
     def set_custom_curriculum_learning_schedule(self, schedule_func_dict):
         for metric in self.curriculum_schedulers:
             if metric in schedule_func_dict:
-                self.curriculum_schedulers[metric].set_custom_get_difficulty(
-                    schedule_func_dict[metric])
+                self.curriculum_schedulers[metric].set_custom_get_difficulty(schedule_func_dict[metric])
 
     def get_start_end_idx(self):
         start_idx = self.data_parallel_rank * self.micro_batch_size
         end_idx = start_idx + self.micro_batch_size
         return start_idx, end_idx
 
     def get_sample_based_on_metric_value(self, metric, value_start, value_end):
         new_samples = None
         for row in range(len(self.curriculum_index_to_sample[metric])):
-            if self.curriculum_index_to_metric[metric][
-                    row] <= value_end and self.curriculum_index_to_metric[metric][
-                        row] > value_start:
+            if self.curriculum_index_to_metric[metric][row] <= value_end and self.curriculum_index_to_metric[metric][
+                    row] > value_start:
                 row_samples = np.copy(self.curriculum_index_to_sample[metric][row])
                 new_samples = row_samples if new_samples is None else np.concatenate(
-                    (new_samples,
-                     row_samples),
-                    axis=None)
+                    (new_samples, row_samples), axis=None)
         return new_samples
 
-    def get_sample_based_on_metric_percentile(self,
-                                              metric,
-                                              percentile_start,
-                                              percentile_end):
+    def get_sample_based_on_metric_percentile(self, metric, percentile_start, percentile_end):
         new_samples = None
         if self.data_1epoch_size is None:
-            self.data_1epoch_size = sum(
-                len(x) for x in self.curriculum_index_to_sample[metric])
-        max_percentile = self.data_efficiency_config[DATA_SAMPLING][CURRICULUM_LEARNING][
-            CURRICULUM_LEARNING_METRICS][metric][CURRICULUM_LEARNING_MAX_DIFFICULTY]
+            self.data_1epoch_size = sum(len(x) for x in self.curriculum_index_to_sample[metric])
+        max_percentile = self.data_efficiency_config[DATA_SAMPLING][CURRICULUM_LEARNING][CURRICULUM_LEARNING_METRICS][
+            metric][CURRICULUM_LEARNING_MAX_DIFFICULTY]
         sample_per_percentile = self.data_1epoch_size // max_percentile
         start_count = sample_per_percentile * percentile_start
         end_count = sample_per_percentile * percentile_end
         if percentile_end == max_percentile:
             end_count = self.data_1epoch_size
         current_count = 0
         for row in range(len(self.curriculum_index_to_sample[metric])):
             row_size = len(self.curriculum_index_to_sample[metric][row])
             if current_count + row_size > start_count:
                 row_start = max(0, start_count - current_count)
                 if current_count + row_size <= end_count:
                     row_end = row_size
                 else:
                     row_end = end_count - current_count
-                row_samples = np.copy(
-                    self.curriculum_index_to_sample[metric][row][row_start:row_end])
+                row_samples = np.copy(self.curriculum_index_to_sample[metric][row][row_start:row_end])
                 new_samples = row_samples if new_samples is None else np.concatenate(
-                    (new_samples,
-                     row_samples),
-                    axis=None)
+                    (new_samples, row_samples), axis=None)
             current_count += row_size
             if current_count >= end_count:
                 break
         return new_samples
 
     def get_new_cluster(self, previous_difficulties):
         cluster_fname = CURRICULUM_LEARNING_CLUSTER_PREFIX
@@ -189,89 +173,65 @@
             new_cluster = None
             need_clustering = 0
             for metric in self.clustering_type:
                 if self.clustering_type[metric] != CURRICULUM_LEARNING_SINGLE_CLUSTER:
                     need_clustering += 1
             if need_clustering > 1:
                 for metric in self.curriculum_schedulers:
-                    if self.clustering_type[
-                            metric] == CURRICULUM_LEARNING_SINGLE_CLUSTER:
+                    if self.clustering_type[metric] == CURRICULUM_LEARNING_SINGLE_CLUSTER:
                         metric_cluster = np.arange(start=0,
                                                    stop=self.one_epoch_total_samples,
                                                    step=1,
                                                    dtype=self.index_dtype)
                     else:
-                        if self.difficulty_type[
-                                metric] == CURRICULUM_LEARNING_VALUE_BASED:
-                            metric_cluster = self.get_sample_based_on_metric_value(
-                                metric,
-                                float('-inf'),
-                                self.current_difficulties[metric])
-                        elif self.difficulty_type[
-                                metric] == CURRICULUM_LEARNING_PERCENTILE_BASED:
+                        if self.difficulty_type[metric] == CURRICULUM_LEARNING_VALUE_BASED:
+                            metric_cluster = self.get_sample_based_on_metric_value(metric, float('-inf'),
+                                                                                   self.current_difficulties[metric])
+                        elif self.difficulty_type[metric] == CURRICULUM_LEARNING_PERCENTILE_BASED:
                             metric_cluster = self.get_sample_based_on_metric_percentile(
-                                metric,
-                                0,
-                                self.current_difficulties[metric])
+                                metric, 0, self.current_difficulties[metric])
                     new_cluster = metric_cluster if new_cluster is None else \
                         np.intersect1d(new_cluster, metric_cluster, assume_unique=True)
                 for cluster in self.data_clusters:
-                    new_cluster = np.setdiff1d(new_cluster,
-                                               cluster[0],
-                                               assume_unique=True)
+                    new_cluster = np.setdiff1d(new_cluster, cluster[0], assume_unique=True)
             else:
                 if len(self.data_clusters) == 0:
-                    new_cluster = np.arange(start=0,
-                                            stop=self.one_epoch_total_samples,
-                                            step=1,
-                                            dtype=self.index_dtype)
+                    new_cluster = np.arange(start=0, stop=self.one_epoch_total_samples, step=1, dtype=self.index_dtype)
                 for metric in self.curriculum_schedulers:
                     if self.clustering_type[metric] != CURRICULUM_LEARNING_SINGLE_CLUSTER:
-                        if self.difficulty_type[
-                                metric] == CURRICULUM_LEARNING_VALUE_BASED:
-                            new_cluster = self.get_sample_based_on_metric_value(
-                                metric,
-                                previous_difficulties[metric],
-                                self.current_difficulties[metric])
-                        elif self.difficulty_type[
-                                metric] == CURRICULUM_LEARNING_PERCENTILE_BASED:
+                        if self.difficulty_type[metric] == CURRICULUM_LEARNING_VALUE_BASED:
+                            new_cluster = self.get_sample_based_on_metric_value(metric, previous_difficulties[metric],
+                                                                                self.current_difficulties[metric])
+                        elif self.difficulty_type[metric] == CURRICULUM_LEARNING_PERCENTILE_BASED:
                             new_cluster = self.get_sample_based_on_metric_percentile(
-                                metric,
-                                previous_difficulties[metric],
-                                self.current_difficulties[metric])
+                                metric, previous_difficulties[metric], self.current_difficulties[metric])
             if new_cluster is not None and len(new_cluster) > 0:
                 logger.info(
                     f"new data cluster (previous_difficulties {previous_difficulties}, current_difficulties {self.current_difficulties}) with size {len(new_cluster)} generated."
                 )
                 self.np_rng.shuffle(new_cluster)
-                cluster_builder = create_mmap_dataset_builder(cluster_path,
-                                                              self.index_dtype)
+                cluster_builder = create_mmap_dataset_builder(cluster_path, self.index_dtype)
                 cluster_builder.add_item_numpy(new_cluster)
                 close_mmap_dataset_builder(cluster_builder, cluster_path)
-                self.data_clusters.append(
-                    MMapIndexedDataset(cluster_path,
-                                       skip_warmup=True))
+                self.data_clusters.append(MMapIndexedDataset(cluster_path, skip_warmup=True))
                 self.data_cluster_sizes.append(len(self.data_clusters[-1][0]))
             else:
                 logger.info(
                     f"new data cluster (previous_difficulties {previous_difficulties}, current_difficulties {self.current_difficulties}) has no matched data thus skipped."
                 )
         dist.barrier(group=self.data_parallel_group)
         if os.path.isfile(f"{cluster_path}.bin"):
             self.data_cluster_paths.append(cluster_fname)
             self.data_cluster_current_position.append(0)
 
     def sample_from_clusters(self):
         num_clusters = len(self.data_clusters)
         weight_sum = sum(self.data_cluster_sizes)
         weights = [x / weight_sum for x in self.data_cluster_sizes]
-        samples = self.np_rng.choice(num_clusters,
-                                     self.global_batch_size,
-                                     replace=True,
-                                     p=weights)
+        samples = self.np_rng.choice(num_clusters, self.global_batch_size, replace=True, p=weights)
         samples = np.bincount(samples, minlength=num_clusters)
         return samples
 
     def reshuffle_clusters(self, cidx):
         cluster_fname = self.data_cluster_paths[cidx]
         cluster_path = self.data_efficiency_config[DATA_SAMPLING][CURRICULUM_LEARNING][
             CURRICULUM_LEARNING_CLUSTER_PATH]
@@ -281,58 +241,51 @@
         cluster_builder = create_mmap_dataset_builder(cluster_path, self.index_dtype)
         cluster_builder.add_item_numpy(cluster)
         close_mmap_dataset_builder(cluster_builder, cluster_path)
         self.data_clusters[cidx] = MMapIndexedDataset(cluster_path, skip_warmup=True)
 
     def get_sample_from_cluster(self, cidx, num_samples):
         start_idx = self.data_cluster_current_position[cidx]
-        samples = list(
-            np.copy(self.data_clusters[cidx][0][start_idx:(start_idx + num_samples)]))
+        samples = list(np.copy(self.data_clusters[cidx][0][start_idx:(start_idx + num_samples)]))
         self.data_cluster_current_position[cidx] += num_samples
         if len(samples) < num_samples:
             num_samples_remained = num_samples - len(samples)
             logger.info(f"reshuffling cluster {cidx}.")
             self.reshuffle_clusters(cidx)
             samples += list(np.copy(self.data_clusters[cidx][0][:num_samples_remained]))
             self.data_cluster_current_position[cidx] = num_samples_remained
         return samples
 
     def get_next_global_batch(self):
-        if self.data_efficiency_config[DATA_SAMPLING][CURRICULUM_LEARNING][
-                CURRICULUM_LEARNING_ENABLED]:
+        if self.data_efficiency_config[DATA_SAMPLING][CURRICULUM_LEARNING][CURRICULUM_LEARNING_ENABLED]:
             self.curriculum_step += 1
             new_cluster = False
             previous_difficulties = {}
             for metric in self.curriculum_schedulers:
-                next_difficulty = self.curriculum_schedulers[metric].update_difficulty(
-                    self.curriculum_step)
+                next_difficulty = self.curriculum_schedulers[metric].update_difficulty(self.curriculum_step)
                 if metric not in self.current_difficulties or \
                     next_difficulty != self.current_difficulties[metric]:
                     new_cluster = True
                 if metric in self.current_difficulties:
                     previous_difficulties[metric] = self.current_difficulties[metric]
                 else:
                     if self.difficulty_type[metric] == CURRICULUM_LEARNING_VALUE_BASED:
                         previous_difficulties[metric] = float('-inf')
-                    elif self.difficulty_type[
-                            metric] == CURRICULUM_LEARNING_PERCENTILE_BASED:
+                    elif self.difficulty_type[metric] == CURRICULUM_LEARNING_PERCENTILE_BASED:
                         previous_difficulties[metric] = 0
                 self.current_difficulties[metric] = next_difficulty
             if new_cluster:
                 self.get_new_cluster(previous_difficulties)
             if self.global_rank == 0:
                 samples_per_cluster = self.sample_from_clusters()
                 batch = []
                 for cidx in range(len(samples_per_cluster)):
-                    batch += self.get_sample_from_cluster(cidx,
-                                                          samples_per_cluster[cidx])
+                    batch += self.get_sample_from_cluster(cidx, samples_per_cluster[cidx])
                 self.np_rng.shuffle(batch)
-                batch = torch.tensor(batch,
-                                     device=get_accelerator().current_device_name(),
-                                     dtype=torch.long).view(-1)
+                batch = torch.tensor(batch, device=get_accelerator().current_device_name(), dtype=torch.long).view(-1)
             else:
                 batch = torch.empty(self.global_batch_size,
                                     device=get_accelerator().current_device_name(),
                                     dtype=torch.long)
             dist.broadcast(batch, 0, group=self.data_parallel_group)
             self.batch = batch.tolist()
 
@@ -352,39 +305,34 @@
     def state_dict(self):
         return {
             CURRICULUM_LEARNING_BATCH: self.batch,
             CURRICULUM_LEARNING_CONSUMED_SAMPLES: self.consumed_samples,
             CURRICULUM_LEARNING_STEP: self.curriculum_step,
             CURRICULUM_LEARNING_CURRENT_DIFFICULTIES: self.current_difficulties,
             CURRICULUM_LEARNING_DATA_CLUSTER_PATHS: self.data_cluster_paths,
-            CURRICULUM_LEARNING_DATA_CLUSTER_CURRENT_POSITION:
-            self.data_cluster_current_position,
+            CURRICULUM_LEARNING_DATA_CLUSTER_CURRENT_POSITION: self.data_cluster_current_position,
             CURRICULUM_LEARNING_NP_RNG_STATE: np.random.get_state()
         }
 
     def load_state_dict(self, state_dict):
         self.batch = state_dict[CURRICULUM_LEARNING_BATCH]
         self.consumed_samples = state_dict[CURRICULUM_LEARNING_CONSUMED_SAMPLES]
         self.curriculum_step = state_dict[CURRICULUM_LEARNING_STEP]
         self.current_difficulties = state_dict[CURRICULUM_LEARNING_CURRENT_DIFFICULTIES]
         self.data_cluster_paths = state_dict[CURRICULUM_LEARNING_DATA_CLUSTER_PATHS]
-        self.data_cluster_current_position = state_dict[
-            CURRICULUM_LEARNING_DATA_CLUSTER_CURRENT_POSITION]
+        self.data_cluster_current_position = state_dict[CURRICULUM_LEARNING_DATA_CLUSTER_CURRENT_POSITION]
         np.random.set_state(state_dict[CURRICULUM_LEARNING_NP_RNG_STATE])
-        cluster_root_path = self.data_efficiency_config[DATA_SAMPLING][
-            CURRICULUM_LEARNING][CURRICULUM_LEARNING_CLUSTER_PATH]
+        cluster_root_path = self.data_efficiency_config[DATA_SAMPLING][CURRICULUM_LEARNING][
+            CURRICULUM_LEARNING_CLUSTER_PATH]
         # Backward compatibility: previously data_cluster_paths were stored as
         # absolute paths. Now we changed it to just the file name so that even
         # if user moved the cluster files, the checkpoint loading still works
         # as long as user set the correct new CURRICULUM_LEARNING_CLUSTER_PATH
         # in deepspeed json config.
         for idx in range(len(self.data_cluster_paths)):
             if '/' in self.data_cluster_paths[idx]:
-                self.data_cluster_paths[idx] = self.data_cluster_paths[idx].split(
-                    '/')[-1]
+                self.data_cluster_paths[idx] = self.data_cluster_paths[idx].split('/')[-1]
         if self.global_rank == 0:
             for cluster_fname in self.data_cluster_paths:
                 cluster_path = f"{cluster_root_path}/{cluster_fname}"
-                self.data_clusters.append(
-                    MMapIndexedDataset(cluster_path,
-                                       skip_warmup=True))
+                self.data_clusters.append(MMapIndexedDataset(cluster_path, skip_warmup=True))
                 self.data_cluster_sizes.append(len(self.data_clusters[-1][0]))
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/data_pipeline/data_sampling/indexed_dataset.py` & `deepspeed-0.9.0/deepspeed/runtime/data_pipeline/data_sampling/indexed_dataset.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,11 +1,14 @@
-'''
-Copyright 2022 The Microsoft DeepSpeed Team
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+"""
 Part of this code was adopted from https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/data/indexed_dataset.py
-'''
+"""
 
 # Copyright (c) Facebook, Inc. and its affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
 # copied from fairseq/fairseq/data/indexed_dataset.py
@@ -46,34 +49,29 @@
                 return 'cached'
             elif magic == MMapIndexedDataset.Index._HDR_MAGIC[:8]:
                 return 'mmap'
             else:
                 return None
     else:
         print(f"Dataset does not exist: {path}")
-        print(
-            "Path should be a basename that both .idx and .bin can be appended to get full filenames."
-        )
+        print("Path should be a basename that both .idx and .bin can be appended to get full filenames.")
         return None
 
 
 def make_builder(out_file, impl, vocab_size=None):
     if impl == 'mmap':
-        return MMapIndexedDatasetBuilder(out_file,
-                                         dtype=__best_fitting_dtype(vocab_size))
+        return MMapIndexedDatasetBuilder(out_file, dtype=__best_fitting_dtype(vocab_size))
     else:
         return IndexedDatasetBuilder(out_file)
 
 
 def make_dataset(path, impl, skip_warmup=False):
     if not IndexedDataset.exists(path):
         print(f"Dataset does not exist: {path}")
-        print(
-            "Path should be a basename that both .idx and .bin can be appended to get full filenames."
-        )
+        print("Path should be a basename that both .idx and .bin can be appended to get full filenames.")
         return None
     if impl == 'infer':
         impl = infer_dataset_impl(path)
     if impl == 'lazy' and IndexedDataset.exists(path):
         return IndexedDataset(path)
     elif impl == 'cached' and IndexedDataset.exists(path):
         return IndexedCachedDataset(path)
@@ -146,18 +144,16 @@
         self.path = path
         self.data_file = None
         self.read_index(path)
 
     def read_index(self, path):
         with open(index_file_path(path), 'rb') as f:
             magic = f.read(8)
-            assert magic == self._HDR_MAGIC, (
-                'Index file doesn\'t match expected format. '
-                'Make sure that --dataset-impl is configured properly.'
-            )
+            assert magic == self._HDR_MAGIC, ('Index file doesn\'t match expected format. '
+                                              'Make sure that --dataset-impl is configured properly.')
             version = f.read(8)
             assert struct.unpack('<Q', version) == (1, )
             code, self.element_size = struct.unpack('<QQ', f.read(16))
             self.dtype = dtypes[code]
             self._len, self.s = struct.unpack('<QQ', f.read(16))
             self.doc_count = struct.unpack('<Q', f.read(8))
             self.dim_offsets = read_longs(f, self._len + 1)
@@ -208,23 +204,23 @@
         return self.sizes[index]
 
     def size(self, index):
         return self.sizes[index]
 
     @staticmethod
     def exists(path):
-        return (os.path.exists(index_file_path(path))
-                and os.path.exists(data_file_path(path)))
+        return (os.path.exists(index_file_path(path)) and os.path.exists(data_file_path(path)))
 
     @property
     def supports_prefetch(self):
         return False  # avoid prefetching to save memory
 
 
 class IndexedCachedDataset(IndexedDataset):
+
     def __init__(self, path):
         super().__init__(path)
         self.cache = None
         self.cache_index = {}
 
     @property
     def supports_prefetch(self):
@@ -269,23 +265,15 @@
             sents = []
             for i in range(*idx.indices(len(self))):
                 sents.append(self[i])
             return sents
 
 
 class IndexedDatasetBuilder(object):
-    element_sizes = {
-        np.uint8: 1,
-        np.int8: 1,
-        np.int16: 2,
-        np.int32: 4,
-        np.int64: 8,
-        np.float64: 4,
-        np.double: 8
-    }
+    element_sizes = {np.uint8: 1, np.int8: 1, np.int16: 2, np.int32: 4, np.int64: 8, np.float64: 4, np.double: 8}
 
     def __init__(self, out_file, dtype=np.int32):
         self.out_file = open(out_file, 'wb')
         self.dtype = dtype
         self.data_offsets = [0]
         self.dim_offsets = [0]
         self.sizes = []
@@ -375,20 +363,23 @@
     # convert to byte offsets
     exscan_from_cumsum_(pointers)
 
     return pointers, bytes_last
 
 
 class MMapIndexedDataset(torch.utils.data.Dataset):
+
     class Index(object):
         _HDR_MAGIC = b'MMIDIDX\x00\x00'
 
         @classmethod
         def writer(cls, path, dtype):
+
             class _Writer(object):
+
                 def __enter__(self):
                     self._file = open(path, 'wb')
 
                     self._file.write(cls._HDR_MAGIC)
                     self._file.write(struct.pack('<Q', 1))
                     self._file.write(struct.pack('<B', code(dtype)))
 
@@ -426,18 +417,16 @@
                     self._file.close()
 
             return _Writer()
 
         def __init__(self, path, skip_warmup=False):
             with open(path, 'rb') as stream:
                 magic_test = stream.read(9)
-                assert self._HDR_MAGIC == magic_test, (
-                    'Index file doesn\'t match expected format. '
-                    'Make sure that --dataset-impl is configured properly.'
-                )
+                assert self._HDR_MAGIC == magic_test, ('Index file doesn\'t match expected format. '
+                                                       'Make sure that --dataset-impl is configured properly.')
                 version = struct.unpack('<Q', stream.read(8))
                 assert (1, ) == version
 
                 dtype_code, = struct.unpack('<B', stream.read(1))
                 self._dtype = dtypes[dtype_code]
                 self._dtype_size = self._dtype().itemsize
 
@@ -448,29 +437,25 @@
             if not skip_warmup:
                 print("    warming up index mmap file...")
                 _warmup_mmap_file(path)
 
             self._bin_buffer_mmap = np.memmap(path, mode='r', order='C')
             self._bin_buffer = memoryview(self._bin_buffer_mmap)
             print("    reading sizes...")
-            self._sizes = np.frombuffer(self._bin_buffer,
-                                        dtype=np.int32,
-                                        count=self._len,
-                                        offset=offset)
+            self._sizes = np.frombuffer(self._bin_buffer, dtype=np.int32, count=self._len, offset=offset)
             print("    reading pointers...")
             self._pointers = np.frombuffer(self._bin_buffer,
                                            dtype=np.int64,
                                            count=self._len,
                                            offset=offset + self._sizes.nbytes)
             print("    reading document index...")
             self._doc_idx = np.frombuffer(self._bin_buffer,
                                           dtype=np.int64,
                                           count=self._doc_count,
-                                          offset=offset + self._sizes.nbytes +
-                                          self._pointers.nbytes)
+                                          offset=offset + self._sizes.nbytes + self._pointers.nbytes)
 
         def __del__(self):
             self._bin_buffer_mmap._mmap.close()
             del self._bin_buffer_mmap
 
         @property
         def dtype(self):
@@ -510,17 +495,15 @@
         self._path = path
         self._index = self.Index(index_file_path(self._path), skip_warmup)
 
         if not skip_warmup:
             print("    warming up data mmap file...")
             _warmup_mmap_file(data_file_path(self._path))
         print("    creating numpy buffer of mmap...")
-        self._bin_buffer_mmap = np.memmap(data_file_path(self._path),
-                                          mode='r',
-                                          order='C')
+        self._bin_buffer_mmap = np.memmap(data_file_path(self._path), mode='r', order='C')
         print("    creating memory view of numpy buffer...")
         self._bin_buffer = memoryview(self._bin_buffer_mmap)
 
     def __del__(self):
         self._bin_buffer_mmap._mmap.close()
         del self._bin_buffer_mmap
         del self._index
@@ -528,48 +511,39 @@
     def __len__(self):
         return len(self._index)
 
     # @lru_cache(maxsize=8)
     def __getitem__(self, idx):
         if isinstance(idx, int):
             ptr, size = self._index[idx]
-            np_array = np.frombuffer(self._bin_buffer,
-                                     dtype=self._index.dtype,
-                                     count=size,
-                                     offset=ptr)
+            np_array = np.frombuffer(self._bin_buffer, dtype=self._index.dtype, count=size, offset=ptr)
             return np_array
         elif isinstance(idx, slice):
             start, stop, step = idx.indices(len(self))
             if step != 1:
                 raise ValueError("Slices into indexed_dataset must be contiguous")
             ptr = self._index._pointers[start]
             sizes = self._index._sizes[idx]
             offsets = list(accumulate(sizes))
             total_size = sum(sizes)
-            np_array = np.frombuffer(self._bin_buffer,
-                                     dtype=self._index.dtype,
-                                     count=total_size,
-                                     offset=ptr)
+            np_array = np.frombuffer(self._bin_buffer, dtype=self._index.dtype, count=total_size, offset=ptr)
             sents = np.split(np_array, offsets[:-1])
             return sents
 
     def get(self, idx, offset=0, length=None):
         """ Retrieves a single item from the dataset with the option to only
         return a portion of the item.
 
         get(idx) is the same as [idx] but get() does not support slicing.
         """
         ptr, size = self._index[idx]
         if length is None:
             length = size - offset
         ptr += offset * np.dtype(self._index.dtype).itemsize
-        np_array = np.frombuffer(self._bin_buffer,
-                                 dtype=self._index.dtype,
-                                 count=length,
-                                 offset=ptr)
+        np_array = np.frombuffer(self._bin_buffer, dtype=self._index.dtype, count=length, offset=ptr)
         return np_array
 
     @property
     def sizes(self):
         return self._index.sizes
 
     def size(self, index):
@@ -587,23 +561,23 @@
 
     @property
     def supports_prefetch(self):
         return False
 
     @staticmethod
     def exists(path):
-        return (os.path.exists(index_file_path(path))
-                and os.path.exists(data_file_path(path)))
+        return (os.path.exists(index_file_path(path)) and os.path.exists(data_file_path(path)))
 
     @property
     def dtype(self):
         return self._index.dtype
 
 
 class MMapIndexedDatasetBuilder(object):
+
     def __init__(self, out_file, dtype=np.int64):
         self._data_file = open(out_file, 'wb')
         self._dtype = dtype
         self._sizes = []
         self._doc_idx = [0]
 
     def add_item(self, tensor):
@@ -622,17 +596,15 @@
 
     def merge_file_(self, another_file):
         # Concatenate index
         index = MMapIndexedDataset.Index(index_file_path(another_file))
         assert index.dtype == self._dtype
 
         total_len = len(index.sizes) + len(self._sizes)
-        print(
-            f"    concat {another_file} size={len(index.sizes)} for a total size of {total_len}"
-        )
+        print(f"    concat {another_file} size={len(index.sizes)} for a total size of {total_len}")
 
         offset = len(self._sizes)
         self._sizes.extend(index.sizes)
         self._doc_idx.extend((offset + index.doc_idx)[1:])
 
         # Concatenate data
         with open(data_file_path(another_file), 'rb') as f:
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/data_pipeline/data_sampling/utils.py` & `deepspeed-0.9.0/deepspeed/runtime/data_pipeline/data_sampling/utils.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-'''
-Copyright 2022 The Microsoft DeepSpeed Team
-'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import math
 import numpy as np
 
 from deepspeed.utils import logger
 from .indexed_dataset import MMapIndexedDatasetBuilder
 
@@ -28,27 +29,22 @@
             return np.int32
         else:
             return np.int64
 
 
 def split_index(start_idx, end_idx, num_partitions):
     partition_size = math.ceil((end_idx - start_idx) / num_partitions)
-    partitions = [[
-        start_idx + x * partition_size,
-        min(end_idx,
-            start_idx + (x + 1) * partition_size)
-    ] for x in range(num_partitions)]
+    partitions = [[start_idx + x * partition_size,
+                   min(end_idx, start_idx + (x + 1) * partition_size)] for x in range(num_partitions)]
     return partitions
 
 
 def split_dataset(dataset, num_workers, worker_id, num_threads):
     worker_splits = split_index(0, len(dataset), num_workers)
-    thread_splits = split_index(worker_splits[worker_id][0],
-                                worker_splits[worker_id][1],
-                                num_threads)
+    thread_splits = split_index(worker_splits[worker_id][0], worker_splits[worker_id][1], num_threads)
     return worker_splits, thread_splits
 
 
 def create_mmap_dataset_builder(fname, dtype):
     logger.info(f"Creating mmap dataset builder at {fname}.")
     return MMapIndexedDatasetBuilder(f"{fname}.bin", dtype=dtype)
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/dataloader.py` & `deepspeed-0.9.0/deepspeed/runtime/dataloader.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,23 +1,25 @@
-'''
-Copyright 2019 The Microsoft DeepSpeed Team
-'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 from torch.utils.data import DataLoader, RandomSampler
 from torch.utils.data.distributed import DistributedSampler
 from deepspeed.accelerator import get_accelerator
 
 from deepspeed.runtime.data_pipeline.data_sampling.data_sampler import DeepSpeedDataSampler
 from deepspeed.runtime.data_pipeline.constants import CURRICULUM_LEARNING, \
     DATA_EFFICIENCY, DATA_SAMPLING_NUM_WORKERS
 from deepspeed.runtime.constants import GRADIENT_ACCUMULATION_STEPS, \
     DATA_PARALLEL_GROUP, GLOBAL_RANK
 
 
 class RepeatingLoader:
+
     def __init__(self, loader):
         """Wraps an iterator to allow for infinite iteration. This is especially useful
         for DataLoader types that we wish to automatically restart upon completion.
 
         Args:
             loader (iterator): The data loader to repeat.
         """
@@ -33,14 +35,15 @@
         except StopIteration:
             self.data_iter = iter(self.loader)
             batch = next(self.data_iter)
         return batch
 
 
 class DeepSpeedDataLoader(object):
+
     def __init__(self,
                  dataset,
                  batch_size,
                  pin_memory,
                  local_rank,
                  tput_timer,
                  collate_fn=None,
@@ -51,38 +54,34 @@
                  dataloader_drop_last=False,
                  deepspeed_dataloader_config={}):
         self.deepspeed_dataloader_config = deepspeed_dataloader_config
         self.tput_timer = tput_timer
         self.batch_size = batch_size
         self.curriculum_learning_enabled = False
         if CURRICULUM_LEARNING in deepspeed_dataloader_config:
-            self.curriculum_learning_enabled = deepspeed_dataloader_config[
-                CURRICULUM_LEARNING]
+            self.curriculum_learning_enabled = deepspeed_dataloader_config[CURRICULUM_LEARNING]
 
         if self.curriculum_learning_enabled:
-            data_sampler = DeepSpeedDataSampler(
-                self.deepspeed_dataloader_config[DATA_EFFICIENCY],
-                len(dataset),
-                self.batch_size,
-                data_parallel_rank,
-                data_parallel_world_size,
-                self.deepspeed_dataloader_config[DATA_PARALLEL_GROUP],
-                self.deepspeed_dataloader_config[GRADIENT_ACCUMULATION_STEPS],
-                self.deepspeed_dataloader_config[GLOBAL_RANK],
-                drop_last=dataloader_drop_last)
+            data_sampler = DeepSpeedDataSampler(self.deepspeed_dataloader_config[DATA_EFFICIENCY],
+                                                len(dataset),
+                                                self.batch_size,
+                                                data_parallel_rank,
+                                                data_parallel_world_size,
+                                                self.deepspeed_dataloader_config[DATA_PARALLEL_GROUP],
+                                                self.deepspeed_dataloader_config[GRADIENT_ACCUMULATION_STEPS],
+                                                self.deepspeed_dataloader_config[GLOBAL_RANK],
+                                                drop_last=dataloader_drop_last)
             device_count = get_accelerator().device_count()
-            num_local_io_workers = self.deepspeed_dataloader_config[
-                DATA_SAMPLING_NUM_WORKERS]
+            num_local_io_workers = self.deepspeed_dataloader_config[DATA_SAMPLING_NUM_WORKERS]
         else:
             if local_rank >= 0:
                 if data_sampler is None:
-                    data_sampler = DistributedSampler(
-                        dataset=dataset,
-                        num_replicas=data_parallel_world_size,
-                        rank=data_parallel_rank)
+                    data_sampler = DistributedSampler(dataset=dataset,
+                                                      num_replicas=data_parallel_world_size,
+                                                      rank=data_parallel_rank)
                 device_count = 1
             else:
                 if data_sampler is None:
                     data_sampler = RandomSampler(dataset)
                 device_count = get_accelerator().device_count()
                 batch_size *= device_count
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/eigenvalue.py` & `deepspeed-0.9.0/deepspeed/runtime/eigenvalue.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,16 +1,20 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import torch
 from deepspeed.utils import log_dist
 import numpy as np
 import logging
 
 
 class Eigenvalue(object):
+
     def __init__(self,
                  verbose=False,
                  max_iter=100,
                  tol=1e-2,
                  stability=0,
                  gas_boundary_resolution=1,
                  layer_name='',
@@ -73,16 +77,15 @@
             if device is None:
                 v = [
                     torch.randn(p.size()) for p in model_block.parameters()
                     if p.grad is not None and p.grad.grad_fn is not None
                 ]
             else:
                 v = [
-                    torch.randn(p.size(),
-                                device=device) for p in model_block.parameters()
+                    torch.randn(p.size(), device=device) for p in model_block.parameters()
                     if p.grad is not None and p.grad.grad_fn is not None
                 ]
             torch.random.set_rng_state(rng_state)
 
             grads = [
                 param.grad for param in model_block.parameters()
                 if param.grad is not None and param.grad.grad_fn is not None
@@ -96,48 +99,40 @@
             param_keys.append(layer_keys)
 
             v = self.normalize(v)
 
             # Disable eigenvalue if the model doesn't support second order gradients computation,
             # e.g. when enabling DS transformer kernel.
             if len(grads) == 0 or len(params) == 0:
-                log_dist(f'The model does NOT support eigenvalue computation.',
-                         ranks=[0],
-                         level=logging.WARNING)
+                log_dist(f'The model does NOT support eigenvalue computation.', ranks=[0], level=logging.WARNING)
                 return []
 
             i = 0
             eigenvalue_current, eigenvalue_previous = 1., 0.
 
             while (i < self.max_iter) and abs(eigenvalue_current) > 0 and (abs(
-                (eigenvalue_current - eigenvalue_previous) /
-                    eigenvalue_current) >= self.tol):  # test convergence criteria
+                (eigenvalue_current - eigenvalue_previous) / eigenvalue_current) >=
+                                                                           self.tol):  # test convergence criteria
                 eigenvalue_previous = eigenvalue_current
 
-                Hv = torch.autograd.grad(grads,
-                                         params,
-                                         grad_outputs=v,
-                                         only_inputs=True,
-                                         retain_graph=True)
+                Hv = torch.autograd.grad(grads, params, grad_outputs=v, only_inputs=True, retain_graph=True)
                 #Hv = [hv.float() for hv in Hv]
                 Hv = [self.nan_to_num(hv).float() for hv in Hv]
 
                 eigenvalue_current = self.inner_product(Hv, v).item()
 
                 v = self.normalize(Hv)
                 v = [x / scale for x in v]
                 i += 1
 
             eigenvalue_current *= scale
             block_eigenvalue.append(eigenvalue_current)
 
             if self.verbose:
-                log_dist(
-                    f'block: {block}, power iteration: {i}, eigenvalue: {eigenvalue_current}',
-                    ranks=[0])
+                log_dist(f'block: {block}, power iteration: {i}, eigenvalue: {eigenvalue_current}', ranks=[0])
 
         block_eigenvalue = self.post_process(block_eigenvalue)
 
         if self.verbose:
             log_dist(f'post processed block_eigenvalue: {block_eigenvalue}', ranks=[0])
 
         # {param_id: (eigenvalue, layer_id)}
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/engine.py` & `deepspeed-0.9.0/deepspeed/runtime/engine.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,18 +1,20 @@
-"""
-Copyright 2019 The Microsoft DeepSpeed Team
-"""
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import os
 import re
 import stat
 import torch
 import hashlib
 from collections import defaultdict, OrderedDict, deque
 from shutil import copyfile
+import gc
 
 from torch.nn.modules import Module
 from torch.nn.parameter import Parameter
 from torch.optim import Optimizer
 from torch.optim.lr_scheduler import _LRScheduler
 
 from typing import Callable, Dict, Union, Iterable
@@ -27,15 +29,15 @@
 from deepspeed.runtime.zero.parameter_offload import DeepSpeedZeRoOffload
 from deepspeed.runtime.zero.config import ZERO_OPTIMIZATION
 
 from deepspeed.runtime.fp16.fused_optimizer import FP16_Optimizer
 from deepspeed.runtime.fp16.unfused_optimizer import FP16_UnfusedOptimizer
 from deepspeed.runtime.bf16_optimizer import BF16_Optimizer
 
-from deepspeed.runtime.config import DeepSpeedConfig, DEEPSPEED_OPTIMIZERS, \
+from deepspeed.runtime.config import DEEPSPEED_OPTIMIZERS, \
     ADAGRAD_OPTIMIZER, ADAM_OPTIMIZER, ADAMW_OPTIMIZER, LAMB_OPTIMIZER, ONEBIT_ADAM_OPTIMIZER, ONEBIT_LAMB_OPTIMIZER, \
     TORCH_ADAM_PARAM, ADAM_W_MODE, ADAM_W_MODE_DEFAULT, ZERO_ONE_ADAM_OPTIMIZER
 
 from deepspeed.runtime.dataloader import DeepSpeedDataLoader
 from deepspeed.runtime.constants import \
     ROUTE_TRAIN, ROUTE_PREDICT, ROUTE_EVAL, \
     PLD_THETA, PLD_GAMMA, BFLOAT16, FP16, AMP, GRADIENT_ACCUMULATION_STEPS, \
@@ -88,15 +90,15 @@
 
 from deepspeed.profiling.flops_profiler.profiler import FlopsProfiler
 from deepspeed.utils.logging import print_json_dist, print_configuration
 
 from deepspeed.accelerator import get_accelerator
 from deepspeed.ops.op_builder import UtilsBuilder
 
-from deepspeed.inference.config import DtypeEnum
+from deepspeed.runtime.config import DtypeEnum
 
 # Set to torch's distributed package or deepspeed.comm based inside DeepSpeedEngine init
 dist = None
 
 MEMORY_OPT_ALLREDUCE_SIZE = 500000000
 
 DeepSpeedOptimizerCallable = \
@@ -106,24 +108,21 @@
 try:
     import apex
     from apex import amp
     APEX_INSTALLED = True
 except ImportError:
     # Fail silently so we don't spam logs unnecessarily if user isn't using amp
     APEX_INSTALLED = False
-    pass
 
 
 def split_half_float_double_sparse(tensors):
     device_type = get_accelerator().device_name()
     supported_types = [
-        "torch.{}.HalfTensor".format(device_type),
-        "torch.{}.FloatTensor".format(device_type),
-        "torch.{}.DoubleTensor".format(device_type),
-        "torch.{}.BFloat16Tensor".format(device_type),
+        "torch.{}.HalfTensor".format(device_type), "torch.{}.FloatTensor".format(device_type),
+        "torch.{}.DoubleTensor".format(device_type), "torch.{}.BFloat16Tensor".format(device_type),
         SparseTensor.type()
     ]
 
     for t in tensors:
         assert t.type() in supported_types, f"attempting to reduce an unsupported grad type: {t.type()}"
 
     buckets = []
@@ -144,14 +143,15 @@
 BACKWARD_REDUCE_GLOBAL_TIMER = 'backward_allreduce'
 STEP_MICRO_TIMER = 'step_microstep'
 STEP_GLOBAL_TIMER = 'step'
 
 
 class EngineTimers(object):
     r"""Wallclock timers for DeepSpeedEngine"""
+
     def __init__(self, enable_micro_timers, enable_global_timers):
         self.forward_timers = []
         self.backward_timers = []
         self.backward_inner_timers = []
         self.backward_reduce_timers = []
         self.step_timers = []
         self.global_timers = []
@@ -160,51 +160,46 @@
         if enable_micro_timers:
             self.forward_timers += [FORWARD_MICRO_TIMER]
             self.backward_timers += [BACKWARD_MICRO_TIMER]
             self.backward_inner_timers += [BACKWARD_INNER_MICRO_TIMER]
             self.backward_reduce_timers += [BACKWARD_REDUCE_MICRO_TIMER]
             self.step_timers += [STEP_MICRO_TIMER]
             self.micro_timers += [
-                FORWARD_MICRO_TIMER,
-                BACKWARD_MICRO_TIMER,
-                BACKWARD_INNER_MICRO_TIMER,
-                BACKWARD_REDUCE_MICRO_TIMER,
+                FORWARD_MICRO_TIMER, BACKWARD_MICRO_TIMER, BACKWARD_INNER_MICRO_TIMER, BACKWARD_REDUCE_MICRO_TIMER,
                 STEP_MICRO_TIMER
             ]
 
         if enable_global_timers:
             self.forward_timers += [FORWARD_GLOBAL_TIMER]
             self.backward_timers += [BACKWARD_GLOBAL_TIMER]
             self.backward_inner_timers += [BACKWARD_INNER_GLOBAL_TIMER]
             self.backward_reduce_timers += [BACKWARD_REDUCE_GLOBAL_TIMER]
             self.step_timers += [STEP_GLOBAL_TIMER]
             self.global_timers += [
-                FORWARD_GLOBAL_TIMER,
-                BACKWARD_GLOBAL_TIMER,
-                BACKWARD_INNER_GLOBAL_TIMER,
-                BACKWARD_REDUCE_GLOBAL_TIMER,
+                FORWARD_GLOBAL_TIMER, BACKWARD_GLOBAL_TIMER, BACKWARD_INNER_GLOBAL_TIMER, BACKWARD_REDUCE_GLOBAL_TIMER,
                 STEP_GLOBAL_TIMER
             ]
 
 
 class DeepSpeedEngine(Module):
     r"""DeepSpeed engine for training."""
+
     def __init__(
         self,
         args,
         model,
         optimizer=None,
         model_parameters=None,
         training_data=None,
         lr_scheduler=None,
         mpu=None,
         dist_init_required=None,
         collate_fn=None,
         config=None,
-        config_params=None,
+        config_class=None,
         dont_change_device=False,
     ):
         super(DeepSpeedEngine, self).__init__()
         self.dont_change_device = dont_change_device
         self.client_optimizer = optimizer
         self.client_lr_scheduler = lr_scheduler
         self.training_data = training_data
@@ -214,14 +209,15 @@
         self.global_steps = 0
         self.global_samples = 0
         self.micro_steps = 0
         self.skipped_steps = 0
         self.gradient_average = True
         self.warn_unscaled_loss = True
         self.config = config
+        self._config = config_class
         self.loaded_checkpoint_mp_world_size = None
         self.loaded_checkpoint_dp_world_size = None
         self.enable_backward_allreduce = True
         self.progressive_layer_drop = None
         self.eigenvalue = None
         self.block_eigenvalue = None
         self.gas_boundary_ctr = 0
@@ -243,23 +239,18 @@
 
         # for debug purposes - can then debug print: debug_get_module_name(module)
         debug_extract_module_and_param_names(model)
 
         # needed for zero_to_fp32 weights reconstruction to remap nameless data to state_dict
         self.param_names = {param: name for name, param in model.named_parameters()}
 
-        # Set config using config_params for backwards compat
-        if self.config is None and config_params is not None:
-            self.config = config_params
-
         from deepspeed.comm import supported_torch_version
         # This supported_torch_version check is for torch1.2 compatibility only
         if supported_torch_version:
-            dist.init_distributed(dist_backend=self.dist_backend,
-                                  dist_init_required=dist_init_required)
+            dist.init_distributed(dist_backend=self.dist_backend, dist_init_required=dist_init_required)
         else:
             if dist_init_required is None:
                 dist_init_required = not dist.is_initialized()
 
             if dist_init_required is False:
                 assert (
                     dist.is_initialized() is True
@@ -267,22 +258,20 @@
             else:
                 if not dist.is_initialized():
                     dist.init_process_group(backend=self.dist_backend)
 
         self._do_args_sanity_check(args)
         self._configure_with_arguments(args, mpu)
         self._do_sanity_check()
-        see_memory_usage(f"DeepSpeed Engine: After args sanity test",
-                         force=self.memory_breakdown())
+        see_memory_usage(f"DeepSpeed Engine: After args sanity test", force=self.memory_breakdown())
         if mpu is not None:
             if self.elasticity_enabled():
                 if not self.is_elastic_model_parallel_supported():
-                    assert not self.elasticity_enabled(), (
-                        "Elasticity is not currently supported" " with model parallelism."
-                    )
+                    assert not self.elasticity_enabled(), ("Elasticity is not currently supported"
+                                                           " with model parallelism.")
 
         self._set_distributed_vars(args)
 
         dist.configure(self._config)
 
         self.monitor = MonitorMaster(self._config.monitor_config)
 
@@ -305,16 +294,15 @@
         # Throughput timer
         self.tput_timer = ThroughputTimer(
             batch_size=self.train_batch_size(),
             steps_per_output=self.steps_per_print(),
             monitor_memory=False,
         )
 
-        log_dist(f"DeepSpeed Flops Profiler Enabled: {self.flops_profiler_enabled()}",
-                 ranks=[0])
+        log_dist(f"DeepSpeed Flops Profiler Enabled: {self.flops_profiler_enabled()}", ranks=[0])
 
         if self.flops_profiler_enabled():
             self.flops_profiler = FlopsProfiler(self.module, self)
 
         if training_data:
             self.training_dataloader = self.deepspeed_io(training_data)
         else:
@@ -346,50 +334,43 @@
         elif self.bfloat16_enabled():
             self.optimizer = self._configure_bf16_optimizer(optimizer=None)
 
         # Bookkeeping for sparse support
         self.sparse_tensor_module_names = set()
         # if self.sparse_gradients_enabled():
         for name, module in self.module.named_modules():
-            if isinstance(module,
-                          (torch.nn.Embedding,
-                           torch.nn.EmbeddingBag)) and self.sparse_gradients_enabled():
+            if isinstance(module, (torch.nn.Embedding, torch.nn.EmbeddingBag)) and self.sparse_gradients_enabled():
                 self.sparse_tensor_module_names.add(name + ".weight")
-                logger.info(
-                    "Will convert {} to sparse tensor during training".format(name))
+                logger.info("Will convert {} to sparse tensor during training".format(name))
 
         self.save_non_zero_checkpoint = False
         self.save_zero_checkpoint = False
         if not isinstance(self.optimizer, DeepSpeedZeRoOffload):
             self._configure_checkpointing(dist_init_required)
 
         if self.eigenvalue_enabled():
             self.eigenvalue = self._configure_eigenvalue()
 
         if self.pld_enabled():
             self.progressive_layer_drop = self._configure_progressive_layer_drop()
 
         if self.curriculum_enabled_legacy():
-            self.curriculum_scheduler_legacy = self._configure_curriculum_scheduler_legacy(
-            )
+            self.curriculum_scheduler_legacy = self._configure_curriculum_scheduler_legacy()
 
         if self.random_ltd_enabled():
             random_ltd_config = self.random_ltd_config()
             random_ltd_config[RANDOM_LTD_GLOBAL_BATCH_SIZE] = self.train_batch_size()
-            random_ltd_config[
-                RANDOM_LTD_MICRO_BATCH_SIZE] = self.train_micro_batch_size_per_gpu()
-            self.random_ltd_scheduler = self._configure_random_ltd_scheduler(
-                random_ltd_config)
+            random_ltd_config[RANDOM_LTD_MICRO_BATCH_SIZE] = self.train_micro_batch_size_per_gpu()
+            self.random_ltd_scheduler = self._configure_random_ltd_scheduler(random_ltd_config)
 
         # Engine timers
 
-        self.engine_timers = EngineTimers(
-            enable_micro_timers=self.wall_clock_breakdown(),
-            enable_global_timers=self.wall_clock_breakdown()
-            or self.flops_profiler_enabled())
+        self.engine_timers = EngineTimers(enable_micro_timers=self.wall_clock_breakdown(),
+                                          enable_global_timers=self.wall_clock_breakdown()
+                                          or self.flops_profiler_enabled())
 
         if self.global_rank == 0:
             self._config.print("DeepSpeedEngine configuration")
             if self.dump_state():
                 print_configuration(self, "DeepSpeedEngine")
 
         # Load pre-installed or JIT compile (un)flatten ops
@@ -414,18 +395,16 @@
                     n += p.ds_numel
                 else:  # if the parameter is not partitioned in zero 3 yet
                     n += p.numel()
                 num_params += n
                 if p.requires_grad:
                     trainable_num_params += n
             if self.global_rank == 0:
-                self.autotuning_model_info[
-                    "num_params"] = num_params * self.mp_world_size
-                self.autotuning_model_info[
-                    "trainable_num_params"] = trainable_num_params * self.mp_world_size
+                self.autotuning_model_info["num_params"] = num_params * self.mp_world_size
+                self.autotuning_model_info["trainable_num_params"] = trainable_num_params * self.mp_world_size
 
             logger.info(f"model parameter = {num_params}")
 
     def get_batch_info(self):
         """Get all training batch related settings.
         Returns:
             train_batch_size (int): The effective training batch size. This is the amount of data
@@ -447,33 +426,29 @@
         (i.e., ``train_micro_batch_size_per_gpu``) is not changed.
         Args:
             train_batch_size (int): The new global batch size for training.
         Raises:
             ValueError: if ``train_batch_size`` is not divisible by the
                 configured micro-batch size and data parallelism.
         """
-        if train_batch_size % (self.train_micro_batch_size_per_gpu() *
-                               self.dp_world_size) != 0:
+        if train_batch_size % (self.train_micro_batch_size_per_gpu() * self.dp_world_size) != 0:
             #print(f'{train_batch_size=} {self.train_micro_batch_size_per_gpu()=} {self.dp_world_size=}')
-            raise ValueError(
-                f'Train batch size must be divisible by micro-batch data parallelism')
-        new_gas = train_batch_size // (self.train_micro_batch_size_per_gpu() *
-                                       self.dp_world_size)
+            raise ValueError(f'Train batch size must be divisible by micro-batch data parallelism')
+        new_gas = train_batch_size // (self.train_micro_batch_size_per_gpu() * self.dp_world_size)
         # overwrite config
         self._config.train_batch_size = train_batch_size
         self._config.gradient_accumulation_steps = new_gas
 
     def set_data_post_process_func(self, post_process_func):
         if self.training_dataloader is not None:
             self.training_dataloader.post_process_func = post_process_func
 
     def set_custom_curriculum_learning_schedule(self, schedule_func_dict):
         if self.training_dataloader is not None and self.curriculum_learning_enabled():
-            self.training_dataloader.data_sampler.set_custom_curriculum_learning_schedule(
-                schedule_func_dict)
+            self.training_dataloader.data_sampler.set_custom_curriculum_learning_schedule(schedule_func_dict)
 
     def get_global_grad_norm(self) -> float:
         """Return the 2-norm of all gradients. If there is model parallelism,
         the norm will be global.
         The computed norm will be cached and reused until the next step() pass.
         .. note::
             In the presence of model parallelism, this is a collective call
@@ -492,16 +467,15 @@
         if "module" in self.__dict__:
             _module = self.__dict__['module']
         if name in dir(self):
             return getattr(self, name)
         elif name in dir(_module):
             return getattr(_module, name)
         else:
-            raise AttributeError(
-                f"'{type(self).__name__}' object has no attribute '{name}'")
+            raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
 
     def checkpoint_tag_validation_enabled(self):
         return self._config.checkpoint_tag_validation_enabled
 
     def checkpoint_tag_validation_fail(self):
         return self._config.checkpoint_tag_validation_fail
 
@@ -567,50 +541,42 @@
     def data_sampling_enabled(self):
         return self._config.data_efficiency_config[DATA_SAMPLING][DATA_SAMPLING_ENABLED]
 
     def data_sampling_config(self):
         return self._config.data_efficiency_config[DATA_SAMPLING]
 
     def curriculum_learning_enabled(self):
-        return self._config.data_efficiency_config[DATA_SAMPLING][CURRICULUM_LEARNING][
-            CURRICULUM_LEARNING_ENABLED]
+        return self._config.data_efficiency_config[DATA_SAMPLING][CURRICULUM_LEARNING][CURRICULUM_LEARNING_ENABLED]
 
     def curriculum_learning_config(self):
         return self._config.data_efficiency_config[DATA_SAMPLING][CURRICULUM_LEARNING]
 
     def random_ltd_enabled(self):
-        return self._config.data_efficiency_config[DATA_ROUTING][RANDOM_LTD][
-            RANDOM_LTD_ENABLED]
+        return self._config.data_efficiency_config[DATA_ROUTING][RANDOM_LTD][RANDOM_LTD_ENABLED]
 
     def random_ltd_config(self):
         return self._config.data_efficiency_config[DATA_ROUTING][RANDOM_LTD]
 
     def random_ltd_initialize(self):
         assert self.random_ltd_enabled()
         random_ltd_config = self.random_ltd_config()
-        random_ltd_queue = deque(
-            [x for x in sorted(random_ltd_config[RANDOM_LTD_LAYER_ID])])
+        random_ltd_queue = deque([x for x in sorted(random_ltd_config[RANDOM_LTD_LAYER_ID])])
         count = 0
         for name, layer in self.module.named_modules():
             if isinstance(layer, RandomLayerTokenDrop):
-                if len(random_ltd_queue) != 0 and str(
-                        random_ltd_queue[0]) in name:  ###[1,2,3]
-                    layer.init_config(random_ltd_config,
-                                      self.random_ltd_scheduler,
-                                      count)
+                if len(random_ltd_queue) != 0 and str(random_ltd_queue[0]) in name:  ###[1,2,3]
+                    layer.init_config(random_ltd_config, self.random_ltd_scheduler, count)
                     random_ltd_queue.popleft()
                     count += 1
 
         if random_ltd_config[RANDOM_LTD_LAYER_NUM] != count:
-            raise ValueError(
-                f'random_ltd_layer_num {random_ltd_config[RANDOM_LTD_LAYER_NUM]} must be \
+            raise ValueError(f'random_ltd_layer_num {random_ltd_config[RANDOM_LTD_LAYER_NUM]} must be \
                 equivalent to the len of random_ltd_layer_id {count}')
 
-        if random_ltd_config[RANDOM_LTD_LAYER_TOKEN_LR_SCHEDULE][
-                RANDOM_LTD_LAYER_TOKEN_LR_ENABLED]:
+        if random_ltd_config[RANDOM_LTD_LAYER_TOKEN_LR_SCHEDULE][RANDOM_LTD_LAYER_TOKEN_LR_ENABLED]:
             assert self.client_lr_scheduler is None
             raise ValueError(f'not yet support')
             #self.lr_scheduler = lr_schedules.WarmupLayerTokenDecayLR(self.optimizer, self.random_ltd_scheduler)
 
     def wall_clock_breakdown(self):
         return self._config.wall_clock_breakdown
 
@@ -663,29 +629,27 @@
 
     def autotuning_metric(self):
         return self._config.autotuning_config.metric
 
     def autotuning_profile_model_info(self):
         return self.autotuning_enabled(
         ) and self._config.autotuning_config.model_info and self._config.autotuning_config.model_info.get(
-            "profile",
-            False)
+            "profile", False)
 
     def sparse_gradients_enabled(self):
         return self._config.sparse_gradients_enabled
 
     def train_batch_size(self):
         return self._config.train_batch_size
 
     def train_micro_batch_size_per_gpu(self):
         return self._config.train_micro_batch_size_per_gpu
 
     def optimizer_name(self):
-        return (self.client_optimizer.__class__.__name__
-                if self.client_optimizer else self._config.optimizer_name)
+        return (self.client_optimizer.__class__.__name__ if self.client_optimizer else self._config.optimizer_name)
 
     def optimizer_params(self):
         return self._config.optimizer_params
 
     def optimizer_legacy_fusion(self):
         return self._config.optimizer_legacy_fusion
 
@@ -695,30 +659,23 @@
     def scheduler_params(self):
         return self._config.scheduler_params
 
     def quantize_training(self):
         return (
             self._config.compression_config[WEIGHT_QUANTIZATION][SHARED_PARAMETERS]
             [WEIGHT_QUANTIZE_IN_FORWARD_ENABLED],
-            self._config.compression_config[WEIGHT_QUANTIZATION][SHARED_PARAMETERS]
-            [WEIGHT_QUANTIZE_ENABLED],
-            self._config.compression_config[WEIGHT_QUANTIZATION][SHARED_PARAMETERS]
-            [WEIGHT_QUANTIZE_GROUPS],
+            self._config.compression_config[WEIGHT_QUANTIZATION][SHARED_PARAMETERS][WEIGHT_QUANTIZE_ENABLED],
+            self._config.compression_config[WEIGHT_QUANTIZATION][SHARED_PARAMETERS][WEIGHT_QUANTIZE_GROUPS],
             self._config.compression_config[WEIGHT_QUANTIZATION][SHARED_PARAMETERS]
             [WEIGHT_QUANTIZE_FP16_MIXED_QUANTIZE],
-            self._config.compression_config[WEIGHT_QUANTIZATION][SHARED_PARAMETERS]
-            [WEIGHT_QUANTIZE_CHANGE_RATIO],
-            self._config.compression_config[WEIGHT_QUANTIZATION][SHARED_PARAMETERS]
-            [WEIGHT_QUANTIZE_TYPE],
-            self._config.compression_config[WEIGHT_QUANTIZATION][SHARED_PARAMETERS]
-            [WEIGHT_QUANTIZE_ROUNDING],
-            self._config.compression_config[WEIGHT_QUANTIZATION][SHARED_PARAMETERS]
-            [WEIGHT_QUANTIZE_VERBOSE],
-            self._config.compression_config[WEIGHT_QUANTIZATION][SHARED_PARAMETERS]
-            [WEIGHT_QUANTIZE_KERNEL],
+            self._config.compression_config[WEIGHT_QUANTIZATION][SHARED_PARAMETERS][WEIGHT_QUANTIZE_CHANGE_RATIO],
+            self._config.compression_config[WEIGHT_QUANTIZATION][SHARED_PARAMETERS][WEIGHT_QUANTIZE_TYPE],
+            self._config.compression_config[WEIGHT_QUANTIZATION][SHARED_PARAMETERS][WEIGHT_QUANTIZE_ROUNDING],
+            self._config.compression_config[WEIGHT_QUANTIZATION][SHARED_PARAMETERS][WEIGHT_QUANTIZE_VERBOSE],
+            self._config.compression_config[WEIGHT_QUANTIZATION][SHARED_PARAMETERS][WEIGHT_QUANTIZE_KERNEL],
         )
 
     def zero_optimization(self):
         return self._config.zero_enabled
 
     def zero_allow_untested_optimizer(self):
         return self._config.zero_allow_untested_optimizer
@@ -736,18 +693,15 @@
         return self._config.zero_config.offload_optimizer
 
     def zero_offload_param(self):
         return self._config.zero_config.offload_param
 
     def zero_use_cpu_optimizer(self):
         if self._config.zero_config.offload_optimizer is not None:
-            return self._config.zero_config.offload_optimizer.device in [
-                OffloadDeviceEnum.cpu,
-                OffloadDeviceEnum.nvme
-            ]
+            return self._config.zero_config.offload_optimizer.device in [OffloadDeviceEnum.cpu, OffloadDeviceEnum.nvme]
         return False
 
     def zero_cpu_offload(self):
         if self._config.zero_config.offload_optimizer is not None:
             return self._config.zero_config.offload_optimizer.device == OffloadDeviceEnum.cpu
         return False
 
@@ -836,18 +790,17 @@
         return self._config.load_universal_checkpoint
 
     @property
     def communication_data_type(self):
         res = self._config.communication_data_type
         if res is not None:
             return res
-        elif self.fp16_enabled() or self.zero_optimization_stage():
+
+        if self.fp16_enabled():
             return torch.float16
-        elif self.bfloat16_enabled():
-            return torch.bfloat16
 
         return torch.float32
 
     def postscale_gradients(self):
         return not self._config.prescale_gradients
 
     def gradient_predivide_factor(self):
@@ -900,85 +853,75 @@
 
         return (model_dtype, grad_accum_dtype)
 
     def _configure_lr_scheduler(self, client_lr_scheduler):
         # First check for scheduler in json configuration
         lr_scheduler = self._scheduler_from_config(self.optimizer)
         if lr_scheduler:
-            log_dist(
-                f"DeepSpeed using configured LR scheduler = {self.scheduler_name()}",
-                ranks=[0])
+            log_dist(f"DeepSpeed using configured LR scheduler = {self.scheduler_name()}", ranks=[0])
             self.lr_scheduler = lr_scheduler
         else:
             if isinstance(client_lr_scheduler, Callable):
-                log_dist('DeepSpeed using client callable to create LR scheduler',
-                         ranks=[0])
+                log_dist('DeepSpeed using client callable to create LR scheduler', ranks=[0])
                 self.lr_scheduler = client_lr_scheduler(self.basic_optimizer)
             else:
                 log_dist('DeepSpeed using client LR scheduler', ranks=[0])
                 self.lr_scheduler = client_lr_scheduler
 
         log_dist(f'DeepSpeed LR Scheduler = {self.lr_scheduler}', ranks=[0])
 
     def _configure_checkpointing(self, dist_init_required):
         self.checkpoint_engine = TorchCheckpointEngine()
 
         if self._config is not None and self._config.nebula_config.enabled:
             try:
                 from deepspeed.runtime.checkpoint_engine.nebula_checkpoint_engine import \
                     NebulaCheckpointEngine
-                self.checkpoint_engine = NebulaCheckpointEngine(
-                    config_params=self._config.nebula_config)
+                self.checkpoint_engine = NebulaCheckpointEngine(config_params=self._config.nebula_config)
             except ImportError as err:
-                logger.error(
-                    f"No torch_nebula was found! Will fall back to torch.save. Details: {err}"
-                )
+                logger.error(f"No torch_nebula was found! Will fall back to torch.save. Details: {err}")
                 self.checkpoint_engine = TorchCheckpointEngine()
 
         dp_rank = self.global_rank
         if self.mpu:
             dp_rank = self.mpu.get_data_parallel_rank()
 
         rank = self.local_rank if self.use_node_local_storage() else dp_rank
 
         # only the first data parallel process needs to store the model checkpoint
         # if you want to use node local storage this must be done by rank 0 on each
         # node
-        self.save_non_zero_checkpoint = (
-            rank == 0) or self.zero_optimization_partition_weights()
+        self.save_non_zero_checkpoint = (rank == 0) or self.zero_optimization_partition_weights()
 
         if self.zero_optimization() or self.bfloat16_enabled():
             param_rank = dist.get_rank(group=self.optimizer.dp_process_group)
 
             # Only the first parameter parallel process needs to store the
             # optimizer state checkpoints for zero
             self.save_zero_checkpoint = param_rank == dp_rank
 
     def _scheduler_from_config(self, optimizer):
         scheduler_name = self.scheduler_name()
         if scheduler_name is not None:
             if hasattr(lr_schedules, scheduler_name):
                 scheduler = getattr(lr_schedules, scheduler_name)
             else:
-                assert hasattr(
-                    torch.optim.lr_scheduler, scheduler_name
-                ), f"DeepSpeed does not recognize LR scheduler {scheduler_name}"
+                assert hasattr(torch.optim.lr_scheduler,
+                               scheduler_name), f"DeepSpeed does not recognize LR scheduler {scheduler_name}"
 
                 scheduler = getattr(torch.optim.lr_scheduler, scheduler_name)
 
             scheduler_params = self.scheduler_params()
             instantiated_scheduler = scheduler(optimizer, **scheduler_params)
             return instantiated_scheduler
         else:
             return None
 
     def _set_distributed_vars(self, args):
-        device_rank = args.device_rank if args is not None and hasattr(
-            args,
-            'device_rank') else self.local_rank
+        device_rank = args.device_rank if args is not None and hasattr(args, 'device_rank') else self.local_rank
         if device_rank >= 0:
             get_accelerator().set_device(device_rank)
             self.device = torch.device(get_accelerator().device_name(), device_rank)
             self.world_size = dist.get_world_size()
             self.global_rank = dist.get_rank()
         else:
             self.world_size = 1
@@ -999,56 +942,31 @@
                 "not sure how to proceed as we're seeing conflicting local rank info."
             os.environ['LOCAL_RANK'] = local_rank
 
         self.local_rank = int(os.environ['LOCAL_RANK'])
         if hasattr(args, 'local_rank'):
             args.local_rank = self.local_rank
 
-        if self.config is None:
-            self.config = (args.deepspeed_config
-                           if hasattr(args,
-                                      "deepspeed_config") else None)
-        self._config = DeepSpeedConfig(self.config, mpu)
-
     # Validate command line arguments
     def _do_args_sanity_check(self, args):
-        if hasattr(args, "deepscale_config") and args.deepscale_config is not None:
-            logger.warning(
-                "************ --deepscale_config is deprecated, please use --deepspeed_config ************"
-            )
-            if hasattr(args, "deepspeed_config"):
-                assert (
-                    args.deepspeed_config is None
-                ), "Not sure how to proceed, we were given both a deepscale_config and deepspeed_config"
-            args.deepspeed_config = args.deepscale_config
-
         assert "LOCAL_RANK" in os.environ or "OMPI_COMM_WORLD_LOCAL_RANK" in os.environ, "DeepSpeed requires the LOCAL_RANK environment " \
             "variable, it is set by the deepspeed launcher, deepspeed.init_distributed, or the torch's launcher. If using a " \
             "different launcher please ensure LOCAL_RANK is set prior to initializing deepspeed."
 
         if hasattr(args, 'local_rank') and args.local_rank != None:
-            assert isinstance(
-                args.local_rank, int), f"args.local_rank of {args.local_rank} is an unknown type {type(args.local_rank)}"
+            assert isinstance(args.local_rank,
+                              int), f"args.local_rank of {args.local_rank} is an unknown type {type(args.local_rank)}"
             if args.local_rank >= 0:
                 env_local_rank = int(os.environ.get("LOCAL_RANK"))
                 assert (
                     env_local_rank == args.local_rank
                 ), f"Mismatch in local rank setting, args.local_rank={args.local_rank} but env['LOCAL_RANK']={env_local_rank}."
 
-        if self.config is None:
-            assert (
-                hasattr(
-                    args, "deepspeed_config") and args.deepspeed_config is not None
-            ), "DeepSpeed requires --deepspeed_config to specify configuration file"
-
     def _is_supported_optimizer(self, optimizer_name):
-        return (optimizer_name in DEEPSPEED_OPTIMIZERS
-                or getattr(torch.optim,
-                           optimizer_name,
-                           None) is not None)
+        return (optimizer_name in DEEPSPEED_OPTIMIZERS or getattr(torch.optim, optimizer_name, None) is not None)
 
     def _supported_optims(self):
         FairseqOptimizer = None
         try:
             from fairseq.optim.fairseq_optimizer import FairseqOptimizer
         except ImportError:
             pass
@@ -1065,82 +983,69 @@
         expected_optim_types += [type(None), Callable]
         assert isinstance(self.client_optimizer, tuple(expected_optim_types)), \
             f'Client Optimizer is of unexpected type {type(self.client_optimizer)}'
 
         if not self.client_optimizer:
             if self.optimizer_name() is not None:
                 assert self._is_supported_optimizer(
-                    self.optimizer_name()
-                ), "{} is not a supported DeepSpeed Optimizer".format(
-                    self.optimizer_name()
-                )
+                    self.optimizer_name()), "{} is not a supported DeepSpeed Optimizer".format(self.optimizer_name())
 
-        if (self.optimizer_name() == LAMB_OPTIMIZER
-                or self.optimizer_name() == ONEBIT_LAMB_OPTIMIZER):
-            assert (
-                self.dynamic_loss_scale()
-            ), "DeepSpeed {} optimizer requires dynamic loss scaling".format(
-                self.optimizer_name()
-            )
+        if (self.optimizer_name() == LAMB_OPTIMIZER or self.optimizer_name() == ONEBIT_LAMB_OPTIMIZER):
+            assert (self.dynamic_loss_scale()), "DeepSpeed {} optimizer requires dynamic loss scaling".format(
+                self.optimizer_name())
 
         # Detect invalid combinations of client optimizer and client scheduler
         if isinstance(self.client_lr_scheduler, _LRScheduler):
             assert isinstance(self.client_optimizer, Optimizer), \
                 f'Client Optimizer (type = {type(self.client_optimizer)} is not instantiated but Client LR Scheduler is instantiated'
 
     def _broadcast_model(self):
+
         def is_replicated(p):
             if hasattr(p, "ds_status") and p.ds_status is not ZeroParamStatus.AVAILABLE:
                 return False
             return True
 
         for p in self.module.parameters():
             # Broadcast the model for different parameters
             if is_moe_param(p):
                 if torch.is_tensor(p) and is_replicated(p):
                     dist.broadcast(p,
                                    groups._get_expert_broadcast_src_rank(p.group_name),
                                    group=self.expert_data_parallel_group[p.group_name])
             else:
                 if torch.is_tensor(p) and is_replicated(p):
-                    dist.broadcast(p,
-                                   groups._get_broadcast_src_rank(),
-                                   group=self.data_parallel_group)
+                    dist.broadcast(p, groups._get_broadcast_src_rank(), group=self.data_parallel_group)
 
     @staticmethod
     def __check_params(model: Module, dtype: torch.dtype) -> None:
         return
-        if not all(param.dtype == dtype
-                   for param in model.parameters()) and dist.get_rank() == 0:
-            raise ValueError(
-                f"{dtype} is enabled but the following parameters have dtype that is "
-                f"not {dtype}: "
-                f"{[(n, p.dtype) for n, p in model.named_parameters() if p.dtype != dtype]}"
-            )
+        if not all(param.dtype == dtype for param in model.parameters()) and dist.get_rank() == 0:
+            raise ValueError(f"{dtype} is enabled but the following parameters have dtype that is "
+                             f"not {dtype}: "
+                             f"{[(n, p.dtype) for n, p in model.named_parameters() if p.dtype != dtype]}")
 
     def _set_client_model(self, model):
         # register client model in _modules so that nn.module methods work correctly
         modules = self.__dict__.get('_modules')
         modules['module'] = model
         # register module attribute in engine but avoid getattr
         self.__dict__['module'] = model
 
     def _configure_distributed_model(self, model):
         self._set_client_model(model)
 
         if self.fp16_enabled():
             if self.zero_optimization_partition_weights() and any(
-                [hasattr(param,
-                         "ds_id") for param in self.module.parameters()]):
+                [hasattr(param, "ds_id") for param in self.module.parameters()]):
                 self.__check_params(self.module, torch.half)
             self.module.half()
         elif self.bfloat16_enabled():
             if self.zero_optimization_partition_weights() and any(
-                    hasattr(param,
-                            'ds_id') for param in self.module.parameters()):
+                    hasattr(param, 'ds_id') for param in self.module.parameters()):
                 self.__check_params(self.module, torch.bfloat16)
             self.module.bfloat16()
         else:
             self.__check_params(self.module, torch.float)
 
         if not self.dont_change_device:
             self.module.to(self.device)
@@ -1186,16 +1091,15 @@
         for name, param in self.module.named_parameters():
             param_id = id(param)
 
             def ids_list(group):
                 return [id(param) for param in group]
 
             occurrence = sum([
-                ids_list(group['params']).count(param_id)
-                if param_id in ids_list(group['params']) else 0
+                ids_list(group['params']).count(param_id) if param_id in ids_list(group['params']) else 0
                 for group in optimizer.param_groups
             ])
             assert occurrence <= 1, f"Parameter with name: {name} occurs multiple times in optimizer.param_groups. Make sure it only appears once to prevent undefined behaviour."
 
     def _do_optimizer_sanity_check(self, basic_optimizer):
         model_dtype, grad_accum_dtype = self.get_data_types()
         zero_enabled = self.zero_optimization()
@@ -1207,117 +1111,99 @@
         if zero_enabled:
             if not is_zero_supported_optimizer(basic_optimizer):
                 assert (
                     self.zero_allow_untested_optimizer()
                 ), 'You are using an untested ZeRO Optimizer. Please add <"zero_allow_untested_optimizer": true> in the configuration file to use it.'
 
                 if self.global_rank == 0:
-                    logger.warning(
-                        "**** You are using ZeRO with an untested optimizer, proceed with caution *****"
-                    )
+                    logger.warning("**** You are using ZeRO with an untested optimizer, proceed with caution *****")
 
             if model_dtype == torch.bfloat16 and grad_accum_dtype == torch.float32 and self.zero_optimization_stage(
             ) == 1:
                 return BFLOAT16
 
             if model_dtype != grad_accum_dtype:
                 raise NotImplementedError(
-                    "Model data type and gradient accumulation data type must be equal to use ZeRO"
-                )
+                    "Model data type and gradient accumulation data type must be equal to use ZeRO")
             return ZERO_OPTIMIZATION
         elif amp_enabled:
             if model_dtype != grad_accum_dtype:
                 raise NotImplementedError(
-                    "Model data type and gradient accumulation data type must be equal to use Amp"
-                )
+                    "Model data type and gradient accumulation data type must be equal to use Amp")
             if model_dtype == torch.bfloat16 or model_dtype == torch.float16:
-                raise NotImplementedError(
-                    "Cannot enable both amp with (legacy) fp16 or bfloat16 mode")
+                raise NotImplementedError("Cannot enable both amp with (legacy) fp16 or bfloat16 mode")
             try:
                 logger.info("Initializing Apex amp from: {}".format(amp.__path__))
             except NameError:
                 # If apex/amp is available it will be imported above
-                raise RuntimeError(
-                    "Unable to import apex/amp, please make sure it is installed")
+                raise RuntimeError("Unable to import apex/amp, please make sure it is installed")
             return AMP
         # data type checks
         elif model_dtype == grad_accum_dtype:
             if model_dtype == torch.bfloat16:
                 raise NotImplementedError(
                     "Bfloat16 wrapper must use a gradient accumulation type of fp32, enable ZeRO to use Bfloat16 gradient accumulation"
                 )
             if model_dtype == torch.float16:
                 return FP16
             # else optimizer_wrapper = None
         elif model_dtype == torch.bfloat16 and grad_accum_dtype == torch.float32:
             return BFLOAT16
         else:
-            raise NotImplementedError(
-                "unsupported mix of model dtype and gradient accummulation type")
+            raise NotImplementedError("unsupported mix of model dtype and gradient accummulation type")
 
         return None
 
     # Configure optimizer
     def _configure_optimizer(self, client_optimizer, model_parameters):
         if client_optimizer is not None:
             if isinstance(client_optimizer, tuple(self._supported_optims())):
                 client_optimizer.param_groups[:] = [
                     pg for pg in client_optimizer.param_groups if len(pg["params"]) != 0
                 ]
-                log_dist(
-                    "Removing param_group that has no 'params' in the client Optimizer",
-                    ranks=[0])
+                log_dist("Removing param_group that has no 'params' in the client Optimizer", ranks=[0])
 
                 basic_optimizer = client_optimizer
                 log_dist('Using client Optimizer as basic optimizer', ranks=[0])
             else:
                 basic_optimizer = client_optimizer(model_parameters)
                 log_dist('Using client callable to create basic optimizer', ranks=[0])
 
-            if self.zero_use_cpu_optimizer() and not isinstance(
-                    basic_optimizer,
-                    deepspeed.ops.adam.DeepSpeedCPUAdam):
+            if self.zero_use_cpu_optimizer() and not isinstance(basic_optimizer, deepspeed.ops.adam.DeepSpeedCPUAdam):
                 if self.zero_force_ds_cpu_optimizer():
                     msg = f'You are using ZeRO-Offload with a client provided optimizer ({type(basic_optimizer)}) which in most cases will yield poor performance. Please either use deepspeed.ops.adam.DeepSpeedCPUAdam or set an optimizer in your ds-config (https://www.deepspeed.ai/docs/config-json/#optimizer-parameters). If you really want to use a custom optimizer w. ZeRO-Offload and understand the performance impacts you can also set <"zero_force_ds_cpu_optimizer": false> in your configuration file.'
                     raise ZeRORuntimeException(msg)
         else:
             basic_optimizer = self._configure_basic_optimizer(model_parameters)
-            log_dist(
-                f"Using DeepSpeed Optimizer param name {self.optimizer_name()} as basic optimizer",
-                ranks=[0])
+            log_dist(f"Using DeepSpeed Optimizer param name {self.optimizer_name()} as basic optimizer", ranks=[0])
 
         self._check_for_duplicates(basic_optimizer)
 
         self.basic_optimizer = basic_optimizer
-        log_dist("DeepSpeed Basic Optimizer = {}".format(
-            basic_optimizer.__class__.__name__),
-                 ranks=[0])
+        log_dist("DeepSpeed Basic Optimizer = {}".format(basic_optimizer.__class__.__name__), ranks=[0])
 
         optimizer_wrapper = self._do_optimizer_sanity_check(basic_optimizer)
 
         if optimizer_wrapper == ZERO_OPTIMIZATION:
             self.optimizer = self._configure_zero_optimizer(basic_optimizer)
         elif optimizer_wrapper == AMP:
             amp_params = self.amp_params()
             log_dist(f"Initializing AMP with these params: {amp_params}", ranks=[0])
-            model, self.optimizer = amp.initialize(
-                self.module, basic_optimizer, **amp_params
-            )
+            model, self.optimizer = amp.initialize(self.module, basic_optimizer, **amp_params)
             self._set_client_model(model)
             self._broadcast_model()
             # TODO: maybe need to broadcast experts differently?
         elif optimizer_wrapper == FP16:
             self.optimizer = self._configure_fp16_optimizer(basic_optimizer)
         elif optimizer_wrapper == BFLOAT16:
             self.optimizer = self._configure_bf16_optimizer(basic_optimizer)
         else:
             self.optimizer = basic_optimizer
 
-        log_dist("DeepSpeed Final Optimizer = {}".format(self.optimizer_name()),
-                 ranks=[0])
+        log_dist("DeepSpeed Final Optimizer = {}".format(self.optimizer_name()), ranks=[0])
 
         self.compression_scheduler = self._configure_compression_scheduler()
         self.quantizer = self._configure_quantization()
 
     def _configure_basic_optimizer(self, model_parameters):
         optimizer_parameters = self.optimizer_params()
         if optimizer_parameters is None:
@@ -1329,30 +1215,26 @@
             )
 
         if self.optimizer_name() in [ADAGRAD_OPTIMIZER, ADAM_OPTIMIZER, ADAMW_OPTIMIZER]:
             torch_adam = optimizer_parameters.pop(TORCH_ADAM_PARAM, False)
             adam_w_mode = optimizer_parameters.pop(ADAM_W_MODE, ADAM_W_MODE_DEFAULT)
 
             # Optimizer name of Adam forces AdamW logic unless adam_w_mode is explicitly set
-            effective_adam_w_mode = self.optimizer_name(
-            ) == ADAMW_OPTIMIZER or adam_w_mode
+            effective_adam_w_mode = self.optimizer_name() == ADAMW_OPTIMIZER or adam_w_mode
 
             if torch_adam:
                 if not effective_adam_w_mode:
-                    optimizer = torch.optim.Adam(model_parameters,
-                                                 **optimizer_parameters)
+                    optimizer = torch.optim.Adam(model_parameters, **optimizer_parameters)
                 else:
-                    optimizer = torch.optim.AdamW(model_parameters,
-                                                  **optimizer_parameters)
+                    optimizer = torch.optim.AdamW(model_parameters, **optimizer_parameters)
             else:
                 if self.zero_use_cpu_optimizer():
                     if self.optimizer_name() == ADAGRAD_OPTIMIZER:
                         from deepspeed.ops.adagrad import DeepSpeedCPUAdagrad
-                        optimizer = DeepSpeedCPUAdagrad(model_parameters,
-                                                        **optimizer_parameters)
+                        optimizer = DeepSpeedCPUAdagrad(model_parameters, **optimizer_parameters)
                     else:
                         from deepspeed.ops.adam import DeepSpeedCPUAdam
                         optimizer = DeepSpeedCPUAdam(model_parameters,
                                                      **optimizer_parameters,
                                                      adamw_mode=effective_adam_w_mode)
                 else:
                     from deepspeed.ops.adam import FusedAdam
@@ -1369,34 +1251,29 @@
             optimizer = FusedLamb(model_parameters, **optimizer_parameters)
         elif self.optimizer_name() == ONEBIT_ADAM_OPTIMIZER:
             assert not self.zero_optimization(), "1bit-Adam is not compatible with ZeRO"
             from deepspeed.runtime.fp16.onebit.adam import OnebitAdam
 
             optimizer = OnebitAdam(model_parameters, self, **optimizer_parameters)
             if not self.fp16_enabled():
-                logger.warning(
-                    f"Currently the convergence of 1-bit Adam is only verified under FP16"
-                )
+                logger.warning(f"Currently the convergence of 1-bit Adam is only verified under FP16")
         elif self.optimizer_name() == ZERO_ONE_ADAM_OPTIMIZER:
             assert not self.zero_optimization(), "0/1 Adam is not compatible with ZeRO"
             from deepspeed.runtime.fp16.onebit.zoadam import ZeroOneAdam
 
             optimizer = ZeroOneAdam(model_parameters, self, **optimizer_parameters)
             if not self.fp16_enabled():
-                logger.warning(
-                    f'Currently the convergence of 0/1 Adam is only verified under FP16')
+                logger.warning(f'Currently the convergence of 0/1 Adam is only verified under FP16')
         elif self.optimizer_name() == ONEBIT_LAMB_OPTIMIZER:
             assert not self.zero_optimization(), "1bit-Lamb is not compatible with ZeRO"
             from deepspeed.runtime.fp16.onebit.lamb import OnebitLamb
 
             optimizer = OnebitLamb(model_parameters, self, **optimizer_parameters)
             if not self.fp16_enabled():
-                logger.warning(
-                    f"Currently the convergence of 1-bit Lamb is only verified under FP16"
-                )
+                logger.warning(f"Currently the convergence of 1-bit Lamb is only verified under FP16")
         else:
             torch_optimizer = getattr(torch.optim, self.optimizer_name())
             optimizer = torch_optimizer(model_parameters, **optimizer_parameters)
         return optimizer
 
     def _configure_compression_scheduler(self):
         return compression_scheduler(self.module, self._config.compression_config)
@@ -1413,15 +1290,16 @@
             q_change_ratio,
             q_type,
             q_rounding,
             q_verbose,
             use_quantizer_kernel,
         ) = self.quantize_training()
         if quantize_enabled and not quantize_weight_in_forward:
-            assert self.fp16_enabled(), "MoQ (quantize in optimization step) weight quantization is only supported for FP16"
+            assert self.fp16_enabled(
+            ), "MoQ (quantize in optimization step) weight quantization is only supported for FP16"
         quantizer = None
         if quantize_enabled and not quantize_weight_in_forward:
             from deepspeed.runtime.quantize import Quantizer
 
             quantizer = Quantizer(
                 q_groups,
                 q_mixed_fp16,
@@ -1457,29 +1335,26 @@
                     mpu=self.mpu,
                     clip_grad=clip_grad,
                     fused_adam_legacy=self.optimizer_legacy_fusion(),
                     timers=timers,
                     has_moe_layers=self.has_moe_layers,
                 )
             else:
-                log_dist(
-                    f'Creating fp16 optimizer with static loss scale: {self.loss_scale()}',
-                    ranks=[0])
+                log_dist(f'Creating fp16 optimizer with static loss scale: {self.loss_scale()}', ranks=[0])
                 optimizer = FP16_Optimizer(
                     optimizer,
                     deepspeed=self,
                     static_loss_scale=self.loss_scale(),
                     mpu=self.mpu,
                     clip_grad=clip_grad,
                     fused_adam_legacy=self.optimizer_legacy_fusion(),
                     has_moe_layers=self.has_moe_layers,
                 )
         else:
-            log_dist(f'Creating fp16 unfused optimizer with dynamic loss scale',
-                     ranks=[0])
+            log_dist(f'Creating fp16 unfused optimizer with dynamic loss scale', ranks=[0])
             optimizer = FP16_UnfusedOptimizer(
                 optimizer,
                 deepspeed=self,
                 static_loss_scale=self.loss_scale(),
                 dynamic_loss_scale=self.dynamic_loss_scale(),
                 dynamic_loss_args=dynamic_loss_args,
                 mpu=self.mpu,
@@ -1494,22 +1369,21 @@
 
         if optimizer is None:
             optimizer = DummyOptim(list(self.module.parameters()))
 
         log_dist('Creating BF16 optimizer', ranks=[0])
 
         timers = self.timers if self.wall_clock_breakdown() else None
-        optimizer = BF16_Optimizer(
-            optimizer,
-            self.param_names,
-            mpu=self.mpu,
-            clip_grad=clip_grad,
-            allgather_bucket_size=self.zero_allgather_bucket_size(),
-            dp_process_group=self.data_parallel_group,
-            timers=timers)
+        optimizer = BF16_Optimizer(optimizer,
+                                   self.param_names,
+                                   mpu=self.mpu,
+                                   clip_grad=clip_grad,
+                                   allgather_bucket_size=self.zero_allgather_bucket_size(),
+                                   dp_process_group=self.data_parallel_group,
+                                   timers=timers)
 
         return optimizer
 
     def _configure_zero_optimizer(self, optimizer):
         zero_stage = self.zero_optimization_stage()
         model_dtype, grad_accum_dtype = self.get_data_types()
         timers = self.timers if self.wall_clock_breakdown() else None
@@ -1524,81 +1398,73 @@
 
         if zero_stage <= ZeroStageEnum.gradients:
             overlap_comm = self.zero_overlap_comm()
             contiguous_gradients = self.zero_contiguous_gradients()
             round_robin_gradients = self.zero_round_robin_gradients()
             assert not isinstance(optimizer, DummyOptim), "zero stage {} requires an optimizer".format(zero_stage)
 
-            log_dist(f'Creating {model_dtype} ZeRO stage {zero_stage} optimizer',
-                     ranks=[0])
+            log_dist(f'Creating {model_dtype} ZeRO stage {zero_stage} optimizer', ranks=[0])
             # Overlap and contiguous grads are meaningless in stage 1 and are ignored
             if zero_stage == ZeroStageEnum.optimizer_states:
                 overlap_comm = False
                 round_robin_gradients = False
                 # Non-MoE requires contiguous grads to be disabled w. stage 1
                 if not self.has_moe_layers:
                     contiguous_gradients = False
 
             if isinstance(self.module, PipelineModule):
                 if overlap_comm:
-                    logger.warning(
-                        "Pipeline parallelism does not support overlapped communication, will be disabled."
-                    )
+                    logger.warning("Pipeline parallelism does not support overlapped communication, will be disabled.")
                     overlap_comm = False
             optimizer = DeepSpeedZeroOptimizer(
                 optimizer,
                 self.param_names,
                 timers=timers,
                 static_loss_scale=self.loss_scale(),
                 dynamic_loss_scale=self.dynamic_loss_scale(),
                 dynamic_loss_args=self.dynamic_loss_scale_args(),
                 clip_grad=self.gradient_clipping(),
                 contiguous_gradients=contiguous_gradients,
                 reduce_bucket_size=self.zero_reduce_bucket_size(),
                 allgather_bucket_size=self.zero_allgather_bucket_size(),
                 dp_process_group=self.data_parallel_group,
-                expert_parallel_group=self.expert_parallel_group
-                if self.has_moe_layers else None,
-                expert_data_parallel_group=self.expert_data_parallel_group
-                if self.has_moe_layers else None,
+                expert_parallel_group=self.expert_parallel_group if self.has_moe_layers else None,
+                expert_data_parallel_group=self.expert_data_parallel_group if self.has_moe_layers else None,
                 reduce_scatter=self.zero_reduce_scatter(),
                 overlap_comm=overlap_comm,
                 cpu_offload=self.zero_cpu_offload(),
                 mpu=self.mpu,
                 postscale_gradients=self.postscale_gradients(),
                 gradient_predivide_factor=self.gradient_predivide_factor(),
                 gradient_accumulation_steps=self.gradient_accumulation_steps(),
                 ignore_unused_parameters=self.zero_ignore_unused_parameters(),
                 partition_grads=zero_stage == ZeroStageEnum.gradients,
                 round_robin_gradients=round_robin_gradients,
                 has_moe_layers=self.has_moe_layers,
-                fp16_master_weights_and_gradients=self.fp16_master_weights_and_gradients(
-                ),
+                fp16_master_weights_and_gradients=self.fp16_master_weights_and_gradients(),
                 communication_data_type=self.communication_data_type,
                 elastic_checkpoint=self.zero_elastic_checkpoint())
 
         elif zero_stage == ZeroStageEnum.weights:
             assert not self.has_moe_layers, "MoE not supported with Stage 3"
             if isinstance(optimizer, DummyOptim):
                 log_dist("Creating ZeRO Offload", ranks=[0])
-                optimizer = DeepSpeedZeRoOffload(
-                    self.module,
-                    timers=timers,
-                    ds_config=self.config,
-                    overlap_comm=self.zero_overlap_comm(),
-                    prefetch_bucket_size=self.zero_prefetch_bucket_size(),
-                    max_reuse_distance=self.zero_max_reuse_distance(),
-                    max_live_parameters=self.zero_max_live_parameters(),
-                    param_persistence_threshold=self.zero_param_persistence_threshold(),
-                    model_persistence_threshold=self.zero_model_persistence_threshold(),
-                    offload_param_config=self.zero_offload_param(),
-                    mpu=self.mpu)
+                optimizer = DeepSpeedZeRoOffload(self.module,
+                                                 timers=timers,
+                                                 ds_config=self.config,
+                                                 overlap_comm=self.zero_overlap_comm(),
+                                                 prefetch_bucket_size=self.zero_prefetch_bucket_size(),
+                                                 max_reuse_distance=self.zero_max_reuse_distance(),
+                                                 max_live_parameters=self.zero_max_live_parameters(),
+                                                 param_persistence_threshold=self.zero_param_persistence_threshold(),
+                                                 model_persistence_threshold=self.zero_model_persistence_threshold(),
+                                                 offload_param_config=self.zero_offload_param(),
+                                                 mpu=self.mpu)
             else:
-                log_dist(f'Creating {model_dtype} ZeRO stage {zero_stage} optimizer',
-                         ranks=[0])
+                log_dist(f'Creating {model_dtype} ZeRO stage {zero_stage} optimizer', ranks=[0])
                 from deepspeed.runtime.zero.stage3 import DeepSpeedZeroOptimizer_Stage3
                 optimizer = DeepSpeedZeroOptimizer_Stage3(
                     self.module,
                     optimizer,
                     timers=timers,
                     ds_config=self.config,
                     static_loss_scale=self.loss_scale(),
@@ -1654,17 +1520,15 @@
 
     @staticmethod
     def is_map_style_dataset(obj):
         return hasattr(obj, "__getitem__") and hasattr(obj, "__len__")
 
     @staticmethod
     def is_iterable_style_dataset(obj):
-        return isinstance(obj,
-                          torch.utils.data.IterableDataset
-                          )  # hasattr(obj, "__iter__") should work as well
+        return isinstance(obj, torch.utils.data.IterableDataset)  # hasattr(obj, "__iter__") should work as well
 
     def dataloader_drop_last(self):
         return self._config.dataloader_drop_last
 
     def was_step_applied(self) -> bool:
         """Returns True if the latest ``step()`` produced in parameter updates.
         Note that a ``False`` return is not an error condition. Steps are frequently
@@ -1679,16 +1543,15 @@
                      dataset,
                      batch_size=None,
                      route=ROUTE_TRAIN,
                      pin_memory=True,
                      data_sampler=None,
                      collate_fn=None,
                      num_local_io_workers=None):
-        if not (self.is_map_style_dataset(dataset)
-                or self.is_iterable_style_dataset(dataset)):
+        if not (self.is_map_style_dataset(dataset) or self.is_iterable_style_dataset(dataset)):
             raise ValueError("Training data must be a torch Dataset")
 
         if batch_size is None:
             batch_size = self.train_micro_batch_size_per_gpu()
 
         if collate_fn is None:
             collate_fn = self.collate_fn
@@ -1712,41 +1575,34 @@
                 rank=data_parallel_rank,
                 shuffle=False,
             )
 
         deepspeed_dataloader_config = {}
         if self.curriculum_learning_enabled():
             deepspeed_dataloader_config = {
-                CURRICULUM_LEARNING:
-                self.curriculum_learning_enabled(),
-                DATA_EFFICIENCY:
-                self.data_efficiency_config(),
-                DATA_PARALLEL_GROUP:
-                self.data_parallel_group,
-                GRADIENT_ACCUMULATION_STEPS:
-                self.gradient_accumulation_steps(),
-                GLOBAL_RANK:
-                self.global_rank,
-                DATA_SAMPLING_NUM_WORKERS:
-                self.data_sampling_config()[DATA_SAMPLING_NUM_WORKERS]
+                CURRICULUM_LEARNING: self.curriculum_learning_enabled(),
+                DATA_EFFICIENCY: self.data_efficiency_config(),
+                DATA_PARALLEL_GROUP: self.data_parallel_group,
+                GRADIENT_ACCUMULATION_STEPS: self.gradient_accumulation_steps(),
+                GLOBAL_RANK: self.global_rank,
+                DATA_SAMPLING_NUM_WORKERS: self.data_sampling_config()[DATA_SAMPLING_NUM_WORKERS]
             }
 
-        return DeepSpeedDataLoader(
-            dataset=dataset,
-            batch_size=batch_size,
-            pin_memory=pin_memory,
-            collate_fn=collate_fn,
-            local_rank=self.local_rank,
-            tput_timer=deepspeed_io_timer,
-            num_local_io_workers=num_local_io_workers,
-            data_sampler=data_sampler,
-            data_parallel_world_size=data_parallel_world_size,
-            data_parallel_rank=data_parallel_rank,
-            dataloader_drop_last=self.dataloader_drop_last(),
-            deepspeed_dataloader_config=deepspeed_dataloader_config)
+        return DeepSpeedDataLoader(dataset=dataset,
+                                   batch_size=batch_size,
+                                   pin_memory=pin_memory,
+                                   collate_fn=collate_fn,
+                                   local_rank=self.local_rank,
+                                   tput_timer=deepspeed_io_timer,
+                                   num_local_io_workers=num_local_io_workers,
+                                   data_sampler=data_sampler,
+                                   data_parallel_world_size=data_parallel_world_size,
+                                   data_parallel_rank=data_parallel_rank,
+                                   dataloader_drop_last=self.dataloader_drop_last(),
+                                   deepspeed_dataloader_config=deepspeed_dataloader_config)
 
     def train(self, mode=True):
         r""""""
 
         self.warn_unscaled_loss = True
         self.module.train(mode)
 
@@ -1765,17 +1621,15 @@
                 if isinstance(l, torch.Tensor):
                     scaled_loss.append(l / self.gradient_accumulation_steps())
                 else:
                     scaled_loss.append(l)
         else:
             scaled_loss = prescaled_loss
             if self.warn_unscaled_loss:
-                logger.warning(
-                    f"DeepSpeed unable to scale loss because of type: {type(prescaled_loss)}"
-                )
+                logger.warning(f"DeepSpeed unable to scale loss because of type: {type(prescaled_loss)}")
                 self.warn_unscaled_loss = False
 
         return scaled_loss
 
     @instrument_w_nvtx
     def forward(self, *inputs, **kwargs):
         r"""Execute forward propagation
@@ -1785,17 +1639,16 @@
         """
 
         if self.autotuning_profile_model_info():
             ma = get_ma_status()
         else:
             see_memory_usage("Engine before forward", force=self.memory_breakdown())
 
-        flops_profiler_active = (self.flops_profiler_enabled() and self.global_steps
-                                 == self.flops_profiler_profile_step()
-                                 and self.global_rank == 0)
+        flops_profiler_active = (self.flops_profiler_enabled()
+                                 and self.global_steps == self.flops_profiler_profile_step() and self.global_rank == 0)
 
         # used to check quantization happens at step 0!
         if self.global_steps == 0 and hasattr(self, "compression_scheduler"):
             self.compression_scheduler.step(step_zero_check=True)
             if self.quantizer:
                 tensor_to_quantize = self.optimizer.bit16_groups if self.zero_optimization_stage(
                 ) == 2 else self.optimizer.fp16_groups
@@ -1816,28 +1669,24 @@
 
         if self.__class__.__name__ != "PipelineEngine":
             # TODO: The above if condition is a HACK since for PipelineEngine
             # it's difficult to inject argument in forward pass.
             if self.module.training and self.curriculum_enabled_legacy():
                 self.curriculum_scheduler_legacy.update_difficulty(self.global_steps + 1)
                 if self.curriculum_params_legacy()["curriculum_type"] == "seqlen":
-                    kwargs.update({
-                        "curriculum_seqlen":
-                        self.curriculum_scheduler_legacy.get_current_difficulty()
-                    })
+                    kwargs.update({"curriculum_seqlen": self.curriculum_scheduler_legacy.get_current_difficulty()})
 
         if self.module.training and self.random_ltd_enabled():
             self.random_ltd_scheduler.update_seq(self.global_steps)
 
         if self.zero_optimization_partition_weights():
             # Enable automated discovery of external parameters by indicating that
             # we are in a forward pass.
             for module in self.module.modules():
                 module._parameters._in_forward = True
-                pass
 
         self._start_timers(self.engine_timers.forward_timers)
 
         if self.training_dataloader is None:
             self.tput_timer.start()
 
         if self.fp16_auto_cast():
@@ -1854,17 +1703,15 @@
 
         if flops_profiler_active:
             self.flops_profiler.stop_profile()
 
         if self.autotuning_profile_model_info():
             activation_mem = get_ma_status() - ma
             self.autotuning_model_info["activation_mem_per_gpu"] = activation_mem
-            print_json_dist(self.autotuning_model_info,
-                            [0],
-                            path=self.autotuning_model_info_path())
+            print_json_dist(self.autotuning_model_info, [0], path=self.autotuning_model_info_path())
             exit()
         else:
             see_memory_usage("Engine after forward", force=self.memory_breakdown())
         return loss
 
     def _cast_inputs_half(self, inputs):
         if isinstance(inputs, (list, tuple)):
@@ -1907,54 +1754,44 @@
 
     @instrument_w_nvtx
     def allreduce_gradients(self, bucket_size=MEMORY_OPT_ALLREDUCE_SIZE):
         assert not (self.bfloat16_enabled() and self.pipeline_parallelism), \
             f'allreduce_gradients() is not valid when bfloat+pipeline_parallelism is enabled'
 
         # Pass (PP) gas boundary flag to optimizer (required for zero)
-        self.optimizer.is_gradient_accumulation_boundary = self.is_gradient_accumulation_boundary(
-        )
+        self.optimizer.is_gradient_accumulation_boundary = self.is_gradient_accumulation_boundary()
         # ZeRO stage >= 2 communicates during non gradient accumulation boundaries as well
         if self.zero_optimization_partition_gradients():
             self.optimizer.overlapping_partition_gradients_reduce_epilogue()
 
         # Communicate only at gradient accumulation boundaries
         elif self.is_gradient_accumulation_boundary():
-            if self.zero_optimization_stage(
-            ) == ZeroStageEnum.optimizer_states and hasattr(self.optimizer,
-                                                            'reduce_gradients'):
-                self.optimizer.reduce_gradients(
-                    pipeline_parallel=self.pipeline_parallelism)
+            if self.zero_optimization_stage() == ZeroStageEnum.optimizer_states and hasattr(
+                    self.optimizer, 'reduce_gradients'):
+                self.optimizer.reduce_gradients(pipeline_parallel=self.pipeline_parallelism)
             else:
                 self.buffered_allreduce_fallback(elements_per_buffer=bucket_size)
 
     @instrument_w_nvtx
-    def backward(self,
-                 loss,
-                 allreduce_gradients=True,
-                 release_loss=False,
-                 retain_graph=False,
-                 scale_wrt_gas=True):
+    def backward(self, loss, allreduce_gradients=True, release_loss=False, retain_graph=False, scale_wrt_gas=True):
         r"""Execute backward pass on the loss
         Arguments:
             loss: Torch tensor on which to execute backward propagation
             allreduce_gradients: is deprecated, ignored, and will soon be removed'
             retain_graph: bool, default: false
                 forward on user defined choice of retain_graph
         """
 
         see_memory_usage("Engine before backward", force=self.memory_breakdown())
 
         if self.scale_wrt_gas is not None:
             scale_wrt_gas = self.scale_wrt_gas
 
         if not allreduce_gradients:
-            logger.warning(
-                f"Argument `allreduce_gradients` is deprecated, ignored, and will soon be removed"
-            )
+            logger.warning(f"Argument `allreduce_gradients` is deprecated, ignored, and will soon be removed")
 
         # scale loss w.r.t. gradient accumulation if needed
         if self.gradient_accumulation_steps() > 1 and scale_wrt_gas:
             loss = self._scale_loss_by_gas(loss.float())
 
         # Log training Loss
         if self.monitor.enabled:
@@ -1971,24 +1808,21 @@
 
         assert self.optimizer is not None and not isinstance(self.optimizer, DummyOptim), \
             "must provide optimizer during init in order to use backward"
 
         self._start_timers(self.engine_timers.backward_inner_timers)
 
         if self.zero_optimization():
-            self.optimizer.is_gradient_accumulation_boundary = self.is_gradient_accumulation_boundary(
-            )
+            self.optimizer.is_gradient_accumulation_boundary = self.is_gradient_accumulation_boundary()
             self.optimizer.backward(loss, retain_graph=retain_graph)
         elif self.amp_enabled():
             # AMP requires delaying unscale when inside gradient accumulation boundaries
             # https://nvidia.github.io/apex/advanced.html#gradient-accumulation-across-iterations
             delay_unscale = not self.is_gradient_accumulation_boundary()
-            with amp.scale_loss(loss,
-                                self.optimizer,
-                                delay_unscale=delay_unscale) as scaled_loss:
+            with amp.scale_loss(loss, self.optimizer, delay_unscale=delay_unscale) as scaled_loss:
                 scaled_loss.backward(retain_graph=retain_graph)
         elif self.fp16_enabled():
             if self.eigenvalue_enabled():
                 self.optimizer.backward(loss, create_graph=True, retain_graph=True)
             else:
                 self.optimizer.backward(loss, retain_graph=retain_graph)
         elif self.bfloat16_enabled():
@@ -2063,30 +1897,25 @@
         """
         Zero parameter grads.
         """
         for param_name, param in self.module.named_parameters():
             param.grad = None
 
     def clip_fp32_gradients(self):
-        clip_grad_norm_(parameters=self.module.parameters(),
-                        max_norm=self.gradient_clipping(),
-                        mpu=self.mpu)
+        clip_grad_norm_(parameters=self.module.parameters(), max_norm=self.gradient_clipping(), mpu=self.mpu)
 
     def _take_model_step(self, lr_kwargs, block_eigenvalue={}):
         if self.gradient_clipping() > 0.0:
-            if not (self.fp16_enabled() or self.bfloat16_enabled() or self.amp_enabled()
-                    or self.zero_optimization()):
+            if not (self.fp16_enabled() or self.bfloat16_enabled() or self.amp_enabled() or self.zero_optimization()):
                 self.clip_fp32_gradients()
             elif self.amp_enabled():
                 # AMP's recommended way of doing clipping
                 # https://nvidia.github.io/apex/advanced.html#gradient-clipping
                 master_params = amp.master_params(self.optimizer)
-                clip_grad_norm_(parameters=master_params,
-                                max_norm=self.gradient_clipping(),
-                                mpu=self.mpu)
+                clip_grad_norm_(parameters=master_params, max_norm=self.gradient_clipping(), mpu=self.mpu)
         self.optimizer.step()
 
         if hasattr(self.optimizer, '_global_grad_norm'):
             self._global_grad_norm = self.optimizer._global_grad_norm
 
         # Quantize the updated parameter if there is no overflow
         if self.quantizer:
@@ -2144,73 +1973,65 @@
         on effective_train_batch.
         """
         see_memory_usage("Engine before step", force=self.memory_breakdown())
 
         # Check early because self.global_steps is incremented at some point here.
         # TODO: Delay self.global_steps increment until very end of this function.
         flops_profiler_active = self.flops_profiler_enabled(
-        ) and self.global_steps == self.flops_profiler_profile_step(
-        ) and self.global_rank == 0
+        ) and self.global_steps == self.flops_profiler_profile_step() and self.global_rank == 0
 
         self._start_timers(self.engine_timers.step_timers)
 
         assert self.optimizer is not None and not isinstance(self.optimizer, DummyOptim), \
             "must provide optimizer during init in order to use step"
 
         report_progress = False
 
         self._step_applied = False  # assume False, will flip to True
 
         # Update the model when we reach gradient accumulation boundaries
         if self.is_gradient_accumulation_boundary():
             self.gas_boundary_ctr += 1
 
-            if (self.eigenvalue_enabled() and
-                (self.gas_boundary_ctr % self.eigenvalue_gas_boundary_resolution() == 0)
+            if (self.eigenvalue_enabled() and (self.gas_boundary_ctr % self.eigenvalue_gas_boundary_resolution() == 0)
                     and self.quantizer.any_precision_switch()):
                 log_dist(f"computing eigenvalue...", ranks=[0])
-                self.block_eigenvalue = self.eigenvalue.compute_eigenvalue(
-                    self.module,
-                    self.device,
-                    self.optimizer.cur_scale)
+                self.block_eigenvalue = self.eigenvalue.compute_eigenvalue(self.module, self.device,
+                                                                           self.optimizer.cur_scale)
 
             if self.progressive_layer_drop:
                 self.progressive_layer_drop.update_state(self.global_steps)
 
-            if (self.eigenvalue_enabled() and not self.gas_boundary_ctr %
-                    self.eigenvalue_gas_boundary_resolution()
+            if (self.eigenvalue_enabled() and not self.gas_boundary_ctr % self.eigenvalue_gas_boundary_resolution()
                     and self.quantizer.any_precision_switch()):
                 self._take_model_step(lr_kwargs, self.block_eigenvalue)
             else:
                 self._take_model_step(lr_kwargs)
 
             report_progress = self.global_rank == 0 if self.global_rank else True
 
-        self.tput_timer.stop(global_step=self.is_gradient_accumulation_boundary(),
-                             report_speed=report_progress)
+        self.tput_timer.stop(global_step=self.is_gradient_accumulation_boundary(), report_speed=report_progress)
 
         self._stop_timers(self.engine_timers.step_timers)
 
         # Log learning rate
         if self.monitor.enabled:
             if self.is_gradient_accumulation_boundary():
                 if self.global_rank == 0:
-                    self.summary_events = [(f"Train/Samples/lr",
-                                            self.get_lr()[0],
-                                            self.global_samples)]
+                    self.summary_events = [(f"Train/Samples/lr", self.get_lr()[0], self.global_samples)]
 
                     if self.fp16_enabled() and hasattr(self.optimizer, "cur_scale"):
                         self.summary_events.append((
                             f"Train/Samples/loss_scale",
                             self.optimizer.cur_scale,
                             self.global_samples,
                         ))
 
-                    if (self.eigenvalue_enabled() and not self.gas_boundary_ctr %
-                            self.eigenvalue_gas_boundary_resolution()):
+                    if (self.eigenvalue_enabled()
+                            and not self.gas_boundary_ctr % self.eigenvalue_gas_boundary_resolution()):
                         ev_values = self.block_eigenvalue.values()
                         for i in range(len(ev_values)):
                             self.summary_events.append((
                                 f"Train/Eigenvalues/ModelBlockParam_{i}",
                                 self.ev_values[i][0],
                                 self.global_samples,
                             ))
@@ -2226,22 +2047,20 @@
                     module_depth=self.flops_profiler_module_depth(),
                     top_modules=self.flops_profiler_top_modules(),
                     detailed=self.flops_profiler_detailed(),
                     output_file=self.flops_profiler_output_file(),
                 )
             self.flops_profiler.end_profile()
 
-        if self.autotuning_enabled() and self.global_steps == (
-                self.autotuning_end_profile_step() + 1):
+        if self.autotuning_enabled() and self.global_steps == (self.autotuning_end_profile_step() + 1):
             self._autotuning_exit()
 
         if self.wall_clock_breakdown():
             # Log micro timing and reset
-            self.timers.log(names=self.engine_timers.micro_timers,
-                            memory_breakdown=self.memory_breakdown())
+            self.timers.log(names=self.engine_timers.micro_timers, memory_breakdown=self.memory_breakdown())
 
         if self.wall_clock_breakdown() or self.flops_profiler_enabled():
             # Log global timing and reset
             if self.is_gradient_accumulation_boundary():
                 if self.monitor.enabled:
                     self._write_monitor()
 
@@ -2267,21 +2086,18 @@
 
     def _autotuning_exit(self):
         if self.global_rank == 0:
             msg = self.timers.get_mean([
                 FORWARD_GLOBAL_TIMER,
                 BACKWARD_GLOBAL_TIMER,
                 STEP_GLOBAL_TIMER,
-            ],
-                                       reset=False)
-            titer = msg[FORWARD_GLOBAL_TIMER] + msg[BACKWARD_GLOBAL_TIMER] + msg[
-                STEP_GLOBAL_TIMER]
+            ], reset=False)
+            titer = msg[FORWARD_GLOBAL_TIMER] + msg[BACKWARD_GLOBAL_TIMER] + msg[STEP_GLOBAL_TIMER]
             msg["latency"] = titer
-            msg["FLOPS_per_gpu"] = self.flops * 1_000_000 * self.gradient_accumulation_steps(
-            ) / titer
+            msg["FLOPS_per_gpu"] = self.flops * 1_000_000 * self.gradient_accumulation_steps() / titer
             msg["throughput"] = self.train_batch_size() * 1_000_000 / \
                 msg["latency"]
             print_json_dist(msg, [0], path=self.autotuning_metric_path())
             log_dist(
                 f"Wrote metrics to {self.autotuning_metric_path()}, {os.path.abspath(self.autotuning_metric_path())}",
                 ranks=[0])
             import atexit
@@ -2347,16 +2163,15 @@
             return self.progressive_layer_drop.get_theta()
         else:
             return None
 
     def _report_progress(self, step):
         lr = self.get_lr()
         mom = self.get_mom()
-        log_dist(f"step={step}, skipped={self.skipped_steps}, lr={lr}, mom={mom}",
-                 ranks=[0])
+        log_dist(f"step={step}, skipped={self.skipped_steps}, lr={lr}, mom={mom}", ranks=[0])
 
     def allreduce_bucket(self, bucket, dp_group):
         tensor = self.flatten(bucket)
 
         tensor_to_allreduce = tensor
 
         if self.communication_data_type != tensor.dtype:
@@ -2364,18 +2179,16 @@
 
         if self.postscale_gradients():
             if self.gradient_predivide_factor() != 1.0:
                 tensor_to_allreduce.mul_(1.0 / self.gradient_predivide_factor())
 
             dist.all_reduce(tensor_to_allreduce, group=dp_group)
             if self.gradient_average:
-                if self.gradient_predivide_factor() != dist.get_world_size(
-                        group=dp_group):
-                    tensor_to_allreduce.mul_(self.gradient_predivide_factor() /
-                                             dist.get_world_size(group=dp_group))
+                if self.gradient_predivide_factor() != dist.get_world_size(group=dp_group):
+                    tensor_to_allreduce.mul_(self.gradient_predivide_factor() / dist.get_world_size(group=dp_group))
         else:
             tensor_to_allreduce.mul_(1. / dist.get_world_size(group=dp_group))
             dist.all_reduce(tensor_to_allreduce, group=dp_group)
 
         if self.communication_data_type != tensor.dtype and tensor is not tensor_to_allreduce:
             tensor.copy_(tensor_to_allreduce)
 
@@ -2409,17 +2222,15 @@
         for param_name, param in self.module.named_parameters():
             if param.grad is None:
                 # In cases where there is an imbalance of empty grads across
                 # ranks we must create empty grads, this will ensure that every
                 # rank is reducing the same size. In some cases it may make
                 # sense in the future to support the ability to average not
                 # w.r.t. world size but with a different value.
-                param.grad = torch.zeros(param.size(),
-                                         dtype=param.dtype,
-                                         device=param.device)
+                param.grad = torch.zeros(param.size(), dtype=param.dtype, device=param.device)
 
             grad_data = param.grad.data
             if param_name in self.sparse_tensor_module_names or grad_data.is_sparse:
                 # Call param.grad without data to avoid problem with setting of updated grads
                 grad_data = SparseTensor(param.grad)
 
             if is_moe_param(param):
@@ -2438,33 +2249,28 @@
                 dp_group = self.mpu.get_data_parallel_group()
             else:
                 dp_group = groups._get_data_parallel_group()
 
             if bucket_type == SparseTensor.type():
                 self.sparse_allreduce_no_retain(bucket, dp_group=dp_group)
             else:
-                self.allreduce_no_retain(bucket,
-                                         dp_group=dp_group,
-                                         numel_per_bucket=elements_per_buffer)
+                self.allreduce_no_retain(bucket, dp_group=dp_group, numel_per_bucket=elements_per_buffer)
 
     def _reduce_expert_gradients(self, expert_grads, elements_per_buffer):
         for ep_name, expert_grads_group in expert_grads.items():
             expert_split_buckets = split_half_float_double_sparse(expert_grads_group)
             for i, bucket_tuple in enumerate(expert_split_buckets):
                 bucket_type, bucket = bucket_tuple
                 if bucket_type == SparseTensor.type():
-                    self.sparse_allreduce_no_retain(
-                        bucket,
-                        groups._get_expert_data_parallel_group(ep_name))
+                    self.sparse_allreduce_no_retain(bucket, groups._get_expert_data_parallel_group(ep_name))
                 else:
                     # Separate between diff groups
-                    self.allreduce_no_retain(
-                        bucket,
-                        dp_group=groups._get_expert_data_parallel_group(ep_name),
-                        numel_per_bucket=elements_per_buffer)
+                    self.allreduce_no_retain(bucket,
+                                             dp_group=groups._get_expert_data_parallel_group(ep_name),
+                                             numel_per_bucket=elements_per_buffer)
 
     def buffered_allreduce_fallback(self, grads=None, elements_per_buffer=500000000):
         if grads is None:
             non_expert_grads, expert_grads = self._get_gradients_for_reduction()
         else:
             assert not self.has_moe_layers, "attempting to reduce grads in unsupported way w.r.t. MoE"
             non_expert_grads = grads
@@ -2499,16 +2305,15 @@
             values = sparse.values.to(self.communication_data_type)
         else:
             indices = sparse.indices
             values = sparse.values
 
         if self.postscale_gradients():
             if self.gradient_average:
-                values.mul_(self.gradient_predivide_factor() /
-                            dist.get_world_size(group=dp_group))
+                values.mul_(self.gradient_predivide_factor() / dist.get_world_size(group=dp_group))
         else:
             values.mul_(1. / dist.get_world_size(group=dp_group))
 
         indices_device_list = self.sparse_all_gather(indices, dp_group)
         values_device_list = self.sparse_all_gather(values, dp_group)
 
         sparse.indices = torch.cat(indices_device_list).to(torch.long)
@@ -2521,44 +2326,33 @@
         max_size = torch.cat(all_sizes).max()
         fill_size = max_size - my_size
 
         assert value.dim() in [1, 2]
         if value.dim() == 1:
             if fill_size > 0:
                 value = torch.cat([value, value.new_empty(fill_size)])
-            tensor_list = [
-                value.new_empty(max_size)
-                for _ in range(dist.get_world_size(group=dp_group))
-            ]
+            tensor_list = [value.new_empty(max_size) for _ in range(dist.get_world_size(group=dp_group))]
         else:
             if fill_size > 0:
                 value = torch.cat([value, value.new_empty(fill_size, value.size()[1])])
             tensor_list = [
                 value.new_empty(max_size,
-                                value.size()[1])
-                for _ in range(dist.get_world_size(group=dp_group))
+                                value.size()[1]) for _ in range(dist.get_world_size(group=dp_group))
             ]
 
         dist.all_gather(tensor_list, value, group=dp_group)
         tensors = []
         for dev_idx, t in enumerate(tensor_list):
             size = all_sizes[dev_idx][0]
-            tensors.append(
-                t.index_select(0,
-                               torch.arange(size,
-                                            dtype=torch.long,
-                                            device=self.device)))
+            tensors.append(t.index_select(0, torch.arange(size, dtype=torch.long, device=self.device)))
 
         return tensors
 
     def all_gather_scalar(self, value, dp_group):
-        tensor_list = [
-            value.new_zeros(value.size())
-            for _ in range(dist.get_world_size(group=dp_group))
-        ]
+        tensor_list = [value.new_zeros(value.size()) for _ in range(dist.get_world_size(group=dp_group))]
         dist.all_gather(tensor_list, value, group=dp_group)
         return tensor_list
 
     def module_state_dict(self, destination=None, prefix="", keep_vars=False):
         sd = self.module.state_dict(destination, prefix, keep_vars)
         if self.random_ltd_enabled():
             sd = remove_random_ltd_state_dict(sd)
@@ -2570,28 +2364,27 @@
                             state_dict,
                             old_moe_load,
                             model=None,
                             mpu=None,
                             num_experts=1,
                             checkpoint_engine=TorchCheckpointEngine()):
         if old_moe_load:
-            expp_rank = groups._get_expert_data_parallel_rank(
-                groups._get_max_expert_size_name())
+            expp_rank = groups._get_expert_data_parallel_rank(groups._get_max_expert_size_name())
 
-            num_local_experts = max(
-                num_experts) // groups._get_expert_parallel_world_size(
-                    groups._get_max_expert_size_name())
+            num_local_experts = max(num_experts) // groups._get_expert_parallel_world_size(
+                groups._get_max_expert_size_name())
             for local_expert_id in range(num_local_experts):
                 global_expert_id = expp_rank * num_local_experts + local_expert_id
-                expert_state_dict = checkpoint_engine.load(DeepSpeedEngine._get_expert_ckpt_name(
-                    checkpoint_path,
-                    -1, # -1 means ignore layer_id
-                    global_expert_id,
-                    tag,
-                    mpu),
+                expert_state_dict = checkpoint_engine.load(
+                    DeepSpeedEngine._get_expert_ckpt_name(
+                        checkpoint_path,
+                        -1,  # -1 means ignore layer_id
+                        global_expert_id,
+                        tag,
+                        mpu),
                     map_location=torch.device('cpu'))
 
                 # Updating global -> local expert ids
                 moe_str_prefix = '.deepspeed_moe.experts.deepspeed_experts.'
                 for key in list(expert_state_dict.keys()):
                     local_key = key.replace(f'{moe_str_prefix}{global_expert_id}',
                                             f'{moe_str_prefix}{local_expert_id}')
@@ -2604,77 +2397,62 @@
                 if isinstance(module, MoE):  # and deepspeed.comm.get_rank() == 0:
                     group_name = module.expert_group_name
                     num_local_experts = module.num_local_experts
                     expp_rank = groups._get_expert_parallel_rank(group_name)
                     # loop all local_experts
                     for local_expert_id in range(num_local_experts):
                         global_expert_id = expp_rank * num_local_experts + local_expert_id
-                        expert_state_dict = checkpoint_engine.load(
-                            DeepSpeedEngine._get_expert_ckpt_name(
-                                checkpoint_path,
-                                moe_layer_id,
-                                global_expert_id,
-                                tag,
-                                mpu),
-                            map_location=torch.device('cpu'))
+                        expert_state_dict = checkpoint_engine.load(DeepSpeedEngine._get_expert_ckpt_name(
+                            checkpoint_path, moe_layer_id, global_expert_id, tag, mpu),
+                                                                   map_location=torch.device('cpu'))
                         # print(expert_state_dict.keys())
                         # Updating global -> local expert ids
                         moe_str_prefix = '.deepspeed_moe.experts.deepspeed_experts.'
                         for key in list(expert_state_dict.keys()):
-                            local_key = key.replace(
-                                f'{moe_str_prefix}{global_expert_id}',
-                                f'{moe_str_prefix}{local_expert_id}')
+                            local_key = key.replace(f'{moe_str_prefix}{global_expert_id}',
+                                                    f'{moe_str_prefix}{local_expert_id}')
                             expert_state_dict[local_key] = expert_state_dict.pop(key)
                         state_dict.update(expert_state_dict)
                     moe_layer_id += 1
 
     def load_module_state_dict(self, state_dict, strict=True, custom_load_fn=None):
         if custom_load_fn:
             custom_load_fn(src=state_dict, dst=self.module)
         else:
-            self.module.load_state_dict(state_dict, # TODO
-                                        strict=strict)
+            self.module.load_state_dict(
+                state_dict,  # TODO
+                strict=strict)
 
     def _get_zero_ckpt_prefix(self, dp_rank, bf16_mode):
         return f'{"bf16_" if bf16_mode else ""}zero_pp_rank_{dp_rank}'
 
-    def _get_rank_zero_ckpt_name(self,
-                                 checkpoints_path,
-                                 tag,
-                                 mp_rank,
-                                 dp_rank,
-                                 bf16_mode):
+    def _get_rank_zero_ckpt_name(self, checkpoints_path, tag, mp_rank, dp_rank, bf16_mode):
         file_prefix = self._get_zero_ckpt_prefix(dp_rank, bf16_mode=bf16_mode)
         zero_ckpt_name = os.path.join(
             checkpoints_path,
             str(tag),
             f"{file_prefix}_mp_rank_{mp_rank:02d}_optim_states.pt",
         )
         return zero_ckpt_name
 
     def _get_zero_ckpt_name(self, checkpoints_path, tag):
         mp_rank = 0 if self.mpu is None else self.mpu.get_model_parallel_rank()
         pp_rank = dist.get_rank(group=self.optimizer.dp_process_group)
         bf16_mode = self.bfloat16_enabled()
-        return self._get_rank_zero_ckpt_name(checkpoints_path,
-                                             tag,
-                                             mp_rank,
-                                             pp_rank,
-                                             bf16_mode)
+        return self._get_rank_zero_ckpt_name(checkpoints_path, tag, mp_rank, pp_rank, bf16_mode)
 
     def _get_ckpt_name(self, checkpoints_path, tag, mp_placeholder=None):
         if mp_placeholder is not None:
             mp_rank_str = mp_placeholder
         else:
             mp_rank = 0 if self.mpu is None else self.mpu.get_model_parallel_rank()
             mp_rank_str = f"{mp_rank:02d}"
 
         if self.zero_optimization_partition_weights():
-            filename = "zero_pp_rank_{}".format(
-                dist.get_rank(group=self.optimizer.dp_process_group))
+            filename = "zero_pp_rank_{}".format(dist.get_rank(group=self.optimizer.dp_process_group))
             ckpt_name = os.path.join(
                 checkpoints_path,
                 str(tag),
                 f"{filename}_mp_rank_{mp_rank_str}_model_states.pt",
             )
         else:
             ckpt_name = os.path.join(
@@ -2682,43 +2460,34 @@
                 str(tag),
                 "mp_rank_" + mp_rank_str + "_model_states.pt",
             )
         return ckpt_name
 
     def _get_optimizer_ckpt_name(self, checkpoints_path, tag, expp_rank):
         mp_rank = 0 if self.mpu is None else self.mpu.get_model_parallel_rank()
-        ckpt_name = os.path.join(
-            checkpoints_path,
-            str(tag),
-            f'expp_rank_{expp_rank}_mp_rank_{mp_rank:02d}_optim_states.pt')
+        ckpt_name = os.path.join(checkpoints_path, str(tag),
+                                 f'expp_rank_{expp_rank}_mp_rank_{mp_rank:02d}_optim_states.pt')
         return ckpt_name
 
     @staticmethod
     def _get_expert_ckpt_name(checkpoints_path, layer_id, expert_id, tag, mpu=None):
         mp_rank = 0 if mpu is None else mpu.get_model_parallel_rank()
         if layer_id <= -1:
             # Used to support old checkpoint loading
-            ckpt_name = os.path.join(
-                checkpoints_path,
-                '' if tag is None else str(tag),
-                f'expert_{expert_id}_mp_rank_{mp_rank:02d}_model_states.pt')
+            ckpt_name = os.path.join(checkpoints_path, '' if tag is None else str(tag),
+                                     f'expert_{expert_id}_mp_rank_{mp_rank:02d}_model_states.pt')
         else:
             # Used to support new checkpoint loading
-            ckpt_name = os.path.join(
-                checkpoints_path,
-                '' if tag is None else str(tag),
-                f'layer_{layer_id}_expert_{expert_id}_mp_rank_{mp_rank:02d}_model_states.pt'
-            )
+            ckpt_name = os.path.join(checkpoints_path, '' if tag is None else str(tag),
+                                     f'layer_{layer_id}_expert_{expert_id}_mp_rank_{mp_rank:02d}_model_states.pt')
         return ckpt_name
 
     def _get_all_ckpt_names(self, checkpoints_path, tag):
         # It is required that (checkpoints_path, tag) are consistent among all ranks.
-        ckpt_file_pattern = self._get_ckpt_name(checkpoints_path,
-                                                tag,
-                                                mp_placeholder="*")
+        ckpt_file_pattern = self._get_ckpt_name(checkpoints_path, tag, mp_placeholder="*")
         import glob
 
         ckpt_files = glob.glob(ckpt_file_pattern)
         ckpt_files.sort()
         return ckpt_files
 
     def load_checkpoint(self,
@@ -2750,25 +2519,22 @@
         after ``engine.save_checkpoint()``. It is because ``engine.module`` is partitioned, and
         ``load_checkpoint()`` wants a pristine model. If insisting to do so, please reinitialize engine
         before ``load_checkpoint()``.
 
         """
 
         if tag is None:
-            latest_tag = "latest_universal" if self.load_universal_checkpoint(
-            ) else "latest"
+            latest_tag = "latest_universal" if self.load_universal_checkpoint() else "latest"
             latest_path = os.path.join(load_dir, latest_tag)
             if os.path.isfile(latest_path):
                 with open(latest_path, "r") as fd:
                     tag = fd.read().strip()
             else:
                 if self.load_universal_checkpoint():
-                    raise ValueError(
-                        f'Invalid for universal checkpoint: {latest_path} does not exist'
-                    )
+                    raise ValueError(f'Invalid for universal checkpoint: {latest_path} does not exist')
                 else:
                     logger.warning(
                         f"Unable to find latest file at {latest_path}, if trying to load latest "
                         "checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint."
                     )
                     return None, None
 
@@ -2782,18 +2548,15 @@
                                                          load_optimizer_states=load_optimizer_states,
                                                          load_lr_scheduler_states=load_lr_scheduler_states,
                                                          load_module_only=load_module_only,
                                                          custom_load_fn=custom_load_fn)
 
         load_zero_checkpoint = self.zero_optimization() or self.bfloat16_enabled()
         if load_zero_checkpoint and load_path is not None:
-            success = self._load_zero_checkpoint(
-                load_dir,
-                tag,
-                load_optimizer_states=load_optimizer_states)
+            success = self._load_zero_checkpoint(load_dir, tag, load_optimizer_states=load_optimizer_states)
             if not success:
                 self.optimizer._restore_from_bit16_weights()
 
         if self.zero_optimization_partition_weights():
             self.optimizer.checkpoint_event_epilogue()
 
         return load_path, client_states
@@ -2806,24 +2569,20 @@
                          load_lr_scheduler_states=True,
                          load_module_only=False,
                          custom_load_fn=None):
 
         from deepspeed.runtime.state_dict_factory import SDLoaderFactory
 
         ckpt_list = self._get_all_ckpt_names(load_dir, tag)
-        sd_loader = SDLoaderFactory.get_sd_loader(
-            ckpt_list,
-            checkpoint_engine=self.checkpoint_engine)
+        sd_loader = SDLoaderFactory.get_sd_loader(ckpt_list, checkpoint_engine=self.checkpoint_engine)
 
         is_pipe_parallel = isinstance(self.module, PipelineModule)
 
         mp_rank = 0 if self.mpu is None else self.mpu.get_model_parallel_rank()
-        load_path, checkpoint, _ = sd_loader.load(
-            self.mp_world_size, mp_rank, is_pipe_parallel=is_pipe_parallel
-        )
+        load_path, checkpoint, _ = sd_loader.load(self.mp_world_size, mp_rank, is_pipe_parallel=is_pipe_parallel)
 
         if checkpoint is None:
             return None, None
 
         if is_pipe_parallel:
             # Pipeline parallelism uses this to load its own checkpoint files.
             self._curr_ckpt_path = os.path.join(load_dir, tag)
@@ -2853,104 +2612,80 @@
             if self.optimizer is not None and self.fp16_enabled():
                 self.optimizer.refresh_fp32_params()
         else:
             if self.has_moe_layers:
                 largest_group_name = groups._get_max_expert_size_name()
                 expp_rank = groups._get_expert_parallel_rank(largest_group_name)
                 optim_load_path = self._get_optimizer_ckpt_name(load_dir, tag, expp_rank)
-                optim_checkpoint = self.checkpoint_engine.load(
-                    optim_load_path,
-                    map_location=torch.device('cpu'))
+                optim_checkpoint = self.checkpoint_engine.load(optim_load_path, map_location=torch.device('cpu'))
             else:
                 optim_checkpoint = checkpoint
 
-            has_zero_optimizer_state = self.zero_optimization() or self.bfloat16_enabled(
-            )
+            has_zero_optimizer_state = self.zero_optimization() or self.bfloat16_enabled()
             if load_optimizer_states and self.optimizer is not None and not has_zero_optimizer_state:
                 if self.fp16_enabled():
-                    self.optimizer.load_state_dict(
-                        optim_checkpoint['optimizer'],
-                        load_optimizer_states=load_optimizer_states)
+                    self.optimizer.load_state_dict(optim_checkpoint['optimizer'],
+                                                   load_optimizer_states=load_optimizer_states)
                 else:
                     self.optimizer.load_state_dict(optim_checkpoint['optimizer'])
 
             if load_lr_scheduler_states and self.lr_scheduler is not None:
                 self.lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])
 
-            if self.random_ltd_enabled(
-            ) and self.random_ltd_scheduler is not None and 'random_ltd' in checkpoint:
+            if self.random_ltd_enabled() and self.random_ltd_scheduler is not None and 'random_ltd' in checkpoint:
                 self.random_ltd_scheduler.load_state_dict(checkpoint['random_ltd'])
 
             if self.training_dataloader is not None and self.curriculum_learning_enabled(
             ) and 'data_sampler' in checkpoint:
-                self.training_dataloader.data_sampler.load_state_dict(
-                    checkpoint['data_sampler'])
+                self.training_dataloader.data_sampler.load_state_dict(checkpoint['data_sampler'])
 
-            def get_sparse_tensor_module_names(original_set,
-                                               loaded_set,
-                                               original_parameters,
-                                               loaded_parameters):
+            def get_sparse_tensor_module_names(original_set, loaded_set, original_parameters, loaded_parameters):
                 result = set()
 
                 for name in original_set:
                     if name in loaded_parameters and name not in loaded_set:
                         continue  # parameter existed in previous model and was not sparse
                     result.add(name)
 
                 for name in loaded_set:
                     if name in original_parameters:
-                        result.add(
-                            name)  # parameter exists in both configs and it was sparse
+                        result.add(name)  # parameter exists in both configs and it was sparse
 
                 return result
 
             if 'sparse_tensor_module_names' in checkpoint:
                 sparse_tensor_module_names = checkpoint['sparse_tensor_module_names']
             elif 'csr_tensor_module_names' in checkpoint:
                 sparse_tensor_module_names = checkpoint['csr_tensor_module_names']
             else:
                 sparse_tensor_module_names = None
             if sparse_tensor_module_names is not None:
                 if load_module_strict:
                     self.sparse_tensor_module_names = sparse_tensor_module_names
                 else:
                     self.sparse_tensor_module_names = get_sparse_tensor_module_names(
-                        self.sparse_tensor_module_names,
-                        sparse_tensor_module_names,
-                        dict(self.module.named_parameters()),
-                        checkpoint["module"])
+                        self.sparse_tensor_module_names, sparse_tensor_module_names,
+                        dict(self.module.named_parameters()), checkpoint["module"])
 
             self.global_steps = checkpoint['global_steps']
-            self.global_samples = checkpoint.get(
-                'global_samples',
-                self.global_steps * self.train_batch_size())
+            self.global_samples = checkpoint.get('global_samples', self.global_steps * self.train_batch_size())
             self.skipped_steps = checkpoint['skipped_steps']
             self.loaded_checkpoint_mp_world_size = checkpoint['mp_world_size']
             deepspeed_states = [
-                'module',
-                'sparse_tensor_module_names',
-                'skipped_steps',
-                'global_steps',
-                'dp_world_size',
-                'mp_world_size',
-                'data_sampler',
-                'random_ltd'
+                'module', 'sparse_tensor_module_names', 'skipped_steps', 'global_steps', 'dp_world_size',
+                'mp_world_size', 'data_sampler', 'random_ltd'
             ]
         client_state = {}
 
         if load_lr_scheduler_states:
             deepspeed_states.append('lr_scheduler')
         if load_optimizer_states:
             deepspeed_states.append('optimizer')
 
-        client_state = {
-            key: value
-            for key,
-            value in checkpoint.items() if not key in deepspeed_states
-        }
+        client_state = {key: value for key, value in checkpoint.items() if not key in deepspeed_states}
 
         if not load_optimizer_states and not load_module_only:
             client_state['optimizer'] = optim_checkpoint['optimizer']
 
         return load_path, client_state
 
     def _load_zero_checkpoint(self, load_dir, tag, load_optimizer_states=True):
@@ -2965,121 +2700,100 @@
                     "of ZeRO's optimizer state partitioning with a new world size is not " \
                     "currently supported.")
             checkpoint_folder = None
             zero_sd_list = self._get_all_zero_checkpoints(load_dir, tag)
             if zero_sd_list is None:
                 return False
 
-        self.optimizer.load_state_dict(
-            state_dict_list=zero_sd_list,
-            load_optimizer_states=load_optimizer_states,
-            load_from_fp32_weights=self.zero_load_from_fp32_weights(),
-            checkpoint_folder=checkpoint_folder)
+        self.optimizer.load_state_dict(state_dict_list=zero_sd_list,
+                                       load_optimizer_states=load_optimizer_states,
+                                       load_from_fp32_weights=self.zero_load_from_fp32_weights(),
+                                       checkpoint_folder=checkpoint_folder)
 
         if self.load_universal_checkpoint():
-            logger.info(
-                f'loaded universal zero checkpoints from {checkpoint_folder} for rank {self.global_rank}'
-            )
+            logger.info(f'loaded universal zero checkpoints from {checkpoint_folder} for rank {self.global_rank}')
         else:
-            logger.info(
-                f"loading {len(zero_sd_list)} zero partition checkpoints for rank {self.global_rank}"
-            )
+            logger.info(f"loading {len(zero_sd_list)} zero partition checkpoints for rank {self.global_rank}")
         return True
 
-    def _get_mp_rank_zero_checkpoint_names(self,
-                                           load_dir,
-                                           tag,
-                                           mp_rank,
-                                           dp_world_size,
-                                           bf16_mode):
+    def _get_mp_rank_zero_checkpoint_names(self, load_dir, tag, mp_rank, dp_world_size, bf16_mode):
         zero_ckpt_names = []
         for dp_rank in range(dp_world_size):
             ckpt_name = self._get_rank_zero_ckpt_name(checkpoints_path=load_dir,
                                                       tag=tag,
                                                       mp_rank=mp_rank,
                                                       dp_rank=dp_rank,
                                                       bf16_mode=bf16_mode)
             zero_ckpt_names.append(ckpt_name)
 
         return zero_ckpt_names
 
     def _get_all_zero_checkpoint_names(self, load_dir, tag, bf16_mode):
         mp_rank = 0 if self.mpu is None else self.mpu.get_model_parallel_rank()
-        zero_ckpt_names = self._get_mp_rank_zero_checkpoint_names(
-            load_dir=load_dir,
-            tag=tag,
-            mp_rank=mp_rank,
-            dp_world_size=self.loaded_checkpoint_dp_world_size,
-            bf16_mode=bf16_mode)
+        zero_ckpt_names = self._get_mp_rank_zero_checkpoint_names(load_dir=load_dir,
+                                                                  tag=tag,
+                                                                  mp_rank=mp_rank,
+                                                                  dp_world_size=self.loaded_checkpoint_dp_world_size,
+                                                                  bf16_mode=bf16_mode)
         for i, ckpt_name in enumerate(zero_ckpt_names):
             if not os.path.exists(ckpt_name):
                 # transparently handle the old file pattern for optim_states
                 if "optim_states.pt" in ckpt_name:
-                    ckpt_name_try = ckpt_name.replace("_optim_states.pt",
-                                                      "optim_states.pt")
+                    ckpt_name_try = ckpt_name.replace("_optim_states.pt", "optim_states.pt")
                     if os.path.exists(ckpt_name_try):
                         zero_ckpt_names[i] = ckpt_name_try
                         continue
 
         return zero_ckpt_names
 
     def _get_all_zero_checkpoint_state_dicts(self, zero_ckpt_names):
         zero_sd_list = []
         for i, ckpt_name in enumerate(zero_ckpt_names):
             _state = None
             if ckpt_name is None:
                 _state = {OPTIMIZER_STATE_DICT: None}
             # Fully load state for current rank
-            elif self.zero_elastic_checkpoint() or dist.get_rank(
-                    group=self.optimizer.dp_process_group) == i:
+            elif self.zero_elastic_checkpoint() or dist.get_rank(group=self.optimizer.dp_process_group) == i:
                 _state = self.checkpoint_engine.load(
                     ckpt_name,
                     map_location='cpu',
                 )
             else:
                 _state = {OPTIMIZER_STATE_DICT: None}
             zero_sd_list.append(_state)
 
         zero_optimizer_sd = [sd[OPTIMIZER_STATE_DICT] for sd in zero_sd_list]
-        logger.info(
-            f"successfully read {len(zero_optimizer_sd)} ZeRO state_dicts for rank {self.global_rank}"
-        )
+        logger.info(f"successfully read {len(zero_optimizer_sd)} ZeRO state_dicts for rank {self.global_rank}")
         return zero_optimizer_sd
 
     def _get_all_zero_checkpoints(self, load_dir, tag):
         for bf16_mode in [self.bfloat16_enabled(), not self.bfloat16_enabled()]:
-            zero_ckpt_names = self._get_all_zero_checkpoint_names(
-                load_dir,
-                tag,
-                bf16_mode)
+            zero_ckpt_names = self._get_all_zero_checkpoint_names(load_dir, tag, bf16_mode)
             if zero_ckpt_names is not None:
                 # Warn if loading checkpoint of different bit16 type
                 if bf16_mode is not self.bfloat16_enabled():
                     checkpoint_bit16 = BFLOAT16 if bf16_mode else FP16
                     engine_bit16 = BFLOAT16 if self.bfloat16_enabled() else FP16
-                    logger.warn(
-                        f'Loading {checkpoint_bit16} zero checkpoints into {engine_bit16} training engine'
-                    )
+                    logger.warn(f'Loading {checkpoint_bit16} zero checkpoints into {engine_bit16} training engine')
                 return self._get_all_zero_checkpoint_state_dicts(zero_ckpt_names)
 
         return None
 
     def _checkpoint_tag_validation(self, tag):
         if self.checkpoint_tag_validation_enabled():
             s_hash = hashlib.sha1(tag.encode())
             bhash = torch.ByteTensor([s_hash.digest()]).flatten().to(self.device)
             max_bhash = bhash.clone()
             min_bhash = bhash.clone()
             dist.all_reduce(max_bhash, op=dist.ReduceOp.MAX)
             dist.all_reduce(min_bhash, op=dist.ReduceOp.MIN)
             valid = all(min_bhash == bhash) and all(max_bhash == bhash)
-            msg = (
-                f"[rank={dist.get_rank()}] The checkpoint tag name '{tag}' is not consistent across "
-                "all ranks. Including rank unique information in checkpoint tag could cause issues when "
-                "restoring with different world sizes.")
+            msg = (f"[rank={dist.get_rank()}] The checkpoint tag name '{tag}' is not consistent across "
+                   "all ranks. Including rank unique information in checkpoint tag could cause issues when "
+                   "restoring with different world sizes.")
             if self.checkpoint_tag_validation_fail():
                 assert valid, msg
             elif not valid:
                 logger.warning(msg)
 
     def save_checkpoint(self, save_dir, tag=None, client_state={}, save_latest=True):
         """Save training checkpoint
@@ -3203,23 +2917,17 @@
                     # truncating extra tensor (shared) storage
                     truncated = moe_state_dict.pop(key).clone().detach()
                     experts_state_dict[str(global_expert_id)][expert_key] = truncated
 
                 # let save the moe parameters
                 for global_expert_id, expert_state_dict in experts_state_dict.items():
                     # save the moe parameters
-                    moe_save_path = self._get_expert_ckpt_name(
-                        save_dir,
-                        moe_layer_id,
-                        global_expert_id,
-                        tag,
-                        self.mpu)
+                    moe_save_path = self._get_expert_ckpt_name(save_dir, moe_layer_id, global_expert_id, tag, self.mpu)
                     if self.random_ltd_enabled():
-                        expert_state_dict = remove_random_ltd_state_dict(
-                            expert_state_dict)
+                        expert_state_dict = remove_random_ltd_state_dict(expert_state_dict)
                     self.checkpoint_engine.save(expert_state_dict, moe_save_path)
                 moe_layer_id += 1
 
         self._curr_ckpt_path = os.path.join(save_dir, tag)
 
         largest_group_name = groups._get_max_expert_size_name()
         expp_rank = groups._get_expert_parallel_rank(largest_group_name)
@@ -3229,40 +2937,35 @@
         # first expert parallel group should save the expert weights
         # since each expert parallel group is a copy of the model's experts
         if exp_dp_rank != 0:
             return
 
         # Save optimizer states. They are different across each exp parallel rank.
         optimizer_state = {
-            'optimizer':
-            self.optimizer.state_dict()
-            if self.optimizer and not self.zero_optimization() else None
+            'optimizer': self.optimizer.state_dict() if self.optimizer and not self.zero_optimization() else None
         }
         # TODO: why use BufferedWriter not the path
         file_path = self._get_optimizer_ckpt_name(save_dir, tag, expp_rank)
         self.checkpoint_engine.save(optimizer_state, file_path)
 
         # get non-moe parameters
         model_state_dict = self._get_non_moe_state_dict(self.module_state_dict())
 
         if expp_rank == 0:
             # TODO: update num experts info,.. in checkpoint
             state = {
                 'module':
                 model_state_dict,
                 'lr_scheduler':
-                self.lr_scheduler.state_dict()
-                if self.lr_scheduler is not None else None,
+                self.lr_scheduler.state_dict() if self.lr_scheduler is not None else None,
                 'data_sampler':
                 self.training_dataloader.data_sampler.state_dict() if
-                (self.training_dataloader is not None
-                 and self.curriculum_learning_enabled()) else None,
+                (self.training_dataloader is not None and self.curriculum_learning_enabled()) else None,
                 'random_ltd':
-                self.random_ltd_scheduler.state_dict()
-                if self.random_ltd_enabled() else None,
+                self.random_ltd_scheduler.state_dict() if self.random_ltd_enabled() else None,
                 'sparse_tensor_module_names':
                 self.sparse_tensor_module_names,
                 'skipped_steps':
                 self.skipped_steps,
                 'global_steps':
                 self.global_steps,
                 'global_samples':
@@ -3276,16 +2979,15 @@
             }
             state.update(client_state)
             logger.info(f'Saving model checkpoint: {save_path}')
             self.checkpoint_engine.save(state, save_path)
         self._curr_save_path = None
 
     def _create_checkpoint_file(self, save_dir, tag, zero_checkpoint):
-        name_function = (self._get_zero_ckpt_name
-                         if zero_checkpoint else self._get_ckpt_name)
+        name_function = (self._get_zero_ckpt_name if zero_checkpoint else self._get_ckpt_name)
         try:
             checkpoint_name = name_function(save_dir, tag)
             path = os.path.dirname(checkpoint_name)
             self.checkpoint_engine.makedirs(path, exist_ok=True)
         except:
             logger.error(f"Failed saving model checkpoint to {save_dir} with tag {tag}")
             return False
@@ -3315,25 +3017,20 @@
         # PipelineEngine expects the save path to be set in self._curr_ckpt_path.
         self._curr_ckpt_path = os.path.join(save_dir, tag)
         module = self.module_state_dict()
         self._curr_ckpt_path = None
 
         state = dict(module=module,
                      buffer_names=self._get_buffer_names(),
-                     optimizer=self.optimizer.state_dict()
-                     if self.optimizer and not zero_optimizer_state else None,
-                     param_shapes=self._get_zero_param_shapes()
-                     if self.optimizer and zero_optimizer_state else None,
-                     lr_scheduler=self.lr_scheduler.state_dict()
-                     if self.lr_scheduler is not None else None,
+                     optimizer=self.optimizer.state_dict() if self.optimizer and not zero_optimizer_state else None,
+                     param_shapes=self._get_zero_param_shapes() if self.optimizer and zero_optimizer_state else None,
+                     lr_scheduler=self.lr_scheduler.state_dict() if self.lr_scheduler is not None else None,
                      data_sampler=self.training_dataloader.data_sampler.state_dict() if
-                     (self.training_dataloader is not None
-                      and self.curriculum_learning_enabled()) else None,
-                     random_ltd=self.random_ltd_scheduler.state_dict()
-                     if self.random_ltd_enabled() else None,
+                     (self.training_dataloader is not None and self.curriculum_learning_enabled()) else None,
+                     random_ltd=self.random_ltd_scheduler.state_dict() if self.random_ltd_enabled() else None,
                      sparse_tensor_module_names=self.sparse_tensor_module_names,
                      skipped_steps=self.skipped_steps,
                      global_steps=self.global_steps,
                      global_samples=self.global_samples,
                      dp_world_size=self.dp_world_size,
                      mp_world_size=self.mp_world_size,
                      ds_config=self.config,
@@ -3415,17 +3112,15 @@
         #logger.info(f"creating recovery script {dst}")
         copyfile(src, dst)
         # make executable
         os.chmod(dst, os.stat(dst).st_mode | stat.S_IEXEC)
 
     def _save_zero_checkpoint(self, save_path, tag):
         zero_checkpoint_name = self._get_zero_ckpt_name(save_path, tag)
-        zero_sd = dict(optimizer_state_dict=self.optimizer.state_dict(),
-                       ds_config=self.config,
-                       ds_version=version)
+        zero_sd = dict(optimizer_state_dict=self.optimizer.state_dict(), ds_config=self.config, ds_version=version)
         self.checkpoint_engine.save(zero_sd, zero_checkpoint_name)
 
         if self.global_rank == 0:
             self._copy_recovery_script(save_path)
         ckpt_type = 'zero' if self.zero_optimization() else 'bf16_zero'
         logger.info(f'{ckpt_type} checkpoint saved {zero_checkpoint_name}')
 
@@ -3447,17 +3142,15 @@
         state_dict = OrderedDict() if dist.get_rank() == 0 else None
         shared_params = {}
 
         def get_layer_state_dict(module, prefix=""):
             # gather one layer at a time to be memory-efficient
             # must use modifier_rank=0 to release GPU memory after each layer gathered
             #see_memory_usage("before GatheredParameters", force=True)
-            with deepspeed.zero.GatheredParameters(list(
-                    module.parameters(recurse=False)),
-                                                   modifier_rank=0):
+            with deepspeed.zero.GatheredParameters(list(module.parameters(recurse=False)), modifier_rank=0):
                 if dist.get_rank() == 0:
                     # handle params
                     for name, param in module.named_parameters(recurse=False):
                         if param is None:
                             continue
                         key = prefix + name
                         # can't rely on param.data_ptr() as it will be reused as weights gets
@@ -3470,16 +3163,15 @@
                         else:
                             state_dict[key] = param.detach().cpu()
                             shared_params[param.ds_id] = key
                         #print(f"param {param.ds_id} {param.shape} {key} ")
 
                     # now buffers - not sure if need to take care of potentially shared weights here
                     for name, buf in module.named_buffers(recurse=False):
-                        if (buf is not None
-                                and name not in module._non_persistent_buffers_set):
+                        if (buf is not None and name not in module._non_persistent_buffers_set):
                             state_dict[prefix + name] = buf.detach().cpu()
             #see_memory_usage("after GatheredParameters", force=True)
 
             for name, child in module.named_children():
                 if child is not None:
                     get_layer_state_dict(child, prefix + name + ".")
 
@@ -3524,19 +3216,33 @@
         if self.zero_optimization_partition_weights():
             if self.zero_gather_16bit_weights_on_model_save():
                 # consolidation is expensive in time and memory and therefore isn't a default
                 state_dict = self._zero3_consolidated_16bit_state_dict()
             else:
                 # the model will be bogus if not consolidated so don't confuse the user by saving it
                 logger.info(
-                    f"Did not save the model {path} because `stage3_gather_16bit_weights_on_model_save` is False"
-                )
+                    f"Did not save the model {path} because `stage3_gather_16bit_weights_on_model_save` is False")
                 return False
         else:
             state_dict = self.module.state_dict()
 
+        tag = f"global_step{self.global_steps}"
+        tag = str(tag)
+        self.checkpoint_engine.create(tag)
+
         if dist.get_rank() == 0:
             self.checkpoint_engine.makedirs(save_dir, exist_ok=True)
-            logger.info(f"Saving model weights to {path}")
+            logger.info(f"Saving model weights to {path}, tag: {tag}")
             self.checkpoint_engine.save(state_dict, path)
 
+        self.checkpoint_engine.commit(tag)
+
         return True
+
+    def empty_partition_cache(self):
+        """
+        Release GPU memory consumed by offloaded model parameters.
+        """
+        if hasattr(self.optimizer, 'empty_partition_cache'):
+            self.optimizer.empty_partition_cache()
+            gc.collect()
+            get_accelerator().empty_cache()
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/fp16/fused_optimizer.py` & `deepspeed-0.9.0/deepspeed/runtime/fp16/fused_optimizer.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
-'''
-Copyright 2019 The Microsoft DeepSpeed Team
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
 
+# DeepSpeed Team
+"""
 Copyright NVIDIA/apex
 This file is adapted from FP16_Optimizer in NVIDIA/apex
-'''
+"""
 
 import torch
 from torch._utils import _flatten_dense_tensors, _unflatten_dense_tensors
 
 from deepspeed.runtime import DeepSpeedOptimizer
 from deepspeed.runtime.utils import get_global_norm, get_grad_norm, CheckOverflow, get_weight_norm
 from deepspeed.runtime.fp16.loss_scaler import INITIAL_LOSS_SCALE, SCALE_WINDOW, MIN_LOSS_SCALE
@@ -19,14 +21,15 @@
 
 class FP16_Optimizer(DeepSpeedOptimizer):
     """
    FP16 Optimizer for training fp16 models. Handles loss scaling.
 
    For usage example please see, TODO:  DeepSpeed V2 Tutorial
     """
+
     def __init__(self,
                  init_optimizer,
                  deepspeed=None,
                  static_loss_scale=1.0,
                  dynamic_loss_scale=False,
                  initial_dynamic_scale=2**32,
                  dynamic_loss_args=None,
@@ -54,28 +57,23 @@
         self._global_grad_norm = 0.
 
         # loop to deal with groups
         for i, param_group in enumerate(self.optimizer.param_groups):
             # push this group to list before modify
             self.fp16_groups.append(param_group['params'])
             # init fp16 weight buffer, flattened
-            self.fp16_groups_flat.append(
-                _flatten_dense_tensors([p.clone().detach()
-                                        for p in self.fp16_groups[i]]))
+            self.fp16_groups_flat.append(_flatten_dense_tensors([p.clone().detach() for p in self.fp16_groups[i]]))
             # set model fp16 weight to slices of flattened buffer
-            updated_params = _unflatten_dense_tensors(self.fp16_groups_flat[i],
-                                                      self.fp16_groups[i])
+            updated_params = _unflatten_dense_tensors(self.fp16_groups_flat[i], self.fp16_groups[i])
             for p, q in zip(self.fp16_groups[i], updated_params):
                 p.data = q.data
             # init master weight, flattened
-            self.fp32_groups_flat.append(
-                self.fp16_groups_flat[i].clone().float().detach())
+            self.fp32_groups_flat.append(self.fp16_groups_flat[i].clone().float().detach())
             # modify optimizer of have flat master weight
-            self.fp32_groups_flat[
-                i].requires_grad = True  # keep this in case internal optimizer uses it
+            self.fp32_groups_flat[i].requires_grad = True  # keep this in case internal optimizer uses it
             param_group['params'] = [self.fp32_groups_flat[i]]
 
         # we may have a way of fusing dynamic scale. Do not support for now
         if dynamic_loss_scale:
             self.dynamic_loss_scale = True
             self.cur_iter = 0
             self.last_overflow_iter = -1
@@ -109,24 +107,21 @@
         else:
             self.clip_grad_norm = torch.nn.utils.clip_grad_norm_
 
         #model parallel object
         self.mpu = mpu
 
         self.overflow = False
-        self.overflow_checker = CheckOverflow(self.fp16_groups,
-                                              mpu=self.mpu,
-                                              deepspeed=deepspeed)
+        self.overflow_checker = CheckOverflow(self.fp16_groups, mpu=self.mpu, deepspeed=deepspeed)
         self.initialize_optimizer_states()
 
     def initialize_optimizer_states(self):
         for i, group in enumerate(self.fp16_groups):
-            self.fp32_groups_flat[i].grad = torch.zeros(
-                self.fp32_groups_flat[i].size(),
-                device=self.fp32_groups_flat[i].device)
+            self.fp32_groups_flat[i].grad = torch.zeros(self.fp32_groups_flat[i].size(),
+                                                        device=self.fp32_groups_flat[i].device)
 
         self.optimizer.step()
 
         for i, group in enumerate(self.fp16_groups):
             self.fp32_groups_flat[i].grad = None
 
         return
@@ -152,51 +147,43 @@
 
         # First compute norm for all group so we know if there is overflow
         grads_groups_flat = []
         norm_groups = []
         for i, group in enumerate(self.fp16_groups):
             grads_groups_flat.append(
                 _flatten_dense_tensors([
-                    torch.zeros(p.size(),
-                                dtype=p.dtype,
-                                device=p.device) if p.grad is None else p.grad
-                    for p in group
+                    torch.zeros(p.size(), dtype=p.dtype, device=p.device) if p.grad is None else p.grad for p in group
                 ]))
             norm_groups.append(get_weight_norm(grads_groups_flat[i], mpu=self.mpu))
 
         self.overflow = self.overflow_checker.check_using_norm(norm_groups)
         prev_scale = self.cur_scale
         self._update_scale(self.overflow)
 
         if self.overflow:
             if self.verbose:
-                logger.info(
-                    "[deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss "
-                    "scale: {}, reducing to {}".format(prev_scale,
-                                                       self.cur_scale))
+                logger.info("[deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss "
+                            "scale: {}, reducing to {}".format(prev_scale, self.cur_scale))
             return self.overflow
 
         scaled_grad_norm = get_global_norm(norm_list=norm_groups)
 
-        combined_scale = self.unscale_and_clip_grads(grads_groups_flat,
-                                                     scaled_grad_norm,
-                                                     apply_scale=False)
+        combined_scale = self.unscale_and_clip_grads(grads_groups_flat, scaled_grad_norm, apply_scale=False)
 
         # Stash unscaled gradient norm
         self._global_grad_norm = scaled_grad_norm / self.cur_scale
 
         # norm is in fact norm*cur_scale
         self.optimizer.step(grads=[[g] for g in grads_groups_flat],
                             output_params=[[p] for p in self.fp16_groups_flat],
                             scale=combined_scale,
                             grad_norms=norm_groups)
         # TODO: we probably don't need this? just to be safe
         for i in range(len(norm_groups)):
-            updated_params = _unflatten_dense_tensors(self.fp16_groups_flat[i],
-                                                      self.fp16_groups[i])
+            updated_params = _unflatten_dense_tensors(self.fp16_groups_flat[i], self.fp16_groups[i])
             for p, q in zip(self.fp16_groups[i], updated_params):
                 p.data = q.data
         return self.overflow
 
     def start_timers(self, name_list):
         if self.timers is not None:
             for name in name_list:
@@ -218,17 +205,15 @@
 
     def get_lr(self):
         """Return the current learning rate."""
         return self.optimizer.param_groups[0]["lr"]
 
     def override_loss_scale(self, loss_scale):
         if loss_scale != self.external_loss_scale:
-            logger.info(
-                f'[deepspeed] setting loss scale from {self.external_loss_scale} -> {loss_scale}'
-            )
+            logger.info(f'[deepspeed] setting loss scale from {self.external_loss_scale} -> {loss_scale}')
         self.custom_loss_scaler = True
         self.external_loss_scale = loss_scale
 
     def step(self, closure=None):
         """
         Not supporting closure.
         """
@@ -269,18 +254,16 @@
 
         grads_groups_flat = []
         for i, group in enumerate(self.fp16_groups):
             data_type = self.fp32_groups_flat[i].dtype
 
             grads_groups_flat.append(
                 _flatten_dense_tensors([
-                    torch.zeros(p.size(),
-                                dtype=data_type,
-                                device=p.device)
-                    if p.grad is None else p.grad.to(data_type) for p in group
+                    torch.zeros(p.size(), dtype=data_type, device=p.device) if p.grad is None else p.grad.to(data_type)
+                    for p in group
                 ]))
 
             for p in group:
                 p.grad = None
 
             self.fp32_groups_flat[i].grad = grads_groups_flat[i]
 
@@ -309,16 +292,15 @@
         #get rid of the fp32 gradients. Not needed anymore
         for group in self.fp32_groups_flat:
             group.grad = None
 
         self.start_timers([UPDATE_FP16])
 
         for i in range(len(self.fp16_groups)):
-            updated_params = _unflatten_dense_tensors(self.fp32_groups_flat[i],
-                                                      self.fp16_groups[i])
+            updated_params = _unflatten_dense_tensors(self.fp32_groups_flat[i], self.fp16_groups[i])
             for p, q in zip(self.fp16_groups[i], updated_params):
                 p.data.copy_(q.data)
 
         self.stop_timers([UPDATE_FP16])
 
         self.log_timers(STEP_TIMERS)
 
@@ -330,17 +312,15 @@
         #all_groups_norm_old = all_groups_norm
         # Need to allreduce (avg) the norms across different ranks because moe params will not be synced during allreduce
         if self.using_pipeline:
             pg = self.deepspeed.mpu.get_data_parallel_group()
         else:
             pg = groups._get_data_parallel_group()
         scaled_norm = all_groups_norm * 1.0 / float(dist.get_world_size(group=pg))
-        scaled_norm_tensor = torch.tensor(scaled_norm,
-                                          device=self.fp32_groups_flat[0].device,
-                                          dtype=torch.float)
+        scaled_norm_tensor = torch.tensor(scaled_norm, device=self.fp32_groups_flat[0].device, dtype=torch.float)
         dist.all_reduce(scaled_norm_tensor, group=pg)
         all_groups_norm = scaled_norm_tensor.item()
         #print(f"old = {all_groups_norm_old} and new = {all_groups_norm} at rank: {deepspeed.comm.get_rank()}")
         return all_groups_norm
 
     def unscale_and_clip_grads(self, grad_groups_flat, total_norm, apply_scale=True):
         # compute combined scale factor for this group
@@ -372,33 +352,27 @@
             scaled_loss = (loss.float()) * self.cur_scale
             scaled_loss.backward(create_graph=create_graph, retain_graph=retain_graph)
 
     def _update_scale(self, skip):
         if self.dynamic_loss_scale:
             prev_scale = self.cur_scale
             if skip:
-                self.cur_scale = max(self.cur_scale / self.scale_factor,
-                                     self.min_loss_scale)
+                self.cur_scale = max(self.cur_scale / self.scale_factor, self.min_loss_scale)
                 self.last_overflow_iter = self.cur_iter
                 if self.verbose:
                     logger.info(f"\nGrad overflow on iteration {self.cur_iter}")
-                    logger.info(
-                        f"Reducing dynamic loss scale from {prev_scale} to {self.cur_scale}"
-                    )
+                    logger.info(f"Reducing dynamic loss scale from {prev_scale} to {self.cur_scale}")
             else:
                 # Ensure self.scale_window updates since last overflow
                 stable_interval = (self.cur_iter - self.last_overflow_iter) - 1
                 if (stable_interval > 0) and (stable_interval % self.scale_window == 0):
                     self.cur_scale *= self.scale_factor
                     if self.verbose:
-                        logger.info(
-                            f"No Grad overflow for {self.scale_window} iterations")
-                        logger.info(
-                            f"Increasing dynamic loss scale from {prev_scale} to {self.cur_scale}"
-                        )
+                        logger.info(f"No Grad overflow for {self.scale_window} iterations")
+                        logger.info(f"Increasing dynamic loss scale from {prev_scale} to {self.cur_scale}")
         else:
             if skip:
                 logger.info("Grad overflow on iteration: %s", self.cur_iter)
                 logger.info("Using static loss scale of: %s", self.cur_scale)
         self.cur_iter += 1
         return
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/fp16/loss_scaler.py` & `deepspeed-0.9.0/deepspeed/runtime/fp16/loss_scaler.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,24 +1,29 @@
-# Copyright 2019 The Microsoft DeepSpeed Team
-# Copyright (c) 2019, NVIDIA CORPORATION.  All rights reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#Taken and modified for DeepSpeed from:
-#    https://github.com/NVIDIA/Megatron-LM/blob/master/fp16/loss_scaler.py
-#Commit: 93ab4bea59dc5cbf97c079d313741866af4deac9
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+"""
+Copyright (c) 2019, NVIDIA CORPORATION.  All rights reserved.
+
+ Licensed under the Apache License, Version 2.0 (the "License");
+ you may not use this file except in compliance with the License.
+ You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+Taken and modified for DeepSpeed from:
+    https://github.com/NVIDIA/Megatron-LM/blob/master/fp16/loss_scaler.py
+Commit: 93ab4bea59dc5cbf97c079d313741866af4deac9
+"""
 
 import torch
 from deepspeed import comm as dist
 from deepspeed.utils import logger
 
 INITIAL_LOSS_SCALE = 'init_scale'
 SCALE_WINDOW = 'scale_window'
@@ -33,14 +38,15 @@
     return t[0]
 
 
 class LossScalerBase:
     """LossScalarBase
     Base class for a loss scaler
     """
+
     def __init__(self, cur_scale):
         self.cur_scale = cur_scale
         self.dynamic = False
 
     @property
     def loss_scale(self):
         return self.cur_scale
@@ -50,27 +56,29 @@
 
     def update_scale(self, overflow):
         pass
 
     def backward(self, loss, retain_graph=False):
         scaled_loss = loss * self.loss_scale
         scaled_loss.backward(retain_graph=retain_graph)
+        # print(f'LossScalerBackward: {scaled_loss=}')
 
 
 class LossScaler(LossScalerBase):
     """
     Class that manages a static loss scale.  This class is intended to interact with
     :class:`FP16_Optimizer`, and should not be directly manipulated by the user.
 
     Use of :class:`LossScaler` is enabled via the ``static_loss_scale`` argument to
     :class:`FP16_Optimizer`'s constructor.
 
     Args:
         scale (float, optional, default=1.0):  The loss scale.
     """
+
     def __init__(self, scale=1):
         super(LossScaler, self).__init__(scale)
 
     # `params` is a list / generator of torch.Variable
     def has_overflow(self, params):
         return False
 
@@ -100,14 +108,15 @@
     always using the highest loss scale possible without incurring overflow.
 
     Args:
         init_scale (float, optional, default=2**32):  Initial loss scale attempted by :class:`DynamicLossScaler.`
         scale_factor (float, optional, default=2.0):  Factor used when adjusting the loss scale. If an overflow is encountered, the loss scale is readjusted to loss scale/``scale_factor``.  If ``scale_window`` consecutive iterations take place without an overflow, the loss scale is readjusted to loss_scale*``scale_factor``.
         scale_window (int, optional, default=1000):  Number of consecutive iterations without an overflow to wait before increasing the loss scale.
     """
+
     def __init__(self,
                  init_scale=2**32,
                  scale_factor=2.,
                  scale_window=1000,
                  min_scale=1,
                  delayed_shift=1,
                  consecutive_hysteresis=False,
@@ -158,16 +167,15 @@
     # `overflow` is boolean indicating whether the gradient overflowed
     def update_scale(self, overflow):
         if overflow:
             # self.cur_scale /= self.scale_factor
             if self.delayed_shift == 1 or self.cur_hysteresis == 1:
                 if (self.cur_scale == self.min_scale) and self.raise_error_at_min_scale:
                     raise Exception(
-                        "Current loss scale already at minimum - cannot decrease scale anymore. Exiting run."
-                    )
+                        "Current loss scale already at minimum - cannot decrease scale anymore. Exiting run.")
                 else:
                     next_scale = max(self.cur_scale / self.scale_factor, self.min_scale)
                     if dist.get_rank() == 0:
                         overflow_msg = f"[deepspeed] OVERFLOW! Rank {dist.get_rank()} Skipping step."
                         if self.dtype == torch.half:
                             overflow_msg += f" Attempted loss scale: {int(self.cur_scale)}, reducing to {int(next_scale)}"
                         logger.info(overflow_msg)
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/fp16/onebit/adam.py` & `deepspeed-0.9.0/deepspeed/runtime/fp16/onebit/adam.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,12 @@
-'''
-Copyright 2020 The Microsoft DeepSpeed Team
-'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+
 import types
 import torch
 import numpy as np
 from deepspeed.accelerator import get_accelerator
 from deepspeed import comm as dist
 
 
@@ -35,22 +37,22 @@
             supports CUDA-Aware communication. (default: False)
         comm_backend_name (string, optional): Set to 'mpi' if needed. (default: 'nccl')
     .. _Adam\\: A Method for Stochastic Optimization:
         https://arxiv.org/abs/1412.6980
     .. _On the Convergence of Adam and Beyond:
         https://openreview.net/forum?id=ryQu7f-RZ
     """
+
     def __init__(self,
                  params,
                  deepspeed=None,
                  lr=1e-3,
                  freeze_step=100000,
                  bias_correction=True,
-                 betas=(0.9,
-                        0.999),
+                 betas=(0.9, 0.999),
                  eps=1e-8,
                  eps_inside_sqrt=False,
                  weight_decay=0.,
                  max_grad_norm=0.,
                  amsgrad=False,
                  cuda_aware=False,
                  comm_backend_name='nccl'):
@@ -85,19 +87,20 @@
 
         # Empty initializer. Set handle based on the comm backend as follows.
         self.comm_backend_handle = None
 
         if self.comm_backend_name == 'nccl':
             TORCH_MAJOR = int(torch.__version__.split('.')[0])
             TORCH_MINOR = int(torch.__version__.split('.')[1])
-            assert TORCH_MAJOR >= 1 and TORCH_MINOR >= 8, "Please use torch 1.8 or greater to enable NCCL backend in 1-bit Adam. Alternatively, please specify 'mpi' as the 'comm_backend_name' in config file to proceed with the MPI backend"
+            assert (
+                (TORCH_MAJOR == 1 and TORCH_MINOR >= 8) or TORCH_MAJOR >= 2
+            ), "Please use torch 1.8 or greater to enable NCCL backend in 1-bit Adam. Alternatively, please specify 'mpi' as the 'comm_backend_name' in config file to proceed with the MPI backend"
             assert dist.is_initialized() == True, "Please initialize the torch distributed backend."
             from deepspeed.runtime.comm.nccl import NcclBackend
-            self.using_pipeline = hasattr(self.deepspeed,
-                                          'pipeline_enable_backward_allreduce')
+            self.using_pipeline = hasattr(self.deepspeed, 'pipeline_enable_backward_allreduce')
             self.comm_backend_handle = NcclBackend(self.deepspeed.mpu)
 
         elif self.comm_backend_name == 'mpi':
             from deepspeed.runtime.comm.mpi import MpiBackend
             self.comm_backend_handle = MpiBackend(cuda_aware)
 
         self.size = self.comm_backend_handle.size
@@ -160,30 +163,25 @@
                 if len(state) == 0:
                     state['step'] = 0
                     # Exponential moving average of gradient values
                     state['exp_avg'] = torch.zeros_like(p.data)
                     # Exponential moving average of squared gradient values
                     state['exp_avg_sq'] = torch.zeros_like(p.data)
 
-                if not self.initialize or (self.adam_freeze_key
-                                           and 'worker_error' not in state.keys()):
+                if not self.initialize or (self.adam_freeze_key and 'worker_error' not in state.keys()):
                     state['tensor_size'] = torch.numel(p.data)
                     state['corrected_tensor_size'] = state['tensor_size']
 
                     if state['tensor_size'] % (self.size * self.divider) != 0:
-                        state['corrected_tensor_size'] += ((self.size * self.divider) -
-                                                           (state['tensor_size'] %
-                                                            (self.size * self.divider)))
-                    state['server_chunk_size'] = state[
-                        'corrected_tensor_size'] // self.size
+                        state['corrected_tensor_size'] += ((self.size * self.divider) - (state['tensor_size'] %
+                                                                                         (self.size * self.divider)))
+                    state['server_chunk_size'] = state['corrected_tensor_size'] // self.size
                     get_accelerator().empty_cache()
-                    state['worker_error'] = torch.zeros(state['corrected_tensor_size'],
-                                                        device=p.device)
-                    state['server_error'] = torch.zeros(state['server_chunk_size'],
-                                                        device=p.device)
+                    state['worker_error'] = torch.zeros(state['corrected_tensor_size'], device=p.device)
+                    state['server_error'] = torch.zeros(state['server_chunk_size'], device=p.device)
                     get_accelerator().empty_cache()
                     self.adam_freeze_key = True
                     if not self.initialize and dist.get_rank() == 0:
                         print("Cupy Buffers Initialized Successfully.")
 
                 exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']
                 beta1, beta2 = group['betas']
@@ -207,30 +205,27 @@
                     else:
                         if self.initialize is True:
                             exp_avg.mul_(beta1).add_(1 - beta1, grad)
                         grad = None
 
                         if self.size > 1:
                             exp_avg.set_(
-                                self.comm_backend_handle.compressed_allreduce(
-                                    exp_avg,
-                                    state['worker_error'],
-                                    state['server_error'],
-                                    self.deepspeed.local_rank))
+                                self.comm_backend_handle.compressed_allreduce(exp_avg, state['worker_error'],
+                                                                              state['server_error'],
+                                                                              self.deepspeed.local_rank))
                         # Because 1-bit compression cannot represent exact zero, it is required to
                         # provide a momentum mask for those params that have constant exact zeros in their
                         # momentums, otherwise the compression error would keep accumulating.
                         # For example, for BERT pre-training seq 128, bert.embeddings.position_embeddings.weight
                         # always have exact zeros in its momentum for row 129 to 512, because it only
                         # learns up to seq length 128 while the model supports up to 512 seq length.
                         # (See example in DeepSpeedExamples/bing_bert/deepspeed_train.py.)
                         if 'exp_avg_mask' in group:
                             if exp_avg.device != group['exp_avg_mask'].device:
-                                group['exp_avg_mask'] = group['exp_avg_mask'].to(
-                                    device=exp_avg.device)
+                                group['exp_avg_mask'] = group['exp_avg_mask'].to(device=exp_avg.device)
                             exp_avg.mul_(group['exp_avg_mask'])
 
                     if self.initialize:
                         update = exp_avg / (exp_avg_sq.sqrt() + group['eps'])
 
                 if self.initialize:
                     if group['weight_decay'] > 0.0:
@@ -268,32 +263,29 @@
         # BERT pre-training seqlen 128 and 512 ), we don't use the exp_avg_mask
         # in checkpoints but always use the one user provided in training script.
         # (See example in DeepSpeedExamples/bing_bert/deepspeed_train.py.)
         # Thus here we keep the exp_avg_mask unchanged when loading checkpoint
         for i, group in enumerate(self.param_groups):
             if 'exp_avg_mask' in group:
                 state_dict['param_groups'][i]['exp_avg_mask'] = group['exp_avg_mask']
-            elif 'exp_avg_mask' not in group and 'exp_avg_mask' in state_dict[
-                    'param_groups'][i]:
+            elif 'exp_avg_mask' not in group and 'exp_avg_mask' in state_dict['param_groups'][i]:
                 state_dict['param_groups'][i].pop('exp_avg_mask')
         super().load_state_dict(state_dict)
         if self.state[self.param_groups[0]['params'][0]]['step'] < self.freeze_step:
             if dist.get_rank() == 0:
                 print("Checkpoint loaded and OnebitAdam warmup stage starts/continues.")
             if self.adam_freeze_key is True:
                 self.adam_freeze_key = False
                 if self.using_pipeline:
                     self.deepspeed.pipeline_enable_backward_allreduce = True
                 else:
                     self.deepspeed.enable_backward_allreduce = True
         else:
             if dist.get_rank() == 0:
-                print(
-                    "Checkpoint loaded and OnebitAdam compression stage starts/continues."
-                )
+                print("Checkpoint loaded and OnebitAdam compression stage starts/continues.")
             if self.adam_freeze_key is False:
                 self.adam_freeze_key = True
                 if self.using_pipeline:
                     self.deepspeed.pipeline_enable_backward_allreduce = False
                 else:
                     self.deepspeed.enable_backward_allreduce = False
         # We reset the compression errors when loading checkpoints for 3 reasons:
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/fp16/onebit/lamb.py` & `deepspeed-0.9.0/deepspeed/runtime/fp16/onebit/lamb.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,12 @@
-'''
-Copyright 2021 The Microsoft DeepSpeed Team
-'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+
 import types
 import torch
 import numpy as np
 from deepspeed import comm as dist
 from torch._utils import _flatten_dense_tensors, _unflatten_dense_tensors
 from deepspeed.accelerator import get_accelerator
 
@@ -50,22 +52,22 @@
     .. _Large Batch Optimization for Deep Learning\\: Training BERT in 76 minutes:
         https://arxiv.org/abs/1904.00962
     .. _Adam\\: A Method for Stochastic Optimization:
         https://arxiv.org/abs/1412.6980
     .. _On the Convergence of Adam and Beyond:
         https://openreview.net/forum?id=ryQu7f-RZ
     """
+
     def __init__(self,
                  params,
                  deepspeed=None,
                  lr=1e-3,
                  freeze_step=100000,
                  bias_correction=True,
-                 betas=(0.9,
-                        0.999),
+                 betas=(0.9, 0.999),
                  eps=1e-8,
                  eps_inside_sqrt=False,
                  weight_decay=0.,
                  max_grad_norm=0.,
                  max_coeff=10.0,
                  min_coeff=0.01,
                  amsgrad=False,
@@ -107,19 +109,20 @@
 
         # Empty initializer. Set handle based on the comm backend as follows.
         self.comm_backend_handle = None
 
         if self.comm_backend_name == 'nccl':
             TORCH_MAJOR = int(torch.__version__.split('.')[0])
             TORCH_MINOR = int(torch.__version__.split('.')[1])
-            assert TORCH_MAJOR >= 1 and TORCH_MINOR >= 8, "Please use torch 1.8 or greater to enable NCCL backend in 1-bit Adam. Alternatively, please specify 'mpi' as the 'comm_backend_name' in config file to proceed with the MPI backend"
+            assert (
+                (TORCH_MAJOR == 1 and TORCH_MINOR >= 8) or TORCH_MAJOR >= 2
+            ), "Please use torch 1.8 or greater to enable NCCL backend in 1-bit Adam. Alternatively, please specify 'mpi' as the 'comm_backend_name' in config file to proceed with the MPI backend"
             assert dist.is_initialized() == True, "Please initialize the torch distributed backend."
             from deepspeed.runtime.comm.nccl import NcclBackend
-            self.using_pipeline = hasattr(self.deepspeed,
-                                          'pipeline_enable_backward_allreduce')
+            self.using_pipeline = hasattr(self.deepspeed, 'pipeline_enable_backward_allreduce')
             self.comm_backend_handle = NcclBackend(self.deepspeed.mpu)
 
         elif self.comm_backend_name == 'mpi':
             from deepspeed.runtime.comm.mpi import MpiBackend
             self.comm_backend_handle = MpiBackend(cuda_aware)
 
         self.size = self.comm_backend_handle.size
@@ -161,32 +164,28 @@
 
         #remove the previous stats
         del self.lamb_coeffs[:]
 
         if self.lamb_freeze_key:
             exp_avg_last_step = []
             for group in self.param_groups:
-                exp_avg_last_step.append(
-                    [self.state[p]['exp_avg'].detach().clone() for p in group['params']])
+                exp_avg_last_step.append([self.state[p]['exp_avg'].detach().clone() for p in group['params']])
             if 'scaling_coeff' not in self.state[self.param_groups[0]['params'][0]]:
                 # Compute the scaling_coeff for each momentum at the end of warmup stage.
                 # This is used to reduce compression error during compression stage.
                 momentum_scales = []
                 for group in self.param_groups:
                     momentum_scales.append([
-                        (torch.norm(self.state[p]['exp_avg']) /
-                         np.sqrt(torch.numel(self.state[p]['exp_avg']))).item()
+                        (torch.norm(self.state[p]['exp_avg']) / np.sqrt(torch.numel(self.state[p]['exp_avg']))).item()
                         for p in group['params']
                     ])
-                united_scale = sum([sum(x) for x in momentum_scales]) / sum(
-                    [len(x) for x in momentum_scales])
+                united_scale = sum([sum(x) for x in momentum_scales]) / sum([len(x) for x in momentum_scales])
                 for i, group in enumerate(self.param_groups):
                     for j, p in enumerate(group['params']):
-                        self.state[p][
-                            'scaling_coeff'] = united_scale / momentum_scales[i][j]
+                        self.state[p]['scaling_coeff'] = united_scale / momentum_scales[i][j]
 
         for group, grads_this_group in zip(self.param_groups, grads_group):
             if grads_this_group is None:
                 grads_this_group = [None] * len(group['params'])
 
             bias_correction = 1 if group['bias_correction'] else 0
 
@@ -197,29 +196,29 @@
                     grad = p.grad.data
                 if grad.is_sparse:
                     raise RuntimeError('1-bit Lamb does not support sparse gradients')
 
                 state = self.state[p]
 
                 # State initialization
-                if len(state) == 0 or (len(state) == 1
-                                       and 'scaling_coeff' in state.keys()):
+                if len(state) == 0 or (len(state) == 1 and 'scaling_coeff' in state.keys()):
                     state['step'] = 0
                     state['lamb_coeff_freeze'] = 0.0
                     state['last_factor'] = 1.0
                     # Exponential moving average of gradient values
                     state['exp_avg'] = torch.zeros_like(p.data)
                     # Exponential moving average of squared gradient values
                     state['exp_avg_sq'] = torch.zeros_like(p.data)
                     state['exp_avg_sq_fresh'] = torch.zeros_like(p.data)
 
                 if not self.initialize:
                     self.lamb_freeze_key = True
 
-                exp_avg, exp_avg_sq, exp_avg_sq_fresh = state['exp_avg'], state['exp_avg_sq'], state['exp_avg_sq_fresh']
+                exp_avg, exp_avg_sq, exp_avg_sq_fresh = state['exp_avg'], state['exp_avg_sq'], state[
+                    'exp_avg_sq_fresh']
                 beta1, beta2 = group['betas']
                 max_coeff = group['max_coeff']
                 min_coeff = group['min_coeff']
 
                 state['step'] += 1
 
                 if self.lamb_freeze_key is False:
@@ -239,16 +238,16 @@
                         if weight_norm != 0 and update_norm != 0:
                             lamb_coeff = (weight_norm / update_norm).item()
                             if lamb_coeff > max_coeff:
                                 lamb_coeff = max_coeff
                             if lamb_coeff < min_coeff:
                                 lamb_coeff = min_coeff
                         if lamb_coeff != 1.0:
-                            state['lamb_coeff_freeze'] = self.coeff_beta * state[
-                                'lamb_coeff_freeze'] + (1 - self.coeff_beta) * lamb_coeff
+                            state['lamb_coeff_freeze'] = self.coeff_beta * state['lamb_coeff_freeze'] + (
+                                1 - self.coeff_beta) * lamb_coeff
                         self.lamb_coeffs.append(lamb_coeff)
                         with torch.no_grad():
                             p.add_(-group['lr'] * lamb_coeff * update)
                 else:
                     # compression stage, update each momentum locally, then
                     # communicate based on the compressed_allreduce below
                     if self.initialize:
@@ -262,118 +261,97 @@
             tensor_size = 0
             for group in self.param_groups:
                 for p in group['params']:
                     momentum_groups.append(self.state[p]['exp_avg'])
                     tensor_size += torch.numel(p.data)
             corrected_tensor_size = tensor_size
             if tensor_size % (self.size * self.divider) != 0:
-                difference = ((self.size * self.divider) - (tensor_size %
-                                                            (self.size * self.divider)))
+                difference = ((self.size * self.divider) - (tensor_size % (self.size * self.divider)))
                 corrected_tensor_size += difference
-                self.dummy_exp_avg[0] = torch.zeros(
-                    difference,
-                    device=momentum_groups[0].data.device)
+                self.dummy_exp_avg[0] = torch.zeros(difference, device=momentum_groups[0].data.device)
                 momentum_groups.append(self.dummy_exp_avg[0])
             self.corrected_tensor_sizes.append(corrected_tensor_size)
             self.server_chunk_sizes.append(corrected_tensor_size // self.size)
 
-            self.exp_avg_flat.append(
-                _flatten_dense_tensors([p.detach().clone() for p in momentum_groups]))
-            updated_params = _unflatten_dense_tensors(self.exp_avg_flat[0],
-                                                      momentum_groups)
+            self.exp_avg_flat.append(_flatten_dense_tensors([p.detach().clone() for p in momentum_groups]))
+            updated_params = _unflatten_dense_tensors(self.exp_avg_flat[0], momentum_groups)
             for p, q in zip(momentum_groups, updated_params):
                 p.data = q.data
 
         if self.initialize and len(self.worker_errors) == 0:
             get_accelerator().empty_cache()
             for i in range(len(self.exp_avg_flat)):
                 self.worker_errors.append(
-                    torch.zeros(self.corrected_tensor_sizes[i],
-                                device=self.exp_avg_flat[i].device))
-                self.server_errors.append(
-                    torch.zeros(self.server_chunk_sizes[i],
-                                device=self.exp_avg_flat[i].device))
+                    torch.zeros(self.corrected_tensor_sizes[i], device=self.exp_avg_flat[i].device))
+                self.server_errors.append(torch.zeros(self.server_chunk_sizes[i], device=self.exp_avg_flat[i].device))
             get_accelerator().empty_cache()
 
         if self.lamb_freeze_key:
             if self.size > 1:
                 for i in range(len(self.exp_avg_flat)):
                     if not self.initialize:
                         get_accelerator().empty_cache()
                         self.worker_errors.append(
-                            torch.zeros(self.corrected_tensor_sizes[i],
-                                        device=self.exp_avg_flat[i].device))
+                            torch.zeros(self.corrected_tensor_sizes[i], device=self.exp_avg_flat[i].device))
                         self.server_errors.append(
-                            torch.zeros(self.server_chunk_sizes[i],
-                                        device=self.exp_avg_flat[i].device))
+                            torch.zeros(self.server_chunk_sizes[i], device=self.exp_avg_flat[i].device))
                         get_accelerator().empty_cache()
                         if dist.get_rank() == 0:
                             print("Cupy Buffers Initialized Successfully.")
 
-                        self.comm_backend_handle.compressed_allreduce(
-                            self.exp_avg_flat[i],
-                            self.worker_errors[0],
-                            self.server_errors[0],
-                            self.deepspeed.local_rank)
+                        self.comm_backend_handle.compressed_allreduce(self.exp_avg_flat[i], self.worker_errors[0],
+                                                                      self.server_errors[0], self.deepspeed.local_rank)
 
                         if dist.get_rank() == 0:
                             print('Pop out errors', flush=True)
                         del self.worker_errors[:]
                         del self.server_errors[:]
                     else:
-                        self.comm_backend_handle.compressed_allreduce(
-                            self.exp_avg_flat[i],
-                            self.worker_errors[i],
-                            self.server_errors[i],
-                            self.deepspeed.local_rank)
+                        self.comm_backend_handle.compressed_allreduce(self.exp_avg_flat[i], self.worker_errors[i],
+                                                                      self.server_errors[i], self.deepspeed.local_rank)
 
         if self.lamb_freeze_key and self.initialize:
             for i, group in enumerate(self.param_groups):
                 bias_correction = 1 if group['bias_correction'] else 0
 
                 for j, p in enumerate(group['params']):
                     state = self.state[p]
-                    exp_avg, exp_avg_sq, exp_avg_sq_fresh = state['exp_avg'], state['exp_avg_sq'], state['exp_avg_sq_fresh']
+                    exp_avg, exp_avg_sq, exp_avg_sq_fresh = state['exp_avg'], state['exp_avg_sq'], state[
+                        'exp_avg_sq_fresh']
                     beta1, beta2 = group['betas']
                     exp_avg.div_(self.state[p]['scaling_coeff'])
                     # Because 1-bit compression cannot represent exact zero, it is required to
                     # provide a momentum mask for those params that have constant exact zeros in their
                     # momentums, otherwise the compression error would keep accumulating.
                     # For example, for BERT pre-training seq 128, bert.embeddings.position_embeddings.weight
                     # always have exact zeros in its momentum for row 129 to 512, because it only
                     # learns up to seq length 128 while the model supports up to 512 seq length.
                     # (See example in DeepSpeedExamples/bing_bert/deepspeed_train.py about how
                     # to add this exp_avg_mask for BERT pre-training.)
                     if 'exp_avg_mask' in group:
                         if exp_avg.device != group['exp_avg_mask'].device:
-                            group['exp_avg_mask'] = group['exp_avg_mask'].to(
-                                device=exp_avg.device)
+                            group['exp_avg_mask'] = group['exp_avg_mask'].to(device=exp_avg.device)
                         exp_avg.mul_(group['exp_avg_mask'])
 
-                    grad_reconstruct = ((exp_avg - exp_avg_last_step[i][j] * beta1) /
-                                        (1 - beta1))
-                    exp_avg_sq_fresh.mul_(beta2).addcmul_(1 - beta2,
-                                                          grad_reconstruct,
-                                                          grad_reconstruct)
+                    grad_reconstruct = ((exp_avg - exp_avg_last_step[i][j] * beta1) / (1 - beta1))
+                    exp_avg_sq_fresh.mul_(beta2).addcmul_(1 - beta2, grad_reconstruct, grad_reconstruct)
                     denom = exp_avg_sq.sqrt() + group['eps']
                     update_prelim = exp_avg / denom
 
                     if group['weight_decay'] > 0.0:
                         update = update_prelim + group['weight_decay'] * p.data
                     else:
                         update = update_prelim
 
                     lamb_coeff = 1.0
                     update_norm = update.pow(2).sum().sqrt()
                     denom_real = exp_avg_sq_fresh.sqrt() + group['eps']
                     factor = (denom / denom_real).max().item()
                     if group['weight_decay'] > 0.0:
-                        update_ratio = min(1.0,
-                                           (update_prelim.pow(2).sum().sqrt() /
-                                            update_norm).item())
+                        update_ratio = min(1.0, (update_prelim.pow(2).sum().sqrt() / update_norm).item())
                         factor = factor * update_ratio + (1.0 - update_ratio)
                     if factor > self.factor_max:
                         factor = self.factor_max
                     if factor < self.factor_min:
                         factor = self.factor_min
                     if factor > state['last_factor'] * (1.0 + self.factor_threshold):
                         factor = state['last_factor'] * (1.0 + self.factor_threshold)
@@ -412,16 +390,15 @@
         # BERT pre-training seqlen 128 and 512 ), we don't use the exp_avg_mask
         # in checkpoints but always use the one user provided in training script.
         # (See example in DeepSpeedExamples/bing_bert/deepspeed_train.py.)
         # Thus here we keep the exp_avg_mask unchanged when loading checkpoint
         for i, group in enumerate(self.param_groups):
             if 'exp_avg_mask' in group:
                 state_dict['param_groups'][i]['exp_avg_mask'] = group['exp_avg_mask']
-            elif 'exp_avg_mask' not in group and 'exp_avg_mask' in state_dict[
-                    'param_groups'][i]:
+            elif 'exp_avg_mask' not in group and 'exp_avg_mask' in state_dict['param_groups'][i]:
                 state_dict['param_groups'][i].pop('exp_avg_mask')
         super().load_state_dict(state_dict)
         # need to reset the fused momentum since loading states will break the linking
         del self.exp_avg_flat[:]
         self.dummy_exp_avg.clear()
         del self.corrected_tensor_sizes[:]
         del self.server_chunk_sizes[:]
@@ -438,17 +415,15 @@
                 for p in group['params']:
                     self.state[p]['lamb_coeff_freeze'] = 0.0
                     self.state[p]['last_factor'] = 1.0
                     if 'scaling_coeff' in self.state[p]:
                         self.state[p].pop('scaling_coeff')
         else:
             if dist.get_rank() == 0:
-                print(
-                    "Checkpoint loaded and OnebitLamb compression stage starts/continues."
-                )
+                print("Checkpoint loaded and OnebitLamb compression stage starts/continues.")
             if self.lamb_freeze_key is False:
                 self.lamb_freeze_key = True
                 if self.using_pipeline:
                     self.deepspeed.pipeline_enable_backward_allreduce = False
                 else:
                     self.deepspeed.enable_backward_allreduce = False
         # We reset the compression errors when loading checkpoints for 3 reasons:
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/fp16/onebit/zoadam.py` & `deepspeed-0.9.0/deepspeed/runtime/fp16/onebit/zoadam.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,12 @@
-'''
-Copyright 2020 The Microsoft DeepSpeed Team
-'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+
 import types
 import torch
 import numpy as np
 from deepspeed.accelerator import get_accelerator
 from deepspeed import comm as dist
 
 
@@ -45,21 +47,21 @@
             supports CUDA-Aware communication. (default: False)
         comm_backend_name (string, optional): Set to 'mpi' if needed. (default: 'nccl')
     .. _Adam\\: A Method for Stochastic Optimization:
         https://arxiv.org/abs/1412.6980
     .. _On the Convergence of Adam and Beyond:
         https://openreview.net/forum?id=ryQu7f-RZ
     """
+
     def __init__(self,
                  params,
                  deepspeed=None,
                  lr=1e-3,
                  bias_correction=True,
-                 betas=(0.9,
-                        0.999),
+                 betas=(0.9, 0.999),
                  eps=1e-8,
                  eps_inside_sqrt=False,
                  weight_decay=0.,
                  max_grad_norm=0.,
                  var_freeze_step=100000,
                  var_update_scaler=16,
                  local_step_scaler=32678,
@@ -98,19 +100,20 @@
 
         # Empty initializer. Set handle based on the comm backend as follows.
         self.comm_backend_handle = None
 
         if self.comm_backend_name == 'nccl':
             TORCH_MAJOR = int(torch.__version__.split('.')[0])
             TORCH_MINOR = int(torch.__version__.split('.')[1])
-            assert TORCH_MAJOR >= 1 and TORCH_MINOR >= 8, "Please use torch 1.8 or greater to enable NCCL backend in 0/1 Adam. Alternatively, please specify 'mpi' as the 'comm_backend_name' in config file to proceed with the MPI backend"
+            assert (
+                (TORCH_MAJOR == 1 and TORCH_MINOR >= 8) or TORCH_MAJOR >= 2
+            ), "Please use torch 1.8 or greater to enable NCCL backend in 0/1 Adam. Alternatively, please specify 'mpi' as the 'comm_backend_name' in config file to proceed with the MPI backend"
             assert dist.is_initialized() == True, "Please initialize the torch distributed backend."
             from deepspeed.runtime.comm.nccl import NcclBackend
-            self.using_pipeline = hasattr(self.deepspeed,
-                                          'pipeline_enable_backward_allreduce')
+            self.using_pipeline = hasattr(self.deepspeed, 'pipeline_enable_backward_allreduce')
             self.comm_backend_handle = NcclBackend(self.deepspeed.mpu)
 
         elif self.comm_backend_name == 'mpi':
             from deepspeed.runtime.comm.mpi import MpiBackend
             self.comm_backend_handle = MpiBackend(cuda_aware)
 
         self.size = self.comm_backend_handle.size
@@ -177,24 +180,20 @@
                     state['local_step_interval'] = 1
                     state['local_step_counter'] = 0
                     state['lrs'] = 0
                     state['tensor_size'] = torch.numel(p.data)
                     state['corrected_tensor_size'] = state['tensor_size']
 
                     if state['tensor_size'] % (self.size * self.divider) != 0:
-                        state['corrected_tensor_size'] += ((self.size * self.divider) -
-                                                           (state['tensor_size'] %
-                                                            (self.size * self.divider)))
-                    state['server_chunk_size'] = state[
-                        'corrected_tensor_size'] // self.size
+                        state['corrected_tensor_size'] += ((self.size * self.divider) - (state['tensor_size'] %
+                                                                                         (self.size * self.divider)))
+                    state['server_chunk_size'] = state['corrected_tensor_size'] // self.size
                     get_accelerator().empty_cache()
-                    state['worker_error'] = torch.zeros(state['corrected_tensor_size'],
-                                                        device=p.device)
-                    state['server_error'] = torch.zeros(state['server_chunk_size'],
-                                                        device=p.device)
+                    state['worker_error'] = torch.zeros(state['corrected_tensor_size'], device=p.device)
+                    state['server_error'] = torch.zeros(state['server_chunk_size'], device=p.device)
                     # Accumulation of momentum, i.e., the u variable in the 0/1 Adam paper
                     state['momentum_accumulator'] = torch.zeros_like(p.data)
                     get_accelerator().empty_cache()
                     # self.freeze_key = True
                     if not self.initialize and dist.get_rank() == 0:
                         print("Cupy Buffers Initialized Successfully.")
 
@@ -209,69 +208,56 @@
                         if state['step'] % state['var_interval'] == 0:
                             exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)
                             exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)
                         else:
                             if self.size > 1:
                                 with torch.no_grad():
                                     grad_onebit = self.comm_backend_handle.compressed_allreduce(
-                                        grad,
-                                        state['worker_error'],
-                                        state['server_error'],
-                                        self.deepspeed.local_rank)
+                                        grad, state['worker_error'], state['server_error'], self.deepspeed.local_rank)
                                     if 'exp_avg_mask' in group:
-                                        if grad_onebit.device != group[
-                                                'exp_avg_mask'].device:
-                                            group['exp_avg_mask'] = group[
-                                                'exp_avg_mask'].to(
-                                                    device=grad_onebit.device)
+                                        if grad_onebit.device != group['exp_avg_mask'].device:
+                                            group['exp_avg_mask'] = group['exp_avg_mask'].to(device=grad_onebit.device)
                                         grad_onebit.mul_(group['exp_avg_mask'])
                                     exp_avg.mul_(beta1).add_(1 - beta1, grad_onebit)
                     else:
                         exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)
                         state['lrs'] += group['lr']
                     grad = None
 
                 if not self.initialize:
                     if self.size > 1:
                         comm_buffer.set_(
-                            self.comm_backend_handle.compressed_allreduce(
-                                comm_buffer,
-                                state['worker_error'],
-                                state['server_error'],
-                                self.deepspeed.local_rank))
+                            self.comm_backend_handle.compressed_allreduce(comm_buffer, state['worker_error'],
+                                                                          state['server_error'],
+                                                                          self.deepspeed.local_rank))
                         if 'exp_avg_mask' in group:
                             if comm_buffer.device != group['exp_avg_mask'].device:
-                                group['exp_avg_mask'] = group['exp_avg_mask'].to(
-                                    device=comm_buffer.device)
+                                group['exp_avg_mask'] = group['exp_avg_mask'].to(device=comm_buffer.device)
                             comm_buffer.mul_(group['exp_avg_mask'])
 
                 if self.initialize:
                     update = exp_avg / (exp_avg_sq.sqrt() + group['eps'])
                     if group['weight_decay'] > 0.0:
                         update += group['weight_decay'] * p.data
                     with torch.no_grad():
                         p.data.add_(-group['lr'] * update)
                         if self.freeze_key is True:
                             comm_buffer.add_(-group['lr'] * update)
-                    if state['step'] % state[
-                            'local_step_interval'] == 0 and self.freeze_key:
+                    if state['step'] % state['local_step_interval'] == 0 and self.freeze_key:
                         with torch.no_grad():
                             p.data.add_(-1 * comm_buffer)
                             comm_buffer.mul_(exp_avg_sq.sqrt() + group['eps'])
                             if self.size > 1:
                                 comm_buffer.copy_(
-                                    self.comm_backend_handle.compressed_allreduce(
-                                        comm_buffer,
-                                        state['worker_error'],
-                                        state['server_error'],
-                                        self.deepspeed.local_rank))
+                                    self.comm_backend_handle.compressed_allreduce(comm_buffer, state['worker_error'],
+                                                                                  state['server_error'],
+                                                                                  self.deepspeed.local_rank))
                                 if 'exp_avg_mask' in group:
                                     if comm_buffer.device != group['exp_avg_mask'].device:
-                                        group['exp_avg_mask'] = group['exp_avg_mask'].to(
-                                            device=comm_buffer.device)
+                                        group['exp_avg_mask'] = group['exp_avg_mask'].to(device=comm_buffer.device)
                                     comm_buffer.mul_(group['exp_avg_mask'])
                             exp_avg.zero_().add_(comm_buffer / state['lrs'], alpha=-1)
                             p.data.add_(comm_buffer / (exp_avg_sq.sqrt() + group['eps']))
                             comm_buffer.zero_()
 
                             state['lrs'] = 0
 
@@ -294,17 +280,16 @@
                                 self.deepspeed.pipeline_enable_backward_allreduce = False
                             else:
                                 self.deepspeed.enable_backward_allreduce = False
                     else:
                         state['local_step_counter'] += 1
                         if state['local_step_counter'] == self.local_step_scaler:
                             state['local_step_counter'] = 0
-                            state['local_step_interval'] = min(
-                                self.local_step_clipper,
-                                state['local_step_interval'] * 2)
+                            state['local_step_interval'] = min(self.local_step_clipper,
+                                                               state['local_step_interval'] * 2)
 
             if not self.initialize:
                 print('Pop out errors', flush=True)
                 self.freeze_key = False
                 state.pop('worker_error')
                 state.pop('server_error')
 
@@ -339,22 +324,21 @@
         # BERT pre-training seqlen 128 and 512 ), we don't use the exp_avg_mask
         # in checkpoints but always use the one user provided in training script.
         # (See example in DeepSpeedExamples/bing_bert/deepspeed_train.py.)
         # Thus here we keep the exp_avg_mask unchanged when loading checkpoint
         for i, group in enumerate(self.param_groups):
             if 'exp_avg_mask' in group:
                 state_dict['param_groups'][i]['exp_avg_mask'] = group['exp_avg_mask']
-            elif 'exp_avg_mask' not in group and 'exp_avg_mask' in state_dict[
-                    'param_groups'][i]:
+            elif 'exp_avg_mask' not in group and 'exp_avg_mask' in state_dict['param_groups'][i]:
                 state_dict['param_groups'][i].pop('exp_avg_mask')
         super().load_state_dict(state_dict)
         if self.state[self.param_groups[0]['params'][0]]['step'] < self.var_freeze_step:
             self.var_freeze_key = False
-            if (self.state[self.param_groups[0]['params'][0]]['step'] + 1
-                ) % self.state[self.param_groups[0]['params'][0]]['var_interval'] == 0:
+            if (self.state[self.param_groups[0]['params'][0]]['step'] +
+                    1) % self.state[self.param_groups[0]['params'][0]]['var_interval'] == 0:
                 if self.using_pipeline:
                     self.deepspeed.pipeline_enable_backward_allreduce = True
                 else:
                     self.deepspeed.enable_backward_allreduce = True
             else:
                 if self.using_pipeline:
                     self.deepspeed.pipeline_enable_backward_allreduce = False
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/fp16/unfused_optimizer.py` & `deepspeed-0.9.0/deepspeed/runtime/fp16/unfused_optimizer.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,13 +1,15 @@
-'''
-Copyright 2019 The Microsoft DeepSpeed Team
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
 
+# DeepSpeed Team
+"""
 Copyright NVIDIA/apex
 This file is adapted from FP16_Optimizer in NVIDIA/apex
-'''
+"""
 
 from deepspeed.moe.utils import split_params_grads_into_shared_and_expert_params
 import torch
 from torch._utils import _flatten_dense_tensors
 
 from deepspeed.runtime import DeepSpeedOptimizer
 from deepspeed.runtime.utils import get_global_norm, CheckOverflow, get_weight_norm
@@ -20,14 +22,15 @@
 
 class FP16_UnfusedOptimizer(DeepSpeedOptimizer):
     """
     FP16 Optimizer without weight fusion to support LAMB optimizer
 
     For usage example please see, TODO:  DeepSpeed V2 Tutorial
     """
+
     def __init__(self,
                  init_optimizer,
                  deepspeed=None,
                  static_loss_scale=1.0,
                  dynamic_loss_scale=False,
                  dynamic_loss_args=None,
                  verbose=True,
@@ -101,17 +104,15 @@
             self.clip_grad_norm = torch.nn.utils.clip_grad_norm
         else:
             self.clip_grad_norm = torch.nn.utils.clip_grad_norm_
 
         self.mpu = mpu
 
         self.overflow = False
-        self.overflow_checker = CheckOverflow(self.fp16_groups,
-                                              mpu=self.mpu,
-                                              deepspeed=deepspeed)
+        self.overflow_checker = CheckOverflow(self.fp16_groups, mpu=self.mpu, deepspeed=deepspeed)
 
         self.initialize_optimizer_states()
 
     def zero_grad(self, set_to_none=False):
         """
         Zero FP16 parameter grads.
         """
@@ -133,53 +134,41 @@
         # First compute norm for all group so we know if there is overflow
         grads_groups_flat = []
         grads_groups = []
         norm_groups = []
         expert_norm_groups = []
         for i, group in enumerate(self.fp16_groups):
             grads = [
-                torch.zeros(p.size(),
-                            dtype=p.dtype,
-                            device=p.device) if p.grad is None else p.grad for p in group
+                torch.zeros(p.size(), dtype=p.dtype, device=p.device) if p.grad is None else p.grad for p in group
             ]
             grads_groups.append(grads)
             grads_groups_flat.append(_flatten_dense_tensors(grads))
             grads_for_norm, expert_grads_for_norm = split_params_grads_into_shared_and_expert_params(group)
             norm_group_value = 0.0
             if len(grads_for_norm) > 0:
-                norm_group_value = get_weight_norm(
-                    _flatten_dense_tensors(grads_for_norm),
-                    mpu=self.mpu)
+                norm_group_value = get_weight_norm(_flatten_dense_tensors(grads_for_norm), mpu=self.mpu)
             norm_groups.append(norm_group_value)
             expert_norm_group_value = 0.0
             if len(expert_grads_for_norm) > 0:
-                expert_norm_group_value = get_weight_norm(
-                    _flatten_dense_tensors(expert_grads_for_norm),
-                    mpu=self.mpu)
+                expert_norm_group_value = get_weight_norm(_flatten_dense_tensors(expert_grads_for_norm), mpu=self.mpu)
             expert_norm_groups.append(expert_norm_group_value)
 
-        self.overflow = self.overflow_checker.check_using_norm(norm_groups +
-                                                               expert_norm_groups)
+        self.overflow = self.overflow_checker.check_using_norm(norm_groups + expert_norm_groups)
         prev_scale = self.cur_scale
 
         self._update_scale(self.overflow)
         if self.overflow:
             if self.verbose:
-                logger.info(
-                    "[deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss "
-                    "scale: {}, reducing to {}".format(prev_scale,
-                                                       self.cur_scale))
+                logger.info("[deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss "
+                            "scale: {}, reducing to {}".format(prev_scale, self.cur_scale))
             return self.overflow
 
         self._global_grad_norm = get_global_norm(norm_list=norm_groups)
-        combined_scale = self.unscale_and_clip_grads(self._global_grad_norm,
-                                                     apply_scale=False)
-        self.optimizer.step(grads=grads_groups,
-                            output_params=self.fp16_groups,
-                            scale=combined_scale)
+        combined_scale = self.unscale_and_clip_grads(self._global_grad_norm, apply_scale=False)
+        self.optimizer.step(grads=grads_groups, output_params=self.fp16_groups, scale=combined_scale)
 
         for fp32_group, fp16_group in zip(self.fp32_groups, self.fp16_groups):
             for idx, (fp32_param, fp16_param) in enumerate(zip(fp32_group, fp16_group)):
 
                 #remove the fp32 grad
                 fp32_param.grad = None
 
@@ -195,17 +184,15 @@
 
     def get_lr(self):
         """Return the current learning rate."""
         return self.optimizer.param_groups[0]["lr"]
 
     def override_loss_scale(self, loss_scale):
         if loss_scale != self.external_loss_scale:
-            logger.info(
-                f'[deepspeed] setting loss scale from {self.external_loss_scale} -> {loss_scale}'
-            )
+            logger.info(f'[deepspeed] setting loss scale from {self.external_loss_scale} -> {loss_scale}')
         self.custom_loss_scaler = True
         self.external_loss_scale = loss_scale
 
     def step(self, closure=None):
         """
         Not supporting closure.
         """
@@ -215,34 +202,30 @@
 
         self.overflow = self.overflow_checker.check()
         prev_scale = self.cur_scale
 
         self._update_scale(self.overflow)
         if self.overflow:
             if self.verbose:
-                logger.info(
-                    "[deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss "
-                    "scale: {}, reducing to {}".format(prev_scale,
-                                                       self.cur_scale))
+                logger.info("[deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss "
+                            "scale: {}, reducing to {}".format(prev_scale, self.cur_scale))
             return self.overflow
 
         norm_groups = []
         for i, group in enumerate(self.fp16_groups):
             grads_for_norm, _ = split_params_grads_into_shared_and_expert_params(group)
             norm_group_value = 0.0
             if len(grads_for_norm) > 0:
                 norm_group_value = get_weight_norm(grads_for_norm, mpu=self.mpu)
             norm_groups.append(norm_group_value)
 
             # copying gradients to fp32 to wor  k with fp32 parameters
             for fp32_param, fp16_param in zip(self.fp32_groups[i], self.fp16_groups[i]):
                 if fp16_param.grad is None:
-                    fp32_param.grad = torch.zeros(fp16_param.size(),
-                                                  dtype=fp32_param.dtype,
-                                                  device=fp32_param.device)
+                    fp32_param.grad = torch.zeros(fp16_param.size(), dtype=fp32_param.dtype, device=fp32_param.device)
                 else:
                     fp32_param.grad = fp16_param.grad.to(fp32_param.dtype)
 
         self._global_grad_norm = get_global_norm(norm_list=norm_groups)
         self.unscale_and_clip_grads(self._global_grad_norm)
 
         self.optimizer.step()
@@ -290,33 +273,27 @@
             scaled_loss = (loss.float()) * self.cur_scale
             scaled_loss.backward(create_graph=create_graph, retain_graph=retain_graph)
 
     def _update_scale(self, skip):
         if self.dynamic_loss_scale:
             prev_scale = self.cur_scale
             if skip:
-                self.cur_scale = max(self.cur_scale / self.scale_factor,
-                                     self.min_loss_scale)
+                self.cur_scale = max(self.cur_scale / self.scale_factor, self.min_loss_scale)
                 self.last_overflow_iter = self.cur_iter
                 if self.verbose:
                     logger.info("Grad overflow on iteration: %s", self.cur_iter)
-                    logger.info(
-                        f"Reducing dynamic loss scale from {prev_scale} to {self.cur_scale}"
-                    )
+                    logger.info(f"Reducing dynamic loss scale from {prev_scale} to {self.cur_scale}")
             else:
                 # Ensure self.scale_window updates since last overflow
                 stable_interval = (self.cur_iter - self.last_overflow_iter) - 1
                 if (stable_interval > 0) and (stable_interval % self.scale_window == 0):
                     self.cur_scale *= self.scale_factor
                     if self.verbose:
-                        logger.info(
-                            f"No Grad overflow for {self.scale_window} iterations")
-                        logger.info(
-                            f"Increasing dynamic loss scale from {prev_scale} to {self.cur_scale}"
-                        )
+                        logger.info(f"No Grad overflow for {self.scale_window} iterations")
+                        logger.info(f"Increasing dynamic loss scale from {prev_scale} to {self.cur_scale}")
         else:
             if skip:
                 logger.info("Grad overflow on iteration %s", self.cur_iter)
                 logger.info("Using static loss scale of %s", self.cur_scale)
         self.cur_iter += 1
         return
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/lr_schedules.py` & `deepspeed-0.9.0/deepspeed/runtime/lr_schedules.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,15 +1,16 @@
-"""
-Copyright 2019 The Microsoft DeepSpeed Team
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
 
+# DeepSpeed Team
+"""
 Implementation of learning rate schedules.
 
 Taken and modified from PyTorch v1.0.1 source
 https://github.com/pytorch/pytorch/blob/v1.1.0/torch/optim/lr_scheduler.py
-
 """
 
 import argparse
 from torch.optim import Optimizer
 import math
 from deepspeed.utils import logger
 
@@ -49,110 +50,65 @@
 WARMUP_LOG_RATE = 'log'
 WARMUP_LINEAR_RATE = 'linear'
 
 TOTAL_NUM_STEPS = 'total_num_steps'
 
 
 def add_tuning_arguments(parser):
-    group = parser.add_argument_group('Convergence Tuning',
-                                      'Convergence tuning configurations')
+    group = parser.add_argument_group('Convergence Tuning', 'Convergence tuning configurations')
 
     # LR scheduler
-    group.add_argument('--lr_schedule',
-                       type=str,
-                       default=None,
-                       help='LR schedule for training.')
+    group.add_argument('--lr_schedule', type=str, default=None, help='LR schedule for training.')
 
     # Learning rate range test
-    group.add_argument("--lr_range_test_min_lr",
-                       type=float,
-                       default=0.001,
-                       help='Starting lr value.')
-    group.add_argument("--lr_range_test_step_rate",
-                       type=float,
-                       default=1.0,
-                       help='scaling rate for LR range test.')
-    group.add_argument("--lr_range_test_step_size",
-                       type=int,
-                       default=1000,
-                       help='training steps per LR change.')
+    group.add_argument("--lr_range_test_min_lr", type=float, default=0.001, help='Starting lr value.')
+    group.add_argument("--lr_range_test_step_rate", type=float, default=1.0, help='scaling rate for LR range test.')
+    group.add_argument("--lr_range_test_step_size", type=int, default=1000, help='training steps per LR change.')
     group.add_argument("--lr_range_test_staircase",
                        type=bool,
                        default=False,
                        help='use staircase scaling for LR range test.')
 
     # OneCycle schedule
     group.add_argument("--cycle_first_step_size",
                        type=int,
                        default=1000,
                        help='size of first step of 1Cycle schedule (training steps).')
     group.add_argument("--cycle_first_stair_count",
                        type=int,
                        default=-1,
                        help='first stair count for 1Cycle schedule.')
-    group.add_argument(
-        "--cycle_second_step_size",
-        type=int,
-        default=-1,
-        help='size of second step of 1Cycle schedule (default first_step_size).')
+    group.add_argument("--cycle_second_step_size",
+                       type=int,
+                       default=-1,
+                       help='size of second step of 1Cycle schedule (default first_step_size).')
     group.add_argument("--cycle_second_stair_count",
                        type=int,
                        default=-1,
                        help='second stair count for 1Cycle schedule.')
-    group.add_argument(
-        "--decay_step_size",
-        type=int,
-        default=1000,
-        help='size of intervals for applying post cycle decay (training steps).')
+    group.add_argument("--decay_step_size",
+                       type=int,
+                       default=1000,
+                       help='size of intervals for applying post cycle decay (training steps).')
 
     # 1Cycle LR
-    group.add_argument("--cycle_min_lr",
-                       type=float,
-                       default=0.01,
-                       help='1Cycle LR lower bound.')
-    group.add_argument("--cycle_max_lr",
-                       type=float,
-                       default=0.1,
-                       help='1Cycle LR upper bound.')
-    group.add_argument("--decay_lr_rate",
-                       type=float,
-                       default=0.0,
-                       help='post cycle LR decay rate.')
+    group.add_argument("--cycle_min_lr", type=float, default=0.01, help='1Cycle LR lower bound.')
+    group.add_argument("--cycle_max_lr", type=float, default=0.1, help='1Cycle LR upper bound.')
+    group.add_argument("--decay_lr_rate", type=float, default=0.0, help='post cycle LR decay rate.')
 
     # 1Cycle Momentum
-    group.add_argument('--cycle_momentum',
-                       default=False,
-                       action='store_true',
-                       help='Enable 1Cycle momentum schedule.')
-    group.add_argument("--cycle_min_mom",
-                       type=float,
-                       default=0.8,
-                       help='1Cycle momentum lower bound.')
-    group.add_argument("--cycle_max_mom",
-                       type=float,
-                       default=0.9,
-                       help='1Cycle momentum upper bound.')
-    group.add_argument("--decay_mom_rate",
-                       type=float,
-                       default=0.0,
-                       help='post cycle momentum decay rate.')
+    group.add_argument('--cycle_momentum', default=False, action='store_true', help='Enable 1Cycle momentum schedule.')
+    group.add_argument("--cycle_min_mom", type=float, default=0.8, help='1Cycle momentum lower bound.')
+    group.add_argument("--cycle_max_mom", type=float, default=0.9, help='1Cycle momentum upper bound.')
+    group.add_argument("--decay_mom_rate", type=float, default=0.0, help='post cycle momentum decay rate.')
 
     # Warmup LR
-    group.add_argument('--warmup_min_lr',
-                       type=float,
-                       default=0,
-                       help='WarmupLR minimum/initial LR value')
-    group.add_argument('--warmup_max_lr',
-                       type=float,
-                       default=0.001,
-                       help='WarmupLR maximum LR value.')
-    group.add_argument('--warmup_num_steps',
-                       type=int,
-                       default=1000,
-                       help='WarmupLR step count for LR warmup.')
+    group.add_argument('--warmup_min_lr', type=float, default=0, help='WarmupLR minimum/initial LR value')
+    group.add_argument('--warmup_max_lr', type=float, default=0.001, help='WarmupLR maximum LR value.')
+    group.add_argument('--warmup_num_steps', type=int, default=1000, help='WarmupLR step count for LR warmup.')
     group.add_argument('--warmup_type',
                        type=str,
                        default=WARMUP_LOG_RATE,
                        help='WarmupLR increasing function during warmup')
     return parser
 
 
@@ -164,40 +120,35 @@
     return lr_sched_args, unknown_args
 
 
 def override_lr_range_test_params(args, params):
     if hasattr(args, LR_RANGE_TEST_MIN_LR) and args.lr_range_test_min_lr is not None:
         params[LR_RANGE_TEST_MIN_LR] = args.lr_range_test_min_lr
 
-    if hasattr(args,
-               LR_RANGE_TEST_STEP_RATE) and args.lr_range_test_step_rate is not None:
+    if hasattr(args, LR_RANGE_TEST_STEP_RATE) and args.lr_range_test_step_rate is not None:
         params[LR_RANGE_TEST_STEP_RATE] = args.lr_range_test_step_rate
 
-    if hasattr(args,
-               LR_RANGE_TEST_STEP_SIZE) and args.lr_range_test_step_size is not None:
+    if hasattr(args, LR_RANGE_TEST_STEP_SIZE) and args.lr_range_test_step_size is not None:
         params[LR_RANGE_TEST_STEP_SIZE] = args.lr_range_test_step_size
 
-    if hasattr(args,
-               LR_RANGE_TEST_STAIRCASE) and args.lr_range_test_staircase is not None:
+    if hasattr(args, LR_RANGE_TEST_STAIRCASE) and args.lr_range_test_staircase is not None:
         params[LR_RANGE_TEST_STAIRCASE] = args.lr_range_test_staircase
 
 
 def override_1cycle_params(args, params):
     if hasattr(args, CYCLE_FIRST_STEP_SIZE) and args.cycle_first_step_size is not None:
         params[CYCLE_FIRST_STEP_SIZE] = args.cycle_first_step_size
 
-    if hasattr(args,
-               CYCLE_FIRST_STAIR_COUNT) and args.cycle_first_stair_count is not None:
+    if hasattr(args, CYCLE_FIRST_STAIR_COUNT) and args.cycle_first_stair_count is not None:
         params[CYCLE_FIRST_STAIR_COUNT] = args.cycle_first_stair_count
 
     if hasattr(args, CYCLE_SECOND_STEP_SIZE) and args.cycle_second_step_size is not None:
         params[CYCLE_SECOND_STEP_SIZE] = args.cycle_second_step_size
 
-    if hasattr(args,
-               CYCLE_SECOND_STAIR_COUNT) and args.cycle_second_stair_count is not None:
+    if hasattr(args, CYCLE_SECOND_STAIR_COUNT) and args.cycle_second_stair_count is not None:
         params[CYCLE_SECOND_STAIR_COUNT] = args.cycle_second_stair_count
 
     if hasattr(args, DECAY_STEP_SIZE) and args.decay_step_size is not None:
         params[DECAY_STEP_SIZE] = args.decay_step_size
 
     # 1Cycle LR params
     if hasattr(args, CYCLE_MIN_LR) and args.cycle_min_lr is not None:
@@ -297,16 +248,15 @@
 def get_torch_optimizer(optimizer):
     if isinstance(optimizer, Optimizer):
         return optimizer
 
     if hasattr(optimizer, 'optimizer') and isinstance(optimizer.optimizer, Optimizer):
         return optimizer.optimizer
 
-    raise TypeError('{} is not a subclass of torch.optim.Optimizer'.format(
-        type(optimizer).__name__))
+    raise TypeError('{} is not a subclass of torch.optim.Optimizer'.format(type(optimizer).__name__))
 
 
 class LRRangeTest(object):
     """Sets the learning rate of each parameter group according to
     learning rate range test (LRRT) policy. The policy increases learning
     rate starting from a base value with a constant frequency, as detailed in
     the paper `A disciplined approach to neural network hyper-parameters: Part1`_.
@@ -339,31 +289,29 @@
         >>>     for batch in data_loader:
         >>>         train_batch(...)
         >>>         scheduler.step()
 
         _A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay:
         https://arxiv.org/abs/1803.09820
 """
+
     def __init__(self,
                  optimizer: Optimizer,
                  lr_range_test_min_lr: float = 1e-3,
                  lr_range_test_step_size: int = 2000,
                  lr_range_test_step_rate: float = 1.0,
                  lr_range_test_staircase: bool = False,
                  last_batch_iteration: int = -1):
 
         self.optimizer = get_torch_optimizer(optimizer)
 
-        if isinstance(lr_range_test_min_lr,
-                      list) or isinstance(lr_range_test_min_lr,
-                                          tuple):
+        if isinstance(lr_range_test_min_lr, list) or isinstance(lr_range_test_min_lr, tuple):
             if len(lr_range_test_min_lr) != len(self.optimizer.param_groups):
-                raise ValueError("expected {} lr_range_test_min_lr, got {}".format(
-                    len(self.optimizer.param_groups),
-                    len(lr_range_test_min_lr)))
+                raise ValueError("expected {} lr_range_test_min_lr, got {}".format(len(self.optimizer.param_groups),
+                                                                                   len(lr_range_test_min_lr)))
             self.min_lr = list(lr_range_test_min_lr)
         else:
             self.min_lr = [lr_range_test_min_lr] * len(self.optimizer.param_groups)
 
         self.step_size = lr_range_test_step_size
         self.step_rate = lr_range_test_step_rate
         self.last_batch_iteration = last_batch_iteration
@@ -380,17 +328,15 @@
         return float(self.last_batch_iteration + 1) / self.step_size
 
     def _get_increase(self):
         return (1 + self.step_rate * self.interval_fn())
 
     def get_lr(self):
         lr_increase = self._get_increase()
-        return [
-            lr_range_test_min_lr * lr_increase for lr_range_test_min_lr in self.min_lr
-        ]
+        return [lr_range_test_min_lr * lr_increase for lr_range_test_min_lr in self.min_lr]
 
     def get_last_lr(self):
         """ Return last computed learning rate by current scheduler.
         """
         assert getattr(self, '_last_lr', None) is not None, "need to call step() first"
         return self._last_lr
 
@@ -476,14 +422,15 @@
         >>>     for batch in data_loader:
         >>>         train_batch(...)
         >>>         scheduler.step()
 
 
     .. _A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay: https://arxiv.org/abs/1803.09820
     """
+
     def __init__(self,
                  optimizer,
                  cycle_min_lr,
                  cycle_max_lr,
                  decay_lr_rate=0.,
                  cycle_first_step_size=2000,
                  cycle_second_step_size=None,
@@ -495,51 +442,36 @@
                  cycle_max_mom=0.9,
                  decay_mom_rate=0.,
                  last_batch_iteration=-1):
 
         self.optimizer = get_torch_optimizer(optimizer)
 
         # Initialize cycle shape
-        self._initialize_cycle(cycle_first_step_size,
-                               cycle_second_step_size,
-                               cycle_first_stair_count,
-                               cycle_second_stair_count,
-                               decay_step_size)
+        self._initialize_cycle(cycle_first_step_size, cycle_second_step_size, cycle_first_stair_count,
+                               cycle_second_stair_count, decay_step_size)
 
         # Initialize cycle lr
-        self._initialize_lr(self.optimizer,
-                            cycle_min_lr,
-                            cycle_max_lr,
-                            decay_lr_rate,
-                            last_batch_iteration)
+        self._initialize_lr(self.optimizer, cycle_min_lr, cycle_max_lr, decay_lr_rate, last_batch_iteration)
 
         # Initialize cyclic momentum
         self.cycle_momentum = cycle_momentum
         if cycle_momentum:
-            self._initialize_momentum(self.optimizer,
-                                      cycle_min_mom,
-                                      cycle_max_mom,
-                                      decay_mom_rate,
+            self._initialize_momentum(self.optimizer, cycle_min_mom, cycle_max_mom, decay_mom_rate,
                                       last_batch_iteration)
 
         # Initialize batch iteration tracker
         self.last_batch_iteration = last_batch_iteration
 
     # Configure cycle shape
 
-    def _initialize_cycle(self,
-                          cycle_first_step_size,
-                          cycle_second_step_size,
-                          cycle_first_stair_count,
-                          cycle_second_stair_count,
-                          decay_step_size):
+    def _initialize_cycle(self, cycle_first_step_size, cycle_second_step_size, cycle_first_stair_count,
+                          cycle_second_stair_count, decay_step_size):
         cycle_first_step_size = float(cycle_first_step_size)
         cycle_second_step_size = float(
-            cycle_second_step_size
-        ) if cycle_second_step_size is not None else cycle_first_step_size
+            cycle_second_step_size) if cycle_second_step_size is not None else cycle_first_step_size
 
         self.total_size = cycle_first_step_size + cycle_second_step_size
         self.step_ratio = cycle_first_step_size / self.total_size
         self.first_stair_count = cycle_first_stair_count
         self.second_stair_count = cycle_first_stair_count if cycle_second_stair_count is None else cycle_second_stair_count
         self.decay_step_size = decay_step_size
 
@@ -547,38 +479,28 @@
             self.skip_lr_decay = True
             self.skip_mom_decay = True
         else:
             self.skip_lr_decay = False
             self.skip_mom_decay = False
 
     # Configure lr schedule
-    def _initialize_lr(self,
-                       optimizer,
-                       cycle_min_lr,
-                       cycle_max_lr,
-                       decay_lr_rate,
-                       last_batch_iteration):
+    def _initialize_lr(self, optimizer, cycle_min_lr, cycle_max_lr, decay_lr_rate, last_batch_iteration):
         self.min_lrs = [cycle_min_lr] * len(optimizer.param_groups)
         if last_batch_iteration == -1:
             for lr, group in zip(self.min_lrs, optimizer.param_groups):
                 group['lr'] = lr
 
         self.max_lrs = [cycle_max_lr] * len(optimizer.param_groups)
         self.decay_lr_rate = decay_lr_rate
 
         if math.isclose(self.decay_lr_rate, 0):
             self.skip_lr_decay = True
 
     # Configure momentum schedule
-    def _initialize_momentum(self,
-                             optimizer,
-                             cycle_min_mom,
-                             cycle_max_mom,
-                             decay_mom_rate,
-                             last_batch_iteration):
+    def _initialize_momentum(self, optimizer, cycle_min_mom, cycle_max_mom, decay_mom_rate, last_batch_iteration):
         if 'betas' not in optimizer.defaults:
             optimizer_name = type(optimizer).__name__
             logger.warn(
                 f"cycle_momentum is disabled because optimizer {optimizer_name} does not support momentum, no betas attribute in defaults"
             )
             self.cycle_momentum = False
             return
@@ -718,14 +640,15 @@
             >>> data_loader = torch.utils.data.DataLoader(...)
             >>> for epoch in range(10):
             >>>     for batch in data_loader:
             >>>         train_batch(...)
             >>>         scheduler.step()
 
     """
+
     def __init__(self,
                  optimizer: Optimizer,
                  warmup_min_lr: float = 0.0,
                  warmup_max_lr: float = 0.001,
                  warmup_num_steps: int = 1000,
                  warmup_type: str = WARMUP_LOG_RATE,
                  last_batch_iteration: int = -1):
@@ -734,33 +657,27 @@
 
         self.min_lrs = self._format_param(self.optimizer, warmup_min_lr, "min_lr")
         self.max_lrs = self._format_param(self.optimizer, warmup_max_lr, "max_lr")
         self.delta_lrs = [big - small for big, small in zip(self.max_lrs, self.min_lrs)]
         self.warmup_num_steps = max(2, warmup_num_steps)
         # Currently only support linear and log function
         if warmup_type not in {WARMUP_LOG_RATE, WARMUP_LINEAR_RATE}:
-            logger.warning(
-                f"Using unknown warmup_type: {warmup_type}. The increasing function "
-                f"is set to default (log)")
+            logger.warning(f"Using unknown warmup_type: {warmup_type}. The increasing function "
+                           f"is set to default (log)")
             warmup_type = WARMUP_LOG_RATE
         self.warmup_type = warmup_type
         self.inverse_log_warm_up = 1.0 / math.log(self.warmup_num_steps)
         self.last_batch_iteration = last_batch_iteration
 
     def get_lr(self):
         if self.last_batch_iteration < 0:
-            logger.warning(
-                "Attempting to get learning rate from scheduler before it has started")
+            logger.warning("Attempting to get learning rate from scheduler before it has started")
             return [0.0]
         gamma = self._get_gamma()
-        return [
-            min_lr + (delta_lr * gamma) for min_lr,
-            delta_lr in zip(self.min_lrs,
-                            self.delta_lrs)
-        ]
+        return [min_lr + (delta_lr * gamma) for min_lr, delta_lr in zip(self.min_lrs, self.delta_lrs)]
 
     def get_last_lr(self):
         """ Return last computed learning rate by current scheduler.
         """
         assert getattr(self, '_last_lr', None) is not None, "need to call step() first"
         return self._last_lr
 
@@ -785,18 +702,16 @@
             elif self.warmup_type == WARMUP_LINEAR_RATE:
                 return self.last_batch_iteration / self.warmup_num_steps
         return 1.0
 
     def _format_param(self, optimizer, param_value, param_name):
         if isinstance(param_value, list) or isinstance(param_value, tuple):
             if len(param_value) != len(optimizer.param_groups):
-                raise ValueError("expected {} value for {}, got {}".format(
-                    len(optimizer.param_groups),
-                    param_name,
-                    FileNotFoundError(param_value)))
+                raise ValueError("expected {} value for {}, got {}".format(len(optimizer.param_groups), param_name,
+                                                                           FileNotFoundError(param_value)))
             return list(param_value)
         return [param_value] * len(optimizer.param_groups)
 
 
 class WarmupDecayLR(WarmupLR):
     """Increase the learning rate of each parameter group from min lr to max lr
         over warmup_num_steps steps, and then decay at linear rate over the remaining training steps.
@@ -815,40 +730,34 @@
             >>> data_loader = torch.utils.data.DataLoader(...)
             >>> for epoch in range(10):
             >>>     for batch in data_loader:
             >>>         train_batch(...)
             >>>         scheduler.step()
 
     """
+
     def __init__(self,
                  optimizer: Optimizer,
                  total_num_steps: int,
                  warmup_min_lr: float = 0.0,
                  warmup_max_lr: float = 0.001,
                  warmup_num_steps: int = 1000,
                  warmup_type: str = WARMUP_LOG_RATE,
                  last_batch_iteration: int = -1):
 
         self.total_num_steps = total_num_steps
-        super(WarmupDecayLR,
-              self).__init__(optimizer,
-                             warmup_min_lr,
-                             warmup_max_lr,
-                             warmup_num_steps,
-                             warmup_type,
-                             last_batch_iteration)
+        super(WarmupDecayLR, self).__init__(optimizer, warmup_min_lr, warmup_max_lr, warmup_num_steps, warmup_type,
+                                            last_batch_iteration)
         if self.total_num_steps < self.warmup_num_steps:
             logger.warning('total_num_steps {} is less than warmup_num_steps {}'.format(
-                total_num_steps,
-                warmup_num_steps))
+                total_num_steps, warmup_num_steps))
 
     def _get_gamma(self):
         if self.last_batch_iteration < self.warmup_num_steps:
             if self.warmup_type == WARMUP_LOG_RATE:
                 return self.inverse_log_warm_up * math.log(self.last_batch_iteration + 1)
             elif self.warmup_type == WARMUP_LINEAR_RATE:
                 return self.last_batch_iteration / self.warmup_num_steps
         return max(
             0.0,
             float(self.total_num_steps - self.last_batch_iteration) /
-            float(max(1.0,
-                      self.total_num_steps - self.warmup_num_steps)))
+            float(max(1.0, self.total_num_steps - self.warmup_num_steps)))
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/pipe/engine.py` & `deepspeed-0.9.0/deepspeed/runtime/pipe/engine.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,11 @@
-# Copyright 2019 The Microsoft DeepSpeed Team
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 from types import MethodType
 
 import torch
 from deepspeed import comm as dist
 
 from deepspeed.utils import logger
@@ -37,26 +40,16 @@
 class PipelineEngine(DeepSpeedEngine):
     """ A training engine hybrid pipeline, data, and model parallel training.
 
     This engine is created by ``deepspeed.initialize()`` when a :class:`PipelineModule`
     is provided.
     """
     ID_TO_DTYPE = [
-        torch.float32,
-        torch.float64,
-        torch.complex64,
-        torch.complex128,
-        torch.float16,
-        torch.bfloat16,
-        torch.uint8,
-        torch.int8,
-        torch.int16,
-        torch.int32,
-        torch.int64,
-        torch.bool
+        torch.float32, torch.float64, torch.complex64, torch.complex128, torch.float16, torch.bfloat16, torch.uint8,
+        torch.int8, torch.int16, torch.int32, torch.int64, torch.bool
     ]
     DTYPE_TO_ID = {dtype: id_ for id_, dtype in enumerate(ID_TO_DTYPE)}
 
     def __init__(self, has_bool_tensors=False, *super_args, **super_kwargs):
         super().__init__(*super_args, **super_kwargs)
         assert isinstance(self.module, PipelineModule), "model must base PipelineModule"
 
@@ -130,16 +123,15 @@
         # Subtract tied parameters if we don't own them
         if self.module.tied_comms:
             tied_params = 0
             for key, d in self.module.tied_comms.items():
                 if self.global_rank != min(d['ranks']):
                     tied_params += sum(p.numel() for p in d['module'].parameters())
             unique_params -= tied_params
-        params_tensor = torch.LongTensor(data=[num_params,
-                                               unique_params]).to(self.device)
+        params_tensor = torch.LongTensor(data=[num_params, unique_params]).to(self.device)
         dist.all_reduce(params_tensor, group=self.grid.get_model_parallel_group())
         params_tensor = params_tensor.tolist()
         total_params = params_tensor[0]
         unique_params = params_tensor[1]
         if self.grid.data_parallel_id == 0:
             logger.info(f'RANK={self.global_rank} '
                         f'STAGE={self.stage_id} '
@@ -152,18 +144,18 @@
         #initialize peer-2-peer communication and allreduce groups
         if self.is_pipe_parallel:
             p2p.init_process_groups(self.grid)
 
         # Pipeline buffers
         self.num_pipe_buffers = 0
         self.pipe_buffers = {
-            'inputs' : [],   # batch input and received activations
-            'labels' : [],   # labels from batch input
-            'outputs' : [],  # activations
-            'output_tensors' : [], # tensor object to preserve backward graph
+            'inputs': [],  # batch input and received activations
+            'labels': [],  # labels from batch input
+            'outputs': [],  # activations
+            'output_tensors': [],  # tensor object to preserve backward graph
         }
         self.pipe_recv_buf = None
         self.grad_layer = None
 
         self.meta_buffer = None
 
         self.first_output_send = True
@@ -174,16 +166,15 @@
 
         #stores the loss for the entire batch
         self.total_loss = None
         self.agg_loss = torch.tensor(0.0, requires_grad=False).to(self.device)
         self.dp_group_loss = torch.tensor(0.0, requires_grad=False).to(self.device)
 
         if self._config.pipeline['activation_checkpoint_interval'] > 0:
-            self.module.activation_checkpoint_interval = self._config.pipeline[
-                'activation_checkpoint_interval']
+            self.module.activation_checkpoint_interval = self._config.pipeline['activation_checkpoint_interval']
 
         self.module.checkpoint_parallel_write_pipeline = self._config.checkpoint_parallel_write_pipeline
 
         if self.is_last_stage():
             self.loss_model = self.module.loss_fn
 
         self.has_attention_mask = self.module.__class__.__name__ == 'GPT2ModelPipe'
@@ -216,19 +207,18 @@
             self.timers('step_microstep').stop()
 
     def set_has_attention_mask(self, value):
         assert isinstance(value, bool)
         self.has_attention_mask = value
 
     def _build_data_iter(self, dataset):
-        sampler = torch.utils.data.distributed.DistributedSampler(
-            dataset,
-            num_replicas=self.dp_world_size,
-            rank=self.mpu.get_data_parallel_rank(),
-            shuffle=False)
+        sampler = torch.utils.data.distributed.DistributedSampler(dataset,
+                                                                  num_replicas=self.dp_world_size,
+                                                                  rank=self.mpu.get_data_parallel_rank(),
+                                                                  shuffle=False)
         # Build a loader and make it repeating.
         pipe_dataloader = self.deepspeed_io(dataset, data_sampler=sampler)
         pipe_dataloader = RepeatingLoader(pipe_dataloader)
         self.set_dataloader(pipe_dataloader)
 
     def _exec_reduce_tied_grads(self):
         # We need to run this first to write to self.averaged_gradients;
@@ -313,16 +303,15 @@
         Args:
             data_iter (Iterator, optional): Iterator of training data.
 
         Returns:
             The arithmetic mean of the losses computed this batch.
         """
         if not torch._C.is_grad_enabled():
-            raise RuntimeError(
-                f'train_batch() requires gradients enabled. Use eval_batch() instead.')
+            raise RuntimeError(f'train_batch() requires gradients enabled. Use eval_batch() instead.')
 
         # Curriculum learning could change activation shape
         if self.curriculum_enabled_legacy():
             new_difficulty = self.curriculum_scheduler_legacy.update_difficulty( \
                 self.global_steps + 1)
             if self.global_steps == 0 or self.curriculum_scheduler_legacy.first_step:
                 self.reset_activation_shape()
@@ -356,36 +345,25 @@
                 print(f'steps: {self.global_steps} '
                       f'loss: {self.agg_train_loss:0.4f} '
                       f'iter time (s): {iter_time:0.3f} '
                       f'samples/sec: {tput:0.3f}')
 
         # Monitoring
         if self.global_rank == 0 and self.monitor.enabled:
-            self.summary_events = [(f'Train/Samples/train_loss',
-                                    self.agg_train_loss.mean().item(),
+            self.summary_events = [(f'Train/Samples/train_loss', self.agg_train_loss.mean().item(),
                                     self.global_samples)]
             self.monitor.write_events(self.summary_events)
 
-        if self.wall_clock_breakdown(
-        ) and self.global_steps % self.steps_per_print() == 0:
-            self.timers.log([
-                'pipe_send_output',
-                'pipe_send_grad',
-                'pipe_recv_input',
-                'pipe_recv_grad'
-            ])
+        if self.wall_clock_breakdown() and self.global_steps % self.steps_per_print() == 0:
+            self.timers.log(['pipe_send_output', 'pipe_send_grad', 'pipe_recv_input', 'pipe_recv_grad'])
 
         # TODO: should return precisely what loss returned and allow others to be queried?
         return self.agg_train_loss
 
-    def eval_batch(self,
-                   data_iter,
-                   return_logits=False,
-                   compute_loss=True,
-                   reduce_output='avg'):
+    def eval_batch(self, data_iter, return_logits=False, compute_loss=True, reduce_output='avg'):
         """Evaluate the pipeline on a batch of data from ``data_iter``. The
         engine will evaluate ``self.train_batch_size()`` total samples
         collectively across all workers.
 
         This method is equivalent to:
 
         .. code-block:: python
@@ -444,17 +422,15 @@
         if self.is_last_stage():
             eval_output = self._reduce_outputs(self.fwd_outputs, reduce=reduce_output)
 
         if compute_loss:
             eval_output = self._bcast_pipe_scalar(eval_output)
 
         if self.global_rank == 0 and self.monitor.enabled:
-            self.summary_events = [(f'Train/Samples/eval_loss',
-                                    eval_output.mean().item(),
-                                    self.global_samples)]
+            self.summary_events = [(f'Train/Samples/eval_loss', eval_output.mean().item(), self.global_samples)]
             self.monitor.write_events(self.summary_events)
 
         # Restore the training iterator
         self.set_dataiterator(train_iterator)
 
         # Reset any buffers that may have been populated during the forward passes.
         #ds_checkpointing.reset()
@@ -506,16 +482,15 @@
             # Average over DP groups
             if reduce_dp and self.is_data_parallel:
                 if torch.is_tensor(reduced):
                     dist.all_reduce(reduced, group=self.mpu.get_data_parallel_group())
                     reduced /= self.dp_world_size
                 else:
                     for idx in range(len(reduced)):
-                        dist.all_reduce(reduced[idx],
-                                        group=self.mpu.get_data_parallel_group())
+                        dist.all_reduce(reduced[idx], group=self.mpu.get_data_parallel_group())
                         reduced[idx] /= self.dp_world_size
 
             return reduced
         else:
             raise NotImplementedError(f'reduction type {reduce} not supported.')
 
     def _bcast_pipe_scalar(self, data, src_rank=None, dtype=torch.float32):
@@ -525,17 +500,15 @@
         assert src_rank in self.grid.pp_group
 
         if self.global_rank == src_rank:
             result = data.clone().detach()
         else:
             result = torch.Tensor([0.]).type(dtype).to(self.device)
 
-        dist.broadcast(tensor=result,
-                       src=src_rank,
-                       group=self.mpu.get_pipe_parallel_group())
+        dist.broadcast(tensor=result, src=src_rank, group=self.mpu.get_pipe_parallel_group())
 
         return result
 
     def _aggregate_total_loss(self):
         # Scale loss, average among DP ranks, and bcast loss to the rest of my DP group
         if self.is_last_stage():
             loss = self._scale_loss_by_gas(self.total_loss)
@@ -546,26 +519,22 @@
             #print(f'RANK={self.global_rank} bcast SENDER src={self.global_rank} group={self.grid.pp_group}', flush=True)
             if self.is_data_parallel:
                 dist.all_reduce(agg_loss, group=self.mpu.get_data_parallel_group())
                 agg_loss /= self.dp_world_size
 
             assert self.global_rank in self.grid.pp_group
             losses = torch.Tensor([self.dp_group_loss, agg_loss]).to(self.device)
-            dist.broadcast(tensor=losses,
-                           src=self.global_rank,
-                           group=self.mpu.get_pipe_parallel_group())
+            dist.broadcast(tensor=losses, src=self.global_rank, group=self.mpu.get_pipe_parallel_group())
 
         else:
             # Get loss from last stage
             src_rank = self.grid.stage_to_global(self.num_stages - 1)
             assert src_rank in self.grid.pp_group
             losses = torch.Tensor([0., 0.]).to(self.device)
-            dist.broadcast(tensor=losses,
-                           src=src_rank,
-                           group=self.grid.get_pipe_parallel_group())
+            dist.broadcast(tensor=losses, src=src_rank, group=self.grid.get_pipe_parallel_group())
             self.dp_group_loss = losses[0].clone().detach()
             agg_loss = losses[1].clone().detach()
 
         return agg_loss
 
     def set_dataloader(self, loader):
         """"""
@@ -634,18 +603,17 @@
         if isinstance(self.pipe_buffers['inputs'][buffer_id], tuple):
             inputs = tuple(t.clone() for t in self.pipe_buffers['inputs'][buffer_id])
         else:
             inputs = self.pipe_buffers['inputs'][buffer_id].clone()
 
         # collect the partitioned input from the previous stage
         if self.is_pipe_partitioned and not self.is_first_stage():
-            part_input = PartitionedTensor.from_meta(
-                meta=inputs[0],
-                local_part=inputs[1],
-                group=self.grid.get_slice_parallel_group())
+            part_input = PartitionedTensor.from_meta(meta=inputs[0],
+                                                     local_part=inputs[1],
+                                                     group=self.grid.get_slice_parallel_group())
 
             inputs = (part_input.full(), *inputs[2:])
             inputs[0].requires_grad = True
             # skip mask
             #inputs[1].requires_grad = True
             part_input = None
             inputs = inputs[0] if len(inputs) == 1 else inputs
@@ -658,26 +626,22 @@
         outputs = super().forward(inputs)
 
         # Partition the outputs if we are not the last stage
         if self.is_pipe_partitioned and not self.is_last_stage():
             if isinstance(outputs, tuple):
                 first_output = outputs[0]
                 # TODO: Improve pipe partitioning to pass multiple tensors that require grads
-                assert all([
-                    torch.is_tensor(elt) and elt.requires_grad is False
-                    for elt in outputs[1:]
-                ])
+                assert all([torch.is_tensor(elt) and elt.requires_grad is False for elt in outputs[1:]])
                 outputs_tail = outputs[1:]
             elif torch.is_tensor(outputs):
                 first_output = outputs
                 outputs_tail = []
             else:
                 raise ValueError("expecting a tensor or a tuple of tensors")
-            part = PartitionedTensor(tensor=first_output,
-                                     group=self.grid.get_slice_parallel_group())
+            part = PartitionedTensor(tensor=first_output, group=self.grid.get_slice_parallel_group())
             # Clear the large output data, but save the computation graph
             first_output.data = torch.zeros(1)
             self.pipe_buffers['output_tensors'][buffer_id] = first_output
             # Inject the partitioned tensor into the output before sending
             outputs = (part.to_meta(), part.data(), *outputs_tail)
             part = None
 
@@ -728,32 +692,30 @@
             self.timers('backward_inner_microstep').start()
             self.timers('backward_inner').start()
 
         # Reconstruct if we previously partitioned the output. We must be
         # careful to also restore the computational graph of the tensors we partitioned.
         if self.is_pipe_partitioned:
             if self.is_grad_partitioned:
-                part_output = PartitionedTensor.from_meta(
-                    meta=outputs[0],
-                    local_part=outputs[1],
-                    group=self.grid.get_slice_parallel_group())
+                part_output = PartitionedTensor.from_meta(meta=outputs[0],
+                                                          local_part=outputs[1],
+                                                          group=self.grid.get_slice_parallel_group())
                 self.pipe_buffers['output_tensors'][buffer_id].data = part_output.full()
                 outputs = (self.pipe_buffers['output_tensors'][buffer_id], *outputs[2:])
             else:
                 # Already restored from partition
                 self.pipe_buffers['output_tensors'][buffer_id].data = outputs[0]
                 outputs = (self.pipe_buffers['output_tensors'][buffer_id], *outputs[1:])
 
         grad_tensors = self.grad_layer
         if self.is_grad_partitioned:
             #print(f'RANK={self.global_rank} BEFORE-BWD restoring grad={self.grad_layer[0].size()} {self.grad_layer[1].size()}')
-            part_grad = PartitionedTensor.from_meta(
-                meta=self.grad_layer[0],
-                local_part=self.grad_layer[1],
-                group=self.grid.get_slice_parallel_group())
+            part_grad = PartitionedTensor.from_meta(meta=self.grad_layer[0],
+                                                    local_part=self.grad_layer[1],
+                                                    group=self.grid.get_slice_parallel_group())
             grad_tensors = (part_grad.full(), *grad_tensors[2:])
             part_grad = None
             #print(f'RANK={self.global_rank} BEFORE-BWD restored grad={self.grad_layer[0].size()} {self.grad_layer[1].size()}')
 
         if self.bfloat16_enabled() and not self.is_last_stage():
             # manually call because we don't call optimizer.backward()
             self.optimizer.clear_lp_grads()
@@ -791,15 +753,15 @@
 
         if self.is_first_stage():
             loaded = None
             if torch.is_tensor(batch[0]):
                 loaded = batch[0].clone().to(self.device).detach()
                 loaded.requires_grad = loaded.is_floating_point()
             else:
-                assert isinstance(batch[0], tuple)
+                assert isinstance(batch[0], (tuple, list))
                 # Assume list or tuple
                 loaded = []
                 for x in batch[0]:
                     assert torch.is_tensor(x)
                     mine = x.clone().detach().to(self.device)
                     mine.requires_grad = mine.is_floating_point()
                     loaded.append(mine)
@@ -861,16 +823,15 @@
             p2p.send(type_tensor, recv_stage)
             count_tensor = torch.LongTensor(data=[len(buffer)]).to(self.device)
             p2p.send(count_tensor, recv_stage)
             for idx, tensor in enumerate(buffer):
                 assert isinstance(tensor, torch.Tensor)
                 send_shape = torch.LongTensor(data=tensor.size()).to(self.device)
                 send_ndims = torch.LongTensor(data=[len(tensor.size())]).to(self.device)
-                send_dtype = torch.LongTensor(data=[self.DTYPE_TO_ID[tensor.dtype]]).to(
-                    self.device)
+                send_dtype = torch.LongTensor(data=[self.DTYPE_TO_ID[tensor.dtype]]).to(self.device)
                 p2p.send(send_dtype, recv_stage)
                 p2p.send(send_ndims, recv_stage)
                 p2p.send(send_shape, recv_stage)
                 # Useful for performance debugging.
                 '''
                 new_bytes = _tensor_bytes(tensor)
                 send_bytes += _tensor_bytes(tensor)
@@ -986,25 +947,22 @@
         inputs = self.pipe_buffers['inputs'][buffer_id]
 
         # Partition the gradient
         if self.is_grad_partitioned:
             if isinstance(inputs, tuple):
                 first_input = inputs[0]
                 assert all([torch.is_tensor(elt) for elt in inputs[1:]])
-                inputs_grad_tail = [
-                    elt.grad for elt in inputs[1:] if elt.grad is not None
-                ]
+                inputs_grad_tail = [elt.grad for elt in inputs[1:] if elt.grad is not None]
             elif torch.is_tensor(inputs):
                 first_input = inputs
                 inputs_grad_tail = []
             else:
                 raise ValueError("expecting a tensor or a tuple of tensors")
             assert torch.is_tensor(first_input)
-            part = PartitionedTensor(tensor=first_input.grad,
-                                     group=self.grid.get_slice_parallel_group())
+            part = PartitionedTensor(tensor=first_input.grad, group=self.grid.get_slice_parallel_group())
 
             inputs = (part.to_meta(), part.data(), *inputs_grad_tail)
 
         # XXX Terrible hack
         # Drop the attention mask from the input buffer here. It does not have
         # a grad that needs to be communicated. We free the buffer immediately
         # after, so no need to restore it. The receiver also has a hack that skips
@@ -1056,17 +1014,15 @@
             assert isinstance(self.pipe_recv_buf, tuple)
             recvd = [None] * len(self.pipe_recv_buf)
             for idx, buffer in enumerate(self.pipe_recv_buf):
                 assert torch.is_tensor(buffer)
                 # XXX hardcode meta type
                 if self.is_pipe_partitioned and idx == 0 and buffer.dtype != torch.long:
                     if self.meta_buffer is None:
-                        self.meta_buffer = torch.zeros(buffer.size(),
-                                                       dtype=torch.long,
-                                                       device=self.device)
+                        self.meta_buffer = torch.zeros(buffer.size(), dtype=torch.long, device=self.device)
                     buffer = self.meta_buffer
 
                 p2p.recv(buffer, self.prev_stage)
                 recvd[idx] = buffer.clone().detach()
 
             # NCCL does not like to send torch.BoolTensor types, so un-cast the
             # attention mask
@@ -1087,30 +1043,27 @@
         if self.wall_clock_breakdown():
             self.timers('pipe_recv_grad').start()
 
         outputs = self.pipe_buffers['outputs'][buffer_id]
         # XXX these shapes are hardcoded for Megatron
         # Restore partitioned output if it was partitioned and we are sending full gradients
         if self.is_pipe_partitioned and not self.is_grad_partitioned:
-            part_output = PartitionedTensor.from_meta(
-                meta=outputs[0],
-                local_part=outputs[1],
-                group=self.grid.get_slice_parallel_group())
+            part_output = PartitionedTensor.from_meta(meta=outputs[0],
+                                                      local_part=outputs[1],
+                                                      group=self.grid.get_slice_parallel_group())
             outputs[0].data = part_output.full()
             outputs = (outputs[0], *outputs[2:])
             # save for backward
             self.pipe_buffers['outputs'][buffer_id] = outputs
 
         # Allocate gradient if necessary
         if self.grad_layer is None:
             if isinstance(outputs, torch.Tensor):
                 s = list(outputs.size())
-                self.grad_layer = self._allocate_buffer(s,
-                                                        dtype=outputs.dtype,
-                                                        num_buffers=1)[0]
+                self.grad_layer = self._allocate_buffer(s, dtype=outputs.dtype, num_buffers=1)[0]
             else:
                 # XXX This is a HACK
                 # When we exchange activations/gradients, the two pipe stages
                 # need to issue the send/recv with the same buffer sizes or
                 # else there is a deadlock. The is_floating_point() filter is
                 # used to avoid sending gradients for tensors that do not
                 # produce gradients. When TP>1, we partition the first
@@ -1119,36 +1072,29 @@
                 # two tensors: a 1/TPth chunk of the original data and also a
                 # small LongTensor storing the metadata used to reconstruct on
                 # the other side. When combined, the floating point filter also
                 # filtered out the metadata tensor. This quick (hacky) fix just
                 # branches on is_grad_partitioned so we don't filter out the
                 # metadata tensor.
                 if self.is_grad_partitioned:
-                    sizes_and_dtypes = [
-                        (list(t.size()),
-                         t.dtype) for t in outputs[:2]
-                    ] + [(list(t.size()),
-                          t.dtype) for t in outputs[2:] if t.is_floating_point()]
+                    sizes_and_dtypes = [(list(t.size()), t.dtype)
+                                        for t in outputs[:2]] + [(list(t.size()), t.dtype)
+                                                                 for t in outputs[2:] if t.is_floating_point()]
                 else:
-                    sizes_and_dtypes = [(list(t.size()),
-                                         t.dtype) for t in outputs
-                                        if t.is_floating_point()]
-                self.grad_layer = self._allocate_buffers(sizes_and_dtypes,
-                                                         num_buffers=1)[0]
+                    sizes_and_dtypes = [(list(t.size()), t.dtype) for t in outputs if t.is_floating_point()]
+                self.grad_layer = self._allocate_buffers(sizes_and_dtypes, num_buffers=1)[0]
 
         if isinstance(self.grad_layer, torch.Tensor):
             p2p.recv(self.grad_layer, self.next_stage)
         else:
             assert isinstance(outputs, tuple)
             for idx, buffer in enumerate(self.grad_layer):
                 # XXX GPT-2 hack
                 if self.is_grad_partitioned and idx == 0 and buffer.dtype != torch.long:
-                    buffer.data = torch.zeros(buffer.size(),
-                                              dtype=torch.long,
-                                              device=self.device)
+                    buffer.data = torch.zeros(buffer.size(), dtype=torch.long, device=self.device)
                 p2p.recv(buffer, self.next_stage)
 
         if self.wall_clock_breakdown():
             self.timers('pipe_recv_grad').stop()
 
     def _exec_optimizer_step(self, lr_kwargs=None):
         if self.wall_clock_breakdown():
@@ -1159,44 +1105,30 @@
         self._force_grad_boundary = True
         self._take_model_step(lr_kwargs)
         self._force_grad_boundary = False
 
         self.mem_status('AFTER STEP')
 
         if self.global_rank == 0 and self.monitor.enabled:
-            self.summary_events = [(f'Train/Samples/lr',
-                                    self.get_lr()[0],
-                                    self.global_samples)]
+            self.summary_events = [(f'Train/Samples/lr', self.get_lr()[0], self.global_samples)]
             if self.fp16_enabled() and hasattr(self.optimizer, 'cur_scale'):
-                self.summary_events.append((f'Train/Samples/loss_scale',
-                                            self.optimizer.cur_scale,
-                                            self.global_samples))
+                self.summary_events.append(
+                    (f'Train/Samples/loss_scale', self.optimizer.cur_scale, self.global_samples))
             self.monitor.write_events(self.summary_events)
 
         if self.wall_clock_breakdown():
             self.timers('step_microstep').stop()
             self.timers('step').stop()
             if self.global_steps % self.steps_per_print() == 0:
                 self.timers.log([
-                    'batch_input',
-                    'forward_microstep',
-                    'backward_microstep',
-                    'backward_inner_microstep',
-                    'backward_allreduce_microstep',
-                    'backward_tied_allreduce_microstep',
-                    'step_microstep'
+                    'batch_input', 'forward_microstep', 'backward_microstep', 'backward_inner_microstep',
+                    'backward_allreduce_microstep', 'backward_tied_allreduce_microstep', 'step_microstep'
                 ])
             if self.global_steps % self.steps_per_print() == 0:
-                self.timers.log([
-                    'forward',
-                    'backward',
-                    'backward_inner',
-                    'backward_allreduce',
-                    'step'
-                ])
+                self.timers.log(['forward', 'backward', 'backward_inner', 'backward_allreduce', 'step'])
 
     def _zero_grads(self, inputs):
         if isinstance(inputs, torch.Tensor):
             if inputs.grad is not None:
                 inputs.grad.data.zero_()
         else:
             for t in inputs:
@@ -1232,18 +1164,15 @@
     def _allocate_buffers(self, shapes_and_dtypes, requires_grad=False, num_buffers=-1):
         buffers = []
         if num_buffers == -1:
             num_buffers = self.num_pipe_buffers
         for count in range(num_buffers):
             buffer = []
             for shape, dtype in shapes_and_dtypes:
-                buffer.append(
-                    self._allocate_zeros(shape,
-                                         dtype=dtype,
-                                         requires_grad=requires_grad))
+                buffer.append(self._allocate_zeros(shape, dtype=dtype, requires_grad=requires_grad))
             buffers.append(buffer)
         return buffers
 
     def forward(self, *args, **kwargs):
         """Disabled for pipeline parallel training. See ``train_batch()``. """
         raise PipelineError("Only train_batch() is accessible in pipeline mode.")
 
@@ -1294,19 +1223,17 @@
         new_cached /= 1024**3
         delta_alloced /= 1024**3
         delta_cached /= 1024**3
         max_alloced /= 1024**3
         max_cached /= 1024**3
 
         print(
-            f'RANK={rank} STAGE={self.stage_id} STEP={self.global_steps} MEMSTATS',
-            msg,
+            f'RANK={rank} STAGE={self.stage_id} STEP={self.global_steps} MEMSTATS', msg,
             f'current alloc={new_alloced:0.4f}GB (delta={delta_alloced:0.4f}GB max={max_alloced:0.4f}GB) '
-            f'current cache={new_cached:0.4f}GB (delta={delta_cached:0.4f}GB max={max_cached:0.4f}GB)'
-        )
+            f'current cache={new_cached:0.4f}GB (delta={delta_cached:0.4f}GB max={max_cached:0.4f}GB)')
 
     def module_state_dict(self):
         """Override hack to save a pipe model and return the directory path of the save.
 
         This method should only be called by DeepSpeed's ``save_checkpoint()``. The
         recommended way of saving a ``PipelineModule`` outside of ``save_checkpoint()``
         is ``save_state_dict()``.
@@ -1314,16 +1241,15 @@
         Returns:
             None
         """
         assert isinstance(self.module, PipelineModule)
         assert self._curr_ckpt_path is not None, \
             "PipelineEngine expects module_state_dict() to be called from save_checkpoint()"
 
-        self.module.save_state_dict(self._curr_ckpt_path,
-                                    checkpoint_engine=self.checkpoint_engine)
+        self.module.save_state_dict(self._curr_ckpt_path, checkpoint_engine=self.checkpoint_engine)
         return None
 
     def load_module_state_dict(self, state_dict, strict=True, custom_load_fn=None):
         """Override hack to instead use a directory path.
 
         This is important because pipeline models checkpoint by layer instead of rank.
 
@@ -1363,14 +1289,12 @@
         self.fwd_outputs = []
 
         # For each step in the schedule
         for step_cmds in pipe_schedule:
             # For each instruction in the step
             for cmd in step_cmds:
                 if type(cmd) not in self._INSTRUCTION_MAP:
-                    raise RuntimeError(
-                        f'{self.__class__.__name__} does not understand instruction {repr(cmd)}'
-                    )
+                    raise RuntimeError(f'{self.__class__.__name__} does not understand instruction {repr(cmd)}')
 
                 # Equivalent to: self._exec_forward_pass(buffer_id=0)
                 self._exec_instr = MethodType(self._INSTRUCTION_MAP[type(cmd)], self)
                 self._exec_instr(**cmd.kwargs)
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/pipe/module.py` & `deepspeed-0.9.0/deepspeed/runtime/pipe/module.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,8 +1,11 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import os
 import glob
 
 import re as regex
 
 from functools import partial
@@ -41,48 +44,42 @@
     .. code-block:: python
 
         layer_specs = [
             LayerSpec(torch.nn.Linear, self.in_dim, self.hidden_dim, bias=False),
             LayerSpec(torch.nn.Linear, self.hidden_hidden, self.out_dim)]
         ]
     """
+
     def __init__(self, typename, *module_args, **module_kwargs):
         self.typename = typename
         self.module_args = module_args
         self.module_kwargs = module_kwargs
 
         if not issubclass(typename, nn.Module):
             raise RuntimeError('LayerSpec only supports torch.nn.Module types.')
 
         if dist.is_initialized():
             self.global_rank = dist.get_rank()
         else:
             self.global_rank = -1
 
     def __repr__(self):
-        return ds_utils.call_to_str(self.typename.__name__,
-                                    self.module_args,
-                                    self.module_kwargs)
+        return ds_utils.call_to_str(self.typename.__name__, self.module_args, self.module_kwargs)
 
     def build(self, log=False):
         """Build the stored specification."""
         if log:
             logger.info(f'RANK={self.global_rank} building {repr(self)}')
 
         return self.typename(*self.module_args, **self.module_kwargs)
 
 
 class TiedLayerSpec(LayerSpec):
-    def __init__(self,
-                 key,
-                 typename,
-                 *module_args,
-                 forward_fn=None,
-                 tied_weight_attr='weight',
-                 **module_kwargs):
+
+    def __init__(self, key, typename, *module_args, forward_fn=None, tied_weight_attr='weight', **module_kwargs):
         super().__init__(typename, *module_args, **module_kwargs)
         self.key = key
         self.forward_fn = forward_fn
         self.tied_weight_attr = tied_weight_attr
 
 
 class PipelineModule(nn.Module):
@@ -116,14 +113,15 @@
         seed_fn(type, optional): The custom seed generating function. Defaults to random seed generator.
         base_seed (int, optional): The starting seed. Defaults to 1234.
         partition_method (str, optional): The method upon which the layers are partitioned. Defaults to 'parameters'.
         activation_checkpoint_interval (int, optional): The granularity activation checkpointing in terms of number of layers. 0 disables activation checkpointing.
         activation_checkpoint_func (callable, optional): The function to use for activation checkpointing. Defaults to ``deepspeed.checkpointing.checkpoint``.
         checkpointable_layers(list, optional): Checkpointable layers may not be checkpointed. Defaults to None which does not additional filtering.
     """
+
     def __init__(self,
                  layers,
                  num_stages=None,
                  topology=None,
                  loss_fn=None,
                  seed_layers=False,
                  seed_fn=None,
@@ -150,17 +148,15 @@
         self.seed_fn = seed_fn
         self.base_seed = base_seed
         if dist.get_rank() == 0:
             try:
                 seed_str = self.seed_fn.__name__
             except AttributeError:
                 seed_str = None
-            print(
-                f'SEED_LAYERS={self.seed_layers} BASE_SEED={self.base_seed} SEED_FN={seed_str}'
-            )
+            print(f'SEED_LAYERS={self.seed_layers} BASE_SEED={self.base_seed} SEED_FN={seed_str}')
 
         # Setup world info
         self.world_group = dist.new_group(ranks=range(dist.get_world_size()))
         self.global_rank = dist.get_rank(group=self.world_group)
         self.world_size = dist.get_world_size(group=self.world_group)
         self.local_rank = int(os.environ.get("LOCAL_RANK", None))
         assert self.local_rank != None
@@ -169,23 +165,21 @@
             self._topo = topology
             self.num_stages = self._topo.get_dim('pipe')
         else:
             self.num_stages = num_stages
             if topology is None:
                 if self.world_size % self.num_stages != 0:
                     raise RuntimeError(
-                        f'num_stages ({self.num_stages}) must divide distributed world size ({self.world_size})'
-                    )
+                        f'num_stages ({self.num_stages}) must divide distributed world size ({self.world_size})')
                 dp = self.world_size // num_stages
                 topology = PipeDataParallelTopology(num_pp=num_stages, num_dp=dp)
                 self._topo = topology
 
         # Construct communicators for pipeline topology
-        self._grid = PipelineParallelGrid(process_group=self.world_group,
-                                          topology=self._topo)
+        self._grid = PipelineParallelGrid(process_group=self.world_group, topology=self._topo)
 
         self.stage_id = self._topo.get_coord(self.global_rank).pipe
 
         # Initialize partition information
         self._layer_specs = list(layers)
         self._num_layers = len(self._layer_specs)
         self._local_start = 0
@@ -241,17 +235,15 @@
                     self.tied_weight_attrs[layer.key] = layer.tied_weight_attr
 
                 if layer.forward_fn is None:
                     # Just use forward()
                     self.forward_funcs.append(self.tied_modules[layer.key])
                 else:
                     # User specified fn with args (module, input)
-                    self.forward_funcs.append(
-                        partial(layer.forward_fn,
-                                self.tied_modules[layer.key]))
+                    self.forward_funcs.append(partial(layer.forward_fn, self.tied_modules[layer.key]))
 
             # LayerSpec objects contain an nn.Module that should be allocated now.
             elif isinstance(layer, LayerSpec):
                 module = layer.build()
                 name = str(layer_idx)
                 self.forward_funcs.append(module)
                 self.fwd_map.update({name: len(self.forward_funcs) - 1})
@@ -300,16 +292,15 @@
                     name = layer.__name__
                 except AttributeError:
                     continue
             if typeregex.search(name):
                 idxs.append(idx)
 
         if len(idxs) == 0:
-            raise RuntimeError(
-                f"Partitioning '{layername}' found no valid layers to partition.")
+            raise RuntimeError(f"Partitioning '{layername}' found no valid layers to partition.")
         return idxs
 
     def forward(self, forward_input):
         # We need to offset the seed by the microbatch ID. Save it in a local var to
         # ensure it is preserved in the closure. Otherwise checkpointed forward funcs
         # will see a different offset.
         self.micro_offset += 1
@@ -323,16 +314,15 @@
             def exec_func(*inputs):
                 # Single tensor inputs need to be unwrapped
                 if len(inputs) == 1:
                     inputs = inputs[0]
                 for idx, layer in enumerate(self.forward_funcs[start:end]):
                     self.curr_layer = idx + self._local_start
                     if self.seed_layers:
-                        new_seed = (self.base_seed *
-                                    local_micro_offset) + self.curr_layer
+                        new_seed = (self.base_seed * local_micro_offset) + self.curr_layer
                         if self.seed_fn:
                             self.seed_fn(new_seed)
                         else:
                             ds_utils.set_random_seed(new_seed)
 
                     inputs = layer(inputs)
                 return inputs
@@ -342,28 +332,24 @@
         if self.activation_checkpoint_interval == 0:
             func = exec_range_func(0, len(self.forward_funcs))
             x = func(forward_input)
         else:
             num_layers = len(self.forward_funcs)
             x = forward_input
             for start_idx in range(0, num_layers, self.activation_checkpoint_interval):
-                end_idx = min(start_idx + self.activation_checkpoint_interval,
-                              num_layers)
+                end_idx = min(start_idx + self.activation_checkpoint_interval, num_layers)
 
                 funcs = self.forward_funcs[start_idx:end_idx]
                 # Since we either pass tensors or tuples of tensors without unpacking, we
                 # need to be careful not to double-wrap tensors with tuple.
                 if not isinstance(x, tuple):
                     x = (x, )
 
                 if self._is_checkpointable(funcs):
-                    x = self.activation_checkpoint_func(
-                        exec_range_func(start_idx,
-                                        end_idx),
-                        *x)
+                    x = self.activation_checkpoint_func(exec_range_func(start_idx, end_idx), *x)
                 else:
                     x = exec_range_func(start_idx, end_idx)(*x)
         return x
 
     def _partition_layers(self, method='uniform'):
         num_stages = self._topo.get_dim('pipe')
         stage_id = self._topo.get_coord(self.global_rank).pipe
@@ -372,27 +358,24 @@
             logger.info(f'Partitioning pipeline stages with method {method}')
 
         method = method.lower()
 
         # Each stage gets a simple uniform number of layers.
         if method == 'uniform':
             num_layers = len(self._layer_specs)
-            self.parts = ds_utils.partition_uniform(num_items=num_layers,
-                                                    num_parts=num_stages)
+            self.parts = ds_utils.partition_uniform(num_items=num_layers, num_parts=num_stages)
         elif method == 'parameters':
             param_counts = self._count_layer_params()
-            self.parts = ds_utils.partition_balanced(weights=param_counts,
-                                                     num_parts=num_stages)
+            self.parts = ds_utils.partition_balanced(weights=param_counts, num_parts=num_stages)
         elif method.startswith('type:'):
             layertype = method.split(':')[1]
             binary_weights = [0] * len(self._layer_specs)
             for idx in self._find_layer_type(layertype):
                 binary_weights[idx] = 1
-            self.parts = ds_utils.partition_balanced(weights=binary_weights,
-                                                     num_parts=num_stages)
+            self.parts = ds_utils.partition_balanced(weights=binary_weights, num_parts=num_stages)
         elif method == 'profile':
             raise NotImplementedError(f'Partitioning method {method} not implemented.')
         else:
             raise NotImplementedError(f'Partitioning method {method} not implemented.')
 
         # Print some information on the partitioning.
         if self.global_rank == 0:
@@ -432,16 +415,15 @@
             weight = getattr(self.tied_modules[key], comm['weight_attr'])
             weight_group_list.append((weight, comm['group']))
         return weight_group_list
 
     def _synchronize_tied_weights(self):
         for key, comm in self.tied_comms.items():
             dist.broadcast(
-                getattr(comm['module'],
-                        comm['weight_attr']),
+                getattr(comm['module'], comm['weight_attr']),
                 src=min(comm['ranks']),
                 group=comm['group'],
             )
 
     def _index_tied_modules(self):
         ''' Build communication structures for tied modules. '''
         tied_comms = {}
@@ -463,22 +445,17 @@
             # TODO: fiber to generate process groups.
             tied_stages = set(self.stage_owner(idx) for idx in tied_layers)
             for dp in range(self._grid.data_parallel_size):
                 for mp in range(self._grid.get_slice_parallel_world_size()):
                     tied_ranks = []
                     for s in sorted(tied_stages):
                         if self._grid.get_slice_parallel_world_size() > 1:
-                            tied_ranks.append(
-                                self._grid.stage_to_global(stage_id=s,
-                                                           data=dp,
-                                                           model=mp))
+                            tied_ranks.append(self._grid.stage_to_global(stage_id=s, data=dp, model=mp))
                         else:
-                            tied_ranks.append(
-                                self._grid.stage_to_global(stage_id=s,
-                                                           data=dp))
+                            tied_ranks.append(self._grid.stage_to_global(stage_id=s, data=dp))
                     group = dist.new_group(ranks=tied_ranks)
 
                     # Record this tied module if we own a local copy of it.
                     if self.global_rank in tied_ranks:
                         assert key in self.tied_modules
                         if key in self.tied_modules:
                             tied_comms[key] = {
@@ -595,35 +572,31 @@
             # We pass cloned tensors to torch.save() to avoid checkpoint bloat which occurs because torch.save()
             # saves the underlying storage rather than the slice of the storage corresponding to individual tensors.
             # This is a problem in DeepSpeed because we often allocate tensors using slices of large flattened buffers.
             # Tensor cloning helps to avoid this problem because the storage of cloned tensors are closer to the true size.
             # It is expected that the garbage collector will reclaim the cloned tensor storage to avoid memory bloat.
             # See https://pytorch.org/docs/stable/notes/serialization.html#preserve-storage-sharing
             orig_state_dict = layer.state_dict()
-            final_state_dict = type(orig_state_dict)(
-                {k: v.clone()
-                 for k,
-                 v in orig_state_dict.items()})
+            final_state_dict = type(orig_state_dict)({k: v.clone() for k, v in orig_state_dict.items()})
             checkpoint_engine.save(final_state_dict, model_ckpt_path)
 
     def load_state_dir(self, load_dir, checkpoint_engine, strict=True):
         for idx, layer in enumerate(self.forward_funcs):
             # Functions, etc. will not have state_dicts
             if not hasattr(layer, 'load_state_dict'):
                 continue
 
             # get all checkpoint files for the layer.
             model_ckpt_list = self.ckpt_layer_path_list(load_dir, idx)
             mp_rank = self._grid.get_slice_parallel_rank()
             mp_world_size = self._grid.get_slice_parallel_world_size()
 
-            sd_loader = SDLoaderFactory.get_sd_loader(
-                model_ckpt_list,
-                version=2.0,
-                checkpoint_engine=checkpoint_engine)
+            sd_loader = SDLoaderFactory.get_sd_loader(model_ckpt_list,
+                                                      version=2.0,
+                                                      checkpoint_engine=checkpoint_engine)
             load_path, checkpoint, _ = sd_loader.load(mp_world_size, mp_rank, module_key=None, is_pipe_parallel=True)
 
             layer.load_state_dict(checkpoint)
 
             # if self._grid.data_parallel_id == 0:
             #     logger.info(
             #         f'RANK={self.global_rank} Loaded layer={idx+self._local_start} file={load_path}'
@@ -632,14 +605,13 @@
         self._synchronize_tied_weights()
 
     def _is_checkpointable(self, funcs):
         # This is an unfortunate hack related to torch and deepspeed activation checkpoint implementations.
         # Some layers like torch.nn.Embedding will not receive grads if checkpointed, which breaks things.
         # I presume it's related to the discrete inputs that cannot require_grad? Need to revisit.
         if self.__class__.__name__ in ('GPTModelPipe', 'GPT2ModelPipe'):
-            return all('ParallelTransformerLayerPipe' in f.__class__.__name__
-                       for f in funcs)
+            return all('ParallelTransformerLayerPipe' in f.__class__.__name__ for f in funcs)
         if self.checkpointable_layers is not None:
             return all(f.__class__.__name__ in self.checkpointable_layers for f in funcs)
 
         params = [f.parameters() for f in funcs if isinstance(f, torch.nn.Module)]
         return any(len(list(p)) > 0 for p in params)
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/pipe/p2p.py` & `deepspeed-0.9.0/deepspeed/runtime/pipe/p2p.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-'''
-Copyright 2019 The Microsoft DeepSpeed Team
-'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import pickle
 import typing
 
 import torch
 from deepspeed import comm as dist
 
@@ -107,20 +108,18 @@
     Args:
         msg (typing.Any): The object to send.
         dest (int): Destination rank.
     """
     # serialize the message
     msg = pickle.dumps(msg)
     # construct a tensor to send
-    msg = torch.ByteTensor(torch.ByteStorage.from_buffer(msg)).to(
-        get_accelerator().device_name())
+    msg = torch.ByteTensor(torch.ByteStorage.from_buffer(msg)).to(get_accelerator().device_name())
 
     # Send meta and message
-    length_tensor = torch.tensor([len(msg)],
-                                 dtype=torch.long).to(get_accelerator().device_name())
+    length_tensor = torch.tensor([len(msg)], dtype=torch.long).to(get_accelerator().device_name())
     dist.send(length_tensor, dst=dest)
     dist.send(msg, dst=dest)
 
 
 def recv_obj(sender: int) -> typing.Any:
     """Receive an arbitrary python object from ``sender``.
 
@@ -131,16 +130,15 @@
         sender (int): The rank sending the message.
     """
     # Get message meta
     length = torch.tensor([0], dtype=torch.long).to(get_accelerator().device_name())
     dist.recv(length, src=sender)
 
     # Receive and deserialize
-    msg = torch.empty(length.item(),
-                      dtype=torch.uint8).to(get_accelerator().device_name())
+    msg = torch.empty(length.item(), dtype=torch.uint8).to(get_accelerator().device_name())
     dist.recv(msg, src=sender)
 
     msg = pickle.loads(msg.cpu().numpy().tobytes())
 
     def _to(x):
         """Recursively move to the current device."""
         if torch.is_tensor(x):
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/pipe/schedule.py` & `deepspeed-0.9.0/deepspeed/runtime/pipe/schedule.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,11 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 from ..utils import call_to_str
 
 from abc import ABC, abstractmethod
 
 
 class PipeSchedule(ABC):
@@ -38,14 +41,15 @@
                 return 1
 
     Args:
         micro_batches (int): The number of micro-batches that comprise a batch.
         stages (int): The number of pipeline stages.
         stage_id (int): The pipe stage that will execute the generated schedule.
     """
+
     def __init__(self, micro_batches, stages, stage_id):
         super().__init__()
         self.micro_batches = micro_batches
         self.stages = stages
         self.stage_id = stage_id
         self.prev_stage = self.stage_id - 1
         self.next_stage = self.stage_id + 1
@@ -127,14 +131,15 @@
             self.it = self.steps()
         return next(self.it)
 
 
 class InferenceSchedule(PipeSchedule):
     """A schedule for inferencing batches using pipeline parallelism.
     """
+
     def steps(self):
         """"""
         prev_micro_batch_id = -1
         total_steps = self.micro_batches + self.stages - 1
         for step_id in range(total_steps):
             cmds = []
             micro_batch_id = step_id - self.stage_id
@@ -184,14 +189,15 @@
 class TrainSchedule(PipeSchedule):
     """A schedule for training a batch using hybrid parallelism.
 
     Pipeline parallelism is extracted through gradient accumulation and thus
     convergence follows that of a data parallel approach with the same batch
     size.
     """
+
     def steps(self):
         """"""
         prev_micro_batch_id = -1
         total_steps = 2 * (self.micro_batches + self.stages - 1)
         for step_id in range(total_steps):
             # Map the step of the pipeline to the micro-batch id and also whether it is a
             # forward or backward pass step.
@@ -202,26 +208,22 @@
             if self._valid_micro_batch(micro_batch_id):
                 curr_buffer = self._buffer_idx(micro_batch_id)
 
             cmds = []
 
             # Exchange activations
             if is_forward:
-                if self._valid_micro_batch(prev_micro_batch_id) and self._valid_stage(
-                        self.prev_stage):
+                if self._valid_micro_batch(prev_micro_batch_id) and self._valid_stage(self.prev_stage):
                     cmds.append(SendGrad(prev_buffer))
-                if self._valid_micro_batch(micro_batch_id) and self._valid_stage(
-                        self.prev_stage):
+                if self._valid_micro_batch(micro_batch_id) and self._valid_stage(self.prev_stage):
                     cmds.append(RecvActivation(curr_buffer))
             else:
-                if self._valid_micro_batch(micro_batch_id) and self._valid_stage(
-                        self.next_stage):
+                if self._valid_micro_batch(micro_batch_id) and self._valid_stage(self.next_stage):
                     cmds.append(RecvGrad(curr_buffer))
-                if self._valid_micro_batch(prev_micro_batch_id) and self._valid_stage(
-                        self.next_stage):
+                if self._valid_micro_batch(prev_micro_batch_id) and self._valid_stage(self.next_stage):
                     cmds.append(SendActivation(prev_buffer))
 
             # First/last stage loads
             if self.stage_id == 0 or self.stage_id == self.stages - 1:
                 if is_forward and self._valid_micro_batch(micro_batch_id):
                     cmds.append(LoadMicroBatch(curr_buffer))
 
@@ -296,14 +298,15 @@
         return micro_batch_id
 
 
 class DataParallelSchedule(PipeSchedule):
     """An example schedule that trains using traditional data parallelism with gradient
     accumulation.
     """
+
     def steps(self):
         """"""
         for step_id in range(self.micro_batches):
             cmds = [
                 LoadMicroBatch(buffer_id=0),
                 ForwardPass(buffer_id=0),
                 BackwardPass(buffer_id=0),
@@ -326,14 +329,15 @@
 
     All keyword arguments are stored as members similar to a ``namedtuple``. These are
     then accessible to the :class:`PipeEngine` during execution.
 
     Args:
         kwargs (optional): keyword arguments to store as members
     """
+
     def __init__(self, **kwargs):
         self.name = self.__class__.__name__
         self.kwargs = kwargs
         for key, val in kwargs.items():
             setattr(self, key, val)
 
     def __repr__(self):
@@ -370,14 +374,15 @@
 
 class BufferOpInstruction(PipeInstruction):
     """A pipeline instruction that operates on pipeline buffer(s).
 
     Args:
         buffer_id (int): the index of the pipeline buffer() to modify.
     """
+
     def __init__(self, buffer_id, **kwargs):
         super().__init__(buffer_id=buffer_id, **kwargs)
 
 
 # IO
 class LoadMicroBatch(BufferOpInstruction):
     """Load a micro-batch into a buffer.
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/pipe/topology.py` & `deepspeed-0.9.0/deepspeed/runtime/pipe/topology.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,11 @@
-# Copyright 2019 The Microsoft DeepSpeed Team
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 from deepspeed import comm as dist
 
 from collections import namedtuple
 from itertools import product as cartesian_product
 
 
@@ -15,14 +18,15 @@
     of the axes defines the layout of the topology. ProcessTopology uses a "row-major"
     layout of the tensor axes, and so axes=['x', 'y'] would map coordinates (x,y) and
     (x,y+1) to adjacent linear indices. If instead axes=['y', 'x'] was used, coordinates
     (x,y) and (x+1,y) would be adjacent.
 
     Some methods return ProcessCoord namedtuples.
     """
+
     def __init__(self, axes, dims):
         """Create a mapping of n-dimensional tensor coordinates to linear indices.
 
         Arguments:
             axes (list): the names of the tensor axes
             dims (list): the dimension (length) of each axis of the topology tensor
         """
@@ -58,20 +62,15 @@
         assert key in self.mapping, f'key {coord_kwargs} invalid'
         return self.mapping[key]
 
     def get_axis_names(self):
         """Return a list of the axis names in the ordering of the topology. """
         return self.axes
 
-    def get_rank_repr(self,
-                      rank,
-                      omit_axes=['data',
-                                 'pipe'],
-                      inner_sep='_',
-                      outer_sep='-'):
+    def get_rank_repr(self, rank, omit_axes=['data', 'pipe'], inner_sep='_', outer_sep='-'):
         """Return a string representation of a rank.
 
         This method is primarily used for checkpointing model data.
 
         For example:
             >>> topo = Topo(axes=['a', 'b'], dims=[2, 2])
             >>> topo.get_rank_repr(rank=3)
@@ -177,14 +176,15 @@
 
         Arguments:
             **filter_kwargs (dict): criteria used to select coordinates.
 
         Returns:
             The list of ranks whose coordinates match filter_kwargs.
         """
+
         def _filter_helper(x):
             for key, val in filter_kwargs.items():
                 if getattr(x, key) != val:
                     return False
             return True
 
         coords = filter(_filter_helper, self.mapping.keys())
@@ -232,20 +232,22 @@
 class PipeDataParallelTopology(ProcessTopology):
     """ A topology specialization for hybrid data and pipeline parallelism.
 
         Uses data parallelism on the last dimension to encourage gradient
         reductions to use high-bandwidth intra-node links and lower-volume
         pipeline communications to use low-bandwidth inter-node links.
     """
+
     def __init__(self, num_pp, num_dp):
         super().__init__(axes=['pipe', 'data'], dims=[num_pp, num_dp])
 
 
 class PipeModelDataParallelTopology(ProcessTopology):
     """ A topology for hybrid pipeline, model, and data parallelism. """
+
     def __init__(self, num_pp, num_mp, num_dp):
         super().__init__(axes=['pipe', 'data', 'model'], dims=[num_pp, num_dp, num_mp])
 
 
 class PipelineParallelGrid:
     """Implements a grid object that stores the data parallel ranks
     corresponding to each of the model parallel stages
@@ -264,14 +266,15 @@
     stores process ranks of adjacent stages for a given data_parallel_id.
     For example if num_stage is 5 then a tuple [7,8] represents stages [3, 4],
     with data_parallel id = 1. A stage wrap around will appear as non-adjacent ranks,
     for example tuple [4,0] with representing wrap-around stage 4 and 0, for
     data_parallel_id = 0, or similarly [9,5] represents wrapped around stages [4,0]
     for data_parallel_id = 1.
     """
+
     def __init__(self, topology=None, process_group=None):
         # TODO use process_group if provided
         self.global_rank = dist.get_rank()
         self.world_size = dist.get_world_size()
         if topology is not None:
             if self.global_rank == 0:
                 print('Using topology:', topology)
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/progressive_layer_drop.py` & `deepspeed-0.9.0/deepspeed/runtime/progressive_layer_drop.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,22 +1,26 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import numpy as np
 from deepspeed.utils import log_dist
 
 
 class ProgressiveLayerDrop(object):
     r""" Progressive Layer Dropping (PLD) for model training.
         This implements the PLD technique for compressed model training
         from this paper: https://arxiv.org/pdf/2010.13369.pdf
     Args:
         theta (float): a hyper-parameter that controls the trade-off between training time and robustness.
         The lower the theta value, the faster the training speed. Default value: 0.5.
         gamma (float): a hyper-parameter that controls how fast the drop ratio increases. Default value: 0.001.
     """
+
     def __init__(self, theta=0.5, gamma=0.001):
         super().__init__()
 
         self.theta = theta
         self.gamma = gamma
         self.current_theta = 1.0
         log_dist(f'Enabled progressive layer dropping (theta = {self.theta})', ranks=[0])
@@ -25,11 +29,12 @@
         kwargs = {'progressive_layer_drop': True, 'pld_theta': self.get_theta()}
         return kwargs
 
     def get_theta(self):
         return self.current_theta
 
     def update_state(self, global_step):
+
         def _prob(x, gamma, p):
             return (1. - p) * np.exp(-gamma * x) + p
 
         self.current_theta = _prob(global_step, self.gamma, self.theta)
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/quantize.py` & `deepspeed-0.9.0/deepspeed/runtime/quantize.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,18 +1,22 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import torch
 import math
 from deepspeed.utils import logger
 from deepspeed.ops.quantizer import ds_quantizer
 
 TWO_D_PARAMS = 6
 
 
 class Quantizer(object):
+
     def __init__(self,
                  q_groups=1,
                  q_mixed_fp16=False,
                  q_change_ratio=0.01,
                  q_type=0,
                  q_rounding=0,
                  q_verbose=False,
@@ -35,25 +39,20 @@
     def any_precision_switch(self):
         # Temporary disabled functionality
         if self.layer_num == 0:
             return True
         result = False
         for index in range(self.layer_num):
             if self.q_start_bits[index] != self.q_target_bits:
-                next_step = self.qsteps + (
-                    TWO_D_PARAMS * (self.layer_num if self.layer_num != 0 else 1))
+                next_step = self.qsteps + (TWO_D_PARAMS * (self.layer_num if self.layer_num != 0 else 1))
                 if next_step >= self.q_period[index]:
                     result = True
         return result
 
-    def quantize(self,
-                 parameter_group,
-                 overflow,
-                 eigenvalue_enabled,
-                 block_eigenvalue={}):
+    def quantize(self, parameter_group, overflow, eigenvalue_enabled, block_eigenvalue={}):
 
         if overflow and not eigenvalue_enabled:
             return
 
         self.step()
 
         self.update_fp16_ratio()
@@ -61,15 +60,16 @@
         for i in range(len(parameter_group)):
             for p in parameter_group[i]:
                 if len(p.size()) > 1 and hasattr(p, "start_bits") and p.start_bits:
                     param_id = id(p)
                     if block_eigenvalue is None:
                         eigenvalue, layer_id = None, 0
                     else:
-                        eigenvalue, layer_id = block_eigenvalue[param_id] if param_id in block_eigenvalue else (None, 0)
+                        eigenvalue, layer_id = block_eigenvalue[param_id] if param_id in block_eigenvalue else (None,
+                                                                                                                0)
                     if eigenvalue is not None:
                         factor = 1 + math.floor(eigenvalue * 4)
                         p.data = self.compute_quantization(p.data, layer_id, factor)
                     else:
                         p.data = self.compute_quantization(p, layer_id)
 
     def step(self):
@@ -87,23 +87,19 @@
             p = 0.
         else:
             p = input_flat.new(input_flat.shape).uniform_(-0.5, 0.5)
 
         if self.q_type == 'symmetric':
             scale = 2 * torch.max(torch.abs(g_min), torch.abs(g_max)) / q_range
             zero_point = 0.
-            input_flat = (input_flat / scale + p).round().clamp(
-                -(q_range >> 1),
-                (q_range >> 1) - 1) * scale
+            input_flat = (input_flat / scale + p).round().clamp(-(q_range >> 1), (q_range >> 1) - 1) * scale
         elif self.q_type == 'asymmetric':
             scale = (g_max - g_min) / q_range
             zero_point = (g_min / scale).round() * scale
-            input_flat = ((input_flat - zero_point) / scale + p).round().clamp(
-                0,
-                (q_range - 1)) * scale + zero_point
+            input_flat = ((input_flat - zero_point) / scale + p).round().clamp(0, (q_range - 1)) * scale + zero_point
         output = input_flat.reshape(inputs.shape).contiguous()
         return output
 
     def quantize_tenary(self, inputs):
         input_flat = inputs.reshape(self.q_groups, -1)
         n = input_flat.shape[1]
         m = input_flat.norm(p=1, dim=1).div(n)
@@ -122,16 +118,15 @@
         m = input_flat.norm(p=1, dim=1, keepdim=True).div(n)
         output = input_flat.sign().mul(m)
         output = output.reshape(inputs.shape).contiguous()
         return output
 
     def mixed_fp16_quantize(self, input, input_q, index):
         if self.q_mixed_fp16 and self.q_start_bits[index] >= (self.q_target_bits - 1):
-            input_q = input * self.quantize_real_ratio + (
-                1 - self.quantize_real_ratio) * input_q
+            input_q = input * self.quantize_real_ratio + (1 - self.quantize_real_ratio) * input_q
             return input_q
         return input_q
 
     def compute_quantization(self, input, index=0, factor=1):
         # fixing the quantization bits based on the training steps
         # when reducing 1 bit at each period, we increase the period
         # to go slowly toward the target quantization bits
@@ -148,23 +143,20 @@
                         f'Quantization settings: current bit-precision = {input.start_bits}, step = {self.qsteps}, quantization period = {input.q_period}, index = {index}'
                     )
         assert (input.start_bits >= input.target_bits), \
             'Quantization bit is lower than target precision bits!'
 
         if self.use_quantizer_kernel:
             if input.start_bits <= 2:
-                raise ValueError(
-                    'Quantization bit is too low, please do it without quantization kernel!'
-                )
-            input_q = ds_quantizer(
-                input.data.clone(),
-                self.q_groups,
-                input.start_bits,
-                asym=False if self.q_type == 'symmetric' else True,
-                sr=False if self.q_rounding == 'nearest_neighbor' else True)
+                raise ValueError('Quantization bit is too low, please do it without quantization kernel!')
+            input_q = ds_quantizer(input.data.clone(),
+                                   self.q_groups,
+                                   input.start_bits,
+                                   asym=False if self.q_type == 'symmetric' else True,
+                                   sr=False if self.q_rounding == 'nearest_neighbor' else True)
         else:
             if input.start_bits >= 3:
                 input_flat = self.quantize_highbit(input.data, input.start_bits)
             elif input.start_bits == 2:
                 assert self.q_type == 'symmetric', 'Quantization type is not symmetric!'
                 assert self.q_rounding == 'nearest', 'Quantization rounding is not nearest_neighbor!'
                 input_flat = self.quantize_tenary(input.data)
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/sparse_tensor.py` & `deepspeed-0.9.0/deepspeed/runtime/sparse_tensor.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,19 +1,22 @@
-"""
-Copyright 2020 The Microsoft DeepSpeed Team
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
 
+# DeepSpeed Team
+"""
 Implementation of a compressed sparse tensor. Similar in
 functionality to TensorFlow's IndexedSlices implementation.
 """
 
 import torch
 
 
 class SparseTensor(object):
     """ Compressed Sparse Tensor """
+
     def __init__(self, dense_tensor=None):
         self.orig_dense_tensor = dense_tensor
         self.is_sparse = dense_tensor.is_sparse
         if dense_tensor is not None:
             if dense_tensor.is_sparse:
                 dense_tensor = dense_tensor.coalesce()
                 self.indices = dense_tensor.indices().flatten()
@@ -25,29 +28,24 @@
             self.dense_size = list(dense_tensor.size())
         else:
             self.indices = None
             self.values = None
             self.dense_size = None
 
     def to_coo_tensor(self):
-        return torch.sparse_coo_tensor(self.indices.unsqueeze(0),
-                                       self.values,
-                                       self.dense_size)
+        return torch.sparse_coo_tensor(self.indices.unsqueeze(0), self.values, self.dense_size)
 
     @staticmethod
     def type():
         return "deepspeed.SparseTensor"
 
     def to_dense(self):
         it = self.indices.unsqueeze(1)
         full_indices = torch.cat([it for _ in range(self.dense_size[1])], dim=1)
-        return self.values.new_zeros(self.dense_size).scatter_add_(
-            0,
-            full_indices,
-            self.values)
+        return self.values.new_zeros(self.dense_size).scatter_add_(0, full_indices, self.values)
 
     def sparse_size(self):
         index_size = list(self.indices.size())
         index_size = index_size[0]
         value_size = list(self.values.size())
         value_size = value_size[0] * value_size[1]
         dense_size = self.dense_size[0] * self.dense_size[1]
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/state_dict_factory.py` & `deepspeed-0.9.0/deepspeed/runtime/state_dict_factory.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-'''
-Copyright 2020 The Microsoft DeepSpeed Team
-'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import torch
 import os
 import copy
 import collections
 import json
 from abc import ABC, abstractmethod
@@ -14,14 +15,15 @@
 
 from .weight_quantizer import WeightQuantization
 
 AUTO_MODULE_KEY = 'auto'
 
 
 class SDLoaderFactory:
+
     @staticmethod
     def get_sd_loader_json(json_file, checkpoint_engine):
         if isinstance(json_file, str):
             with open(json_file) as f:
                 data = json.load(f)
         else:
             assert isinstance(json_file, dict)
@@ -29,34 +31,31 @@
         sd_type = data['type']
         ckpt_list = data['checkpoints']
         version = data['version']
         ckpt_type = data.get('parallelization', 'pp')
         mp_size = data.get('mp_size', 0)
         if sd_type.lower() in ['bloom', 'ds_model']:
             return data
-        return SDLoaderFactory.get_sd_loader(ckpt_list,
-                                             checkpoint_engine,
-                                             sd_type,
-                                             version)
+        return SDLoaderFactory.get_sd_loader(ckpt_list, checkpoint_engine, sd_type, version)
 
     @staticmethod
     def get_sd_loader(ckpt_list, checkpoint_engine, sd_type='Megatron', version=None):
         if sd_type == 'Megatron':
             return MegatronSDLoader(ckpt_list, version, checkpoint_engine)
         else:
             assert False, '{} checkpoint type is not supported'.format(sd_type)
 
 
 class SDLoaderBase(ABC):
+
     def __init__(self, ckpt_list, version, checkpoint_engine):
         self.module_key = None
         self.ckpt_list = ckpt_list
         self.version = version
-        self.checkpoint_engine = TorchCheckpointEngine(
-        ) if checkpoint_engine is None else checkpoint_engine
+        self.checkpoint_engine = TorchCheckpointEngine() if checkpoint_engine is None else checkpoint_engine
         self.check_ckpt_list()
 
     def load(self,
              mp_world_size,
              mp_rank,
              module_key=AUTO_MODULE_KEY,
              is_pipe_parallel=False,
@@ -95,17 +94,17 @@
         if num_ckpt == mp_world_size:
             assert os.path.exists(load_path)
             #logger.info(f'rank: {mp_rank} loading checkpoint: {load_path}')
             sd = self.checkpoint_engine.load(load_path, map_location=lambda storage, \
                 loc: storage)
 
             if quantize:
-                quantizer = WeightQuantization(mlp_extra_grouping=mlp_extra_grouping,
-                                               mp_size=mp_world_size)
-                sd_module, all_scales = quantizer.sd_quantize_megatron(self.get_module(sd), quantize_bits, quantize_groups)
+                quantizer = WeightQuantization(mlp_extra_grouping=mlp_extra_grouping, mp_size=mp_world_size)
+                sd_module, all_scales = quantizer.sd_quantize_megatron(self.get_module(sd), quantize_bits,
+                                                                       quantize_groups)
                 self.set_module(sd, sd_module)
             else:
                 all_scales = None
         elif num_ckpt > mp_world_size:
             sd, all_scales, merge_count = self.merge_state_dict(mp_world_size, mp_rank, quantize, \
                 quantize_bits, quantize_groups, mlp_extra_grouping)
         else:
@@ -114,47 +113,37 @@
         return load_path, sd, (all_scales, merge_count)
 
     def get_merge_state_dicts(self, mp_world_size, mp_rank):
         num_ckpt = len(self.ckpt_list)
         assert num_ckpt % mp_world_size == 0, 'Invalid checkpoints and world size for sd merge'
 
         num_to_merge = num_ckpt // mp_world_size
-        ckpt_list = [
-            self.ckpt_list[i] for i in range(num_to_merge * mp_rank,
-                                             num_to_merge * (mp_rank + 1))
-        ]
+        ckpt_list = [self.ckpt_list[i] for i in range(num_to_merge * mp_rank, num_to_merge * (mp_rank + 1))]
 
         logger.info(f"mp_rank: {mp_rank}, ckpt_list: {ckpt_list}")
-        sd_list = [
-            self.checkpoint_engine.load(ckpt,
-                                        map_location=lambda storage,
-                                        loc: storage) for ckpt in ckpt_list
-        ]
+        sd_list = [self.checkpoint_engine.load(ckpt, map_location=lambda storage, loc: storage) for ckpt in ckpt_list]
         return sd_list
 
     def get_split_state_dict(self, mp_world_size, mp_rank):
         num_ckpt = len(self.ckpt_list)
         assert mp_world_size % num_ckpt == 0, 'Invalid checkpoints and world size for sd split'
 
         num_to_split = mp_world_size // num_ckpt
         ckpt_index = mp_rank // num_to_split
         ckpt_offset = mp_rank % num_to_split
 
-        logger.info(
-            f"mp_rank: {mp_rank}, ckpt_list: {self.ckpt_list[ckpt_index]}, offset: {ckpt_offset}"
-        )
-
-        sd = self.checkpoint_engine.load(self.ckpt_list[ckpt_index],
-                                         map_location=lambda storage,
-                                         loc: storage)
+        logger.info(f"mp_rank: {mp_rank}, ckpt_list: {self.ckpt_list[ckpt_index]}, offset: {ckpt_offset}")
+
+        sd = self.checkpoint_engine.load(self.ckpt_list[ckpt_index], map_location=lambda storage, loc: storage)
 
         return sd, num_to_split, ckpt_offset
 
     def _choose_module_key(self, sd):
-        assert not ('module' in sd and 'model' in sd), "checkpoint has both 'model' and 'module' keys, not sure how to proceed"
+        assert not ('module' in sd
+                    and 'model' in sd), "checkpoint has both 'model' and 'module' keys, not sure how to proceed"
         assert 'module' in sd or 'model' in sd, "checkpoint contains neither 'model' or 'module' keys, not sure how to proceed"
         if 'module' in sd:
             return 'module'
         elif 'model' in sd:
             return 'model'
 
     def get_module(self, sd):
@@ -174,48 +163,36 @@
             sd[self.module_key] = module
         return sd
 
     def check_ckpt_list(self):
         #logger.info(f'checkpoint file list: {self.ckpt_list}')
         assert len(self.ckpt_list) > 0
 
-        sd = self.checkpoint_engine.load(self.ckpt_list[0],
-                                         map_location=lambda storage,
-                                         loc: storage)
+        sd = self.checkpoint_engine.load(self.ckpt_list[0], map_location=lambda storage, loc: storage)
 
         # check checkpoint count is same with saved mp_world_size
         if 'mp_world_size' in sd.keys():
-            assert len(self.ckpt_list) == sd['mp_world_size'], f"checkpoint count {len(self.ckpt_list)} is different from saved mp_world_size {sd['mp_world_size']}"
+            assert len(self.ckpt_list) == sd[
+                'mp_world_size'], f"checkpoint count {len(self.ckpt_list)} is different from saved mp_world_size {sd['mp_world_size']}"
 
     @abstractmethod
-    def merge_state_dict(self,
-                         mp_world_size,
-                         mp_rank,
-                         quantize,
-                         quantize_bits,
-                         groups,
-                         mlp_extra_grouping):
+    def merge_state_dict(self, mp_world_size, mp_rank, quantize, quantize_bits, groups, mlp_extra_grouping):
         pass
 
     @abstractmethod
-    def split_state_dict(self,
-                         mp_world_size,
-                         mp_rank,
-                         quantize,
-                         quantize_bits,
-                         groups,
-                         mlp_extra_grouping):
+    def split_state_dict(self, mp_world_size, mp_rank, quantize, quantize_bits, groups, mlp_extra_grouping):
         pass
 
     @abstractmethod
     def sanity_check(self, ckpt_file_name):
         pass
 
 
 class MegatronSDLoader(SDLoaderBase):
+
     def __init__(self, ckpt_list, version, checkpoint_engine):
         super().__init__(ckpt_list, version, checkpoint_engine)
         """
         ## Q/K/V data need special processing
         key: transformer.layers.0.attention.query_key_value.weight, shape: torch.Size([3192, 4256])
         key: transformer.layers.0.attention.query_key_value.bias, shape: torch.Size([3192])
 
@@ -336,48 +313,35 @@
 
         client_sd_list = [self.get_module(sd) for sd in sd_list]
         keys = client_sd_list[0].keys()
 
         ckpt_ver = self.get_checkpoint_version(ds_sd)
         logger.info(f"checkpoint version: {ckpt_ver}")
         if quantize:
-            quantizer = WeightQuantization(mlp_extra_grouping=mlp_extra_grouping,
-                                           mp_size=mp_world_size)
+            quantizer = WeightQuantization(mlp_extra_grouping=mlp_extra_grouping, mp_size=mp_world_size)
 
         for key in keys:
             value_list = [sd[key] for sd in client_sd_list]
 
             if "attention.dense.weight" in key or "mlp.dense_4h_to_h.weight" in key:
                 if quantize:
-                    value_list = quantizer.Quantize(value_list,
-                                                    quantize_bits,
-                                                    groups,
-                                                    key=key,
-                                                    merge_dim=1)
+                    value_list = quantizer.Quantize(value_list, quantize_bits, groups, key=key, merge_dim=1)
                 new_client_sd[key] = torch.cat(value_list, axis=1)
             elif "attention.query_key_value" in key:
                 if quantize and "attention.query_key_value.weight" in key:
-                    value_list = quantizer.Quantize(value_list,
-                                                    quantize_bits,
-                                                    groups,
-                                                    key=key)
+                    value_list = quantizer.Quantize(value_list, quantize_bits, groups, key=key)
                     new_client_sd[key] = torch.cat(value_list, axis=0)
                 else:
                     if quantize:
                         new_client_sd[key] = torch.cat(value_list, axis=0)
                     else:
-                        new_client_sd[key] = self.merge_query_key_value(
-                            value_list,
-                            ckpt_ver)
+                        new_client_sd[key] = self.merge_query_key_value(value_list, ckpt_ver)
             elif "mlp.dense_h_to_4h.weight" in key or "word_embeddings.weight" in key or "mlp.dense_h_to_4h.bias" in key:
                 if quantize and "mlp.dense_h_to_4h.weight" in key:
-                    value_list = quantizer.Quantize(value_list,
-                                                    quantize_bits,
-                                                    groups,
-                                                    key=key)
+                    value_list = quantizer.Quantize(value_list, quantize_bits, groups, key=key)
                 new_client_sd[key] = torch.cat(value_list, axis=0)
             else:
                 new_client_sd[key] = value_list[0]
         if quantize:
             all_scales = quantizer.merge_scales()
         ds_sd = self.set_module(ds_sd, new_client_sd)
 
@@ -398,16 +362,15 @@
 
         client_sd = self.get_module(sd)
 
         ckpt_ver = self.get_checkpoint_version(ds_sd)
         logger.info(f"checkpoint version: {ckpt_ver}")
 
         if quantize:
-            quantizer = WeightQuantization(mlp_extra_grouping=mlp_extra_grouping,
-                                           mp_size=mp_world_size)
+            quantizer = WeightQuantization(mlp_extra_grouping=mlp_extra_grouping, mp_size=mp_world_size)
 
         for key in client_sd.keys():
             value = client_sd[key]
 
             if "attention.dense.weight" in key or "mlp.dense_4h_to_h.weight" in key:
                 assert value.shape[1] % num_to_split == 0
                 split_size = value.shape[1] // num_to_split
@@ -415,19 +378,15 @@
                     q_vals = quantizer.Quantize([value], quantize_bits, groups, key)
                     value = q_vals[0]
                 new_client_sd[key] = torch.split(value, split_size, dim=1)[ckpt_offset]
             elif "attention.query_key_value" in key:
                 if quantize and "attention.query_key_value.weight" in key:
                     q_vals = quantizer.Quantize([value], quantize_bits, groups, key)
                     value = q_vals[0]
-                new_client_sd[key] = self.split_query_key_value(
-                    value,
-                    num_to_split,
-                    ckpt_offset,
-                    ckpt_ver)
+                new_client_sd[key] = self.split_query_key_value(value, num_to_split, ckpt_offset, ckpt_ver)
             elif "mlp.dense_h_to_4h.weight" in key or "word_embeddings.weight" in key or "mlp.dense_h_to_4h.bias" in key or "final_linear.weight" in key:
                 assert value.shape[0] % num_to_split == 0
                 split_size = value.shape[0] // num_to_split
                 if quantize and "mlp.dense_h_to_4h.weight" in key:
                     q_vals = quantizer.Quantize([value], quantize_bits, groups, key)
                     value = q_vals[0]
                 new_client_sd[key] = torch.split(value, split_size, dim=0)[ckpt_offset]
@@ -439,36 +398,30 @@
 
         ds_sd = self.set_module(ds_sd, new_client_sd)
 
         return ds_sd, (all_scales if quantize else None)
 
     def sanity_check(self, ckpt_file_name):
         keys_to_check = [
-            "attention.dense.weight",
-            "mlp.dense_4h_to_h.weight",
-            "attention.query_key_value",
-            "mlp.dense_h_to_4h.weight",
-            "mlp.dense_h_to_4h.bias"
+            "attention.dense.weight", "mlp.dense_4h_to_h.weight", "attention.query_key_value",
+            "mlp.dense_h_to_4h.weight", "mlp.dense_h_to_4h.bias"
         ]
 
-        sd = self.checkpoint_engine.load(ckpt_file_name,
-                                         map_location=lambda storage,
-                                         loc: storage)
+        sd = self.checkpoint_engine.load(ckpt_file_name, map_location=lambda storage, loc: storage)
 
         # partial_key is a sub-string of one key in the sd
         def check_key_exist(partial_key, sd):
             keys = sd.keys()
             found = False
             for k in keys:
                 if partial_key in k:
                     found = True
                     break
             return found
 
         for key in keys_to_check:
-            assert check_key_exist(key, self.get_module(sd)), f'key: {key} is not found in the checkpoint {ckpt_file_name}'
+            assert check_key_exist(key,
+                                   self.get_module(sd)), f'key: {key} is not found in the checkpoint {ckpt_file_name}'
 
     def get_checkpoint_version(self, state_dict):
         # Use 0 if version info doesn't exist
-        return self.version if self.version is not None else state_dict.get(
-            'checkpoint_version',
-            0)
+        return self.version if self.version is not None else state_dict.get('checkpoint_version', 0)
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/swap_tensor/aio_config.py` & `deepspeed-0.9.0/deepspeed/runtime/swap_tensor/aio_config.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,11 +1,11 @@
-'''
-Copyright 2020 The Microsoft DeepSpeed Team.
-Licensed under the MIT license.
-'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 from deepspeed.runtime.config_utils import get_scalar_param
 from deepspeed.runtime.swap_tensor.constants import *
 
 AIO_DEFAULT_DICT = {
     AIO_BLOCK_SIZE: AIO_BLOCK_SIZE_DEFAULT,
     AIO_QUEUE_DEPTH: AIO_QUEUE_DEPTH_DEFAULT,
@@ -15,30 +15,15 @@
 }
 
 
 def get_aio_config(param_dict):
     if AIO in param_dict.keys() and param_dict[AIO] is not None:
         aio_dict = param_dict[AIO]
         return {
-            AIO_BLOCK_SIZE:
-            get_scalar_param(aio_dict,
-                             AIO_BLOCK_SIZE,
-                             AIO_BLOCK_SIZE_DEFAULT),
-            AIO_QUEUE_DEPTH:
-            get_scalar_param(aio_dict,
-                             AIO_QUEUE_DEPTH,
-                             AIO_QUEUE_DEPTH_DEFAULT),
-            AIO_THREAD_COUNT:
-            get_scalar_param(aio_dict,
-                             AIO_THREAD_COUNT,
-                             AIO_THREAD_COUNT_DEFAULT),
-            AIO_SINGLE_SUBMIT:
-            get_scalar_param(aio_dict,
-                             AIO_SINGLE_SUBMIT,
-                             AIO_SINGLE_SUBMIT_DEFAULT),
-            AIO_OVERLAP_EVENTS:
-            get_scalar_param(aio_dict,
-                             AIO_OVERLAP_EVENTS,
-                             AIO_OVERLAP_EVENTS_DEFAULT)
+            AIO_BLOCK_SIZE: get_scalar_param(aio_dict, AIO_BLOCK_SIZE, AIO_BLOCK_SIZE_DEFAULT),
+            AIO_QUEUE_DEPTH: get_scalar_param(aio_dict, AIO_QUEUE_DEPTH, AIO_QUEUE_DEPTH_DEFAULT),
+            AIO_THREAD_COUNT: get_scalar_param(aio_dict, AIO_THREAD_COUNT, AIO_THREAD_COUNT_DEFAULT),
+            AIO_SINGLE_SUBMIT: get_scalar_param(aio_dict, AIO_SINGLE_SUBMIT, AIO_SINGLE_SUBMIT_DEFAULT),
+            AIO_OVERLAP_EVENTS: get_scalar_param(aio_dict, AIO_OVERLAP_EVENTS, AIO_OVERLAP_EVENTS_DEFAULT)
         }
 
     return AIO_DEFAULT_DICT
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/swap_tensor/async_swapper.py` & `deepspeed-0.9.0/deepspeed/runtime/swap_tensor/async_swapper.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,24 +1,26 @@
-"""
-Copyright 2020 The Microsoft DeepSpeed Team.
-Licensed under the MIT license.
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
 
+# DeepSpeed Team
+"""
 Functionality of swapping tensors to/from (NVMe) storage devices.
 """
 import torch
 
 from deepspeed import comm as dist
 from deepspeed.utils.logging import logger
 from deepspeed.runtime.swap_tensor.utils import swap_out_tensors, SwapBuffer
 
 INVALID_BUFFER_INDEX = -1
 ASYNC_SWAPPER_WAIT_TIMER = 'async_swap_gradient_wait'
 
 
 class AsyncTensorSwapper(object):
+
     def __init__(self, aio_handle, numel_alignment, timers):
         self.free_buffer_index = []
         self.swapping_buffer_index = []
         self.ready_buffer_index = []
         self.current_buffer_index = INVALID_BUFFER_INDEX
         self.all_buffers = []
         self.aio_handle = aio_handle
@@ -66,17 +68,15 @@
         for tensor, swap_path in zip(tensor_list, path_list):
             self._swap_out_tensor(tensor, swap_path)
 
     def _report_statistics(self, message):
         if dist.get_rank() == 0:
             element_size = torch.tensor([], dtype=self.dtype).element_size()
             swapped_GB = (self.num_elements_swapped * element_size) / (1024**3)
-            logger.debug(
-                f'{message} num_elems = {self.num_elements_swapped}, {swapped_GB:5.2f} GB'
-            )
+            logger.debug(f'{message} num_elems = {self.num_elements_swapped}, {swapped_GB:5.2f} GB')
 
     def _swap_out_tensor(self, tensor, swap_path):
         assert len(self.all_buffers) > 0
 
         aligned_numel = self._io_aligned_numel(tensor.numel())
         assert aligned_numel <= self.max_numel
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/swap_tensor/constants.py` & `deepspeed-0.9.0/deepspeed/runtime/swap_tensor/constants.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,14 +1,14 @@
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 """
-"Copyright 2020 The Microsoft DeepSpeed Team.
-Licensed under the MIT license.
+AIO
 """
-#########################################
-# AIO
-#########################################
 AIO_FORMAT = '''
 "aio": {
   "block_size": 1048576,
   "queue_depth": 8,
   "thread_count": 1,
   "single_submit": false,
   "overlap_events": true
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/swap_tensor/optimizer_utils.py` & `deepspeed-0.9.0/deepspeed/runtime/swap_tensor/optimizer_utils.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,11 +1,12 @@
-"""
-Copyright 2020 The Microsoft DeepSpeed Team.
-Licensed under the MIT license.
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
 
+# DeepSpeed Team
+"""
 Functionality of swapping tensors to/from (NVMe) storage devices.
 """
 
 import os
 import torch
 
 from deepspeed import comm as dist
@@ -13,21 +14,23 @@
 from deepspeed.runtime.swap_tensor.constants import *
 from deepspeed.runtime.swap_tensor.utils import swap_in_tensors, swap_out_tensors, \
     MIN_AIO_BYTES, AIO_ALIGNED_BYTES, get_sized_buffers
 from deepspeed.runtime.swap_tensor.utils import SwapBufferManager, SwapBufferPool
 
 
 class FlattenedTensorSwapInfo(object):
+
     def __init__(self, path, length, offset):
         self.path = path
         self.offset = offset
         self.length = length
 
 
 class OptimizerStateSwapInfo(object):
+
     def __init__(self, parameter, numel, base_folder):
         self.tensors = []
         self.param_id = id(parameter)
         self.swap_folder = base_folder
         self.swap_paths = []
         self.swapped_gradients = {}
         self.unswapped_gradients = {}
@@ -62,39 +65,30 @@
         for tensor in self.tensors:
             tensor.data = torch.Tensor()
 
     def get_or_create_gradient_paths(self, offsets, lengths):
         gradient_paths = []
         for offset, length in zip(offsets, lengths):
             if not offset in self.swapped_gradients.keys():
-                path = os.path.join(
-                    self.swap_folder,
-                    f'{self.param_id}_gradient_{offset}_{length}.tensor.swp')
-                self.swapped_gradients[offset] = FlattenedTensorSwapInfo(
-                    path,
-                    length,
-                    offset)
+                path = os.path.join(self.swap_folder, f'{self.param_id}_gradient_{offset}_{length}.tensor.swp')
+                self.swapped_gradients[offset] = FlattenedTensorSwapInfo(path, length, offset)
 
             gradient_paths.append(self.swapped_gradients[offset].path)
 
         return gradient_paths
 
     def set_swap_buffers(self, buffers):
         compute_lengths = [self.numel()] * len(self.tensors)
         compute_buffers = get_sized_buffers(buffers, compute_lengths)
         for t, buffer in zip(self.tensors, compute_buffers):
             t.data = buffer.data
 
     def get_swap_gradient_buffers(self, swap_buffer):
         assert self.numel() <= swap_buffer.numel()
-        return [
-            swap_buffer.narrow(0,
-                               grad.offset,
-                               grad.length) for grad in self.swapped_gradients.values()
-        ]
+        return [swap_buffer.narrow(0, grad.offset, grad.length) for grad in self.swapped_gradients.values()]
 
     def get_swap_gradient_paths(self):
         return [grad.path for grad in self.swapped_gradients.values()]
 
     def get_unpinned_state_tensors(self):
         return [t for t in self.tensors if not t.is_pinned()]
 
@@ -112,32 +106,23 @@
 
 
 SWAPPER_DEBUG_MODE = False
 SWAP_OUT_GRADIENT_TIMER = 'swap_out_gradient'
 
 
 class OptimizerSwapper(object):
-    def __init__(self,
-                 swap_config,
-                 aio_config,
-                 base_folder,
-                 optimizer,
-                 largest_numel,
-                 device,
-                 dtype,
-                 timers):
+
+    def __init__(self, swap_config, aio_config, base_folder, optimizer, largest_numel, device, dtype, timers):
         self.swap_config = swap_config
         self.aio_config = aio_config
 
         # NVMe swap management
         self.swap_params_info = {}
         self.swap_element_size = torch.tensor([], dtype=dtype).element_size()
-        self.swap_folder = os.path.join(base_folder,
-                                        'optimizer',
-                                        f'rank{dist.get_rank()}')
+        self.swap_folder = os.path.join(base_folder, 'optimizer', f'rank{dist.get_rank()}')
         os.makedirs(self.swap_folder, exist_ok=True)
 
         self.optimizer = optimizer
 
         # Read/Write alignment for each thread during Intra-request parallelism
         self.min_aio_bytes = max(MIN_AIO_BYTES, aio_config[AIO_BLOCK_SIZE])
         self.aligned_bytes = AIO_ALIGNED_BYTES * aio_config[AIO_THREAD_COUNT]
@@ -187,119 +172,95 @@
             self._start_timer(SWAP_OUT_GRADIENT_TIMER)
             pinned_buffers = gradient_swapper.release_buffers()
             self.swap_buffer_manager.free(pinned_buffers)
             self._stop_timer(SWAP_OUT_GRADIENT_TIMER)
             self.timer_names.add(SWAP_OUT_GRADIENT_TIMER)
             self.timer_names.update(gradient_swapper.get_timer_names())
 
-    def _swap_out_gradients(self,
-                            parameter,
-                            gradient_offsets,
-                            gradient_tensors,
-                            gradient_swapper):
+    def _swap_out_gradients(self, parameter, gradient_offsets, gradient_tensors, gradient_swapper):
         if not id(parameter) in self.swap_params_info.keys():
             return
 
         swap_info = self.swap_params_info[id(parameter)]
 
         swappable_tensors = []
         swappable_offsets = []
         swappable_lengths = []
 
-        aligned_gradients, aligned_offsets = self._adjust_for_misaligned_lengths(
-            tensors=gradient_tensors,
-            offsets=gradient_offsets
-        )
+        aligned_gradients, aligned_offsets = self._adjust_for_misaligned_lengths(tensors=gradient_tensors,
+                                                                                 offsets=gradient_offsets)
 
         self._start_timer(SWAP_OUT_GRADIENT_TIMER)
         for tensor, offset in zip(aligned_gradients, aligned_offsets):
             if not self.swappable_tensor(param=tensor):
                 swap_info.unswapped_gradients[offset] = tensor
                 continue
 
             swappable_tensors.append(tensor)
             swappable_offsets.append(offset)
             swappable_lengths.append(tensor.numel())
 
         if len(swappable_tensors) > 0:
             if not gradient_swapper.has_buffers():
-                pinned_buffers = self.swap_buffer_manager.allocate_all(
-                    num_elems=self.largest_numel,
-                    dtype=self.dtype)
+                pinned_buffers = self.swap_buffer_manager.allocate_all(num_elems=self.largest_numel, dtype=self.dtype)
 
                 gradient_swapper.add_buffers(pinned_buffers)
 
-            swappable_paths = swap_info.get_or_create_gradient_paths(
-                swappable_offsets,
-                swappable_lengths)
+            swappable_paths = swap_info.get_or_create_gradient_paths(swappable_offsets, swappable_lengths)
 
-            gradient_swapper.swap_out_tensors(tensor_list=swappable_tensors,
-                                              path_list=swappable_paths)
+            gradient_swapper.swap_out_tensors(tensor_list=swappable_tensors, path_list=swappable_paths)
 
         self._stop_timer(SWAP_OUT_GRADIENT_TIMER)
         self.timer_names.add(SWAP_OUT_GRADIENT_TIMER)
 
-    def _initialize_from_swapped_fp16_params(self,
-                                             aio_handle,
-                                             fp16_partitions_info,
-                                             fp16_num_elems,
-                                             fp16_pinned_buffers,
-                                             fp32_parameters):
+    def _initialize_from_swapped_fp16_params(self, aio_handle, fp16_partitions_info, fp16_num_elems,
+                                             fp16_pinned_buffers, fp32_parameters):
         assert len(fp32_parameters) == len(fp16_partitions_info)
         assert len(fp32_parameters) == len(fp16_num_elems)
         assert all([buffer.is_pinned() for buffer in fp16_pinned_buffers])
 
-        fp32_swap_paths = self._get_swap_paths(parameters=fp32_parameters,
-                                               num_elems=fp16_num_elems)
+        fp32_swap_paths = self._get_swap_paths(parameters=fp32_parameters, num_elems=fp16_num_elems)
 
-        fp32_pinned_buffers = self.swap_buffer_manager.allocate_all(
-            num_elems=self.largest_numel,
-            dtype=self.dtype)
+        fp32_pinned_buffers = self.swap_buffer_manager.allocate_all(num_elems=self.largest_numel, dtype=self.dtype)
 
         fp16_buffer_numel = [buf.numel() for buf in fp16_pinned_buffers]
         assert all([numel >= self.largest_numel for numel in fp16_buffer_numel]), \
         f"numel of fp16 buffers {fp16_buffer_numel} is too small for initializing fp32 params {self.largest_numel}"
 
         fp32_swap_buffers = SwapBufferPool(fp32_pinned_buffers)
         fp16_swap_buffers = SwapBufferPool(fp16_pinned_buffers)
 
         curr_index = 0
         while curr_index < len(fp32_parameters):
-            fp16_pinned_tensors = self._swap_in_fp16_params(
-                aio_handle=aio_handle,
-                fp16_num_elems=fp16_num_elems[curr_index:],
-                fp16_partitions_info=fp16_partitions_info[curr_index:],
-                fp16_swap_buffers=fp16_swap_buffers)
+            fp16_pinned_tensors = self._swap_in_fp16_params(aio_handle=aio_handle,
+                                                            fp16_num_elems=fp16_num_elems[curr_index:],
+                                                            fp16_partitions_info=fp16_partitions_info[curr_index:],
+                                                            fp16_swap_buffers=fp16_swap_buffers)
 
             if dist.get_rank() == 0 and SWAPPER_DEBUG_MODE:
                 for i, tensor in enumerate(fp16_pinned_tensors):
                     true_index = curr_index + i
                     logger.info(
                         f'swap_in_fp16_param: fp32_id = {id(fp32_parameters[true_index])} index = {true_index} orig_num_elem = {fp16_num_elems[true_index]}, swap_num_elem = {fp16_pinned_tensors[i].numel()}'
                     )
 
-            swap_out_count = self._swap_out_fp16_params(
-                aio_handle=aio_handle,
-                fp32_swap_paths=fp32_swap_paths[curr_index:],
-                fp32_swap_buffers=fp32_swap_buffers,
-                fp16_pinned_tensors=fp16_pinned_tensors)
+            swap_out_count = self._swap_out_fp16_params(aio_handle=aio_handle,
+                                                        fp32_swap_paths=fp32_swap_paths[curr_index:],
+                                                        fp32_swap_buffers=fp32_swap_buffers,
+                                                        fp16_pinned_tensors=fp16_pinned_tensors)
             assert swap_out_count == len(fp16_pinned_tensors), \
             f"{swap_out_count} does not match {len(fp16_pinned_tensors)}"
 
             fp16_swap_buffers.reset()
             fp32_swap_buffers.reset()
             curr_index += swap_out_count
 
         self.swap_buffer_manager.free(fp32_pinned_buffers)
 
-    def _swap_in_fp16_params(self,
-                             aio_handle,
-                             fp16_num_elems,
-                             fp16_partitions_info,
-                             fp16_swap_buffers):
+    def _swap_in_fp16_params(self, aio_handle, fp16_num_elems, fp16_partitions_info, fp16_swap_buffers):
         assert len(fp16_num_elems) > 0
 
         swapped_fp16_tensors = []
         swap_tensors = []
         swap_paths = []
         unswapped_srcs = []
         unswapped_dsts = []
@@ -326,52 +287,42 @@
         for src, dst in zip(unswapped_srcs, unswapped_dsts):
             dst.data.copy_(src.data)
 
         assert len(swap_tensors) == aio_handle.wait()
 
         return swapped_fp16_tensors
 
-    def _swap_out_fp16_params(self,
-                              aio_handle,
-                              fp32_swap_paths,
-                              fp32_swap_buffers,
-                              fp16_pinned_tensors):
+    def _swap_out_fp16_params(self, aio_handle, fp32_swap_paths, fp32_swap_buffers, fp16_pinned_tensors):
 
         assert len(fp16_pinned_tensors) <= len(fp32_swap_paths)
         swap_out_count = 0
         for i, fp16_tensor in enumerate(fp16_pinned_tensors):
             if not fp32_swap_buffers.has_space(fp16_tensor.numel()):
                 fp32_swap_buffers.swap_out(aio_handle)
                 fp32_swap_buffers.reset()
 
-            pinned_tensor, _ = fp32_swap_buffers.insert_tensor(
-                fp16_tensor,
-                fp32_swap_paths[i],
-                self._io_aligned_numel(fp16_tensor.numel())
-                )
+            pinned_tensor, _ = fp32_swap_buffers.insert_tensor(fp16_tensor, fp32_swap_paths[i],
+                                                               self._io_aligned_numel(fp16_tensor.numel()))
             assert pinned_tensor is not None
             swap_out_count += 1
 
         if len(fp32_swap_buffers.get_swap_tensors()) > 0:
             fp32_swap_buffers.swap_out(aio_handle)
 
         return swap_out_count
 
     def _initialize_parameters(self, parameters, src_tensors, aio_handle):
         assert len(parameters) == len(src_tensors)
 
-        swap_paths = self._get_swap_paths(parameters=parameters,
-                                          num_elems=[src.numel() for src in src_tensors])
+        swap_paths = self._get_swap_paths(parameters=parameters, num_elems=[src.numel() for src in src_tensors])
 
         SWAP_INIT_TIMER = "swap_init_write"
         self._start_timer(SWAP_INIT_TIMER)
 
-        pinned_buffers = self.swap_buffer_manager.allocate_all(
-            num_elems=self.largest_numel,
-            dtype=self.dtype)
+        pinned_buffers = self.swap_buffer_manager.allocate_all(num_elems=self.largest_numel, dtype=self.dtype)
         assert pinned_buffers is not None
 
         self._swap_out_unpinned_tensors(aio_handle=aio_handle,
                                         unpinned_tensors=src_tensors,
                                         dest_paths=swap_paths,
                                         pinned_buffers=pinned_buffers)
 
@@ -393,19 +344,15 @@
             for p, numel in zip(parameters, num_elems)
         ]
         assert len(swap_info_list) == len(num_elems)
 
         swap_paths = [info.swap_paths[0] for info in swap_info_list]
         return swap_paths
 
-    def _swap_out_unpinned_tensors(self,
-                                   aio_handle,
-                                   unpinned_tensors,
-                                   dest_paths,
-                                   pinned_buffers):
+    def _swap_out_unpinned_tensors(self, aio_handle, unpinned_tensors, dest_paths, pinned_buffers):
 
         swap_buffer_count = len(pinned_buffers)
         unpinned_tensor_count = len(unpinned_tensors)
 
         for i in range(0, unpinned_tensor_count, swap_buffer_count):
             swap_tensor_count = min((unpinned_tensor_count - i), swap_buffer_count)
 
@@ -437,16 +384,15 @@
             remainder = orig_tensor.numel() % self.numel_alignment
             if remainder == 0:
                 new_tensors.append(orig_tensor)
                 new_offsets.append(orig_offset)
                 continue
 
             # Split into two by making remainder a tensor
-            aligned_length = (orig_tensor.numel() //
-                              self.numel_alignment) * self.numel_alignment
+            aligned_length = (orig_tensor.numel() // self.numel_alignment) * self.numel_alignment
             new_tensors.append(orig_tensor.narrow(0, 0, aligned_length))
             new_offsets.append(orig_offset)
 
             # remainder tensor
             new_tensors.append(orig_tensor.narrow(0, aligned_length, remainder))
             new_offsets.append(orig_offset + aligned_length)
 
@@ -485,18 +431,17 @@
             if state_tensors:
                 swap_info.add_state_tensors(state_tensors)
 
     def _create_param_swap_info(self, parameter, numel):
         param_id = id(parameter)
         assert not param_id in self.swap_params_info
 
-        self.swap_params_info[param_id] = OptimizerStateSwapInfo(
-            parameter=parameter,
-            numel=numel,
-            base_folder=self.swap_folder)
+        self.swap_params_info[param_id] = OptimizerStateSwapInfo(parameter=parameter,
+                                                                 numel=numel,
+                                                                 base_folder=self.swap_folder)
         swap_info = self.swap_params_info[param_id]
 
         self._update_param_state_info(swap_info, parameter)
 
         return swap_info
 
     def _get_param_swap_info(self, parameter):
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/swap_tensor/partitioned_optimizer_swapper.py` & `deepspeed-0.9.0/deepspeed/runtime/swap_tensor/partitioned_optimizer_swapper.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,11 +1,12 @@
-"""
-Copyright 2020 The Microsoft DeepSpeed Team
-Licensed under the MIT license.
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
 
+# DeepSpeed Team
+"""
 Functionality of swapping optimizer tensors to/from (NVMe) storage devices.
 """
 
 import torch
 
 from deepspeed.utils.logging import logger
 from deepspeed.ops.op_builder import AsyncIOBuilder
@@ -21,85 +22,56 @@
 
 SWAP_IN_PARAM_TIMER = 'swap_in_param'
 SWAP_OUT_PARAM_TIMER = 'swap_out_param'
 SWAP_IN_GRADIENT_TIMER = 'swap_in_gradient'
 
 
 class PartitionedOptimizerSwapper(OptimizerSwapper):
-    def __init__(self,
-                 swap_config,
-                 aio_config,
-                 base_folder,
-                 optimizer,
-                 largest_numel,
-                 device,
-                 dtype,
-                 timers):
-        super(PartitionedOptimizerSwapper,
-              self).__init__(swap_config,
-                             aio_config,
-                             base_folder,
-                             optimizer,
-                             largest_numel,
-                             device,
-                             dtype,
-                             timers)
+
+    def __init__(self, swap_config, aio_config, base_folder, optimizer, largest_numel, device, dtype, timers):
+        super(PartitionedOptimizerSwapper, self).__init__(swap_config, aio_config, base_folder, optimizer,
+                                                          largest_numel, device, dtype, timers)
 
         aio_op = AsyncIOBuilder().load()
-        self.aio_handle = aio_op.aio_handle(aio_config[AIO_BLOCK_SIZE],
-                                            aio_config[AIO_QUEUE_DEPTH],
-                                            aio_config[AIO_SINGLE_SUBMIT],
-                                            aio_config[AIO_OVERLAP_EVENTS],
+        self.aio_handle = aio_op.aio_handle(aio_config[AIO_BLOCK_SIZE], aio_config[AIO_QUEUE_DEPTH],
+                                            aio_config[AIO_SINGLE_SUBMIT], aio_config[AIO_OVERLAP_EVENTS],
                                             aio_config[AIO_THREAD_COUNT])
 
         # Overlap swapping out
         self.gradient_swapper = AsyncTensorSwapper(aio_handle=self.aio_handle,
                                                    numel_alignment=self.numel_alignment,
                                                    timers=self.timers)
 
-        self.print_exclude_list += [
-            'aio_handle',
-            'gradient_swapper',
-            'print_exclude_list'
-        ]
+        self.print_exclude_list += ['aio_handle', 'gradient_swapper', 'print_exclude_list']
 
         if dist.get_rank() == 0:
-            print_object(obj=self,
-                         name='PartitionedOptimizerSwapper',
-                         exclude_list=self.print_exclude_list)
+            print_object(obj=self, name='PartitionedOptimizerSwapper', exclude_list=self.print_exclude_list)
 
     def initialize_parameters(self, parameters, src_tensors):
-        self._initialize_parameters(parameters=parameters,
-                                    src_tensors=src_tensors,
-                                    aio_handle=self.aio_handle)
-
-    def initialize_from_swapped_fp16_params(self,
-                                            fp16_partitions_info,
-                                            fp16_num_elems,
-                                            fp16_pinned_buffers,
+        self._initialize_parameters(parameters=parameters, src_tensors=src_tensors, aio_handle=self.aio_handle)
+
+    def initialize_from_swapped_fp16_params(self, fp16_partitions_info, fp16_num_elems, fp16_pinned_buffers,
                                             fp32_parameters):
-        self._initialize_from_swapped_fp16_params(
-            aio_handle=self.aio_handle,
-            fp16_partitions_info=fp16_partitions_info,
-            fp16_num_elems=fp16_num_elems,
-            fp16_pinned_buffers=fp16_pinned_buffers,
-            fp32_parameters=fp32_parameters)
+        self._initialize_from_swapped_fp16_params(aio_handle=self.aio_handle,
+                                                  fp16_partitions_info=fp16_partitions_info,
+                                                  fp16_num_elems=fp16_num_elems,
+                                                  fp16_pinned_buffers=fp16_pinned_buffers,
+                                                  fp32_parameters=fp32_parameters)
 
     def flush_gradients(self):
         self._flush_gradient_swapper(self.gradient_swapper)
 
     def swap_in_optimizer_state(self, parameter, async_parameter=None):
         swap_info = self._get_param_swap_info(parameter)
         if swap_info is None:
             return
 
         self._flush_gradient_swapper(self.gradient_swapper)
 
-        required_buffer_count = len(
-            swap_info.tensors) + (1 if swap_info.has_gradients() else 0)
+        required_buffer_count = len(swap_info.tensors) + (1 if swap_info.has_gradients() else 0)
         aligned_numel = self._io_aligned_numel(swap_info.numel())
         pinned_buffers = self.swap_buffer_manager.allocate(num_elems=aligned_numel,
                                                            count=required_buffer_count,
                                                            dtype=parameter.dtype)
         assert pinned_buffers is not None
         self.allocated_swap_buffers = pinned_buffers.copy()
 
@@ -107,45 +79,38 @@
         self._swap_in_parameter(aio_handle=self.aio_handle,
                                 parameter=parameter,
                                 dest_buffers=pinned_buffers[:required_buffer_count])
         self._stop_timer(SWAP_IN_PARAM_TIMER)
         self.timer_names.add(SWAP_IN_PARAM_TIMER)
 
         self._start_timer(SWAP_IN_GRADIENT_TIMER)
-        self._swap_in_gradients(aio_handle=self.aio_handle,
-                                parameter=parameter,
-                                dest_buffer=pinned_buffers[-1])
+        self._swap_in_gradients(aio_handle=self.aio_handle, parameter=parameter, dest_buffer=pinned_buffers[-1])
         self._stop_timer(SWAP_IN_GRADIENT_TIMER)
         self.timer_names.add(SWAP_IN_GRADIENT_TIMER)
 
     def swap_out_optimizer_state(self, parameter, async_swap=False):
         swap_info = self._get_param_swap_info(parameter=parameter)
 
         if swap_info is None:
             return
 
         self._start_timer(SWAP_OUT_PARAM_TIMER)
         pinned_tensors, pinned_paths, unpinned_tensors, unpinned_paths = self._separate_pinned_tensors(swap_info)
-        swap_bytes = sum([
-            self._io_aligned_numel(t.numel()) * t.element_size()
-            for t in swap_info.tensors
-        ])
+        swap_bytes = sum([self._io_aligned_numel(t.numel()) * t.element_size() for t in swap_info.tensors])
 
         WRITE_TIMER = 'swap_submit_write'
         self._start_timer(WRITE_TIMER)
 
         swap_out_tensors(self.aio_handle, pinned_tensors, pinned_paths)
         assert self.aio_handle.wait() == len(pinned_tensors)
         for t in pinned_tensors:
             t.data = torch.Tensor()
 
         if len(unpinned_tensors) > 0:
-            pinned_buffers = self.swap_buffer_manager.allocate_all(
-                num_elems=self.largest_numel,
-                dtype=self.dtype)
+            pinned_buffers = self.swap_buffer_manager.allocate_all(num_elems=self.largest_numel, dtype=self.dtype)
             self._swap_out_unpinned_tensors(aio_handle=self.aio_handle,
                                             unpinned_tensors=unpinned_tensors,
                                             dest_paths=unpinned_paths,
                                             pinned_buffers=pinned_buffers)
             self.allocated_swap_buffers += pinned_buffers
 
             for t in unpinned_tensors:
@@ -172,27 +137,25 @@
     def _swap_in_parameter(self, aio_handle, parameter, dest_buffers):
         swap_info = self._get_param_swap_info(parameter)
         if swap_info is None:
             return
 
         assert len(swap_info.tensors) <= len(dest_buffers)
 
-        swap_lengths = [self._io_aligned_numel(swap_info.numel())] * len(
-            swap_info.tensors)
+        swap_lengths = [self._io_aligned_numel(swap_info.numel())] * len(swap_info.tensors)
         swap_buffers = get_sized_buffers(dest_buffers, swap_lengths)
 
         READ_TIMER = 'swap_submit_read_param'
         WAIT_TIMER = 'swap_wait_read_param'
 
         self._start_timer(READ_TIMER)
         swap_in_tensors(aio_handle, swap_buffers, swap_info.swap_paths)
         self._stop_timer(READ_TIMER)
 
-        swap_bytes = sum(
-            [buffer.numel() * buffer.element_size() for buffer in swap_buffers])
+        swap_bytes = sum([buffer.numel() * buffer.element_size() for buffer in swap_buffers])
 
         self._start_timer(WAIT_TIMER)
         aio_handle.wait()
         self._stop_timer(WAIT_TIMER)
 
         compute_lengths = [swap_info.numel()] * len(swap_info.tensors)
         compute_buffers = get_sized_buffers(dest_buffers, compute_lengths)
@@ -219,19 +182,15 @@
                 unpinned_paths.append(path)
 
         return pinned_tensors, pinned_paths, unpinned_tensors, unpinned_paths
 
     def _swap_in_pinned_gradients(self, aio_handle, parameter, gradient_tensor):
         swap_info = self.swap_params_info[id(parameter)]
         param_gradients = swap_info.swapped_gradients.values()
-        swap_buffers = [
-            gradient_tensor.narrow(0,
-                                   grad.offset,
-                                   grad.length) for grad in param_gradients
-        ]
+        swap_buffers = [gradient_tensor.narrow(0, grad.offset, grad.length) for grad in param_gradients]
         swap_paths = [grad.path for grad in param_gradients]
         SWAP_READ_GRADIENTS = 'swap_submit_read_gradient'
         SWAP_WAIT_GRADIENTS = 'swap_submit_wait_gradient'
 
         self._start_timer(SWAP_READ_GRADIENTS)
         swap_in_tensors(aio_handle, swap_buffers, swap_paths)
         self._stop_timer(SWAP_READ_GRADIENTS)
@@ -252,9 +211,8 @@
 
         parameter.grad = dest_buffer.narrow(0, 0, parameter.numel())
 
         if swap_info.swapped_gradients:
             self._swap_in_pinned_gradients(aio_handle, parameter, parameter.grad)
 
         if swap_info.unswapped_gradients:
-            self._retrieve_unswapped_grad_partitions(swap_info=swap_info,
-                                                     dest_buffer=parameter.grad)
+            self._retrieve_unswapped_grad_partitions(swap_info=swap_info, dest_buffer=parameter.grad)
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/swap_tensor/partitioned_param_swapper.py` & `deepspeed-0.9.0/deepspeed/runtime/swap_tensor/partitioned_param_swapper.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,11 +1,12 @@
-"""
-Copyright 2020 The Microsoft DeepSpeed Team.
-Licensed under the MIT license.
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
 
+# DeepSpeed Team
+"""
 Functionality of swapping tensors to/from (NVMe) storage devices.
 """
 
 import os
 import shutil
 from enum import Enum
 import torch
@@ -29,14 +30,15 @@
     NOT_AVAILABLE = 2
 
     # partitioned params are being read from some non-memory device.
     INFLIGHT = 3
 
 
 class AsyncPartitionedParameterSwapper(object):
+
     def __init__(self, ds_config, model_dtype):
 
         aio_op = AsyncIOBuilder().load(verbose=False)
         self.aio_handle = aio_op.aio_handle
         self.dtype = model_dtype
 
         #set swap buffers, create aio handles
@@ -70,63 +72,54 @@
         self.partitioned_swap_buffer = None
         self.partitioned_swap_pool = None
 
         self.invalid_buffer = torch.tensor(1).half()
 
         if dist.get_rank() == 0:
             exclude_list = ['aio_read_handle', 'aio_write_handle', 'buffers']
-            print_object(obj=self,
-                         name='AsyncPartitionedParameterSwapper',
-                         exclude_list=exclude_list)
+            print_object(obj=self, name='AsyncPartitionedParameterSwapper', exclude_list=exclude_list)
 
     def available_swap_in_buffers(self):
         return len(self.available_buffer_ids)
 
     def _configure_aio(self, ds_config):
         self.swap_config = ds_config.zero_config.offload_param
         torch_dtype_string = str(self.dtype).split(".")[1]
-        self.swap_folder = os.path.join(self.swap_config.nvme_path,
-                                        'zero_stage_3',
-                                        f'{torch_dtype_string}params',
+        self.swap_folder = os.path.join(self.swap_config.nvme_path, 'zero_stage_3', f'{torch_dtype_string}params',
                                         f'rank{dist.get_rank()}')
         shutil.rmtree(self.swap_folder, ignore_errors=True)
         os.makedirs(self.swap_folder, exist_ok=True)
 
         self.swap_element_size = torch.tensor([], dtype=self.dtype).element_size()
 
         self.aio_config = ds_config.aio_config
 
         # Read/Write alignment for each thread during Intra-request parallelism
         self.min_aio_bytes = max(MIN_AIO_BYTES, self.aio_config[AIO_BLOCK_SIZE])
         self.aligned_bytes = AIO_ALIGNED_BYTES * self.aio_config[AIO_THREAD_COUNT]
         self.numel_alignment = self.aligned_bytes // self.swap_element_size
 
         self.elements_per_buffer = self.swap_config.buffer_size
-        self.aligned_elements_per_buffer = self._io_aligned_numel(
-            self.elements_per_buffer)
+        self.aligned_elements_per_buffer = self._io_aligned_numel(self.elements_per_buffer)
         self.param_buffer_count = self.swap_config.buffer_count
 
         self.available_buffer_ids = [i for i in range(self.param_buffer_count)]
         self.reserved_buffer_ids = []
         self.buffers = get_accelerator().pin_memory(
             torch.empty(int(self.aligned_elements_per_buffer * self.param_buffer_count),
                         dtype=self.dtype,
                         requires_grad=False))
 
-        self.aio_read_handle = self.aio_handle(self.aio_config[AIO_BLOCK_SIZE],
-                                               self.aio_config[AIO_QUEUE_DEPTH],
-                                               self.aio_config[AIO_SINGLE_SUBMIT],
-                                               self.aio_config[AIO_OVERLAP_EVENTS],
+        self.aio_read_handle = self.aio_handle(self.aio_config[AIO_BLOCK_SIZE], self.aio_config[AIO_QUEUE_DEPTH],
+                                               self.aio_config[AIO_SINGLE_SUBMIT], self.aio_config[AIO_OVERLAP_EVENTS],
                                                self.aio_config[AIO_THREAD_COUNT])
 
-        self.aio_write_handle = self.aio_handle(self.aio_config[AIO_BLOCK_SIZE],
-                                                self.aio_config[AIO_QUEUE_DEPTH],
+        self.aio_write_handle = self.aio_handle(self.aio_config[AIO_BLOCK_SIZE], self.aio_config[AIO_QUEUE_DEPTH],
                                                 self.aio_config[AIO_SINGLE_SUBMIT],
-                                                self.aio_config[AIO_OVERLAP_EVENTS],
-                                                self.aio_config[AIO_THREAD_COUNT])
+                                                self.aio_config[AIO_OVERLAP_EVENTS], self.aio_config[AIO_THREAD_COUNT])
 
         self.swap_out_params = []
 
     #Check if partitioned param or numel in a tensor is swappable or not
     def swappable_tensor(self, param=None, numel=None):
         if param is not None:
             assert numel is None, "Both parma and numel cannot be provided"
@@ -143,16 +136,15 @@
         paths = []
         for param in params:
             param_id = param.ds_id
             if param_id in self.id_to_path.keys():
                 param_path = self.id_to_path[param_id]
             else:
                 assert not must_exist, f"Path for param id {param_id} does not exist"
-                param_path = os.path.join(self.swap_folder,
-                                          f'{param_id}_param.tensor.swp')
+                param_path = os.path.join(self.swap_folder, f'{param_id}_param.tensor.swp')
 
                 self.id_to_path[param_id] = param_path
             paths.append(param_path)
 
         return paths
 
     def _get_swap_buffers(self, params):
@@ -173,26 +165,24 @@
     def _allocate_and_return_buffers_for_swap_in(self, params):
         compute_buffers = []
         swap_buffers = []
 
         for param in params:
             param_id = param.ds_id
             assert param_id in self.param_id_to_numel.keys(), f" Number of elements in param {param_id} is unknown"
-            assert param_id not in self.param_id_to_buffer_id.keys(), f"param {param_id} already assigned swap buffer id {self.param_id_to_buffer_id[param_id]}"
-            assert param_id not in self.param_id_to_swap_buffer.keys(), f"param {param_id} has already been assigned a swap buffer"
+            assert param_id not in self.param_id_to_buffer_id.keys(
+            ), f"param {param_id} already assigned swap buffer id {self.param_id_to_buffer_id[param_id]}"
+            assert param_id not in self.param_id_to_swap_buffer.keys(
+            ), f"param {param_id} has already been assigned a swap buffer"
 
             buffer_id = self.available_buffer_ids.pop()
-            print_rank_0(
-                f"param {param.ds_id} is assigned swap in buffer id {buffer_id}  ")
+            print_rank_0(f"param {param.ds_id} is assigned swap in buffer id {buffer_id}  ")
             self.param_id_to_buffer_id[param_id] = buffer_id
             aligned_swap_numel = self._io_aligned_numel(self.param_id_to_numel[param_id])
-            swap_buffer = self.buffers.narrow(
-                0,
-                int(buffer_id * self.aligned_elements_per_buffer),
-                aligned_swap_numel)
+            swap_buffer = self.buffers.narrow(0, int(buffer_id * self.aligned_elements_per_buffer), aligned_swap_numel)
 
             self.param_id_to_swap_buffer[param_id] = swap_buffer
             compute_buffer = swap_buffer.narrow(0, 0, self.param_id_to_numel[param_id])
             compute_buffers.append(compute_buffer)
             swap_buffers.append(swap_buffer)
 
         return compute_buffers, swap_buffers
@@ -213,17 +203,15 @@
 
         assert self.pending_reads == self.aio_read_handle.wait()
 
         self.pending_reads = 0
 
         for param, swap_in_buffer in zip(self.inflight_params, self.inflight_swap_in_buffers):
             param_id = param.ds_id
-            compute_buffer = swap_in_buffer.narrow(0,
-                                                   0,
-                                                   self.param_id_to_numel[param_id])
+            compute_buffer = swap_in_buffer.narrow(0, 0, self.param_id_to_numel[param_id])
             param.ds_tensor.data = compute_buffer.data
             param.ds_tensor.status = PartitionedParamStatus.AVAILABLE
 
         self.available_params.update([param.ds_id for param in self.inflight_params])
         self.available_numel += self.inflight_numel
 
         self.inflight_params = []
@@ -285,15 +273,16 @@
             param.ds_tensor.status = PartitionedParamStatus.INFLIGHT
 
         self.pending_reads += len(params)
 
     #assigns an in memory buffer and swaps in from nvme
     def swap_in(self, params, async_op=True, swap_in_buffers=None):
 
-        assert all([param.ds_tensor.status == PartitionedParamStatus.NOT_AVAILABLE for param in params]), "Some params are already available or in flight"
+        assert all([param.ds_tensor.status == PartitionedParamStatus.NOT_AVAILABLE
+                    for param in params]), "Some params are already available or in flight"
         swap_in_paths = self._get_swap_paths(params)
 
         if swap_in_buffers is None:
             if len(self.available_buffer_ids) < len(swap_in_paths):
                 ids = [p.ds_id for p in params]
                 print_rank_0(
                     f'Not enough swap in buffers {len(self.available_buffer_ids)} for {len(swap_in_paths)} params, ids = {ids}',
@@ -301,15 +290,17 @@
                 print_rank_0(
                     f'Num inflight: params {len(self.inflight_params)}, buffers {len(self.inflight_swap_in_buffers)}, numel = {self.inflight_numel}',
                     force=True)
                 print_rank_0(
                     f'Num available params: count = {len(self.available_params)}, ids = {self.available_params}, numel = {self.available_numel}',
                     force=True)
 
-            assert len(swap_in_paths) <= len(self.available_buffer_ids), f"Not enough buffers {len(self.available_buffer_ids)} for swapping {len(swap_in_paths)}"
+            assert len(swap_in_paths) <= len(
+                self.available_buffer_ids
+            ), f"Not enough buffers {len(self.available_buffer_ids)} for swapping {len(swap_in_paths)}"
             compute_buffers, swap_in_buffers = self._allocate_and_return_buffers_for_swap_in(params)
             inflight_numel = sum([t.numel() for t in compute_buffers])
         else:
             inflight_numel = sum([t.numel() for t in swap_in_buffers])
 
         swap_in_tensors(self.aio_read_handle, swap_in_buffers, swap_in_paths)
 
@@ -318,16 +309,15 @@
         if not async_op:
             self.synchronize_reads()
 
     # Enables swapping into buffer that is out the control of swapper. This is always synchronous
     def swap_into_buffer(self, param, dest_buffer):
         assert param.ds_tensor.status == PartitionedParamStatus.NOT_AVAILABLE, f"param {param.ds_id} is already available or inflight"
 
-        require_swap_buffer = not (dest_buffer.is_pinned()
-                                   and self._is_io_aligned(dest_buffer.numel()))
+        require_swap_buffer = not (dest_buffer.is_pinned() and self._is_io_aligned(dest_buffer.numel()))
 
         if require_swap_buffer:
             assert len(self.available_buffer_ids) > 0, f"No buffer available to swap param {param.ds_id}."
             compute_buffers, swap_in_buffers = self._allocate_and_return_buffers_for_swap_in([param])
             inflight_numel = compute_buffers[0].numel()
         else:
             swap_in_buffers = [dest_buffer]
@@ -344,37 +334,34 @@
             # Release swap buffer memory assignment. Note, this will mark the parameter not available.
             self.remove_partition_and_release_buffers([param])
 
     #assign a buffer to a param and return the buffer
     def get_buffer(self, param, numel):
         param_id = param.ds_id
 
-        assert self.available_swap_in_buffers() > 0, f"No swap buffers to allocate for fp16 param {param_id} of numel = {numel}"
+        assert self.available_swap_in_buffers(
+        ) > 0, f"No swap buffers to allocate for fp16 param {param_id} of numel = {numel}"
         assert numel < self.elements_per_buffer, f"More elements {numel} than buffer size {self.elements_per_buffer}"
 
         self.param_id_to_numel[param_id] = numel
         buffer_id = self.available_buffer_ids.pop()
         self.param_id_to_buffer_id[param_id] = buffer_id
         aligned_swap_numel = self._io_aligned_numel(self.param_id_to_numel[param_id])
-        swap_buffer = self.buffers.narrow(
-            0,
-            int(buffer_id * self.aligned_elements_per_buffer),
-            aligned_swap_numel)
+        swap_buffer = self.buffers.narrow(0, int(buffer_id * self.aligned_elements_per_buffer), aligned_swap_numel)
 
         self.param_id_to_swap_buffer[param_id] = swap_buffer
         compute_buffer = swap_buffer.narrow(0, 0, self.param_id_to_numel[param_id])
         print_rank_0(f"param {param.ds_id} is assigned swap in buffer id {buffer_id}")
         return compute_buffer
 
     def reserve_available_buffers(self):
         buffers = []
         for id in self.available_buffer_ids:
             buffers.append(
-                self.buffers.narrow(0,
-                                    int(id * self.aligned_elements_per_buffer),
+                self.buffers.narrow(0, int(id * self.aligned_elements_per_buffer),
                                     int(self.aligned_elements_per_buffer)))
             self.reserved_buffer_ids.append(id)
 
         self.available_buffer_ids = []
         return buffers
 
     def release_reserved_buffers(self):
@@ -386,37 +373,31 @@
         remainder = numel % self.numel_alignment
         return numel if remainder == 0 else (numel + self.numel_alignment - remainder)
 
     def _is_io_aligned(self, numel):
         return (numel % self.numel_alignment) == 0
 
     def reserve_partitioned_swap_space(self, partition_num_elems):
-        aligned_numel = sum(
-            [self._io_aligned_numel(numel) for numel in partition_num_elems])
+        aligned_numel = sum([self._io_aligned_numel(numel) for numel in partition_num_elems])
         self.partitioned_swap_buffer = get_accelerator().pin_memory(
-            torch.zeros(aligned_numel,
-                        device='cpu',
-                        dtype=self.dtype))
+            torch.zeros(aligned_numel, device='cpu', dtype=self.dtype))
         self.partitioned_swap_pool = SwapBufferPool([self.partitioned_swap_buffer])
 
     def swap_out_partitioned_params(self, dst_fp16_params, src_fp32_params):
         assert self.partitioned_swap_buffer is not None, f'partitioned swap buffers for fp16 params not initialized'
         assert self.partitioned_swap_pool is not None, f'partitioned swap pool for fp16 params not initialized'
         assert len(dst_fp16_params) == len(src_fp32_params), \
         f'mismatch in number of fp16 params {len(dst_fp16_params)} and fp32 params {len(src_fp32_params)}'
 
         fp16_swap_paths = self._get_swap_paths(dst_fp16_params, must_exist=True)
         self.synchronize_writes()
         self.partitioned_swap_pool.reset()
         for i, fp32_tensor in enumerate(src_fp32_params):
-            swap_tensor, _ = self.partitioned_swap_pool.insert_tensor(
-                fp32_tensor,
-                fp16_swap_paths[i],
-                self._io_aligned_numel(fp32_tensor.numel())
-            )
+            swap_tensor, _ = self.partitioned_swap_pool.insert_tensor(fp32_tensor, fp16_swap_paths[i],
+                                                                      self._io_aligned_numel(fp32_tensor.numel()))
             assert swap_tensor is not None
             dst_fp16_params[i].ds_tensor.status = PartitionedParamStatus.AVAILABLE
 
         self.partitioned_swap_pool.swap_out(self.aio_write_handle)
 
         for param in dst_fp16_params:
             param.ds_tensor.status = PartitionedParamStatus.NOT_AVAILABLE
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/swap_tensor/pipelined_optimizer_swapper.py` & `deepspeed-0.9.0/deepspeed/runtime/swap_tensor/pipelined_optimizer_swapper.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,32 +1,28 @@
-"""
-Copyright 2020 The Microsoft DeepSpeed Team.
-Licensed under the MIT license.
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
 
+# DeepSpeed Team
+"""
 Functionality of swapping optimizer tensors to/from (NVMe) storage devices.
 """
 
 from deepspeed.ops.op_builder import AsyncIOBuilder
 from deepspeed import comm as dist
 
 from deepspeed.runtime.swap_tensor.constants import *
 from deepspeed.runtime.swap_tensor.utils import swap_in_tensors, swap_out_tensors, print_object
 from deepspeed.runtime.swap_tensor.async_swapper import AsyncTensorSwapper
 from deepspeed.runtime.swap_tensor.utils import get_sized_buffer
 from deepspeed.runtime.swap_tensor.optimizer_utils import OptimizerSwapper
 
 
 class OptimizerSwapOp(object):
-    def __init__(self,
-                 aio_handle,
-                 read_op,
-                 param_info,
-                 allocated_buffers,
-                 state_buffers,
-                 num_ops):
+
+    def __init__(self, aio_handle, read_op, param_info, allocated_buffers, state_buffers, num_ops):
         self.aio_handle = aio_handle
         self.read_op = read_op
         self.param_info = param_info
         self.allocated_buffers = allocated_buffers
         self.state_buffers = state_buffers
         self.wait_required = True
         self.num_ops = num_ops
@@ -49,90 +45,55 @@
 SWAP_OUT_STATE_TIMER = 'swap_out_state'
 SWAP_OUT_GRADIENT_TIMER = 'swap_out_gradient'
 ASYNC_SWAP_IN_STATE_TIMER = "async_swap_in_state"
 ASYNC_SWAP_OUT_STATE_TIMER = 'async_swap_out_state'
 
 
 class PipelinedOptimizerSwapper(OptimizerSwapper):
-    def __init__(self,
-                 swap_config,
-                 aio_config,
-                 base_folder,
-                 optimizer,
-                 largest_numel,
-                 device,
-                 dtype,
-                 timers):
-        super(PipelinedOptimizerSwapper,
-              self).__init__(swap_config,
-                             aio_config,
-                             base_folder,
-                             optimizer,
-                             largest_numel,
-                             device,
-                             dtype,
-                             timers)
+
+    def __init__(self, swap_config, aio_config, base_folder, optimizer, largest_numel, device, dtype, timers):
+        super(PipelinedOptimizerSwapper, self).__init__(swap_config, aio_config, base_folder, optimizer, largest_numel,
+                                                        device, dtype, timers)
 
         aio_op = AsyncIOBuilder().load()
-        self.write_aio_handle = aio_op.aio_handle(aio_config[AIO_BLOCK_SIZE],
-                                                  aio_config[AIO_QUEUE_DEPTH],
-                                                  aio_config[AIO_SINGLE_SUBMIT],
-                                                  aio_config[AIO_OVERLAP_EVENTS],
+        self.write_aio_handle = aio_op.aio_handle(aio_config[AIO_BLOCK_SIZE], aio_config[AIO_QUEUE_DEPTH],
+                                                  aio_config[AIO_SINGLE_SUBMIT], aio_config[AIO_OVERLAP_EVENTS],
                                                   aio_config[AIO_THREAD_COUNT])
 
-        self.read_aio_handle = aio_op.aio_handle(aio_config[AIO_BLOCK_SIZE],
-                                                 aio_config[AIO_QUEUE_DEPTH],
-                                                 aio_config[AIO_SINGLE_SUBMIT],
-                                                 aio_config[AIO_OVERLAP_EVENTS],
+        self.read_aio_handle = aio_op.aio_handle(aio_config[AIO_BLOCK_SIZE], aio_config[AIO_QUEUE_DEPTH],
+                                                 aio_config[AIO_SINGLE_SUBMIT], aio_config[AIO_OVERLAP_EVENTS],
                                                  aio_config[AIO_THREAD_COUNT])
 
         # Overlap gradient swap out
         self.gradient_swapper = AsyncTensorSwapper(aio_handle=self.write_aio_handle,
                                                    numel_alignment=self.numel_alignment,
                                                    timers=self.timers)
 
         self.async_swap_in = swap_config.pipeline_read
         self.async_swap_out = swap_config.pipeline_write
 
-        self.swap_ops = {
-            SYNC_SWAP_IN: None,
-            ASYNC_SWAP_IN: None,
-            SYNC_SWAP_OUT: None,
-            ASYNC_SWAP_OUT: None
-        }
+        self.swap_ops = {SYNC_SWAP_IN: None, ASYNC_SWAP_IN: None, SYNC_SWAP_OUT: None, ASYNC_SWAP_OUT: None}
 
         self.print_exclude_list += [
-            'gradient_swapper',
-            'read_aio_handle',
-            'write_aio_handle',
-            'swap_ops',
-            'print_exclude_list'
+            'gradient_swapper', 'read_aio_handle', 'write_aio_handle', 'swap_ops', 'print_exclude_list'
         ]
 
         if dist.get_rank() == 0:
-            print_object(obj=self,
-                         name='PipelinedOptimizerSwapper',
-                         exclude_list=self.print_exclude_list)
+            print_object(obj=self, name='PipelinedOptimizerSwapper', exclude_list=self.print_exclude_list)
 
     def initialize_parameters(self, parameters, src_tensors):
-        self._initialize_parameters(parameters=parameters,
-                                    src_tensors=src_tensors,
-                                    aio_handle=self.write_aio_handle)
-
-    def initialize_from_swapped_fp16_params(self,
-                                            fp16_partitions_info,
-                                            fp16_num_elems,
-                                            fp16_pinned_buffers,
+        self._initialize_parameters(parameters=parameters, src_tensors=src_tensors, aio_handle=self.write_aio_handle)
+
+    def initialize_from_swapped_fp16_params(self, fp16_partitions_info, fp16_num_elems, fp16_pinned_buffers,
                                             fp32_parameters):
-        self._initialize_from_swapped_fp16_params(
-            aio_handle=self.write_aio_handle,
-            fp16_partitions_info=fp16_partitions_info,
-            fp16_num_elems=fp16_num_elems,
-            fp16_pinned_buffers=fp16_pinned_buffers,
-            fp32_parameters=fp32_parameters)
+        self._initialize_from_swapped_fp16_params(aio_handle=self.write_aio_handle,
+                                                  fp16_partitions_info=fp16_partitions_info,
+                                                  fp16_num_elems=fp16_num_elems,
+                                                  fp16_pinned_buffers=fp16_pinned_buffers,
+                                                  fp32_parameters=fp32_parameters)
 
     def flush_gradients(self):
         self._flush_gradient_swapper(self.gradient_swapper)
 
     def swap_in_optimizer_state(self, parameter, async_parameter):
         assert parameter is not None
         assert self.swap_ops[SYNC_SWAP_IN] is None
@@ -142,26 +103,24 @@
         self._start_timer(SWAP_IN_STATE_TIMER)
 
         if self.swap_ops[ASYNC_SWAP_IN]:
             assert self.swap_ops[ASYNC_SWAP_IN].is_parameter(parameter)
             self.swap_ops[SYNC_SWAP_IN] = self.swap_ops[ASYNC_SWAP_IN]
             self.swap_ops[ASYNC_SWAP_IN] = None
         else:
-            self.swap_ops[SYNC_SWAP_IN] = self._swap_in_optimizer_state(
-                aio_handle=self.read_aio_handle,
-                parameter=parameter)
+            self.swap_ops[SYNC_SWAP_IN] = self._swap_in_optimizer_state(aio_handle=self.read_aio_handle,
+                                                                        parameter=parameter)
 
         if self.swap_ops[SYNC_SWAP_IN]:
             self.swap_ops[SYNC_SWAP_IN].wait()
 
         if self.async_swap_in and async_parameter is not None:
             assert self.swap_ops[ASYNC_SWAP_IN] is None
-            self.swap_ops[ASYNC_SWAP_IN] = self._swap_in_optimizer_state(
-                aio_handle=self.read_aio_handle,
-                parameter=async_parameter)
+            self.swap_ops[ASYNC_SWAP_IN] = self._swap_in_optimizer_state(aio_handle=self.read_aio_handle,
+                                                                         parameter=async_parameter)
 
         self._stop_timer(SWAP_IN_STATE_TIMER)
         self.timer_names.add(SWAP_IN_STATE_TIMER)
 
     def swap_out_optimizer_state(self, parameter, async_swap):
         self._start_timer(SWAP_OUT_STATE_TIMER)
 
@@ -205,18 +164,17 @@
         swap_buffers = swap_in_op.state_buffers.copy()
 
         param_info = swap_in_op.param_info
         self._update_param_state_info(param_info, parameter)
         unpinned_tensors = param_info.get_unpinned_state_tensors()
 
         if len(unpinned_tensors) > 0:
-            new_alloc_buffers = self.swap_buffer_manager.allocate(
-                num_elems=self._io_aligned_numel(param_info.numel()),
-                count=len(unpinned_tensors),
-                dtype=param_info.dtype())
+            new_alloc_buffers = self.swap_buffer_manager.allocate(num_elems=self._io_aligned_numel(param_info.numel()),
+                                                                  count=len(unpinned_tensors),
+                                                                  dtype=param_info.dtype())
             assert new_alloc_buffers is not None
 
             allocated_buffers += new_alloc_buffers
             swap_buffers += new_alloc_buffers
 
             for pinned_dst, unpinned_src in zip(new_alloc_buffers, unpinned_tensors):
                 dst = get_sized_buffer(pinned_dst, unpinned_src.numel())
@@ -237,21 +195,19 @@
         return swap_out_op
 
     def _swap_in_optimizer_state(self, aio_handle, parameter):
         param_info = self._get_param_swap_info(parameter)
         if param_info is None:
             return None
 
-        required_buffer_count = len(
-            param_info.tensors) + (1 if param_info.has_gradients() else 0)
+        required_buffer_count = len(param_info.tensors) + (1 if param_info.has_gradients() else 0)
         aligned_numel = self._io_aligned_numel(param_info.numel())
-        allocated_buffers = self.swap_buffer_manager.allocate(
-            num_elems=aligned_numel,
-            count=required_buffer_count,
-            dtype=parameter.dtype)
+        allocated_buffers = self.swap_buffer_manager.allocate(num_elems=aligned_numel,
+                                                              count=required_buffer_count,
+                                                              dtype=parameter.dtype)
         assert allocated_buffers is not None, \
         f"PipelinedOptimizerSwapper ran out of swap buffers, try increasing 'buffer_count'"
 
         state_buffers = allocated_buffers[:len(param_info.tensors)]
         param_info.set_swap_buffers(state_buffers)
 
         swap_buffers = state_buffers.copy()
@@ -262,16 +218,15 @@
             if param_info.swapped_gradients:
                 swap_buffers += param_info.get_swap_gradient_buffers(parameter.grad)
                 swap_paths += param_info.get_swap_gradient_paths()
 
         swap_in_tensors(aio_handle, swap_buffers, swap_paths)
 
         if param_info.unswapped_gradients:
-            self._retrieve_unswapped_grad_partitions(swap_info=param_info,
-                                                     dest_buffer=parameter.grad)
+            self._retrieve_unswapped_grad_partitions(swap_info=param_info, dest_buffer=parameter.grad)
 
         swap_in_op = OptimizerSwapOp(aio_handle=aio_handle,
                                      param_info=param_info,
                                      read_op=True,
                                      allocated_buffers=allocated_buffers,
                                      state_buffers=state_buffers,
                                      num_ops=len(swap_buffers))
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/swap_tensor/utils.py` & `deepspeed-0.9.0/deepspeed/runtime/swap_tensor/utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,11 +1,12 @@
-"""
-Copyright 2020 The Microsoft DeepSpeed Team
-Licensed under the MIT license.
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
 
+# DeepSpeed Team
+"""
 Functionality of swapping tensors to/from (NVMe) storage devices.
 """
 
 import torch
 from deepspeed.utils.logging import logger
 from deepspeed.accelerator import get_accelerator
 
@@ -30,14 +31,15 @@
     for arg in sorted(vars(obj)):
         if not arg in exclude_list:
             dots = '.' * (29 - len(arg))
             logger.info('  {} {} {}'.format(arg, dots, getattr(obj, arg)))
 
 
 class SwapBuffer(object):
+
     def __init__(self, buffer):
         self.buffer = buffer
         self.reset()
 
     def reset(self):
         self.offset = 0
         self.swap_tensors = {}
@@ -88,14 +90,15 @@
         return self.compute_tensors.get(offset, None)
 
     def get_swap_path(self, offset):
         return self.swap_paths(offset, None)
 
 
 class SwapBufferPool(object):
+
     def __init__(self, buffers):
         assert all([buf.is_pinned() for buf in buffers])
         self.buffers = [SwapBuffer(buf) for buf in buffers]
         self.current_index = 0
 
     def reset(self):
         self.current_index = 0
@@ -171,28 +174,25 @@
         return self.buffers[self.current_index]
 
     def _get_used_buffers(self):
         return self.buffers[:self.current_index + 1]
 
 
 class SwapBufferManager(object):
+
     def __init__(self, num_elems, count, dtype):
         self.num_elems = num_elems
         self.count = count
         self.dtype = dtype
         self.all_buffers = [
-            get_accelerator().pin_memory(
-                torch.zeros(num_elems,
-                            device='cpu',
-                            dtype=dtype)) for _ in range(count)
+            get_accelerator().pin_memory(torch.zeros(num_elems, device='cpu', dtype=dtype)) for _ in range(count)
         ]
         self.free_buffer_index = [i for i in range(count)]
         self.used_buffer_index = {}
-        self.gigabytes = (self.all_buffers[0].element_size() * num_elems * count) / (1024
-                                                                                     **3)
+        self.gigabytes = (self.all_buffers[0].element_size() * num_elems * count) / (1024**3)
 
         if dist.get_rank() == 0:
             exclude_list = ['all_buffers']
             print_object(obj=self, name='SwapBufferManager', exclude_list=exclude_list)
 
     def allocate(self, num_elems, count, dtype):
         assert dtype == self.dtype
@@ -207,17 +207,15 @@
         for i in used_indices:
             tmp_buffer = self.all_buffers[i].narrow(0, 0, num_elems)
             buffers.append(tmp_buffer)
             self.used_buffer_index[id(tmp_buffer)] = i
         return buffers
 
     def allocate_all(self, num_elems, dtype):
-        return self.allocate(num_elems=num_elems,
-                             count=len(self.free_buffer_index),
-                             dtype=dtype)
+        return self.allocate(num_elems=num_elems, count=len(self.free_buffer_index), dtype=dtype)
 
     def free(self, buffers):
         buffer_ids = []
         for buf in buffers:
             buffer_ids.append(id(buf))
 
         assert all([b_id in self.used_buffer_index for b_id in buffer_ids])
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/utils.py` & `deepspeed-0.9.0/deepspeed/runtime/utils.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,46 +1,52 @@
-'''
-Copyright 2019 The Microsoft DeepSpeed Team
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
 
+# DeepSpeed Team
+"""
 Copyright NVIDIA/Megatron
 
 Helper functions and classes from multiple sources.
-'''
+"""
 
 from collections.abc import Iterable
 from deepspeed.moe.utils import is_moe_param
 import os
 import psutil
 import gc
 from math import sqrt
 from math import floor
 from bisect import bisect_left
 
 import torch
 from deepspeed import comm as dist
 
 try:
-    from torch._six import inf as inf
+    from torch._six import inf
 except ModuleNotFoundError:
-    from torch import inf as inf
+    from torch import inf
 
 from deepspeed.utils import groups, logger
 from deepspeed.runtime.constants import PIPE_REPLICATED
 from numpy import prod
 from deepspeed.accelerator import get_accelerator
 
+from deepspeed.module_inject.policy import transpose
+from torch.nn import functional as F
+
 torch_memory_reserved = get_accelerator().memory_reserved
 torch_max_memory_reserved = get_accelerator().max_memory_reserved
 
 
 class DummyOptim():
     """
     Dummy optimizer presents model parameters as a param group, this is
     primarily used to allow ZeRO-3 without an optimizer
     """
+
     def __init__(self, params):
         self.param_groups = []
         self.param_groups.append({'params': params})
 
 
 def noop_decorator(func):
     return func
@@ -165,19 +171,16 @@
         return {k: move_to_device(v, device, criterion_func) for k, v in item.items()}
     else:
         return item
 
 
 class CheckOverflow(object):
     '''Checks for overflow in gradient across parallel process'''
-    def __init__(self,
-                 param_groups=None,
-                 mpu=None,
-                 zero_reduce_scatter=False,
-                 deepspeed=None):
+
+    def __init__(self, param_groups=None, mpu=None, zero_reduce_scatter=False, deepspeed=None):
         self.mpu = mpu
         self.params = [] if param_groups else None
         self.zero_reduce_scatter = zero_reduce_scatter
         self.deepspeed = deepspeed
         self.has_moe_params = False
         if param_groups:
             for group in param_groups:
@@ -192,21 +195,17 @@
         overflow_gpu = get_accelerator().FloatTensor([overflow])
         if self.has_moe_params:
             # In this case, we need to do an all_reduce across
             # the expert_parallel_group, so that if there was
             # an overflow due to expert weights, we detect it
 
             # Only need to check groups.get_largest_expert_parallel_group()
-            dist.all_reduce(overflow_gpu,
-                            op=dist.ReduceOp.MAX,
-                            group=groups._get_max_expert_parallel_group())
+            dist.all_reduce(overflow_gpu, op=dist.ReduceOp.MAX, group=groups._get_max_expert_parallel_group())
         if self.mpu is not None:
-            dist.all_reduce(overflow_gpu,
-                            op=dist.ReduceOp.MAX,
-                            group=self.mpu.get_model_parallel_group())
+            dist.all_reduce(overflow_gpu, op=dist.ReduceOp.MAX, group=self.mpu.get_model_parallel_group())
         elif reduce_overflow:
             dist.all_reduce(overflow_gpu, op=dist.ReduceOp.MAX)
             dist.barrier()
         overflow = overflow_gpu[0].item()
         return bool(overflow)
 
     def check(self, param_groups=None):
@@ -243,39 +242,26 @@
         overflow_gpu = get_accelerator().ByteTensor([overflow])
         # deepspeeed.comm.all_reduce(overflow_gpu,
         #                             op=deepspeed.comm.ReduceOp.MAX,
         #                             group=mpu.get_model_parallel_group())
         if has_moe_params:
             # All reduce this across expert_parallel_group, so that if an expert
             # overflows, we detect it here
-            dist.all_reduce(overflow_gpu,
-                            op=dist.ReduceOp.MAX,
-                            group=groups._get_max_expert_parallel_group())
+            dist.all_reduce(overflow_gpu, op=dist.ReduceOp.MAX, group=groups._get_max_expert_parallel_group())
         if self.zero_reduce_scatter:
-            dist.all_reduce(overflow_gpu,
-                            op=dist.ReduceOp.MAX,
-                            group=dist.get_world_group())
+            dist.all_reduce(overflow_gpu, op=dist.ReduceOp.MAX, group=dist.get_world_group())
         elif self.mpu is not None:
             if self.deepspeed is not None:
-                using_pipeline = hasattr(self.deepspeed,
-                                         'pipeline_enable_backward_allreduce')
-                if (using_pipeline
-                        and self.deepspeed.pipeline_enable_backward_allreduce is False
-                    ) or (not using_pipeline
-                          and self.deepspeed.enable_backward_allreduce is False):
-                    dist.all_reduce(overflow_gpu,
-                                    op=dist.ReduceOp.MAX,
-                                    group=self.mpu.get_data_parallel_group())
-            dist.all_reduce(overflow_gpu,
-                            op=dist.ReduceOp.MAX,
-                            group=self.mpu.get_model_parallel_group())
+                using_pipeline = hasattr(self.deepspeed, 'pipeline_enable_backward_allreduce')
+                if (using_pipeline and self.deepspeed.pipeline_enable_backward_allreduce is False) or (
+                        not using_pipeline and self.deepspeed.enable_backward_allreduce is False):
+                    dist.all_reduce(overflow_gpu, op=dist.ReduceOp.MAX, group=self.mpu.get_data_parallel_group())
+            dist.all_reduce(overflow_gpu, op=dist.ReduceOp.MAX, group=self.mpu.get_model_parallel_group())
         elif self.deepspeed is not None and self.deepspeed.enable_backward_allreduce is False:
-            dist.all_reduce(overflow_gpu,
-                            op=dist.ReduceOp.MAX,
-                            group=dist.get_world_group())
+            dist.all_reduce(overflow_gpu, op=dist.ReduceOp.MAX, group=dist.get_world_group())
 
         overflow = overflow_gpu[0].item()
         return bool(overflow)
 
     # `x` is a torch.Tensor
     @staticmethod
     def _has_inf_or_nan(x, i):
@@ -304,25 +290,24 @@
     rank = dist.get_rank()
     if rank == 0:
         t_i = -1
         for v_i, v in enumerate(x.data.contiguous().view(-1)):
             if not math.isfinite(float(v)):
                 t_i = v_i
                 break
-        logger.info(
-            f"rank {rank} detected overflow {cpu_sum} in tensor {i}:{t_i} shape {x.shape}"
-        )
+        logger.info(f"rank {rank} detected overflow {cpu_sum} in tensor {i}:{t_i} shape {x.shape}")
 
 
 def get_global_norm(norm_list):
     """ Compute total from a list of norms
     """
     total_norm = 0.0
     for norm in norm_list:
         total_norm += norm**2.0
+    # logger.info(f'norm_list = {norm_list} global = {sqrt(total_norm)}')
     return sqrt(total_norm)
 
 
 def clip_grad_norm_(parameters, max_norm, norm_type=2, mpu=None):
     """Clips gradient norm of an iterable of parameters.
 
     This has been adapted from Nvidia megatron. We add norm averaging
@@ -349,36 +334,31 @@
     max_norm = float(max_norm)
     norm_type = float(norm_type)
     if norm_type == inf:
         total_norm = max(p.grad.data.abs().max() for p in parameters)
         total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
         # Take max across all GPUs.
         if mpu is not None:
-            dist.all_reduce(total_norm_cuda,
-                            op=dist.ReduceOp.MAX,
-                            group=mpu.get_model_parallel_group())
+            dist.all_reduce(total_norm_cuda, op=dist.ReduceOp.MAX, group=mpu.get_model_parallel_group())
         total_norm = total_norm_cuda[0].item()
     else:
         total_norm = 0
         for p in parameters:
             if mpu is not None:
-                if (mpu.get_model_parallel_rank()
-                        == 0) or is_model_parallel_parameter(p):
+                if (mpu.get_model_parallel_rank() == 0) or is_model_parallel_parameter(p):
                     param_norm = p.grad.data.norm(norm_type)
                     total_norm += param_norm.item()**norm_type
             else:
                 param_norm = p.grad.data.float().norm(norm_type)
                 total_norm += param_norm.item()**norm_type
 
         # Sum across all model parallel GPUs.
         total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
         if mpu is not None:
-            dist.all_reduce(total_norm_cuda,
-                            op=dist.ReduceOp.SUM,
-                            group=mpu.get_model_parallel_group())
+            dist.all_reduce(total_norm_cuda, op=dist.ReduceOp.SUM, group=mpu.get_model_parallel_group())
         total_norm = total_norm_cuda[0].item()**(1. / norm_type)
 
     # Need to average total_norm across different GPUs due to the presence of moe params
     pg = groups._get_data_parallel_group()
     scaled_norm = total_norm * 1.0 / float(dist.get_world_size(group=pg))
 
     scaled_norm_tensor = get_accelerator().FloatTensor([float(scaled_norm)])
@@ -415,17 +395,15 @@
 
     norm_type = float(norm_type)
     if norm_type == inf:
         total_norm = max(p.grad.data.abs().max() for p in parameters)
         total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
         # Take max across all GPUs.
         if mpu is not None:
-            dist.all_reduce(total_norm_cuda,
-                            op=dist.ReduceOp.MAX,
-                            group=mpu.get_model_parallel_group())
+            dist.all_reduce(total_norm_cuda, op=dist.ReduceOp.MAX, group=mpu.get_model_parallel_group())
         total_norm = total_norm_cuda[0].item()
     else:
         total_norm = 0.
         tensor_mp_rank = bwc_tensor_model_parallel_rank(mpu=mpu)
         for p in parameters:
             # Pipeline parallelism may replicate parameters. Avoid multi-counting.
             if hasattr(p, PIPE_REPLICATED) and p.ds_pipe_replicated:
@@ -438,21 +416,18 @@
 
             param_norm = p.grad.data.float().norm(norm_type)
             total_norm += param_norm.item()**norm_type
 
         # Sum across all model parallel GPUs.
         total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
         if mpu is not None:
-            dist.all_reduce(total_norm_cuda,
-                            op=dist.ReduceOp.SUM,
-                            group=mpu.get_model_parallel_group())
+            dist.all_reduce(total_norm_cuda, op=dist.ReduceOp.SUM, group=mpu.get_model_parallel_group())
         total_norm = total_norm_cuda[0].item()**(1. / norm_type)
 
-    if total_norm == float(
-            'inf') or total_norm == -float('inf') or total_norm != total_norm:
+    if total_norm == float('inf') or total_norm == -float('inf') or total_norm != total_norm:
         total_norm = -1
 
     return total_norm
 
 
 def get_grad_zeros(parameters, mpu=None):
     """Compute the number of grads with zero values.
@@ -484,17 +459,15 @@
 
         count_zeros = p.grad.numel() - torch.count_nonzero(p.grad)
         total_zeros += count_zeros.item()
 
     # Sum across all model parallel GPUs.
     total_zeros_cuda = get_accelerator().FloatTensor([float(total_zeros)])
     if mpu is not None:
-        dist.all_reduce(total_zeros_cuda,
-                        op=dist.ReduceOp.SUM,
-                        group=mpu.get_model_parallel_group())
+        dist.all_reduce(total_zeros_cuda, op=dist.ReduceOp.SUM, group=mpu.get_model_parallel_group())
     total_zeros = total_zeros_cuda[0].item()
 
     return total_zeros
 
 
 def get_weight_norm(parameters, norm_type=2, mpu=None):
     """Get norm of an iterable of parameters.
@@ -518,17 +491,15 @@
 
     norm_type = float(norm_type)
     if norm_type == inf:
         total_norm = max(p.data.abs().max() for p in parameters)
         total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
         # Take max across all GPUs.
         if mpu is not None:
-            dist.all_reduce(total_norm_cuda,
-                            op=dist.ReduceOp.MAX,
-                            group=mpu.get_model_parallel_group())
+            dist.all_reduce(total_norm_cuda, op=dist.ReduceOp.MAX, group=mpu.get_model_parallel_group())
         total_norm = total_norm_cuda[0].item()
     else:
         total_norm = 0.
         tensor_mp_rank = bwc_tensor_model_parallel_rank(mpu=mpu)
         for p in parameters:
             # Pipeline parallelism may replicate parameters. Avoid multi-counting.
             if hasattr(p, PIPE_REPLICATED) and p.ds_pipe_replicated:
@@ -541,21 +512,18 @@
 
             param_norm = p.data.float().norm(norm_type)
             total_norm += param_norm**norm_type
 
         # Sum across all model parallel GPUs.
         total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
         if mpu is not None:
-            dist.all_reduce(total_norm_cuda,
-                            op=dist.ReduceOp.SUM,
-                            group=mpu.get_model_parallel_group())
+            dist.all_reduce(total_norm_cuda, op=dist.ReduceOp.SUM, group=mpu.get_model_parallel_group())
         total_norm = total_norm_cuda[0].item()**(1. / norm_type)
 
-    if total_norm == float(
-            'inf') or total_norm == -float('inf') or total_norm != total_norm:
+    if total_norm == float('inf') or total_norm == -float('inf') or total_norm != total_norm:
         total_norm = -1
 
     return total_norm
 
 
 def prefix_sum_inc(weights):
     """ Compute an inclusive prefix sum.
@@ -599,19 +567,15 @@
     step = chunksize
     for p in range(1, num_parts):
         # Jump to the next bucket
         while (step < num_items) and (weights[step] < bsum):
             step += chunksize
 
         # Find the end index of partition p
-        parts[p] = bisect_left(weights,
-                               bsum,
-                               lo=step - chunksize,
-                               hi=min(step,
-                                      num_items))
+        parts[p] = bisect_left(weights, bsum, lo=step - chunksize, hi=min(step, num_items))
         # Nothing more to partition, return early
         if parts[p] == num_items:
             # See if the current partition is overweight.
             part_size = weights[-1] - weights[parts[p - 1]]
             return parts, part_size < bottleneck
 
         # Next partition target
@@ -651,14 +615,15 @@
     parts, success = _lprobe(weights_, num_parts, bottleneck)
     assert success
 
     return parts
 
 
 class PartitionedTensor:
+
     def __init__(self, tensor, group, partition_meta=None):
         super().__init__()
 
         self.group = group
         self.num_parts = dist.get_world_size(group=self.group)
         self.rank = dist.get_rank(group=self.group)
 
@@ -692,44 +657,37 @@
 
         return part_obj
 
     def _partition_tensor(self, tensor):
         partition = partition_uniform(num_items=tensor.numel(), num_parts=self.num_parts)
         start = partition[self.rank]
         length = partition[self.rank + 1] - start
-        tensor_part = tensor.detach().contiguous().view(-1).narrow(
-            0,
-            start=start,
-            length=length).clone()
+        tensor_part = tensor.detach().contiguous().view(-1).narrow(0, start=start, length=length).clone()
 
         return tensor_part, partition
 
     def full(self, device=None):
         if device is None:
             device = self.orig_device
 
         # Allocate the full tensor as a flat buffer.
         full_numel = prod(self.full_size())
-        flat_tensor = torch.zeros([full_numel],
-                                  dtype=self.local_data.dtype,
-                                  device=device)
+        flat_tensor = torch.zeros([full_numel], dtype=self.local_data.dtype, device=device)
 
         # Prepare all-gather buffer
         partition_tensors = []
         for part_id in range(self.num_parts):
             part_size = self.partition[part_id + 1] - self.partition[part_id]
             buf = flat_tensor.narrow(0, start=self.partition[part_id], length=part_size)
             if part_id == self.rank:
                 buf.copy_(self.local_data)
             partition_tensors.append(buf)
 
         # Collect the full tensor
-        dist.all_gather(partition_tensors,
-                        partition_tensors[self.rank],
-                        group=self.group)
+        dist.all_gather(partition_tensors, partition_tensors[self.rank], group=self.group)
 
         for i in range(len(partition_tensors)):
             partition_tensors[i].data = torch.zeros(1)
             partition_tensors[i] = None
 
         return flat_tensor.view(self.full_size()).clone().detach()
 
@@ -794,53 +752,49 @@
     new_cached /= 1024**3
     delta_alloced /= 1024**3
     delta_cached /= 1024**3
     max_alloced /= 1024**3
     max_cached /= 1024**3
 
     print(
-        f'RANK={rank} MEMSTATS',
-        msg,
-        f'device={get_accelerator().current_device_name()} '
+        f'RANK={rank} MEMSTATS', msg, f'device={get_accelerator().current_device_name()} '
         f'current alloc={new_alloced:0.4f}GB (delta={delta_alloced:0.4f}GB max={max_alloced:0.4f}GB) '
-        f'current cache={new_cached:0.4f}GB (delta={delta_cached:0.4f}GB max={max_cached:0.4f}GB)'
-    )
+        f'current cache={new_cached:0.4f}GB (delta={delta_cached:0.4f}GB max={max_cached:0.4f}GB)')
 
 
 def get_ma_status():
     if dist.is_initialized() and not dist.get_rank() == 0:
         return 0
     return get_accelerator().memory_allocated()
 
 
 def empty_cache():
     get_accelerator().empty_cache()
+    get_accelerator().reset_peak_memory_stats()
 
 
 def see_memory_usage(message, force=False):
     if not force:
         return
     if dist.is_initialized() and not dist.get_rank() == 0:
         return
 
     # python doesn't do real-time garbage collection so do it explicitly to get the correct RAM reports
     gc.collect()
 
     # Print message except when distributed but not rank 0
     logger.info(message)
-    logger.info(
-        f"MA {round(get_accelerator().memory_allocated() / (1024 * 1024 * 1024),2 )} GB \
+    logger.info(f"MA {round(get_accelerator().memory_allocated() / (1024 * 1024 * 1024),2 )} GB \
         Max_MA {round(get_accelerator().max_memory_allocated() / (1024 * 1024 * 1024),2)} GB \
         CA {round(torch_memory_reserved() / (1024 * 1024 * 1024),2)} GB \
         Max_CA {round(torch_max_memory_reserved() / (1024 * 1024 * 1024))} GB ")
 
     vm_stats = psutil.virtual_memory()
     used_GB = round(((vm_stats.total - vm_stats.available) / (1024**3)), 2)
-    logger.info(
-        f'CPU Virtual Memory:  used = {used_GB} GB, percent = {vm_stats.percent}%')
+    logger.info(f'CPU Virtual Memory:  used = {used_GB} GB, percent = {vm_stats.percent}%')
 
     # get the peak memory to report correct data, so reset the counter for the next call
     get_accelerator().reset_peak_memory_stats()
 
 
 def call_to_str(base, *args, **kwargs):
     """Construct a string representation of a call.
@@ -911,40 +865,30 @@
     assert all([torch.is_tensor(t) for t in input_tensors]), f'expected list of only tensors'
 
     norm_type = float(norm_type)
     if norm_type == inf:
         total_norm = max(t.data.abs().max() for t in input_tensors)
         total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
         if mpu is not None:
-            dist.all_reduce(total_norm_cuda,
-                            op=dist.ReduceOp.MAX,
-                            group=mpu.get_model_parallel_group())
+            dist.all_reduce(total_norm_cuda, op=dist.ReduceOp.MAX, group=mpu.get_model_parallel_group())
             total_norm = total_norm_cuda[0].item()
     else:
-        total_norm = sum(
-            [t.data.float().norm(norm_type).item()**norm_type for t in input_tensors])
+        total_norm = sum([t.data.float().norm(norm_type).item()**norm_type for t in input_tensors])
         total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
         if mpu is not None:
-            dist.all_reduce(total_norm_cuda,
-                            op=dist.ReduceOp.SUM,
-                            group=mpu.get_model_parallel_group())
+            dist.all_reduce(total_norm_cuda, op=dist.ReduceOp.SUM, group=mpu.get_model_parallel_group())
         total_norm = total_norm_cuda[0].item()**(1. / norm_type)
 
-    if total_norm == float(
-            'inf') or total_norm == -float('inf') or total_norm != total_norm:
+    if total_norm == float('inf') or total_norm == -float('inf') or total_norm != total_norm:
         total_norm = -1
 
     return total_norm
 
 
-def clip_tensors_by_global_norm(input_tensors,
-                                max_norm=1.0,
-                                global_norm=None,
-                                mpu=None,
-                                eps=1e-6):
+def clip_tensors_by_global_norm(input_tensors, max_norm=1.0, global_norm=None, mpu=None, eps=1e-6):
     """Clip list of tensors by global norm.
     Args:
         input_tensors: List of tensors to be clipped
         global_norm (float, optional): Precomputed norm. Defaults to None.
         mpu (optional): model parallelism unit. Defaults to None.
         eps (float, optional): epsilon value added to grad norm. Defaults to 1e-6
     Returns:
@@ -964,56 +908,68 @@
 
 def align_dense_tensors(tensor_list, alignment):
     num_elements = sum(t.numel() for t in tensor_list)
     remaining = num_elements % alignment
 
     if remaining:
         elements_to_add = alignment - remaining
-        pad_tensor = torch.zeros(elements_to_add,
-                                 device=tensor_list[0].device,
-                                 dtype=tensor_list[0].dtype)
+        pad_tensor = torch.zeros(elements_to_add, device=tensor_list[0].device, dtype=tensor_list[0].dtype)
         padded_tensor_list = tensor_list + [pad_tensor]
     else:
         padded_tensor_list = tensor_list
 
     return padded_tensor_list
 
 
-def all_gather_dp_groups(partitioned_param_groups,
-                         dp_process_group,
-                         start_alignment_factor,
-                         allgather_bucket_size):
+def all_gather_dp_groups(partitioned_param_groups, dp_process_group, start_alignment_factor, allgather_bucket_size):
     for group_id, partitioned_params in enumerate(partitioned_param_groups):
         # Sequential AllGather Best of both worlds
         partition_id = dist.get_rank(group=dp_process_group[group_id])
         dp_world_size = dist.get_world_size(group=dp_process_group[group_id])
 
-        num_shards = max(
-            1,
-            partitioned_params[partition_id].numel() * dp_world_size //
-            allgather_bucket_size)
+        num_shards = max(1, partitioned_params[partition_id].numel() * dp_world_size // allgather_bucket_size)
 
         shard_size = partitioned_params[partition_id].numel() // num_shards
 
         # Enforce nccl/rccl alignment of start location of each shard
         shard_size = shard_size - (shard_size % start_alignment_factor)
 
         num_elements = shard_size
 
         assert shard_size * num_shards <= partitioned_params[partition_id].numel()
 
         for shard_id in range(num_shards):
 
             if shard_id == (num_shards - 1):
-                num_elements = partitioned_params[partition_id].numel(
-                ) - shard_id * shard_size
+                num_elements = partitioned_params[partition_id].numel() - shard_id * shard_size
 
             shard_list = []
             for dp_id in range(dp_world_size):
-                curr_shard = partitioned_params[dp_id].narrow(0,
-                                                              shard_id * shard_size,
-                                                              num_elements).detach()
+                curr_shard = partitioned_params[dp_id].narrow(0, shard_id * shard_size, num_elements).detach()
                 shard_list.append(curr_shard)
 
-            dist.all_gather(shard_list,
-                            shard_list[partition_id],
-                            dp_process_group[group_id])
+            dist.all_gather(shard_list, shard_list[partition_id], dp_process_group[group_id])
+
+
+class TLinear(torch.nn.Linear):
+
+    def __init__(self, orig_layer, name=""):
+        self.name = name
+        super().__init__(orig_layer.weight.shape[1], orig_layer.weight.shape[0], bias=(orig_layer.bias is not None))
+        self.weight.data = transpose(orig_layer.weight.data)
+        self.bias = orig_layer.bias
+        self._fwd_func = self._fwd_bias_add if self.bias is not None else self._fwd
+
+    def _fwd(self, input):
+        return F.linear(input, self.weight)
+
+    def _fwd_bias_add(self, input):
+        return F.linear(input, self.weight, bias=self.bias)
+
+    def forward(self, input):
+        return self._fwd_func(input)
+
+
+def get_inactive_params(param_list):
+    from deepspeed.runtime.zero.partition_parameters import ZeroParamStatus
+    return [param for param in param_list if (hasattr(param, 'ds_id') and \
+                            param.ds_status == ZeroParamStatus.NOT_AVAILABLE)]
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/weight_quantizer.py` & `deepspeed-0.9.0/deepspeed/runtime/weight_quantizer.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,31 +1,34 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import torch
 from ..module_inject.replace_policy import HFBertLayerPolicy, replace_policies
 from deepspeed.accelerator import get_accelerator
 
 
 class WeightQuantization(object):
+
     def __init__(self, mlp_extra_grouping=True, mp_size=1):
         self.dense_scales = []
         self.qkv_scales = []
         self.mlp4hh_scales = []
         self.mlph4h_scales = []
         self.mlp_extra_grouping = mlp_extra_grouping
         self.mp_size = mp_size
 
     def quantize_data(self, data, quantize_bits, groups, key=None):
         data_groups = torch.split(data.float().view(-1), data.numel() // groups)
         max_d = [max(g.max(), g.min().abs()) for g in data_groups]
         data_scale = [float(1 << quantize_bits) / (2 * mx + 1e-5) for mx in max_d]
         data_int = [(g * s) for g, s in zip(data_groups, data_scale)]
         data_int = [
-            di.round().clamp(-(1 << (quantize_bits - 1)),
-                             (((1 << (quantize_bits - 1)) - 1))) for di in data_int
+            di.round().clamp(-(1 << (quantize_bits - 1)), (((1 << (quantize_bits - 1)) - 1))) for di in data_int
         ]
         data_int = torch.cat(data_int).reshape(data.shape)
         data_int = data_int.to(torch.int8)
         data_scale = torch.cat([s.unsqueeze(0).unsqueeze(0) for s in data_scale])
         return data_int, data_scale
 
     def is_mlp(self, data, merge_count=1):
@@ -33,79 +36,63 @@
                 (self.mp_size *data.shape[1] * merge_count) / data.shape[0] == 4)
 
     def is_qkv(self, data):
         return ((self.mp_size * data.shape[0]) / data.shape[1] == 3 or \
                 (self.mp_size * data.shape[1]) / data.shape[0] == 3)
 
     def Quantize(self, value_list, quantize_bits, groups, key, merge_dim=0):
-        if self.mlp_extra_grouping and self.is_mlp(value_list[0],
-                                                   merge_count=len(value_list)):
+        if self.mlp_extra_grouping and self.is_mlp(value_list[0], merge_count=len(value_list)):
             groups *= 2
         q_scale = []
         index = 0
         for data in value_list:
             data_int, data_scale = self.quantize_data(data, quantize_bits, groups, key)
             q_scale.append(data_scale)
             value_list[index] = data_int
             index += 1
-        q_scale = (
-            1 /
-            torch.cat(q_scale,
-                      dim=merge_dim).to(
-                          get_accelerator().current_device_name()).view(-1).unsqueeze(0))
+        q_scale = (1 /
+                   torch.cat(q_scale, dim=merge_dim).to(get_accelerator().current_device_name()).view(-1).unsqueeze(0))
         if "mlp.dense_4h_to_h.weight" in key:
             self.mlp4hh_scales.append(q_scale)
         elif "mlp.dense_h_to_4h.weight" in key:
             self.mlph4h_scales.append(q_scale)
         elif "attention.query_key_value.weight" in key:
             self.qkv_scales.append(q_scale)
         else:
             self.dense_scales.append(q_scale)
         return value_list
 
     def merge_layer_scales(self, layer_scales):
         max_dim = max([s.shape[-1] for s in layer_scales])
         layer_scales = [
-            torch.cat((s,
-                       torch.zeros((1,
-                                    max_dim - s.shape[-1]),
-                                   device=get_accelerator().current_device_name())),
+            torch.cat((s, torch.zeros((1, max_dim - s.shape[-1]), device=get_accelerator().current_device_name())),
                       dim=-1) if s.shape[-1] < max_dim else s for s in layer_scales
         ]
         return torch.cat(layer_scales).unsqueeze(0)
 
     def merge_scales(self):
         all_scales = []
         for dense_scale, qkv_scale, m4hh_scale, mh4h_scale in \
             zip(self.dense_scales, self.qkv_scales, self.mlp4hh_scales, self.mlph4h_scales):
-            all_scales.append(
-                self.merge_layer_scales([qkv_scale,
-                                         dense_scale,
-                                         mh4h_scale,
-                                         m4hh_scale]))
+            all_scales.append(self.merge_layer_scales([qkv_scale, dense_scale, mh4h_scale, m4hh_scale]))
         return torch.cat(all_scales)
 
     def merge_scales_split(self, split_count):
         all_scales = [[] for _ in range(split_count)]
         for dense_scale, qkv_scale, m4hh_scale, mh4h_scale in \
             zip(self.dense_scales, self.qkv_scales, self.mlp4hh_scales, self.mlph4h_scales):
             dense_scale = torch.split(dense_scale, dense_scale.numel() // split_count)
             qkv_scale = torch.split(qkv_scale, qkv_scale.numel() // split_count)
             m4hh_scale = torch.split(m4hh_scale, m4hh_scale.numel() // split_count)
             mh4h_scale = torch.split(mh4h_scale, mh4h_scale.numel() // split_count)
             for s in range(split_count):
                 all_scales[s].append(
                     torch.cat([
-                        torch.cat((qkv_scale[s],
-                                   torch.zeros_like(qkv_scale[s])),
-                                  dim=1),
-                        torch.cat((dense_scale[s],
-                                   torch.zeros_like(dense_scale[s])),
-                                  dim=1),
-                        mh4h_scale[s],
+                        torch.cat((qkv_scale[s], torch.zeros_like(qkv_scale[s])), dim=1),
+                        torch.cat((dense_scale[s], torch.zeros_like(dense_scale[s])), dim=1), mh4h_scale[s],
                         m4hh_scale[s]
                     ]).unsqueeze(0))
             for scales_a in all_scales:
                 torch.cat(scales_a)
         return all_scales
 
     def sd_quantize_megatron(self, sd, quantize_bits, groups):
@@ -135,16 +122,15 @@
                 if self.mlp_extra_grouping and self.is_mlp(keys[key]):
                     data_quantized, data_scale = self.quantize_data(keys[key], quantize_bits, groups * 2)
                 elif policy_cls is HFBertLayerPolicy and self.is_qkv(keys[key]):
                     data_quantized, data_scale = self.quantize_data(keys[key], quantize_bits, groups * 3)
                 else:
                     data_quantized, data_scale = self.quantize_data(keys[key], quantize_bits, groups)
                 keys[key].copy_(data_quantized)
-                layer_scales.append((1 / data_scale.to(
-                    get_accelerator().current_device_name()).view(-1).unsqueeze(0)))
+                layer_scales.append((1 / data_scale.to(get_accelerator().current_device_name()).view(-1).unsqueeze(0)))
             all_scales.append(self.merge_layer_scales(layer_scales))
             return layer
 
         def _quantize_module(model, policies):
             for name, child in model.named_children():
                 if child.__class__ in policies:
                     quantize_fn, replace_policy = policies[child.__class__]
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/zero/config.py` & `deepspeed-0.9.0/deepspeed/runtime/zero/config.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,12 +1,11 @@
-'''Copyright The Microsoft DeepSpeed Team'''
-"""
-Copyright (c) Microsoft Corporation
-Licensed under the MIT license.
-"""
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 from pydantic import Field, validator
 import sys
 from typing import Optional
 from enum import Enum
 from deepspeed.runtime.config_utils import get_scalar_param, pp_int, DeepSpeedConfigModel
 from deepspeed.utils import logger
@@ -31,33 +30,31 @@
     "cpu_offload": [true|false] (deprecated),
     "cpu_offload_params" : [true|false] (deprecated),
     "cpu_offload_use_pin_memory": [true|false] (deprecated),
     "sub_group_size" : 1000000000000,
     "offload_param": {...},
     "offload_optimizer": {...},
     "ignore_unused_parameters": [true|false],
-    "round_robin_gradients": [true|false]
+    "round_robin_gradients": [true|false],
+    "memory_efficient_linear": [true|false]
     }
 }
 """
 
 ZERO_OPTIMIZATION = "zero_optimization"
 
 
 def read_zero_config_deprecated(param_dict):
     zero_config_dict = {}
     zero_config_dict["stage"] = 1 if param_dict[ZERO_OPTIMIZATION] else 0
     if zero_config_dict["stage"] > 0:
-        zero_config_dict["allgather_bucket_size"] = get_scalar_param(
-            param_dict,
-            "allgather_size",
-            5e8)
+        zero_config_dict["allgather_bucket_size"] = get_scalar_param(param_dict, "allgather_size", 5e8)
     logger.warning(
-        "DeepSpeedConfig: this format of ZeRO optimization setup is deprecated. Please use the following format: {}"
-        .format(ZERO_FORMAT))
+        "DeepSpeedConfig: this format of ZeRO optimization setup is deprecated. Please use the following format: {}".
+        format(ZERO_FORMAT))
     return zero_config_dict
 
 
 def get_zero_config(param_dict):
     if ZERO_OPTIMIZATION in param_dict:
         zero_config_dict = param_dict[ZERO_OPTIMIZATION]
         if isinstance(zero_config_dict, bool):
@@ -157,17 +154,15 @@
     parameters). Used by ZeRO3-Offload and ZeRO-Infinity
     """
 
     cpu_offload_param: bool = Field(
         None,
         deprecated=True,
         new_param="offload_param",
-        new_param_fn=(
-            lambda val: DeepSpeedZeroOffloadParamConfig(device=OffloadDeviceEnum.cpu)
-            if val else None),
+        new_param_fn=(lambda val: DeepSpeedZeroOffloadParamConfig(device=OffloadDeviceEnum.cpu) if val else None),
     )
     """ Deprecated, please use ``offload_param`` """
 
     cpu_offload_use_pin_memory: bool = Field(
         None,
         deprecated=True,
         new_param="offload_param or offload_optimizer",
@@ -175,76 +170,64 @@
     )
     """ Deprecated, please use ``offload_param`` or ``offload_optimizer`` """
 
     cpu_offload: bool = Field(
         None,
         deprecated=True,
         new_param="offload_optimizer",
-        new_param_fn=(
-            lambda val: DeepSpeedZeroOffloadOptimizerConfig(device=OffloadDeviceEnum.cpu)
-            if val else None),
+        new_param_fn=(lambda val: DeepSpeedZeroOffloadOptimizerConfig(device=OffloadDeviceEnum.cpu) if val else None),
     )
     """ Deprecated, please use ``offload_optimizer`` """
 
-    prefetch_bucket_size: int = Field(pp_int(5e7),
-                                      ge=0,
-                                      alias="stage3_prefetch_bucket_size")
+    prefetch_bucket_size: int = Field(pp_int(5e7), ge=0, alias="stage3_prefetch_bucket_size")
     """
     Maximum number of parameter elements to fetch ahead of use. Used by ZeRO3,
     ZeRO3-Offload, ZeRO-Infinity, and ZeRO-Inference.
     """
 
-    param_persistence_threshold: int = Field(pp_int(1e5),
-                                             ge=0,
-                                             alias="stage3_param_persistence_threshold")
+    param_persistence_threshold: int = Field(pp_int(1e5), ge=0, alias="stage3_param_persistence_threshold")
     """
     Do not partition parameters smaller than this threshold. Smaller values use
     less memory, but can greatly increase communication (especially
     latency-bound messages).
     """
 
-    model_persistence_threshold: int = Field(pp_int(sys.maxsize,
-                                                    "sys.maxsize"),
+    model_persistence_threshold: int = Field(pp_int(sys.maxsize, "sys.maxsize"),
                                              ge=0,
                                              alias="stage3_model_persistence_threshold")
     """
     Maximum number of parameter elements that can be persisted in GPU and not
     partitioned. This imposes an upper bound on the number of unpartitioned
     parameters resulting from param_persistence_threshold setting. Used by
     ZeRO3-Offload, ZeRO-Infinity and ZeRO-Inference.
     """
 
-    max_live_parameters: int = Field(pp_int(1e9),
-                                     ge=0,
-                                     alias="stage3_max_live_parameters")
+    max_live_parameters: int = Field(pp_int(1e9), ge=0, alias="stage3_max_live_parameters")
     """
     The maximum number of parameters resident per GPU before releasing. Smaller
     values use less memory, but perform more communication.
     """
 
     max_reuse_distance: int = Field(pp_int(1e9), ge=0, alias="stage3_max_reuse_distance")
     """
     Do not release a parameter if it will be reused within this threshold of
     parameters. Smaller values use less memory, but perform more communication.
     """
 
-    gather_16bit_weights_on_model_save: bool = Field(
-        False,
-        alias="stage3_gather_16bit_weights_on_model_save")
+    gather_16bit_weights_on_model_save: bool = Field(False, alias="stage3_gather_16bit_weights_on_model_save")
     """
     Consolidate the weights before saving the model by ``save_16bit_model()``.
     Since the weights are partitioned across GPUs, they aren’t part of
     ``state_dict``, so this function automatically gathers the weights when
     this option is enabled and then saves the fp16 model weights.
     """
 
-    stage3_gather_fp16_weights_on_model_save: bool = Field(
-        False,
-        deprecated=True,
-        new_param="gather_16bit_weights_on_model_save")
+    stage3_gather_fp16_weights_on_model_save: bool = Field(False,
+                                                           deprecated=True,
+                                                           new_param="gather_16bit_weights_on_model_save")
     """ Deprecated, please use ``gather_16bit_weights_on_model_save`` """
 
     ignore_unused_parameters: bool = True
     """
     Unused parameters in modules may be unexpected in static networks, but
     could be normal in dynamic networks. This controls whether or not training
     should terminate with an error message when unused parameters are detected.
@@ -262,16 +245,19 @@
     """
     Stage 1 and 2 optimization for CPU offloading that parallelizes gradient
     copying to CPU memory among ranks by fine-grained gradient partitioning.
     Performance benefit grows with gradient accumulation steps (more copying
     between optimizer steps) or GPU count (increased parallelism).
     """
 
+    memory_efficient_linear: bool = True
+    """
+    Use memory efficient linear implementation, for Stage 3.
+    """
+
     # Validators
     @validator("overlap_comm")
     def overlap_comm_valid(cls, field_value, values):
         if field_value is None:
-            assert (
-                "stage" in values
-            ), "DeepSpeedZeroConfig: 'stage' must be defined before 'overlap_comm'"
+            assert ("stage" in values), "DeepSpeedZeroConfig: 'stage' must be defined before 'overlap_comm'"
             field_value = values["stage"] == ZeroStageEnum.weights
         return field_value
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/zero/contiguous_memory_allocator.py` & `deepspeed-0.9.0/deepspeed/runtime/zero/contiguous_memory_allocator.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,20 +1,24 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import torch
 
 from deepspeed import comm as dist
 
 
 def print_rank_0(message):
     if dist.get_rank() == 0:
         print(message)
 
 
 class ContiguousMemoryAllocator(object):
+
     def __init__(self, size, dtype, device):
         self.buffer = torch.zeros(size, dtype=dtype, device=device)
 
         #address to contiguous size available
         self.contiguous_sizes = {}
 
         self.contiguous_sizes[0] = size
@@ -94,29 +98,27 @@
         free_before = self.total_free
         tensor_id = id(tensor)
         tensor_size = tensor.numel()
         self._release_tensor(tensor_id)
         self._unassign_params(tensor_id)
         self.total_free += tensor_size
         print_rank_0(
-            f"Free before release {free_before}. Released {tensor.numel()}. Total free after {self.total_free}."
-        )
+            f"Free before release {free_before}. Released {tensor.numel()}. Total free after {self.total_free}.")
         assert self.total_free - tensor_size == free_before, "Release bookkeeping error"
 
     def release_tensor_with_id(self, tensor_id):
         free_before = self.total_free
         assert tensor_id in self.tensor_map.keys(), "Invalid tensor id"
         tensor = self.tensor_map[tensor_id]
         tensor_size = tensor.numel()
         self._release_tensor(tensor_id)
         self._unassign_params(tensor_id)
         self.total_free += tensor_size
         print_rank_0(
-            f"Free before release {free_before}. Released {tensor.numel()}. Total free after {self.total_free}."
-        )
+            f"Free before release {free_before}. Released {tensor.numel()}. Total free after {self.total_free}.")
         assert self.total_free - tensor_size == free_before, "Release bookkeeping error"
 
     #shows the current memory allocation at specified resolution
     def print_allocation(self, resolution=200):
         total_size = self.buffer.numel() * 1.0
         empty = []
         for addr, size in self.contiguous_sizes.items():
@@ -132,17 +134,15 @@
         return self.max_allocated
 
     #to be called after defragmentation that moves the tensor buffers
     #this call reassigns the data of all the parameters using the tensor buffers
     def _reset_param_data(self):
         for id, tensor in self.tensor_map.items():
             for param in self.id_to_params[id]:
-                param.data = tensor.narrow(0,
-                                           0,
-                                           param.numel()).view(param.data.shape).data
+                param.data = tensor.narrow(0, 0, param.numel()).view(param.data.shape).data
 
     def _unassign_params(self, tensor_id):
         if tensor_id in self.id_to_params.keys():
             del self.id_to_params[tensor_id]
 
     def _release_tensor(self, tensor_id):
         assert tensor_id in self.tensor_addresses, f"Tensor id {tensor_id} not found"
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/zero/linear.py` & `deepspeed-0.9.0/deepspeed/runtime/zero/linear.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,8 +1,11 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 #Linear Module to use with ZeRO Stage 3 to allow for parameter memory release
 #after the module execution during forward
 #Instead of saving variables using save_for_backward, we save variable ids
 #Allowing us to retrieve the variable without creating pointer to it
 #Which allows for underlying tensor to be garbage collected
 #When partitioned as needed by the Zero Stage 3 optimizer
@@ -27,35 +30,29 @@
 
 
 def print_rank_0(message, debug=False, force=False):
     if dist.get_rank() == 0 and (debug or force):
         print(message)
 
 
-device = get_accelerator().device_name()
-if device == 'cuda':
-    try:
-        autocast_custom_fwd = torch.cuda.amp.custom_fwd
-        autocast_custom_bwd = torch.cuda.amp.custom_bwd
-    except (ImportError, AttributeError) as exp:
-        autocast_custom_fwd = noop_decorator
-        autocast_custom_bwd = noop_decorator
-else:
+try:
+    autocast_custom_fwd = get_accelerator().amp().custom_fwd
+    autocast_custom_bwd = get_accelerator().amp().custom_bwd
+except (ImportError, AttributeError) as exp:
     autocast_custom_fwd = noop_decorator
     autocast_custom_bwd = noop_decorator
 
 
 class LinearFunctionForZeroStage3(torch.autograd.Function):
 
     # Note that both forward and backward are @staticmethods
     @staticmethod
     @autocast_custom_fwd
     # bias is an optional argument
     def forward(ctx, input, weight, bias=None):
-        #print("In ZeRO Linear Function")
 
         weight_id = id(weight)
         bias_id = id(bias)
 
         #ctx.save_for_backward(input, weight, bias)
         ctx.save_for_backward(input, torch.tensor(weight_id), torch.tensor(bias_id))
 
@@ -100,17 +97,15 @@
             grad_input = grad_output.matmul(weight)
             #print(f"Computed grad input {grad_input.shape}")
         if ctx.needs_input_grad[1]:
             #print("Computing grad weight")
             dim = grad_output.dim()
             if dim > 2:
                 grad_weight = grad_output.reshape(-1,
-                                                  grad_output.shape[-1]).t().matmul(
-                                                      input.reshape(-1,
-                                                                    input.shape[-1]))
+                                                  grad_output.shape[-1]).t().matmul(input.reshape(-1, input.shape[-1]))
             else:
                 grad_weight = grad_output.t().matmul(input)
             #print(f"Computed grad weight grad_weight {grad_weight.shape}")
         if bias is not None and ctx.needs_input_grad[2]:
             #print("Computing grad bias")
             grad_bias = grad_output.sum(0)
             #print("Done computing grad bias")
@@ -185,11 +180,9 @@
             bound = 1 / math.sqrt(fan_in)
             init.uniform_(self.bias, -bound, bound)
 
     def forward(self, input: Tensor) -> Tensor:
         return LinearFunctionForZeroStage3.apply(input, self.weight, self.bias)
 
     def extra_repr(self) -> str:
-        return 'in_features={}, out_features={}, bias={}'.format(
-            self.in_features,
-            self.out_features,
-            self.bias is not None)
+        return 'in_features={}, out_features={}, bias={}'.format(self.in_features, self.out_features, self.bias
+                                                                 is not None)
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/zero/offload_config.py` & `deepspeed-0.9.0/deepspeed/runtime/zero/offload_config.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,12 +1,11 @@
-'''Copyright The Microsoft DeepSpeed Team'''
-"""
-Copyright (c) Microsoft Corporation
-Licensed under the MIT license.
-"""
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 from pydantic import Field, validator
 from enum import Enum
 from pathlib import Path
 from deepspeed.runtime.config_utils import DeepSpeedConfigModel, pp_int
 
 
@@ -84,11 +83,12 @@
     """
     For tile-based optimizer step processing, overlap write of previous tile
     with computation of current tile.
     """
 
     fast_init: bool = False
     """ Enable fast optimizer initialization when offloading to NVMe. """
+
     @validator("pipeline_read", "pipeline_write", always=True)
     def set_pipeline(cls, field_value, values):
         values["pipeline"] = field_value or values.get("pipeline", False)
         return field_value
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/zero/parameter_offload.py` & `deepspeed-0.9.0/deepspeed/runtime/zero/parameter_offload.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,11 +1,11 @@
-"""
-"Copyright 2022 The Microsoft DeepSpeed Team.
-Licensed under the MIT license.
-"""
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import sys
 import torch
 from collections import OrderedDict
 from deepspeed.runtime.utils import see_memory_usage
 from deepspeed.runtime.zero.offload_config import OffloadDeviceEnum
 from deepspeed.runtime.zero.partition_parameters import _init_external_params
@@ -18,36 +18,61 @@
 
 
 def is_builtin_type(obj):
     # https://stackoverflow.com/a/17795199
     return obj.__class__.__module__ == '__builtin__' or obj.__class__.__module__ == "builtins"
 
 
+def isinstance_namedtuple(obj: object) -> bool:
+    """
+    Is this an instance of namedtuple/NamedTuple?
+    From: https://stackoverflow.com/a/62692640
+
+    Args:
+        obj (object): An object.
+
+    Returns:
+        bool: True if namedtuple/NamedTuple else False.
+    """
+    return isinstance(obj, tuple) and hasattr(obj, '_asdict') and hasattr(obj, '_fields')
+
+
 # ensure we only warn once, otherwise every iteration will trigger a warning
 warned = False
 
 
-#apply torch.autograd.Function that calls a backward_function to tensors in output
 def _apply_to_tensors_only(module, functional, backward_function, outputs):
+    """
+    Apply a torch.autograd.Function that calls a `backward_function` to every Tensor in `outputs`.
+
+    Args:
+        module (torch.nn.Module):  A torch module
+        functional (Type[torch.autograd.Function]): The function class to apply.
+        backward_function (Callable[[torch.nn.Module], None]): A backward_function to pass to
+            `functional.apply`.
+        outputs (Any): The output of `module`.
+
+    Returns:
+        Any: The output of `module`.
+    """
     if isinstance(outputs, (tuple, list)):
         touched_outputs = []
         for output in outputs:
-            touched_output = _apply_to_tensors_only(module,
-                                                    functional,
-                                                    backward_function,
-                                                    output)
+            touched_output = _apply_to_tensors_only(module, functional, backward_function, output)
             touched_outputs.append(touched_output)
+
+        if isinstance_namedtuple(outputs):
+            # namedtuples require a slightly different syntax.
+            return outputs.__class__(*touched_outputs)
+
         return outputs.__class__(touched_outputs)
     elif isinstance(outputs, dict):
         # apply inplace to avoid recreating dict inherited objects
         for key in outputs.keys():
-            outputs[key] = _apply_to_tensors_only(module,
-                                                  functional,
-                                                  backward_function,
-                                                  outputs[key])
+            outputs[key] = _apply_to_tensors_only(module, functional, backward_function, outputs[key])
         return outputs
 
     elif isinstance(outputs, torch.Tensor):
         # this also applies to torch.Tensor's subclasses like torch.nn.parameter.Parameter
         touched_outputs = functional.apply(module, backward_function, outputs)
 
         # restore zero param attributes if those get stripped by `backward_function`
@@ -63,38 +88,33 @@
                     "The ZeRO-3 hooks designed to trigger before or after backward pass of the module relies on knowing the input and "
                     "output tensors and therefore may not get triggered properly.")
                 warned = True
         return outputs
 
 
 #for each tensor in outputs run the forward_function and register backward_function as hook
-def _apply_forward_and_backward_to_tensors_only(module,
-                                                forward_function,
-                                                backward_function,
-                                                outputs):
+def _apply_forward_and_backward_to_tensors_only(module, forward_function, backward_function, outputs):
     if type(outputs) is tuple:
         touched_outputs = []
         for output in outputs:
-            touched_output = _apply_forward_and_backward_to_tensors_only(
-                module,
-                forward_function,
-                backward_function,
-                output)
+            touched_output = _apply_forward_and_backward_to_tensors_only(module, forward_function, backward_function,
+                                                                         output)
             touched_outputs.append(touched_output)
         return tuple(touched_outputs)
     elif type(outputs) is torch.Tensor:
         forward_function(outputs)
         if outputs.requires_grad:
             outputs.register_hook(backward_function)
         return outputs
     else:
         return outputs
 
 
 class ZeROOrderedDict(OrderedDict):
+
     def __init__(self, parent_module, *args, **kwargs):
         """A replacement for ``collections.OrderedDict`` to detect external ZeRO params.
 
         Args:
             parent_module (``collections.OrderedDict``): the collection to replace
         """
 
@@ -109,17 +129,15 @@
         if param is None:
             return param
 
         if param.ds_status == ZeroParamStatus.NOT_AVAILABLE:
             if self._parent_module._parameters._in_forward:
                 register_external_parameter(FWD_MODULE_STACK[-1], param)
                 param.all_gather()
-                print_rank_0(
-                    f'Registering external parameter from getter {key} ds_id = {param.ds_id}',
-                    force=False)
+                print_rank_0(f'Registering external parameter from getter {key} ds_id = {param.ds_id}', force=False)
 
         return param
 
 
 def _inject_parameters(module, cls):
     for module in module.modules():
         if cls == ZeROOrderedDict:
@@ -129,14 +147,15 @@
 
         for key, param in module._parameters.items():
             new_param[key] = param
         module._parameters = new_param
 
 
 class PreBackwardFunction(torch.autograd.Function):
+
     @staticmethod
     def forward(ctx, module, pre_backward_function, outputs):
         ctx.module = module
         ctx.pre_backward_function = pre_backward_function
         if not hasattr(module, "applied_pre_backward_ref_cnt"):
             module.applied_pre_backward_ref_cnt = 0
         module.applied_pre_backward_ref_cnt += 1
@@ -148,14 +167,15 @@
     def backward(ctx, *args):
         #print(f"Before Backward: {ctx.module.__class__.__name__}")
         ctx.pre_backward_function(ctx.module)
         return (None, None) + args
 
 
 class PostBackwardFunction(torch.autograd.Function):
+
     @staticmethod
     def forward(ctx, module, pre_backward_function, output):
         ctx.module = module
         if output.requires_grad:
             #TODO SOME TIMES post backward does not seem to be triggered debug in detail
             #Should only cause increase in memory not correctness issue
             #if output.grad_fn.__class__.__name__ == 'ViewBackward':
@@ -175,14 +195,15 @@
         if ctx.module.ds_grads_remaining == 0:
             ctx.pre_backward_function(ctx.module)
             #print(f"After Backward: {ctx.module.__class__.__name__}")
         return (None, None) + args
 
 
 class DeepSpeedZeRoOffload(object):
+
     def __init__(self,
                  module,
                  timers,
                  ds_config,
                  overlap_comm=True,
                  prefetch_bucket_size=50000000,
                  max_reuse_distance=1000000000,
@@ -190,16 +211,15 @@
                  param_persistence_threshold=100000,
                  model_persistence_threshold=sys.maxsize,
                  offload_param_config=None,
                  mpu=None):
 
         see_memory_usage("DeepSpeedZeRoOffload initialize [begin]", force=True)
 
-        print_rank_0(f"initialized {__class__.__name__} with args: {locals()}",
-                     force=False)
+        print_rank_0(f"initialized {__class__.__name__} with args: {locals()}", force=False)
 
         self.module = module
         self.dtype = list(module.parameters())[0].dtype
         self.offload_device = None
         self.offload_param_pin_memory = False
 
         if offload_param_config is not None and offload_param_config.device != OffloadDeviceEnum.none:
@@ -211,24 +231,22 @@
         for m in module.modules():
             _init_external_params(m)
 
         _inject_parameters(module, ZeROOrderedDict)
 
         self.param_numel_persistence_threshold = int(param_persistence_threshold)
         self.model_persistence_threshold = int(model_persistence_threshold)
-        self.persistent_parameters = self.mark_persistent_parameters(
-            self.param_numel_persistence_threshold,
-            self.model_persistence_threshold)
+        self.persistent_parameters = self.mark_persistent_parameters(self.param_numel_persistence_threshold,
+                                                                     self.model_persistence_threshold)
 
         self.param_coordinators = {}
         self._prefetch_bucket_sz = int(prefetch_bucket_size)
         self._max_reuse_distance_in_numel = int(max_reuse_distance)
         self._max_available_parameters_in_numel = int(max_live_parameters)
-        self.__allgather_stream = get_accelerator().Stream(
-        ) if overlap_comm else get_accelerator().default_stream()
+        self.__allgather_stream = get_accelerator().Stream() if overlap_comm else get_accelerator().default_stream()
 
         self.forward_hooks = []
         self.backward_hooks = []
         self.setup_zero_stage3_hooks()
         print_rank_0(
             f'Created module hooks: forward = {len(self.forward_hooks)}, backward = {len(self.backward_hooks)}',
             force=False)
@@ -236,33 +254,34 @@
         see_memory_usage("DeepSpeedZeRoOffload initialize [end]", force=True)
 
     @instrument_w_nvtx
     def partition_all_parameters(self):
         """Partitioning Parameters that were not partitioned usually if parameters
         of modules whose input parameters do not require grad computation do not
         trigger post call and will therefore will remain unpartitioned"""
-        self.get_param_coordinator(training=self.module.training).release_and_reset_all(
-            self.module)
+        self.get_param_coordinator(training=self.module.training).release_and_reset_all(self.module)
         for param in iter_params(self.module, recurse=True):
             if param.ds_status != ZeroParamStatus.NOT_AVAILABLE:
                 raise RuntimeError(f"{param.ds_summary()} expected to be released")
 
     def get_param_coordinator(self, training):
         if not training in self.param_coordinators:
             self.param_coordinators[training] = PartitionedParameterCoordinator(
                 prefetch_bucket_sz=self._prefetch_bucket_sz,
                 max_reuse_distance_in_numel=self._max_reuse_distance_in_numel,
-                max_available_parameters_in_numel=self.
-                _max_available_parameters_in_numel,
+                max_available_parameters_in_numel=self._max_available_parameters_in_numel,
                 allgather_stream=self.__allgather_stream,
                 prefetch_nvme=self.offload_device == OffloadDeviceEnum.nvme,
             )
 
         return self.param_coordinators[training]
 
+    def empty_partition_cache(self):
+        self.partition_all_parameters()
+
     def _convert_to_zero_parameters(self, ds_config, module, mpu):
         non_zero_params = [p for p in module.parameters() if not is_zero_param(p)]
         if non_zero_params:
             zero_params = [p for p in module.parameters() if is_zero_param(p)]
             if zero_params:
                 zero_params[0].convert_to_zero_parameters(param_list=non_zero_params)
             else:
@@ -287,17 +306,16 @@
 
         for hook in self.forward_hooks:
             hook.remove()
 
         for hook in self.backward_hooks:
             hook.remove()
 
-        print_rank_0(
-            f'Deleted module hooks: forward = {num_forward_hooks}, backward = {num_backward_hooks}',
-            force=False)
+        print_rank_0(f'Deleted module hooks: forward = {num_forward_hooks}, backward = {num_backward_hooks}',
+                     force=False)
 
     def setup_zero_stage3_hooks(self):
         self.hierarchy = 0
 
         #reset step if in inference mode
         @instrument_w_nvtx
         def _end_of_forward_hook(module, *args):
@@ -317,15 +335,15 @@
         persistent_params = []
         total_persistent_parameters = 0
         params_count = 0
         for _, param in self.module.named_parameters(recurse=True):
             if param.ds_numel + total_persistent_parameters > model_threshold:
                 continue
 
-            if param.ds_numel < param_threshold:
+            if param.ds_numel <= param_threshold:
                 params_count += 1
                 param.ds_persist = True
                 persistent_params.append(param)
                 total_persistent_parameters += param.ds_numel
 
         print_rank_0(
             f"Parameter Offload: Total persistent parameters: {total_persistent_parameters} in {params_count} params",
@@ -361,27 +379,22 @@
                     outputs = []
                     output = output if isinstance(output, dict) else vars(output)
                     for name, val in output.items():
                         if not name.startswith('__') and torch.is_tensor(val):
                             outputs.append(val)
                     output = outputs
 
-            for item in filter(
-                    lambda item: is_zero_param(item) or hasattr(item,
-                                                                'ds_param_alias'),
-                    output):
+            for item in filter(lambda item: is_zero_param(item) or hasattr(item, 'ds_param_alias'), output):
                 key = id(item) if hasattr(item, 'ds_id') else id(item.ds_param_alias)
-                actual_external_param = item if hasattr(item,
-                                                        'ds_id') else item.ds_param_alias
+                actual_external_param = item if hasattr(item, 'ds_id') else item.ds_param_alias
 
                 if not any(key in m._external_params for m in FWD_MODULE_STACK):
                     actual_external_param.is_external_param = True
                     module_to_register = FWD_MODULE_STACK[-1]
-                    register_external_parameter(module_to_register,
-                                                actual_external_param)
+                    register_external_parameter(module_to_register, actual_external_param)
                     print_rank_0(
                         f'Registering dangling parameter for module {module_to_register.__class__.__name__}, ds_id = {actual_external_param.ds_id}.',
                         force=False)
 
                     # It's possible that the parameter was already external to the completed module. If so, remove it the
                     # registration as it will be covered by the outer module instead.
                     if key in module._external_params:
@@ -391,29 +404,27 @@
                         unregister_external_parameter(module, actual_external_param)
 
                     actual_external_param.all_gather()
 
             self.post_sub_module_forward_function(module)
 
         def _pre_backward_module_hook(module, inputs, output):
+
             @instrument_w_nvtx
             def _run_before_backward_function(sub_module):
                 # some models (e.g. Albert) may run multiple forwards on the same layer in a loop
                 # before doing backwards, so each backward will need a pre-fetch - using reference
                 # counting to support this scenario
                 #print(f"COUNTER before: {sub_module.applied_pre_backward_ref_cnt}")
                 if sub_module.applied_pre_backward_ref_cnt > 0:
                     self.pre_sub_module_backward_function(sub_module)
                     sub_module.applied_pre_backward_ref_cnt -= 1
                 #print(f"COUNTER after: {sub_module.applied_pre_backward_ref_cnt}")
 
-            return _apply_to_tensors_only(module,
-                                          PreBackwardFunction,
-                                          _run_before_backward_function,
-                                          output)
+            return _apply_to_tensors_only(module, PreBackwardFunction, _run_before_backward_function, output)
 
         #This is an alternate to doing _post_backward_module_hook
         #it uses tensor.register_hook instead of using torch.autograd.Function
         def _alternate_post_backward_module_hook(module, inputs):
             module.ds_grads_remaining = 0
 
             #print(f"Before Forward {module.__class__.__name__}")
@@ -424,93 +435,79 @@
                     #print(f"After backward {module.__class__.__name__}")
                     self.post_sub_module_backward_function(module)
 
             def _run_before_forward_function(input):
                 if input.requires_grad:
                     module.ds_grads_remaining += 1
 
-            return _apply_forward_and_backward_to_tensors_only(
-                module,
-                _run_before_forward_function,
-                _run_after_backward_hook,
-                inputs)
+            return _apply_forward_and_backward_to_tensors_only(module, _run_before_forward_function,
+                                                               _run_after_backward_hook, inputs)
 
         def _post_backward_module_hook(module, inputs):
             module.ds_grads_remaining = 0
 
             @instrument_w_nvtx
             def _run_after_backward_function(sub_module):
                 if sub_module.ds_grads_remaining == 0:
                     self.post_sub_module_backward_function(sub_module)
 
-            return _apply_to_tensors_only(module,
-                                          PostBackwardFunction,
-                                          _run_after_backward_function,
-                                          inputs)
+            return _apply_to_tensors_only(module, PostBackwardFunction, _run_after_backward_function, inputs)
 
         # Pre forward hook
-        self.forward_hooks.append(
-            module.register_forward_pre_hook(_pre_forward_module_hook))
+        self.forward_hooks.append(module.register_forward_pre_hook(_pre_forward_module_hook))
 
         # Post forward hook
-        self.forward_hooks.append(
-            module.register_forward_hook(_post_forward_module_hook))
+        self.forward_hooks.append(module.register_forward_hook(_post_forward_module_hook))
 
         # Pre backward hook
-        self.backward_hooks.append(
-            module.register_forward_hook(_pre_backward_module_hook))
+        self.backward_hooks.append(module.register_forward_hook(_pre_backward_module_hook))
 
         # post backward hook
-        self.backward_hooks.append(
-            module.register_forward_pre_hook(_post_backward_module_hook))
+        self.backward_hooks.append(module.register_forward_pre_hook(_post_backward_module_hook))
 
     @torch.no_grad()
     def pre_sub_module_forward_function(self, sub_module):
-        see_memory_usage(f"Before sub module function {sub_module.__class__.__name__}",
-                         force=False)
+        see_memory_usage(f"Before sub module function {sub_module.__class__.__name__}", force=False)
 
         global FWD_MODULE_STACK
         FWD_MODULE_STACK.append(sub_module)
 
         param_coordinator = self.get_param_coordinator(training=sub_module.training)
         param_coordinator.trace_prologue(sub_module)
         if param_coordinator.is_record_trace():
             param_coordinator.record_module(sub_module)
         param_coordinator.fetch_sub_module(sub_module)
 
-        see_memory_usage(
-            f"Before sub module function {sub_module.__class__.__name__} after fetch",
-            force=False)
+        see_memory_usage(f"Before sub module function {sub_module.__class__.__name__} after fetch", force=False)
 
     @torch.no_grad()
     def post_sub_module_forward_function(self, sub_module):
-        see_memory_usage(
-            f"After sub module function {sub_module.__class__.__name__} {sub_module.id} before release",
-            force=False)
+        see_memory_usage(f"After sub module function {sub_module.__class__.__name__} {sub_module.id} before release",
+                         force=False)
 
         param_coordinator = self.get_param_coordinator(training=sub_module.training)
         param_coordinator.release_sub_module(sub_module)
 
-        see_memory_usage(
-            f"After sub module function {sub_module.__class__.__name__}  {sub_module.id} after release",
-            force=False)
+        see_memory_usage(f"After sub module function {sub_module.__class__.__name__}  {sub_module.id} after release",
+                         force=False)
 
     @torch.no_grad()
     def pre_sub_module_backward_function(self, sub_module):
-        param_coordinator = self.get_param_coordinator(training=sub_module.training)
+        assert sub_module.training, "backward pass is invalid for module in evaluation mode"
+        param_coordinator = self.get_param_coordinator(training=True)
         param_coordinator.trace_prologue(sub_module)
         if param_coordinator.is_record_trace():
             param_coordinator.record_module(sub_module)
         param_coordinator.fetch_sub_module(sub_module)
 
     @torch.no_grad()
     def post_sub_module_backward_function(self, sub_module):
+        assert sub_module.training, "backward pass is invalid for module in evaluation mode"
         see_memory_usage(
             f"After sub module backward function {sub_module.__class__.__name__} {sub_module.id} before release",
             force=False)
 
-        self.get_param_coordinator(
-            training=sub_module.training).release_sub_module(sub_module)
+        self.get_param_coordinator(training=True).release_sub_module(sub_module)
 
         see_memory_usage(
             f"After sub module backward function {sub_module.__class__.__name__} {sub_module.id} after release",
             force=False)
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/zero/partition_parameters.py` & `deepspeed-0.9.0/deepspeed/runtime/zero/partition_parameters.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,53 +1,88 @@
-"""
-"Copyright 2020 The Microsoft DeepSpeed Team.
-Licensed under the MIT license.
-"""
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import math
 import os
 import types
 from typing import Callable, Iterable
 from enum import Enum
 import functools
 import itertools
 from typing import List
-
 import torch
 from torch import Tensor
 from deepspeed import comm as dist
 from torch.nn import Module
 from torch.nn import Parameter
 
 from .linear import zero3_linear_wrap
 
 import deepspeed
 from ..utils import get_only_unique_item, see_memory_usage
+from deepspeed.runtime.zero.config import DeepSpeedZeroConfig
 from deepspeed.runtime.zero.utils import assert_ints_same_as_other_ranks
 from deepspeed.runtime.zero.offload_config import OffloadDeviceEnum
+from deepspeed.runtime.config_utils import get_config_default
 from deepspeed.utils import instrument_w_nvtx, logger
 from deepspeed.comm.comm import init_distributed
-from deepspeed.utils.debug import (debug_param2name_id_shape,
-                                   debug_param2name_id_shape_device,
-                                   debug_module2name,
-                                   debug_param2name_id,
-                                   debug_param2name_id_shape_status)
+from deepspeed.utils.debug import (debug_param2name_id_shape, debug_param2name_id_shape_device, debug_module2name,
+                                   debug_param2name_id, debug_param2name_id_shape_status)
 from deepspeed.accelerator import get_accelerator
 from ..swap_tensor.partitioned_param_swapper import AsyncPartitionedParameterSwapper, PartitionedParamStatus
 
 param_count = 0
 partitioned_param_data_shape = [0]
 zero_init_enabled = False
 
 
+class NoGatherHandle:
+
+    def __init__(self, param: Parameter) -> None:
+        if param.ds_status != ZeroParamStatus.INFLIGHT:
+            raise RuntimeError(f"expected param {param.ds_summary()} to be available")
+
+        param.data = param.ds_tensor.data.to(device=get_accelerator().current_device_name(),
+                                             non_blocking=True).view(param.ds_shape)
+        self.__param = param
+
+    def wait(self) -> None:
+        get_accelerator().current_stream().synchronize()
+        self.__param.ds_status = ZeroParamStatus.AVAILABLE
+
+
+class NoGatherCoalescedHandle:
+
+    def __init__(self, params: List[Parameter]) -> None:
+        self.__params = params
+        self.__complete = False
+
+        for param in self.__params:
+            if param.ds_status != ZeroParamStatus.INFLIGHT:
+                raise RuntimeError(f"expected param {param.ds_summary()} to not be available")
+            param.data = param.ds_tensor.data.to(device=get_accelerator().current_device_name(),
+                                                 non_blocking=True).view(param.ds_shape)
+
+    @instrument_w_nvtx
+    def wait(self) -> None:
+        if self.__complete:
+            return
+
+        get_accelerator().current_stream().synchronize()
+        for param in self.__params:
+            assert param.ds_status == ZeroParamStatus.INFLIGHT, f"expected param {param.ds_summary()} to be inflight"
+            param.ds_status = ZeroParamStatus.AVAILABLE
+
+        self.__complete = True
+
+
 def _dist_allgather_fn(input_tensor: Tensor, output_tensor: Tensor, group=None):
-    return instrument_w_nvtx(dist.allgather_fn)(output_tensor,
-                                                input_tensor,
-                                                group=group,
-                                                async_op=True)
+    return instrument_w_nvtx(dist.allgather_fn)(output_tensor, input_tensor, group=group, async_op=True)
 
 
 def print_rank_0(message, debug=False, force=False):
     rank = dist.get_rank()
     if rank == 0 and (debug or force):
         print(message)
     # other variations
@@ -72,17 +107,15 @@
     if not hasattr(module, '_external_params'):
         module._external_params = {}
 
         def external_parameters(self):
             return self._external_params.items()
 
         def all_parameters(self):
-            return itertools.chain(self.named_parameters(self,
-                                                         recurse=False),
-                                   external_parameters(self))
+            return itertools.chain(self.named_parameters(self, recurse=False), external_parameters(self))
 
         module.ds_external_parameters = types.MethodType(external_parameters, module)
         module.all_parameters = types.MethodType(all_parameters, module)
 
 
 def register_external_parameter(module, parameter):
     """Instruct DeepSpeed to coordinate ``parameter``'s collection and partitioning in
@@ -146,16 +179,15 @@
     Raises:
         RuntimeError: If ``parameter`` is not of type ``torch.nn.Parameter``.
         RuntimeError: If ``parameter`` is not a registered external parameter of ``module``.
     """
     if not isinstance(parameter, torch.nn.Parameter):
         raise RuntimeError('Parameter is not a torch.nn.Parameter')
 
-    if not hasattr(module,
-                   '_external_params') or id(parameter) not in module._external_params:
+    if not hasattr(module, '_external_params') or id(parameter) not in module._external_params:
         raise RuntimeError('Parameter is not a registered external parameter of module.')
 
     key = id(parameter)
     del module._external_params[key]
 
 
 class ZeroParamType(Enum):
@@ -184,30 +216,30 @@
 
 _orig_torch_empty = torch.empty
 _orig_torch_zeros = torch.zeros
 _orig_torch_ones = torch.ones
 _orig_torch_full = torch.full
 
 
-def zero_wrapper_for_fp_tensor_constructor(fn: Callable,
-                                           target_fp_dtype: torch.dtype) -> Callable:
+def zero_wrapper_for_fp_tensor_constructor(fn: Callable, target_fp_dtype: torch.dtype) -> Callable:
+
     def wrapped_fn(*args, **kwargs) -> Tensor:
         if kwargs.get("device", None) is None:
-            kwargs['device'] = torch.device(get_accelerator().device_name(
-                os.environ["LOCAL_RANK"]))
+            kwargs['device'] = torch.device(get_accelerator().device_name(os.environ["LOCAL_RANK"]))
         tensor: Tensor = fn(*args, **kwargs)
         if tensor.is_floating_point():
             tensor = tensor.to(target_fp_dtype)
 
         return tensor
 
     return wrapped_fn
 
 
 def get_new_tensor_fn_for_dtype(dtype: torch.dtype) -> Callable:
+
     def new_tensor(cls, *args) -> Tensor:
         device = torch.device(get_accelerator().device_name(os.environ["LOCAL_RANK"]))
         tensor = _orig_torch_empty(0, device=device).new_empty(*args)
         if tensor.is_floating_point():
             tensor = tensor.to(dtype)
 
         return tensor
@@ -246,23 +278,22 @@
 temp_contiguous_tensor = None
 empty_buffers = {}
 
 
 # Inserts _post_init_method at the end of init method
 # for all sub classes of torch.nn.Module
 class InsertPostInitMethodToModuleSubClasses(object):
-    def __init__(self,
-                 enabled=True,
-                 mem_efficient_linear=True,
-                 ds_config=None,
-                 dtype=None):
+
+    def __init__(self, enabled=True, mem_efficient_linear=True, ds_config=None, dtype=None):
         self.mem_efficient_linear = mem_efficient_linear
         self.enabled = enabled
         self._set_dtype(ds_config, dtype)
-        assert self.dtype in [torch.half, torch.bfloat16, torch.float], f"Invalid data type {self.dtype}, allowed values are [torch.half, torch.bfloat16, torch.float]"
+        assert self.dtype in [
+            torch.half, torch.bfloat16, torch.float
+        ], f"Invalid data type {self.dtype}, allowed values are [torch.half, torch.bfloat16, torch.float]"
 
     def __enter__(self):
         global zero_init_enabled
         if not self.enabled:
             return
         zero_init_enabled = True
 
@@ -276,14 +307,15 @@
             since the Init context manager partitions child modules immediately after
             they are initialized, without modifying apply we would entirely skip
             any initialization done by parent modules.
 
             to get around this issue, we wrap the function passed to Module.apply
             so that the applied function is applied to child modules correctly.
             """
+
             def get_wrapped_fn_to_apply(fn_to_apply: Callable) -> Callable:
                 if hasattr(fn_to_apply, "wrapped"):
                     return fn_to_apply
 
                 @functools.wraps(fn_to_apply)
                 def wrapped_fn_to_apply(module_to_apply_fn_to: Module) -> None:
                     """gathers parameters before calling apply function. afterwards
@@ -292,27 +324,22 @@
 
                     takes the following steps:
                     1. allgathers parameters for the current module being worked on
                     2. calls the original function
                     3. broadcasts root rank's parameters to the other ranks
                     4. re-partitions the parameters
                     """
-                    if not all(
-                            is_zero_param(p)
-                            for p in module_to_apply_fn_to.parameters(recurse=False)):
-                        raise RuntimeError(
-                            f"not all parameters for {module_to_apply_fn_to.__class__.__name__}, "
-                            f"were zero params, is it possible that the parameters were "
-                            f"overwritten after they were initialized? "
-                            f"params: {[p for p in module_to_apply_fn_to.parameters(recurse=False)]} "
-                        )
+                    if not all(is_zero_param(p) for p in module_to_apply_fn_to.parameters(recurse=False)):
+                        raise RuntimeError(f"not all parameters for {module_to_apply_fn_to.__class__.__name__}, "
+                                           f"were zero params, is it possible that the parameters were "
+                                           f"overwritten after they were initialized? "
+                                           f"params: {[p for p in module_to_apply_fn_to.parameters(recurse=False)]} ")
 
                     params_to_apply_fn_to: Iterable[Parameter] = list(
-                        sorted(module_to_apply_fn_to.parameters(recurse=False),
-                               key=lambda p: p.ds_id))
+                        sorted(module_to_apply_fn_to.parameters(recurse=False), key=lambda p: p.ds_id))
 
                     for param in params_to_apply_fn_to:
                         param.all_gather()
 
                     fn_to_apply(module_to_apply_fn_to)
 
                     for param in params_to_apply_fn_to:
@@ -328,47 +355,44 @@
             @functools.wraps(orig_module_apply_fn)
             def wrapped_apply(module: Module, fn_to_apply: Callable) -> None:
                 orig_module_apply_fn(module, get_wrapped_fn_to_apply(fn_to_apply))
 
             return wrapped_apply
 
         def partition_after(f):
+
             @functools.wraps(f)
             def wrapper(module, *args, **kwargs):
 
                 # important logic: We want to run post_init only after child's __init__ is
                 # completed, and do nothing after __init__ of any of its parents and grandparents in
                 # the inheritance ancestry. This way the partitioning will need to happen only once
                 # when the whole object is ready to be partitioned and not before. This is because
                 # often the child module will need to tweak the weights - for example running a
                 # custom weights init function. So if a parent created the weights param, the child
                 # won't need to gather it in order to tweak it
 
-                print_rank_0(f'Before initializing {module.__class__.__name__}',
-                             force=False)
+                print_rank_0(f'Before initializing {module.__class__.__name__}', force=False)
 
                 is_child_module = False
                 if not hasattr(module, "_ds_child_entered"):
                     # child's __init__ was called, since parents all see the same object they can now skip post_init
                     is_child_module = True
                     setattr(module, "_ds_child_entered", True)
 
                 f(module, *args, **kwargs)
 
                 if is_child_module:
                     # child's __init__ is done, now we can run a single post_init on the child object
                     delattr(module, "_ds_child_entered")
 
-                    print_rank_0(f'Running post_init for {module.__class__.__name__}',
-                                 force=False)
+                    print_rank_0(f'Running post_init for {module.__class__.__name__}', force=False)
                     self._post_init_method(module)
 
-                print_rank_0(
-                    f'After initializing followed by post init for {module.__class__.__name__}',
-                    force=False)
+                print_rank_0(f'After initializing followed by post init for {module.__class__.__name__}', force=False)
 
             return wrapper
 
         def _enable_class(cls):
             cls._old_init = cls.__init__
             cls.__init__ = partition_after(cls.__init__)
 
@@ -383,22 +407,19 @@
         # holding onto some methods so we can put them back the way they were in __exit__
         torch.nn.modules.module.Module._old_init_subclass = torch.nn.modules.module.Module.__init_subclass__
         torch.nn.modules.module.Module._old_apply = torch.nn.modules.module.Module.apply
         torch.Tensor.__old_new__ = torch.Tensor.__new__
 
         # Replace .__init__() for future subclasses of torch.nn.Module
         torch.nn.modules.module.Module.__init_subclass__ = classmethod(_init_subclass)
-        torch.nn.modules.module.Module.apply = apply_with_gather(
-            torch.nn.modules.module.Module._old_apply)
+        torch.nn.modules.module.Module.apply = apply_with_gather(torch.nn.modules.module.Module._old_apply)
 
         torch.Tensor.__new__ = get_new_tensor_fn_for_dtype(self.dtype)
-        torch.empty = zero_wrapper_for_fp_tensor_constructor(_orig_torch_empty,
-                                                             self.dtype)
-        torch.zeros = zero_wrapper_for_fp_tensor_constructor(_orig_torch_zeros,
-                                                             self.dtype)
+        torch.empty = zero_wrapper_for_fp_tensor_constructor(_orig_torch_empty, self.dtype)
+        torch.zeros = zero_wrapper_for_fp_tensor_constructor(_orig_torch_zeros, self.dtype)
         torch.ones = zero_wrapper_for_fp_tensor_constructor(_orig_torch_ones, self.dtype)
         torch.full = zero_wrapper_for_fp_tensor_constructor(_orig_torch_full, self.dtype)
 
         if self.mem_efficient_linear:
             print_rank_0(
                 "nn.functional.linear has been overridden with a more memory efficient version. This will persist unless manually reset.",
                 force=False)
@@ -408,16 +429,15 @@
     def __exit__(self, exc_type, exc_value, traceback):
         if not self.enabled:
             return
 
         shutdown_init_context()
 
         if dist.get_rank() == 0:
-            logger.info("finished initializing model with %.2fB parameters",
-                        param_count / 1e9)
+            logger.info("finished initializing model with %.2fB parameters", param_count / 1e9)
 
         # Now that we cleaned up the metaclass injection, raise the exception.
         if exc_type is not None:
             return False
 
     # To be implemented by inheriting classes
     def _post_init_method(self, module):
@@ -467,27 +487,29 @@
     #        if self.mem_efficient_linear:
     #            torch.nn.functional.linear = self.linear_bk
 
     zero_init_enabled = False
 
 
 class AllGatherHandle:
+
     def __init__(self, handle, param: Parameter) -> None:
         if param.ds_status != ZeroParamStatus.INFLIGHT:
             raise RuntimeError(f"expected param {param.ds_summary()} to be available")
 
         self.__handle = handle
         self.__param = param
 
     def wait(self) -> None:
         instrument_w_nvtx(self.__handle.wait)()
         self.__param.ds_status = ZeroParamStatus.AVAILABLE
 
 
 class AllGatherCoalescedHandle:
+
     def __init__(
         self,
         allgather_handle,
         params: List[Parameter],
         partitions: List[Tensor],
         world_size: int,
     ) -> None:
@@ -495,16 +517,15 @@
         self.__params = params
         self.__partitions = partitions
         self.__world_size = world_size
         self.__complete = False
 
         for param in self.__params:
             if param.ds_status != ZeroParamStatus.INFLIGHT:
-                raise RuntimeError(
-                    f"expected param {param.ds_summary()} to not be available")
+                raise RuntimeError(f"expected param {param.ds_summary()} to not be available")
 
     @instrument_w_nvtx
     def wait(self) -> None:
         if self.__complete:
             return
 
         instrument_w_nvtx(self.__allgather_handle.wait)()
@@ -514,34 +535,48 @@
         for param in self.__params:
             assert param.ds_status == ZeroParamStatus.INFLIGHT, f"expected param {param.ds_summary()} to be inflight"
             partitions: List[Tensor] = []
             for rank in range(self.__world_size):
                 param_start = rank * param.ds_tensor.ds_numel
                 if param_start < param.ds_numel:
                     part_to_copy = self.__partitions[rank].narrow(
-                        0,
-                        param_offset,
-                        min(param.ds_numel - param_start,
-                            param.ds_tensor.ds_numel))
+                        0, param_offset, min(param.ds_numel - param_start, param.ds_tensor.ds_numel))
                     partitions.append(part_to_copy)
-
             param.data = instrument_w_nvtx(torch.cat)(partitions).view(param.ds_shape)
             param.ds_status = ZeroParamStatus.AVAILABLE
 
             for part_to_copy in partitions:
                 part_to_copy.record_stream(get_accelerator().current_stream())
 
             param_offset += param.ds_tensor.ds_numel
 
         self.__complete = True
 
 
+def _no_gather_coalesced(params: Iterable[Parameter]) -> AllGatherCoalescedHandle:
+    for param in params:
+        if param.ds_status != ZeroParamStatus.NOT_AVAILABLE:
+            raise RuntimeError(param.ds_summary())
+        param.ds_status = ZeroParamStatus.INFLIGHT
+
+    params = sorted(params, key=lambda p: p.ds_id)
+    if len(params) == 1:
+        param, = params
+        return NoGatherHandle(param)
+    return NoGatherCoalescedHandle(params)
+
+
 # Replaces all parameters in module with Scattered Parameters
 class Init(InsertPostInitMethodToModuleSubClasses):
     param_id = 0
+    param_persistence_threshold = get_config_default(DeepSpeedZeroConfig, "param_persistence_threshold")
+    model_persistence_threshold = get_config_default(DeepSpeedZeroConfig, "model_persistence_threshold")
+    num_persisted_parameters = 0
+    num_persisted_elements = 0
+    apply_param_persistence = False
 
     def __init__(self,
                  module=None,
                  data_parallel_group=None,
                  mem_efficient_linear=True,
                  remote_device=None,
                  pin_memory=False,
@@ -647,74 +682,70 @@
             .. code-block:: python
 
                 model = deepspeed.zero.Init(module=model)
         """
         if config is not None:
             config_dict_or_path = config
             logger.warning(
-                f'zero.Init: the `config` argument is deprecated. Please use `config_dict_or_path` instead.'
-            )
-
-        _ds_config = deepspeed.runtime.config.DeepSpeedConfig(
-            config_dict_or_path,
-            mpu) if config_dict_or_path is not None else None
-        super().__init__(enabled=enabled,
-                         mem_efficient_linear=mem_efficient_linear,
-                         ds_config=_ds_config,
-                         dtype=dtype)
+                f'zero.Init: the `config` argument is deprecated. Please use `config_dict_or_path` instead.')
+        _ds_config = deepspeed.runtime.config.DeepSpeedConfig(config_dict_or_path,
+                                                              mpu) if config_dict_or_path is not None else None
+        if _ds_config is not None:
+            mem_efficient_linear = _ds_config.zero_config.memory_efficient_linear
+        super().__init__(enabled=enabled, mem_efficient_linear=mem_efficient_linear, ds_config=_ds_config, dtype=dtype)
         if not dist.is_initialized():
             init_distributed()
             assert dist.is_initialized(), "Parameters cannot be scattered without initializing deepspeed.comm"
         if data_parallel_group is None:
             self.ds_process_group = dist.get_world_group()
         else:
             self.ds_process_group = data_parallel_group
 
         self.rank = dist.get_rank(group=self.ds_process_group)
         self.world_size = dist.get_world_size(group=self.ds_process_group)
 
         # Local device is the device where the parameters are consumed, must be default device.
         # It is the device where parameters are fully instantiated using allgather
-        self.local_device = torch.device(get_accelerator().device_name(
-            os.environ["LOCAL_RANK"]))
+        self.local_device = torch.device(get_accelerator().device_name(os.environ["LOCAL_RANK"]))
         get_accelerator().set_device(self.local_device)
 
-        if _ds_config is not None and _ds_config.zero_config.offload_param is not None:
-            remote_device = _ds_config.zero_config.offload_param.device
-            pin_memory = _ds_config.zero_config.offload_param.pin_memory
+        if _ds_config is not None:
+            self._update_persist_config(_ds_config)
+
+            if _ds_config.zero_config.offload_param is not None:
+                remote_device = _ds_config.zero_config.offload_param.device
+                pin_memory = _ds_config.zero_config.offload_param.pin_memory
 
         self._validate_remote_device(remote_device, _ds_config)
 
         # Remote device is the device where parameter partitions are stored
         # It can be same as local_device or it could be CPU or NVMe.
-        self.remote_device = self.local_device if remote_device in [
-            None,
-            OffloadDeviceEnum.none
-        ] else remote_device
-        self.pin_memory = pin_memory if (
-            self.remote_device in [OffloadDeviceEnum.cpu,
-                                   OffloadDeviceEnum.nvme]) else False
+        self.remote_device = self.local_device if remote_device in [None, OffloadDeviceEnum.none] else remote_device
+        self.pin_memory = pin_memory if (self.remote_device in [OffloadDeviceEnum.cpu, OffloadDeviceEnum.nvme
+                                                                ]) else False
 
         # Enable fp16 param swapping to NVMe
         if self.remote_device == OffloadDeviceEnum.nvme:
             self.param_swapper = AsyncPartitionedParameterSwapper(_ds_config, self.dtype)
         else:
             self.param_swapper = None
 
         # If we are provided an already-allocated module to prepare.
         if module is not None:
             assert isinstance(module, torch.nn.Module)
             self._convert_to_zero_parameters(module.parameters(recurse=True))
 
-        self.use_all_gather_base = False
-        if dist.has_allgather_base():
-            self.use_all_gather_base = True
-        else:
-            logger.info(
-                f"_all_gather_base API is not available in torch {torch.__version__}")
+        self.use_all_gather_into_tensor = dist.has_all_gather_into_tensor()
+        if not self.use_all_gather_into_tensor:
+            logger.info(f"all_gather_into_tensor API is not available in torch {torch.__version__}")
+
+    def _update_persist_config(self, ds_config):
+        Init.apply_param_persistence = True
+        Init.param_persistence_threshold = ds_config.zero_config.param_persistence_threshold
+        Init.model_persistence_threshold = ds_config.zero_config.model_persistence_threshold // self.world_size
 
     def _convert_to_zero_parameters(self, param_list):
         for param in param_list:
             if is_zero_param(param):
                 continue
             self._convert_to_deepspeed_param(param)
             param.partition()
@@ -733,26 +764,23 @@
 
                 assert ds_config.zero_config.offload_param.nvme_path is not None, \
                 f'"nvme_path" in DeepSpeed Config cannot be None if remote device is {OffloadDeviceEnum.nvme}'
 
     def _post_init_method(self, module):
         #see_memory_usage(f"Before converting parmas in {module.__class__.__name__}", force=False)
         print_rank_0(f'Converting Params in {module.__class__.__name__}', force=False)
-        see_memory_usage(
-            f"Before converting and partitioning parmas in {module.__class__.__name__}",
-            force=False)
+        see_memory_usage(f"Before converting and partitioning parmas in {module.__class__.__name__}", force=False)
 
         global param_count
         for name, param in module.named_parameters(recurse=False):
             param_count += param.numel()
             if not is_zero_param(param):
                 self._convert_to_deepspeed_param(param)
                 print_rank_0(
-                    f"Partitioning param {debug_param2name_id_shape(param)} module={debug_module2name(module)}"
-                )
+                    f"Partitioning param {debug_param2name_id_shape(param)} module={debug_module2name(module)}")
 
                 if get_accelerator().on_accelerator(param):
                     dist.broadcast(param, 0, self.ds_process_group)
                 else:
                     if dist.get_rank() == 0:
                         logger.warn(f"param `{name}` in {module.__class__.__name__} "
                                     f"not on GPU so was not broadcasted from rank 0")
@@ -780,15 +808,20 @@
         param.ds_tensor = None
 
         # Keeps track of how many active sub-modules need this param at any given point in time
         param.ds_active_sub_modules = set()
 
         # If this flag is true, then the parameters are replicated throughput training
         # And only partitioned before the step
-        param.ds_persist = False
+        if Init.apply_param_persistence and param.ds_numel <= Init.param_persistence_threshold and Init.num_persisted_elements + param.ds_numel <= Init.model_persistence_threshold:
+            param.ds_persist = True
+            Init.num_persisted_parameters += 1
+            Init.num_persisted_elements += param.ds_numel
+        else:
+            param.ds_persist = False
 
         param.is_external_param = False
 
         # The group that the parameter is scattered across.
         param.ds_process_group = self.ds_process_group
 
         # This is set to the Async Param swapper if remote device is nvme
@@ -802,20 +835,22 @@
         def all_gather(param_list=None, async_op=False, hierarchy=0):
             cls = param
             if param_list is None:
                 param_list = [cls]
             return self._all_gather(param_list, async_op=async_op, hierarchy=hierarchy)
 
         @instrument_w_nvtx
-        def all_gather_coalesced(params: Iterable[Parameter],
-                                 safe_mode: bool = False) -> AllGatherCoalescedHandle:
+        def all_gather_coalesced(params: Iterable[Parameter], safe_mode: bool = False) -> AllGatherCoalescedHandle:
 
             # fetches from nvme if the partition is not available and in nvme
             self._ensure_availability_of_partitioned_params(params)
 
+            if self.world_size == 1:
+                return _no_gather_coalesced(params)
+
             for param in params:
                 if param.ds_status != ZeroParamStatus.NOT_AVAILABLE:
                     raise RuntimeError(param.ds_summary())
                 param.ds_status = ZeroParamStatus.INFLIGHT
 
             # ensure that each rank has params in same order. the allgather
             # is done by flattening the parameter list into a single tensor that
@@ -841,87 +876,65 @@
                 param, = params
                 param_buffer = torch.empty(
                     math.ceil(param.ds_numel / self.world_size) * self.world_size,
                     dtype=param.dtype,
                     device=get_accelerator().current_device_name(),
                     requires_grad=False,
                 )
-                handle = _dist_allgather_fn(
-                    param.ds_tensor.to(get_accelerator().current_device_name()),
-                    param_buffer,
-                    self.ds_process_group)
-                param.data = param_buffer.narrow(0,
-                                                 0,
-                                                 param.ds_numel).view(param.ds_shape).to(
-                                                     param.device)
+                handle = _dist_allgather_fn(param.ds_tensor.to(get_accelerator().current_device_name()), param_buffer,
+                                            self.ds_process_group)
+                param.data = param_buffer.narrow(0, 0, param.ds_numel).view(param.ds_shape).to(param.device)
                 return AllGatherHandle(handle, param)
             else:
                 partition_sz = sum(p.ds_tensor.ds_numel for p in params)
                 flat_tensor = torch.empty(partition_sz * self.world_size,
-                                          dtype=get_only_unique_item(p.dtype
-                                                                     for p in params),
+                                          dtype=get_only_unique_item(p.dtype for p in params),
                                           device=get_accelerator().current_device_name(),
                                           requires_grad=False)
                 partitions: List[Parameter] = []
                 for i in range(self.world_size):
-                    partitions.append(
-                        flat_tensor.narrow(0,
-                                           partition_sz * i,
-                                           partition_sz))
-
-                instrument_w_nvtx(torch.cat)([
-                    p.ds_tensor.to(get_accelerator().current_device_name())
-                    for p in params
-                ],
+                    partitions.append(flat_tensor.narrow(0, partition_sz * i, partition_sz))
+
+                instrument_w_nvtx(torch.cat)([p.ds_tensor.to(get_accelerator().current_device_name()) for p in params],
                                              out=partitions[self.rank])
-                handle = _dist_allgather_fn(partitions[self.rank],
-                                            flat_tensor,
-                                            self.ds_process_group)
+                handle = _dist_allgather_fn(partitions[self.rank], flat_tensor, self.ds_process_group)
 
                 return AllGatherCoalescedHandle(
                     allgather_handle=handle,
                     params=params,
                     partitions=partitions,
                     world_size=self.world_size,
                 )
 
         def partition(param_list=None, hierarchy=0, has_been_updated=False):
             cls = param
-            print_rank_0(
-                f"{'--'*hierarchy}----Partitioning param {debug_param2name_id_shape_device(cls)}"
-            )
+            print_rank_0(f"{'--'*hierarchy}----Partitioning param {debug_param2name_id_shape_device(cls)}")
             if param_list is None:
                 param_list = [cls]
             self._partition(param_list, has_been_updated=has_been_updated)
 
         def reduce_gradients_at_owner(param_list=None, hierarchy=0):
             cls = param
             if param_list is None:
                 param_list = [cls]
             print_rank_0(
                 f"{'--'*hierarchy}----Reducing Gradients for param with ids {[param.ds_id for param in param_list]} to owner"
             )
             self._reduce_scatter_gradients(param_list)
 
-        def partition_gradients(param_list=None,
-                                partition_buffers=None,
-                                hierarchy=0,
-                                accumulate=False):
+        def partition_gradients(param_list=None, partition_buffers=None, hierarchy=0, accumulate=False):
             cls = param
             print_rank_0(
-                f"{'--'*hierarchy}----Partitioning param gradient with id {debug_param2name_id_shape_device(cls)}"
-            )
+                f"{'--'*hierarchy}----Partitioning param gradient with id {debug_param2name_id_shape_device(cls)}")
             if param_list is None:
                 param_list = [cls]
                 if isinstance(partition_buffers, torch.Tensor):
                     partition_buffers = [partition_buffers]
 
-            self._partition_gradients(param_list,
-                                      partition_buffers=partition_buffers,
-                                      accumulate=accumulate)
+            self._partition_gradients(param_list, partition_buffers=partition_buffers, accumulate=accumulate)
 
         def aligned_size():
             return self._aligned_size(param)
 
         def padding_size():
             return self._padding_size(param)
 
@@ -946,14 +959,15 @@
                 "active_sub_modules": slf.ds_active_sub_modules,
             }
 
         def convert_to_zero_parameters(param_list):
             self._convert_to_zero_parameters(param_list)
 
         def allgather_before(func: Callable) -> Callable:
+
             def wrapped(*args, **kwargs):
                 param.all_gather()
                 return func(*args, **kwargs)
 
             return wrapped
 
         # Collectives for gathering and partitioning parameters
@@ -1007,17 +1021,15 @@
         self._ensure_availability_of_partitioned_params(param_list)
 
         handles = []
         all_gather_list = []
         for param in param_list:
             if param.ds_status == ZeroParamStatus.NOT_AVAILABLE:
                 if async_op:
-                    handle = self._allgather_param(param,
-                                                   async_op=async_op,
-                                                   hierarchy=hierarchy)
+                    handle = self._allgather_param(param, async_op=async_op, hierarchy=hierarchy)
                     param.ds_status = ZeroParamStatus.INFLIGHT  # if async_op else ZeroParamStatus.AVAILABLE
                     handles.append(handle)
                 else:
                     all_gather_list.append(param)
 
         if not async_op:
             if len(param_list) == 1:
@@ -1046,17 +1058,15 @@
     @instrument_w_nvtx
     def _partition_param(self, param, buffer=None, has_been_updated=False):
         assert param.ds_status is not ZeroParamStatus.INFLIGHT, f" {param} Cannot partition a param in flight"
 
         global reuse_buffers
         #print_rank_0(f"Param id {param.ds_id} status is {param.ds_status}")
         if param.ds_status is ZeroParamStatus.AVAILABLE:
-            print_rank_0(
-                f"Partitioning param id {param.ds_id} reuse buffers {reuse_buffers}",
-                force=False)
+            print_rank_0(f"Partitioning param id {param.ds_id} reuse buffers {reuse_buffers}", force=False)
             # if reuse_buffers and False:
             #     numel = buffer.numel()
             #     buffer = param.data.view(-1)
             #     print_rank_0(
             #         "Returning buffer for param {param.ds_id} with numel {param.ds_numel} to empty buffers",
             #         force=False)
             #     if numel in empty_buffers:
@@ -1064,56 +1074,50 @@
 
             # if deepspeed.comm.get_rank():
             #    print(f"Releasing {param.data.numel()}")
             if param.ds_tensor is not None and not has_been_updated:
 
                 #param.data = param.ds_tensor.data
 
-                see_memory_usage(
-                    f'Before partitioning param {param.ds_id} {param.shape}',
-                    force=False)
+                see_memory_usage(f'Before partitioning param {param.ds_id} {param.shape}', force=False)
                 # param.data does not store anything meaningful in partitioned state
                 free_param(param)
-                see_memory_usage(f'After partitioning param {param.ds_id} {param.shape}',
-                                 force=False)
+                see_memory_usage(f'After partitioning param {param.ds_id} {param.shape}', force=False)
 
                 if param.ds_tensor.final_location == OffloadDeviceEnum.nvme:
-                    print_rank_0(
-                        f"Param {param.ds_id} partition released since it exists in nvme",
-                        force=False)
+                    print_rank_0(f"Param {param.ds_id} partition released since it exists in nvme", force=False)
                     param.nvme_swapper.remove_partition_and_release_buffers([param])
 
                 return
 
             tensor_size = self._aligned_size(param)
             partition_size = tensor_size // self.world_size
 
             if param.ds_tensor is None:
                 final_location = None
                 if self.remote_device == OffloadDeviceEnum.nvme and self.param_swapper.swappable_tensor(
                         numel=partition_size):
                     final_location = OffloadDeviceEnum.nvme
                     buffer = self.param_swapper.get_buffer(param, partition_size)
-                    partitioned_tensor = torch.empty(0,
-                                                     dtype=param.dtype,
-                                                     device=buffer.device)
+                    partitioned_tensor = torch.empty(0, dtype=param.dtype, device=buffer.device)
                     partitioned_tensor.data = buffer.data
-                    print_rank_0(
-                        f"ID {param.ds_id} Initializing partition for the first time for nvme offload."
-                    )
+                    print_rank_0(f"ID {param.ds_id} Initializing partition for the first time for nvme offload.")
 
                 else:
-                    partitioned_tensor = torch.empty(
-                        partition_size,
-                        dtype=param.dtype,
-                        device=OffloadDeviceEnum.cpu if self.remote_device
-                        == OffloadDeviceEnum.nvme else self.remote_device)
-                    if self.pin_memory:
-                        partitioned_tensor = get_accelerator().pin_memory(
-                            partitioned_tensor)
+                    if param.ds_persist:
+                        device = self.local_device
+                    elif self.remote_device == OffloadDeviceEnum.nvme:
+                        device = OffloadDeviceEnum.cpu
+                    else:
+                        device = self.remote_device
+
+                    partitioned_tensor = torch.empty(partition_size, dtype=param.dtype, device=device)
+
+                    if device == OffloadDeviceEnum.cpu and self.pin_memory:
+                        partitioned_tensor = get_accelerator().pin_memory(partitioned_tensor)
 
                 partitioned_tensor.requires_grad = False
                 param.ds_tensor = partitioned_tensor
                 param.ds_tensor.ds_numel = partition_size
                 param.ds_tensor.status = PartitionedParamStatus.AVAILABLE
                 param.ds_tensor.final_location = final_location
 
@@ -1131,47 +1135,35 @@
             else:
                 # partitioned_tensor = torch.zeros(partition_size,
                 #                                  dtype=param.dtype,
                 #                                  device=self.remote_device )
 
                 if start < param.ds_numel:
                     elements_to_copy = param.ds_numel - start
-                    param.ds_tensor.narrow(0,
-                                           0,
-                                           elements_to_copy).copy_(
-                                               one_dim_param.narrow(
-                                                   0,
-                                                   start,
-                                                   elements_to_copy))
+                    param.ds_tensor.narrow(0, 0,
+                                           elements_to_copy).copy_(one_dim_param.narrow(0, start, elements_to_copy))
 
             #print(f"Remote device {self.remote_device}")
 
             #param.ds_tensor = partitioned_tensor
 
             #param.data = param.ds_tensor.data
 
             # param.data does not store anything meaningful in partitioned state
 
-            see_memory_usage(f'Before partitioning param {param.ds_id} {param.shape}',
-                             force=False)
+            see_memory_usage(f'Before partitioning param {param.ds_id} {param.shape}', force=False)
             free_param(param)
-            see_memory_usage(f'After partitioning param {param.ds_id} {param.shape}',
-                             force=False)
+            see_memory_usage(f'After partitioning param {param.ds_id} {param.shape}', force=False)
 
             if param.ds_tensor.final_location == OffloadDeviceEnum.nvme:
                 self.param_swapper.swap_out_and_release([param])
-                print_rank_0(
-                    f"ID {param.ds_id} Offloaded to nvme offload and buffers released.")
-                see_memory_usage(
-                    f"ID {param.ds_id} Offloaded to nvme offload and buffers released.",
-                    force=False)
+                print_rank_0(f"ID {param.ds_id} Offloaded to nvme offload and buffers released.")
+                see_memory_usage(f"ID {param.ds_id} Offloaded to nvme offload and buffers released.", force=False)
 
-            print_rank_0(
-                f"ID {param.ds_id} partitioned type {param.dtype} dev {param.device} shape {param.shape}"
-            )
+            print_rank_0(f"ID {param.ds_id} partitioned type {param.dtype} dev {param.device} shape {param.shape}")
 
     def _param_status(self, param):
         if param.ds_tensor is not None:
             print_rank_0(
                 f"Param id {param.ds_id}, param status: {param.ds_status}, param numel {param.ds_numel}, partitioned numel {param.ds_tensor.numel()}, data numel {param.data.numel()}"
             )
         else:
@@ -1190,17 +1182,15 @@
         print_rank_0(
             f"{'--'* hierarchy}---- Before allocating allgather param {debug_param2name_id_shape_status(param)} partition size={partition_size}"
         )
 
         see_memory_usage(
             f'Before allocate allgather param {debug_param2name_id_shape_status(param)} partition_size={partition_size} ',
             force=False)
-        flat_tensor = torch.zeros(aligned_param_size,
-                                  dtype=param.dtype,
-                                  device=param.device).view(-1)
+        flat_tensor = torch.zeros(aligned_param_size, dtype=param.dtype, device=param.device).view(-1)
         see_memory_usage(
             f'After allocate allgather param {debug_param2name_id_shape_status(param)} {aligned_param_size} {partition_size} ',
             force=False)
 
         get_accelerator().synchronize()
 
         print_rank_0(
@@ -1208,165 +1198,145 @@
         )
         #        if not flat_tensor.numel() > 100000:
         #            replicated_tensor = flat_tensor.narrow(0,
         #                                                   0,
         #                                                   param.ds_numel).view(param.ds_shape)
         #            param.data = replicated_tensor.data
         #            return None
-        if self.use_all_gather_base:
-            # try the _all_gather_base on PyTorch master branch
-            handle = dist.all_gather_base(flat_tensor,
-                                          param.ds_tensor.to(
-                                              get_accelerator().device_name()),
-                                          group=self.ds_process_group,
-                                          async_op=async_op)
+        if self.use_all_gather_into_tensor:
+            # try the all_gather_into_tensor on PyTorch master branch
+            handle = dist.all_gather_into_tensor(flat_tensor,
+                                                 param.ds_tensor.to(get_accelerator().device_name()),
+                                                 group=self.ds_process_group,
+                                                 async_op=async_op)
         else:
             partitions = []
             for i in range(self.world_size):
-                partitions.append(
-                    flat_tensor.narrow(0,
-                                       partition_size * i,
-                                       partition_size))
+                partitions.append(flat_tensor.narrow(0, partition_size * i, partition_size))
 
                 if i == dist.get_rank(group=self.ds_process_group):
                     partitions[i].data.copy_(param.ds_tensor.data, non_blocking=True)
 
-            handle = dist.all_gather(partitions,
-                                     partitions[self.rank],
-                                     group=self.ds_process_group,
-                                     async_op=async_op)
+            handle = dist.all_gather(partitions, partitions[self.rank], group=self.ds_process_group, async_op=async_op)
 
         replicated_tensor = flat_tensor.narrow(0, 0, param.ds_numel).view(param.ds_shape)
         param.data = replicated_tensor.data
         return handle
 
     def _allgather_params_coalesced(self, param_list, hierarchy=0):
         """ blocking call
         avoid explicit memory copy in _allgather_params
         """
         if len(param_list) == 0:
             return
+
+        if self.world_size == 1:
+            handle = _no_gather_coalesced(param_list)
+            handle.wait()
+            return None
+
         # collect local tensors and partition sizes
         partition_sizes = []
         local_tensors = []
         for param in param_list:
             partition_sizes.append(param.ds_tensor.ds_numel)
             local_tensors.append(param.ds_tensor.to(get_accelerator().device_name()))
 
         # allocate memory for allgather params
         allgather_params = []
         for psize in partition_sizes:
             tensor_size = psize * self.world_size
-            flat_tensor = torch.empty(tensor_size,
-                                      dtype=param_list[0].dtype,
-                                      device=self.local_device).view(-1)
+            flat_tensor = torch.empty(tensor_size, dtype=param_list[0].dtype, device=self.local_device).view(-1)
             flat_tensor.requires_grad = False
             allgather_params.append(flat_tensor)
 
         # launch
         launch_handles = []
         # backend = get_backend(self.ds_process_group)
         # with _batch_p2p_manager(backend):
         for param_idx, param in enumerate(param_list):
             input_tensor = local_tensors[param_idx].view(-1)
 
-            if self.use_all_gather_base:
-                # try the _all_gather_base from Pytorch master
-                h = dist.all_gather_base(allgather_params[param_idx],
-                                         input_tensor,
-                                         group=self.ds_process_group,
-                                         async_op=True)
+            if self.use_all_gather_into_tensor:
+                # try the all_gather_into_tensor from Pytorch master
+                h = dist.all_gather_into_tensor(allgather_params[param_idx],
+                                                input_tensor,
+                                                group=self.ds_process_group,
+                                                async_op=True)
             else:
                 output_list = []
                 for i in range(self.world_size):
                     psize = partition_sizes[param_idx]
                     partition = allgather_params[param_idx].narrow(0, i * psize, psize)
                     output_list.append(partition)
                     if not get_accelerator().on_accelerator(partition):
                         logger.warning(
-                            f'param {param_idx}, partition {i} is not on CUDA, partition shape {partition.size()}'
-                        )
+                            f'param {param_idx}, partition {i} is not on CUDA, partition shape {partition.size()}')
 
                 # back to old all_gather function signature
-                h = dist.all_gather(output_list,
-                                    input_tensor,
-                                    group=self.ds_process_group,
-                                    async_op=True)
+                h = dist.all_gather(output_list, input_tensor, group=self.ds_process_group, async_op=True)
             launch_handles.append(h)
 
         # Wait ensures the operation is enqueued, but not necessarily complete.
         launch_handles[-1].wait()
 
         # assign to param.data (not copy)
         for i, param in enumerate(param_list):
             gathered_tensor = allgather_params[i]
-            param.data = gathered_tensor.narrow(0,
-                                                0,
-                                                param.ds_numel).view(param.ds_shape).data
+            param.data = gathered_tensor.narrow(0, 0, param.ds_numel).view(param.ds_shape).data
 
         # guarantee the communication to be completed
         get_accelerator().synchronize()
 
         return None
 
     def _allgather_params(self, param_list, hierarchy=0):
         if len(param_list) == 0:
             return
 
         partition_size = sum([param.ds_tensor.ds_numel for param in param_list])
 
         tensor_size = partition_size * self.world_size
-        flat_tensor = torch.empty(tensor_size,
-                                  dtype=param_list[0].dtype,
-                                  device=self.local_device)
+        flat_tensor = torch.empty(tensor_size, dtype=param_list[0].dtype, device=self.local_device)
         flat_tensor.requires_grad = False
         partitions = []
         for i in range(self.world_size):
             start = partition_size * i
 
             partitions.append(flat_tensor.narrow(0, start, partition_size))
 
             if i == self.rank:
                 offset = 0
                 for param in param_list:
                     param_numel = param.ds_tensor.ds_numel
 
-                    partitions[i].narrow(0,
-                                         offset,
-                                         param_numel).copy_(param.ds_tensor.data)
+                    partitions[i].narrow(0, offset, param_numel).copy_(param.ds_tensor.data)
 
                     offset += param_numel
 
-        dist.all_gather(partitions,
-                        partitions[self.rank],
-                        group=self.ds_process_group,
-                        async_op=False)
+        dist.all_gather(partitions, partitions[self.rank], group=self.ds_process_group, async_op=False)
         param_offset = 0
 
         for param in param_list:
             param_partition_size = param.ds_tensor.ds_numel
             param_size = param.ds_numel
-            replicated_tensor = torch.empty(param.ds_shape,
-                                            dtype=param.dtype,
-                                            device=self.local_device)
+            replicated_tensor = torch.empty(param.ds_shape, dtype=param.dtype, device=self.local_device)
 
             for i in range(self.world_size):
 
                 start = i * partition_size
 
                 param_start = i * param_partition_size
 
                 if param_start < param_size:
                     numel_to_copy = min(param_size - param_start, param_partition_size)
 
                     part_to_copy = partitions[i].narrow(0, param_offset, numel_to_copy)
 
-                    replicated_tensor.view(-1).narrow(0,
-                                                      param_start,
-                                                      numel_to_copy).copy_(part_to_copy)
+                    replicated_tensor.view(-1).narrow(0, param_start, numel_to_copy).copy_(part_to_copy)
             #param_offset += param.data.numel()
             param_offset += param.ds_tensor.ds_numel
 
             param.data = replicated_tensor.data
 
         return None
 
@@ -1390,20 +1360,15 @@
             # to be copied in
             partition_size = param.ds_tensor.ds_numel
             start = self.rank * partition_size
             end = start + partition_size
             #print_rank_0("REduce scatter was executed for praam {param.ds_id}")
             if start < param.ds_numel and end > param.ds_numel:
                 elements = param.ds_numel - start
-                param.grad.view(-1).narrow(0,
-                                           start,
-                                           elements).copy_(
-                                               reduced_partition.narrow(0,
-                                                                        0,
-                                                                        elements))
+                param.grad.view(-1).narrow(0, start, elements).copy_(reduced_partition.narrow(0, 0, elements))
 
     def _reduce_scatter_gradient(self, param):
 
         partition_size = param.ds_tensor.ds_numel
         #output = torch.empty(partition_size, dtype=param.dtype, device=param.device)
 
         total_size = partition_size * self.world_size
@@ -1414,61 +1379,47 @@
             start = i * partition_size
             end = start + partition_size
 
             #print("before reduce scatter gradients")
             if start < param.ds_numel and end <= param.ds_numel:
                 input = param.grad.view(-1).narrow(0, start, partition_size)
             else:
-                input = torch.zeros(partition_size,
-                                    dtype=param.dtype,
-                                    device=param.device)
+                input = torch.zeros(partition_size, dtype=param.dtype, device=param.device)
 
                 if start < param.ds_numel:
                     elements = param.ds_numel - start
-                    input.narrow(0,
-                                 0,
-                                 elements).copy_(
-                                     param.grad.view(-1).narrow(0,
-                                                                start,
-                                                                elements))
+                    input.narrow(0, 0, elements).copy_(param.grad.view(-1).narrow(0, start, elements))
             #print("after reduce scatter gradients")
             input_list.append(input)
 
         rank = dist.get_rank(group=self.ds_process_group)
-        handle = dist.reduce_scatter(input_list[rank],
-                                     input_list,
-                                     group=self.ds_process_group,
-                                     async_op=True)
+        handle = dist.reduce_scatter(input_list[rank], input_list, group=self.ds_process_group, async_op=True)
 
         return handle, input_list[rank]
 
     def _partition_gradients(self, param_list, partition_buffers=None, accumulate=False):
         if partition_buffers is None:
             partition_buffers = [None] * len(param_list)
 
         for param, partition_buffer in zip(param_list, partition_buffers):
-            self._partition_gradient(param,
-                                     partition_buffer=partition_buffer,
-                                     accumulate=accumulate)
+            self._partition_gradient(param, partition_buffer=partition_buffer, accumulate=accumulate)
 
     def _partition_gradient(self, param, partition_buffer=None, accumulate=False):
         #import pdb;pdb.set_trace()
         # param.grad=None
         # param.grad.test()
         print_rank_0(
             f"Partitioning param {param.ds_id} gradient of size {param.grad.numel()} type {param.grad.dtype} part_size {param.ds_tensor.ds_numel}"
         )
         see_memory_usage("Before partitioning gradients", force=False)
         partition_size = param.ds_tensor.ds_numel
 
         if partition_buffer is None:
             assert not accumulate, "No buffer to accumulate to"
-            partition_buffer = torch.zeros(partition_size,
-                                           dtype=param.dtype,
-                                           device=param.device)
+            partition_buffer = torch.zeros(partition_size, dtype=param.dtype, device=param.device)
         else:
             assert partition_buffer.numel(
             ) >= partition_size, f"The partition buffer size {partition_buffer.numel()} should match the size of param.ds_tensor {partition_size}"
 
         rank = dist.get_rank(group=self.ds_process_group)
         start = partition_size * rank
         end = start + partition_size
@@ -1492,17 +1443,15 @@
                 dest_tensor.add_(src_tensor)
 
             # if source and destination are on different device, copy first to src
             # then add and move back to the destination. This seems to run faster
             # when src is gpu and dest is cpu
             # adding directly to cpu is very slow
             else:
-                acc_tensor = torch.empty(src_tensor.numel(),
-                                         dtype=param.dtype,
-                                         device=param.device)
+                acc_tensor = torch.empty(src_tensor.numel(), dtype=param.dtype, device=param.device)
 
                 acc_tensor.copy_(dest_tensor)
                 acc_tensor.add_(src_tensor)
                 dest_tensor.copy_(acc_tensor)
 
             # partition_buffer.view(-1).narrow(
             #     0,
@@ -1513,14 +1462,15 @@
 
         #print("after partition gradients")
         param.grad.data = dest_tensor_full_buffer.data
         see_memory_usage("After partitioning gradients", force=False)
 
 
 class GatheredParameters:
+
     def __init__(self, params, modifier_rank=None, fwd_module=None, enabled=True):
         """A context that collects parameters that were partitioned via a
         :class:`deepspeed.zero.Init` context. The parameters are partitioned
         again upon exit.
 
         Args:
             params (``torch.nn.Parameter``): A single parameter, or an iterable of parameters (list, tuple, generator) of parameters to collect.
@@ -1623,16 +1573,15 @@
         self.params = [p for p in params if hasattr(p, "ds_id")]
         self.src_rank = None
         if modifier_rank is not None:
             if self.params[0].ds_process_group == dist.get_world_group():
                 self.src_rank = modifier_rank
             else:
                 # A group was specified; convert DP rank to global rank
-                self.src_rank = dist.get_global_rank(self.params[0].ds_process_group,
-                                                     modifier_rank)
+                self.src_rank = dist.get_global_rank(self.params[0].ds_process_group, modifier_rank)
         self.fwd_module = fwd_module
         if self.fwd_module is not None:
             # is a no-op if already registered
             for p in self.params:
                 register_external_parameter(self.fwd_module, p)
 
     def __enter__(self):
@@ -1643,16 +1592,11 @@
     def __exit__(self, *exc):
         if not self.enabled:
             return
         if self.src_rank is None:
             self.params[0].partition(param_list=self.params, has_been_updated=False)
             return
 
-        handles = [
-            dist.broadcast(p,
-                           self.src_rank,
-                           group=p.ds_process_group,
-                           async_op=True) for p in self.params
-        ]
+        handles = [dist.broadcast(p, self.src_rank, group=p.ds_process_group, async_op=True) for p in self.params]
         for h in handles:
             h.wait()
         self.params[0].partition(param_list=self.params, has_been_updated=True)
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/zero/partitioned_param_coordinator.py` & `deepspeed-0.9.0/deepspeed/runtime/zero/partitioned_param_coordinator.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,11 +1,11 @@
-"""
-"Copyright 2020 The Microsoft DeepSpeed Team.
-Licensed under the MIT license.
-"""
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 from dataclasses import dataclass
 import collections
 from collections import UserDict
 from typing import Deque, Set
 
 from deepspeed import comm as dist
@@ -20,16 +20,15 @@
 def debug_rank0(message: str) -> None:
     if dist.get_rank() == 0:
         logger.debug(message)
 
 
 @instrument_w_nvtx
 def get_all_parameters(sub_module, recurse=False):
-    return itertools.chain(sub_module.named_parameters(recurse=recurse),
-                           sub_module.ds_external_parameters())
+    return itertools.chain(sub_module.named_parameters(recurse=recurse), sub_module.ds_external_parameters())
 
 
 def iter_params(module: Module, recurse=False) -> Iterable[Parameter]:
     return map(lambda pair: pair[1], get_all_parameters(module, recurse))
 
 
 class ZeRoTraceMode(Enum):
@@ -39,25 +38,23 @@
     COMPLETE = 2
     # Recorded trace does not match current forward+backward or forward pass.
     INVALID = 3
 
 
 class PartitionedParameterCoordinator:
     """Handles partitioning and gathering of parameters."""
+
     class __InflightParamRegistry(UserDict):
         """registry for parameters in flight"""
-        def __setitem__(self,
-                        param: Parameter,
-                        handle: AllGatherCoalescedHandle) -> None:
+
+        def __setitem__(self, param: Parameter, handle: AllGatherCoalescedHandle) -> None:
             if param in self.data:
                 raise RuntimeError(f"{param.ds_summary()} already in registry")
             if param.ds_status != ZeroParamStatus.INFLIGHT:
-                raise RuntimeError(
-                    f"attempted to add non-inflight parameter to registry {param.ds_summary()}"
-                )
+                raise RuntimeError(f"attempted to add non-inflight parameter to registry {param.ds_summary()}")
             self.data[param] = handle
 
     @dataclass
     class __ParamInTrace:
         param: Parameter
         step_id_last_used_at: int
 
@@ -74,18 +71,16 @@
         # keeps track of the number of submodules invoked so far.
         self.__step_id: int = 0
         # network tracing mode
         self.__trace_mode: ZeRoTraceMode = ZeRoTraceMode.RECORD
         # sequence of submodules/parameters in forward pass + backward pass
         self.__submodule_order: Iterable[Module] = []
         self.__param_order: Iterable[__class__.__ParamInTrace] = []
-        self.__most_recent_step_id_param_fetched_for = collections.defaultdict(
-            lambda: int(-1e10))
-        self.__step_id_module_fetched_for = collections.defaultdict(
-            lambda: collections.deque())
+        self.__most_recent_step_id_param_fetched_for = collections.defaultdict(lambda: int(-1e10))
+        self.__step_id_module_fetched_for = collections.defaultdict(lambda: collections.deque())
         # number of available params, and max number of available params
         self.__n_available_params: int = 0
         self.__max_n_available_params: int = max_available_parameters_in_numel
         # max distance between two use of the module beyond which module is released
         self.__max_reuse_dist_in_numel: int = max_reuse_distance_in_numel
         # queue for parameters to fetch. parameters will be popped off the left
         # side of the dequeue as they are fetched
@@ -118,16 +113,15 @@
 
     Bookkeeping operations used to track where we are in the forward/backward pass
     """
 
     def _clear_trace_structures(self) -> None:
         self.__submodule_order = []
         self.__param_order = []
-        self.__most_recent_step_id_param_fetched_for = collections.defaultdict(
-            lambda: int(-1e10))
+        self.__most_recent_step_id_param_fetched_for = collections.defaultdict(lambda: int(-1e10))
         self.__param_queue = None
 
     def is_complete_trace(self) -> bool:
         return self.__trace_mode == ZeRoTraceMode.COMPLETE
 
     def is_invalid_trace(self) -> bool:
         return self.__trace_mode == ZeRoTraceMode.INVALID
@@ -140,94 +134,91 @@
             raise RuntimeError("attempted to invalidate already invalid trace")
         self.__trace_mode = ZeRoTraceMode.INVALID
         self._clear_trace_structures()
 
     def trace_prologue(self, sub_module: Module) -> None:
         if self.is_complete_trace():
             # sub_module must match expectation else invalidate trace cache
+            if len(self.__submodule_order) <= self.__step_id:
+                print_rank_0(
+                    f"Invalidate trace cache @ step {self.__step_id} and module {sub_module.id}: "
+                    f"cache has only {len(self.__submodule_order)} modules",
+                    force=True)
+                self._invalidate_trace()
+                return
+
             if sub_module != self.__submodule_order[self.__step_id]:
                 expected_module_id = self.__submodule_order[self.__step_id].id
-                debug_rank0(
+                print_rank_0(
                     f"Invalidate trace cache @ step {self.__step_id}: "
-                    f"expected module {expected_module_id}, but got module {sub_module.id}"
-                )
+                    f"expected module {expected_module_id}, but got module {sub_module.id}",
+                    force=True)
                 self._invalidate_trace()
 
     def record_module(self, sub_module: Module) -> None:
         """adds sub module to trace"""
         if not self.is_record_trace():
-            raise RuntimeError(
-                f"attempted to record trace when status = {self.__trace_mode}")
+            raise RuntimeError(f"attempted to record trace when status = {self.__trace_mode}")
 
         self.__submodule_order.append(sub_module)
         self.__step_id_module_fetched_for[sub_module.id].append(self.__step_id)
 
     def record_parameters(self, sub_module: Module) -> None:
         """adds sub module to trace"""
         if not self.is_record_trace():
-            raise RuntimeError(
-                f"attempted to record trace when status = {self.__trace_mode}")
+            raise RuntimeError(f"attempted to record trace when status = {self.__trace_mode}")
 
         step_id = self.__step_id_module_fetched_for[sub_module.id].popleft()
         for param in sorted(set(iter_params(sub_module)), key=lambda p: p.ds_id):
-            self.__param_order.append(
-                __class__.__ParamInTrace(param=param,
-                                         step_id_last_used_at=step_id))
+            self.__param_order.append(__class__.__ParamInTrace(param=param, step_id_last_used_at=step_id))
 
     def construct_parameter_trace_from_module_trace(self):
         """use module trace to construct parameter trace"""
         self.__param_order = []
         for sub_module in self.__submodule_order:
             self.record_parameters(sub_module)
 
     def reset_step(self) -> None:
         """indicate that we have completed one fwd+bwd for the model"""
         if self.__inflight_param_registry:
-            raise RuntimeError(
-                f"still have inflight params "
-                f"{[p.ds_summary for p in self.__inflight_param_registry.keys()]}")
+            raise RuntimeError(f"still have inflight params "
+                               f"{[p.ds_summary for p in self.__inflight_param_registry.keys()]}")
 
         if not self.is_complete_trace():  # not self.trace_complete:
             # Make sure that recorded submodule orders are identical across ranks
             assert_ints_same_as_other_ranks([m.id for m in self.__submodule_order])
 
             if self.is_record_trace():
                 # Successfully recorded a trace
                 self.construct_parameter_trace_from_module_trace()
                 # Make sure that recorded parameter orders are identical across ranks
-                assert_ints_same_as_other_ranks(
-                    [p.param.ds_id for p in self.__param_order])
-                assert_ints_same_as_other_ranks(
-                    [p.step_id_last_used_at for p in self.__param_order])
+                assert_ints_same_as_other_ranks([p.param.ds_id for p in self.__param_order])
+                assert_ints_same_as_other_ranks([p.step_id_last_used_at for p in self.__param_order])
 
                 self.__submodule_order = tuple(self.__submodule_order)  # freeze
                 self.__param_order = tuple(self.__param_order)  # freeze
                 self.__trace_mode = ZeRoTraceMode.COMPLETE
                 print_rank_0(
-                    f"completed record trace: {[m.id for m in self.__submodule_order]}",
+                    f"completed record trace of {len(self.__submodule_order)} sub modules: {[m.id for m in self.__submodule_order]}",
                     force=False)
             else:
                 # Enable trace recording for next forward/backward pass
                 self.__trace_mode = ZeRoTraceMode.RECORD
 
         self.__param_queue = collections.deque(self.__param_order)  # reset fetch queue
-        self.__most_recent_step_id_param_fetched_for = collections.defaultdict(
-            lambda: int(-1e10))
-        self.__step_id_module_fetched_for = collections.defaultdict(
-            lambda: collections.deque())
+        self.__most_recent_step_id_param_fetched_for = collections.defaultdict(lambda: int(-1e10))
+        self.__step_id_module_fetched_for = collections.defaultdict(lambda: collections.deque())
         self.__step_id = 0
         self.__n_available_params = 0
 
     def _dump_params(self, tag, sub_module, params, step_id=None):
         if step_id is None:
             step_id = self.__step_id
         param_names = [debug_param2name_id(p) for p in params]
-        print(
-            f'{tag} step = {step_id} mod = {debug_module2name_id(sub_module)} p_names = {param_names}'
-        )
+        print(f'{tag} step = {step_id} mod = {debug_module2name_id(sub_module)} p_names = {param_names}')
 
     def _dump_param_ids(self, tag, mod_id, p_ids, step_id=None):
         if step_id is None:
             step_id = self.__step_id
         print(f'{tag} mod = {mod_id}, step = {step_id}, p_ids = {p_ids}')
 
     """Fetch and Release
@@ -259,19 +250,17 @@
 
         # wait for parameters in the immediately needed submodule to become available
         for param in params_to_fetch:
             param.ds_active_sub_modules.add(current_submodule.id)
             debug_rank0(f"-wait: {param.ds_summary()}")
             if param in self.__inflight_param_registry:
                 with get_accelerator().stream(self.__allgather_stream):
-                    while self.__ongoing_fetch_events and self.__ongoing_fetch_events[
-                            0].query():
+                    while self.__ongoing_fetch_events and self.__ongoing_fetch_events[0].query():
                         self.__ongoing_fetch_events.popleft()
-                    if len(self.__ongoing_fetch_events
-                           ) > self.__max_ongoing_fetch_events:
+                    if len(self.__ongoing_fetch_events) > self.__max_ongoing_fetch_events:
                         self.__ongoing_fetch_events.popleft().synchronize()
 
                     self.__inflight_param_registry.pop(param).wait()
 
                     event = get_accelerator().Event()
                     event.record()
                     self.__ongoing_fetch_events.append(event)
@@ -284,51 +273,44 @@
         if self.is_complete_trace():
             # go through the parameters we need for the current module and pop them
             # off the fetch queue so that they aren't prefetched later.
             # if params have already been popped off the fetch queue by earlier
             # prefetches we won't look for them here
             discarded_from_prefetch_queue = set()
             params_not_already_fetched = set(
-                filter(
-                    lambda p: self.__most_recent_step_id_param_fetched_for[p] < self.
-                    __step_id,
-                    params_to_fetch))
-            while self.__param_queue and len(discarded_from_prefetch_queue) < len(
-                    params_not_already_fetched):
+                filter(lambda p: self.__most_recent_step_id_param_fetched_for[p] < self.__step_id, params_to_fetch))
+            while self.__param_queue and len(discarded_from_prefetch_queue) < len(params_not_already_fetched):
                 param_in_trace = self.__param_queue.popleft()
                 self.__most_recent_step_id_param_fetched_for[
                     param_in_trace.param] = param_in_trace.step_id_last_used_at
                 discarded_from_prefetch_queue.add(param_in_trace.param)
 
             if discarded_from_prefetch_queue != params_not_already_fetched:
                 raise RuntimeError(
                     f"tracing error at step {self.__step_id}: \n"
                     f"module id: {current_submodule.id}, training: {current_submodule.training}\n"
                     f"expected the next {len(params_not_already_fetched)} parameters in the "
                     f"parameter fetch queue to be {tuple(p.ds_summary(use_debug_name=True) for p in params_not_already_fetched)} \n"
-                    f"but got \n {tuple(p.ds_summary(use_debug_name=True) for p in discarded_from_prefetch_queue)}."
-                )
+                    f"but got \n {tuple(p.ds_summary(use_debug_name=True) for p in discarded_from_prefetch_queue)}.")
 
             def _is_currently_on_nvme(param):
                 if param.nvme_swapper is None:
                     return False
 
                 return param.ds_tensor.final_location == OffloadDeviceEnum.nvme \
                     and param.ds_tensor.status == PartitionedParamStatus.NOT_AVAILABLE
 
             # kick off all gather for params in the next few submodules (prefetch)
             if self.__prefetch_bucket_sz > 0:
-                max_params_to_prefetch = min(
-                    self.__max_n_available_params - self.__n_available_params,
-                    self.__prefetch_bucket_sz)
+                max_params_to_prefetch = min(self.__max_n_available_params - self.__n_available_params,
+                                             self.__prefetch_bucket_sz)
                 params_to_prefetch = set()
                 numel_prefetching = 0
                 while self.__param_queue and numel_prefetching < max_params_to_prefetch:
-                    param_in_trace: __class__.__ParamInTrace = self.__param_queue.popleft(
-                    )
+                    param_in_trace: __class__.__ParamInTrace = self.__param_queue.popleft()
 
                     if _is_currently_on_nvme(param_in_trace.param):
                         # nvme prefetch is handled elsewhere. Need to break here to preserve fetch order
                         self.__param_queue.appendleft(param_in_trace)
                         break
 
                     do_prefetch = param_in_trace.param.ds_status == ZeroParamStatus.NOT_AVAILABLE
@@ -354,18 +336,16 @@
         self.__step_id += 1
 
     @instrument_w_nvtx
     @torch.no_grad()
     def release_sub_module(self, submodule: Module) -> None:
         """release the parameters of a sub module, assuming they meet conditions to
         be released."""
-        params_to_release = (self.__params_to_release(submodule,
-                                                      self.__step_id)
-                             if self.is_complete_trace() else set(
-                                 p.ds_id for p in iter_params(submodule)))
+        params_to_release = (self.__params_to_release(submodule, self.__step_id) if self.is_complete_trace() else set(
+            p.ds_id for p in iter_params(submodule)))
         for param in iter_params(submodule):
             param.ds_active_sub_modules.discard(submodule.id)
             if param.ds_id in params_to_release and not param.is_external_param:
                 self.__release_param(param)
 
     @instrument_w_nvtx
     @torch.no_grad()
@@ -400,39 +380,33 @@
 
             for param in partitioned_params:
                 assert param.ds_status == ZeroParamStatus.INFLIGHT, param.ds_summary()
                 self.__inflight_param_registry[param] = handle
 
             # Release swap buffers for persisted params on nvme since they will never be partitioned or evicted from GPU
             swap_persisted_params = [
-                p for p in partitioned_params
-                if p.ds_persist and p.ds_tensor.final_location == OffloadDeviceEnum.nvme
+                p for p in partitioned_params if p.ds_persist and p.ds_tensor.final_location == OffloadDeviceEnum.nvme
             ]
             if swap_persisted_params:
-                swap_persisted_params[
-                    0].nvme_swapper.remove_partition_and_release_buffers(
-                        swap_persisted_params)
+                swap_persisted_params[0].nvme_swapper.remove_partition_and_release_buffers(swap_persisted_params)
 
     @instrument_w_nvtx
     def __release_param(self, param: Parameter) -> None:
         if param.ds_status == ZeroParamStatus.AVAILABLE and not param.ds_active_sub_modules:
             debug_rank0(f"-release: {param.ds_summary()}")
             param.partition()
             self.__n_available_params -= param.ds_numel
 
     @instrument_w_nvtx
     @functools.lru_cache(maxsize=None)
-    def __params_to_release(self,
-                            submodule_to_release: Module,
-                            step_id: int) -> Set[int]:
+    def __params_to_release(self, submodule_to_release: Module, step_id: int) -> Set[int]:
         if not self.is_complete_trace():
             raise RuntimeError("expected trace to be complete")
 
-        params_to_release = set(p.ds_id for p in iter_params(submodule_to_release)
-                                if not p.ds_persist)
+        params_to_release = set(p.ds_id for p in iter_params(submodule_to_release) if not p.ds_persist)
 
         # Problem: When prefetcher scans the param trace, it skips AVAILABLE params.
         # This creates issues if those params are released before the skipped uses:
         # 1) It hurts performance as the skipped uses are never prefetched.
         # 2) For nvme params, we run out of swap buffers because the prefetch order
         # diverges from the trace.
         # Solution: Don't release params whose reuse was skipped by prefetch. This is
@@ -466,16 +440,16 @@
 
         numel_considered = 0
         swap_in_params = []
         for param_in_trace in self.__param_queue:
             param = param_in_trace.param
             if param.nvme_swapper is None:
                 continue
-            if (numel_considered > 2 * numel_in_flight or len(swap_in_params) >=
-                    param.nvme_swapper.available_swap_in_buffers()):
+            if (numel_considered > 2 * numel_in_flight
+                    or len(swap_in_params) >= param.nvme_swapper.available_swap_in_buffers()):
                 break
             if param.ds_tensor.status == PartitionedParamStatus.NOT_AVAILABLE:
                 swap_in_params.append(param)
             numel_considered += param.ds_numel
 
         if swap_in_params:
             swap_in_params[0].nvme_swapper.swap_in(swap_in_params, async_op=True)
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/zero/stage3.py` & `deepspeed-0.9.0/deepspeed/runtime/zero/stage3.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,11 +1,11 @@
-"""
-"Copyright 2020 The Microsoft DeepSpeed Team.
-Licensed under the MIT license.
-"""
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import sys
 import gc
 import collections
 from typing import Deque, Dict, Tuple
 
 from deepspeed.runtime import ZeROOptimizer
@@ -29,15 +29,15 @@
 # with gradient partitioning and without
 pg_correctness_test = False
 
 
 def print_rank_0(message, debug=False, force=False):
     rank = dist.get_rank()
     if rank == 0 and (debug or force):
-        print(message)
+        logger.info(message)
     # other variations
     # - print for all ranks w/o interleaving
     # printflock(f"[{rank}] {message}")
     # - print to log file per rank
     # log_rank_file(rank, message)
 
 
@@ -69,14 +69,15 @@
 
     For more details please see ZeRO: Memory Optimization Towards Training A Trillion Parameter Models
     https://arxiv.org/abs/1910.02054
 
     For usage examples, refer to TODO: DeepSpeed Tutorial
 
     """
+
     def __init__(self,
                  module,
                  init_optimizer,
                  timers,
                  ds_config,
                  static_loss_scale=1.0,
                  dynamic_loss_scale=False,
@@ -102,16 +103,15 @@
                  gradient_predivide_factor=1.0,
                  gradient_accumulation_steps=1,
                  elastic_checkpoint=False,
                  aio_config=None):
 
         see_memory_usage("Stage 3 initialize beginning", force=True)
 
-        print_rank_0(f"initialized {__class__.__name__} with args: {locals()}",
-                     force=False)
+        print_rank_0(f"initialized {__class__.__name__} with args: {locals()}", force=False)
 
         if dist.get_rank() == 0:
             logger.info(f"Reduce bucket size {reduce_bucket_size}")
             logger.info(f"Prefetch bucket size {prefetch_bucket_size}")
         # The fused optimizer does all the work. We need this layer for two reason:
         # 1. maintain same user API from apex.fp16_utils
         # 2. keep common stuff here in case we need to add ne552w fused optimizer later
@@ -143,47 +143,43 @@
         self.offload_optimizer_pin_memory = False
         self.offload_optimizer_fast_init = False
         self.offload_param = False
         self.offload_param_pin_memory = False
         self.params_in_nvme_and_cpu = False
         self.max_params_in_cpu = 0
 
-        self.parameter_offload = DeepSpeedZeRoOffload(
-            module=module,
-            timers=timers,
-            ds_config=ds_config,
-            overlap_comm=overlap_comm,
-            prefetch_bucket_size=prefetch_bucket_size,
-            max_reuse_distance=max_reuse_distance,
-            max_live_parameters=max_live_parameters,
-            param_persistence_threshold=param_persistence_threshold,
-            model_persistence_threshold=model_persistence_threshold,
-            offload_param_config=offload_optimizer_config,
-            mpu=mpu)
+        self.parameter_offload = DeepSpeedZeRoOffload(module=module,
+                                                      timers=timers,
+                                                      ds_config=ds_config,
+                                                      overlap_comm=overlap_comm,
+                                                      prefetch_bucket_size=prefetch_bucket_size,
+                                                      max_reuse_distance=max_reuse_distance,
+                                                      max_live_parameters=max_live_parameters,
+                                                      param_persistence_threshold=param_persistence_threshold,
+                                                      model_persistence_threshold=model_persistence_threshold,
+                                                      offload_param_config=offload_optimizer_config,
+                                                      mpu=mpu)
 
         self.persistent_parameters = self.parameter_offload.persistent_parameters
         self._configure_offloading(offload_optimizer_config, offload_param_config)
 
         self.module = module
         self.elastic_checkpoint = elastic_checkpoint
 
-        self.__inf_or_nan_tracker: Tensor = torch.zeros(
-            1,
-            dtype=torch.bool,
-            device=get_accelerator().current_device_name(),
-            requires_grad=False)
+        self.__inf_or_nan_tracker: Tensor = torch.zeros(1,
+                                                        dtype=torch.bool,
+                                                        device=get_accelerator().current_device_name(),
+                                                        requires_grad=False)
 
-        self.deepspeed_adam_offload = (self.offload_optimizer
-                                       and type(init_optimizer) == DeepSpeedCPUAdam)
+        self.deepspeed_adam_offload = (self.offload_optimizer and type(init_optimizer) == DeepSpeedCPUAdam)
 
-        self.device = get_accelerator().current_device_name(
-        ) if not self.offload_optimizer else OffloadDeviceEnum.cpu
+        self.device = get_accelerator().current_device_name() if not self.offload_optimizer else OffloadDeviceEnum.cpu
         ### streams used for overlapping computation with communication
-        self.__reduce_and_partition_stream = get_accelerator().Stream(
-        ) if overlap_comm else get_accelerator().default_stream()
+        self.__reduce_and_partition_stream = get_accelerator().Stream() if overlap_comm else get_accelerator(
+        ).default_stream()
 
         ############################################################################
 
         self.__n_caching_allocator_flushes = 0
 
         #-------------Stage 3 Setup-------------------#
 
@@ -208,15 +204,16 @@
         self.gradient_predivide_factor = gradient_predivide_factor
         self.postscale_gradients = postscale_gradients
         self.gradient_accumulation_steps = gradient_accumulation_steps
         self.micro_step_id = 0
         self.reduce_bucket_size = int(reduce_bucket_size)
 
         if self.reduce_scatter:
-            assert self.communication_data_type in (torch.float16, torch.bfloat16, torch.float32), f"ZeRO-3 supports only float16 or bfloat16 communication_data_type with reduce scatter enabled. Got: '{self.communication_data_type}'"
+            valid_reduce_scatter_dtypes = (torch.float16, torch.bfloat16, torch.float32)
+            assert self.communication_data_type in valid_reduce_scatter_dtypes, f"ZeRO-3 supports {valid_reduce_scatter_dtypes} communication_data_type with reduce scatter enabled. Got: '{self.communication_data_type}'"
             assert self.gradient_predivide_factor == 1.0, "gradient_predivide_factor != 1.0 is not yet supported with ZeRO-3 with reduce scatter enabled"
             assert self.postscale_gradients, "pre-scale gradients is not yet supported with ZeRO-3 with reduce scatter enabled"
 
         # Holds the mode parameter
         # The param.data may not hold any meaningful data
         # when param's status is NOT_AVAILABLE or IN_FLGHT
         self.fp16_groups = []
@@ -254,16 +251,15 @@
 
         # Trainable parameters
         self.trainable_param_groups = self._get_trainable_parameter_groups()
 
         see_memory_usage("Before creating fp16 partitions", force=True)
         self._create_fp16_partitions_with_defragmentation(self.trainable_param_groups)
         num_fp16_subgroups = len(self.fp16_partitioned_groups_flat)
-        see_memory_usage(f"After creating fp16 partitions: {num_fp16_subgroups}",
-                         force=True)
+        see_memory_usage(f"After creating fp16 partitions: {num_fp16_subgroups}", force=True)
 
         # Optimizer tensor swapping
         if self.swap_optimizer:
             self._configure_tensor_swapping(offload_optimizer_config, aio_config)
 
         self.__params_in_ipg_bucket: List[Parameter] = []
         self.is_gradient_accumulation_boundary: bool = True
@@ -296,22 +292,18 @@
                 self.param_id[unique_id] = count
                 self.param_dict[count] = param
                 self.params_already_reduced.append(False)
                 count = count + 1
 
         #Largest partitioned param
         largest_partitioned_param_numel = max([
-            max([
-                max(tensor.numel(),
-                    tensor.ds_numel) for tensor in fp16_partitioned_group
-            ]) for fp16_partitioned_group in self.fp16_partitioned_groups
+            max([max(tensor.numel(), tensor.ds_numel) for tensor in fp16_partitioned_group])
+            for fp16_partitioned_group in self.fp16_partitioned_groups
         ])
-        print_rank_0(
-            f'Largest partitioned param numel = {largest_partitioned_param_numel}',
-            force=False)
+        print_rank_0(f'Largest partitioned param numel = {largest_partitioned_param_numel}', force=False)
 
         self._setup_for_real_optimizer()
         self.grad_position = {}
         self.set_grad_positions()
 
         if self.offload_optimizer:
             self.norm_for_param_grads = {}
@@ -347,17 +339,15 @@
 
     def destroy(self):
         self.parameter_offload.destroy()
 
     def _get_trainable_parameter_groups(self):
         param_groups = []
         for param_group in self.optimizer.param_groups:
-            trainable_params = {
-                "params": [p for p in param_group["params"] if p.requires_grad]
-            }
+            trainable_params = {"params": [p for p in param_group["params"] if p.requires_grad]}
             param_groups.append(trainable_params)
         return param_groups
 
     def _setup_for_real_optimizer(self):
         see_memory_usage("Before creating fp32 partitions", force=True)
         self._create_fp32_partitions()
         see_memory_usage("After creating fp32 partitions", force=True)
@@ -373,39 +363,33 @@
         dist.barrier()
 
         if dist.get_rank() == 0:
             logger.info(f"optimizer state initialized")
 
         # IPG
         if self.contiguous_gradients:
-            self.__ipg_bucket_flat_buffer: Tensor = torch.empty(
-                self.reduce_bucket_size,
-                dtype=self.dtype,
-                device=get_accelerator().current_device_name())
+            self.__ipg_bucket_flat_buffer: Tensor = torch.empty(self.reduce_bucket_size,
+                                                                dtype=self.dtype,
+                                                                device=get_accelerator().current_device_name())
 
         grad_partitions_flat_buffer = None
         self.__param_id_to_grad_partition: Dict[int, Tensor] = {}
 
         all_params = list(itertools.chain.from_iterable(self.fp16_groups))
 
-        grad_partitions_flat_buffer: Tensor = torch.zeros(sum(p.partition_numel()
-                                                              for p in all_params),
+        grad_partitions_flat_buffer: Tensor = torch.zeros(sum(p.partition_numel() for p in all_params),
                                                           dtype=self.dtype,
                                                           device=self.device)
         if self.offload_optimizer_pin_memory:
-            grad_partitions_flat_buffer = get_accelerator().pin_memory(
-                grad_partitions_flat_buffer)
+            grad_partitions_flat_buffer = get_accelerator().pin_memory(grad_partitions_flat_buffer)
 
         offset = 0
         for param in all_params:
-            self.__param_id_to_grad_partition[
-                param.ds_id] = grad_partitions_flat_buffer.narrow(
-                    0,
-                    offset,
-                    param.partition_numel())
+            self.__param_id_to_grad_partition[param.ds_id] = grad_partitions_flat_buffer.narrow(
+                0, offset, param.partition_numel())
             offset += param.partition_numel()
 
     def _link_all_hp_params(self):
         for p in self.module.parameters():
             p._z3_optimizer = self
 
     def set_lr(self, lr):
@@ -473,31 +457,29 @@
             self.params_in_nvme_and_cpu = offload_param_config.device == OffloadDeviceEnum.nvme
             self.max_params_in_cpu = offload_param_config.max_in_cpu
             print_rank_0(
                 f"FP16 params swapping is {self.params_in_nvme_and_cpu}, Max params in CPU is {self.max_params_in_cpu}",
                 force=False)
 
     def _configure_tensor_swapping(self, offload_optimizer_config, aio_config):
-        nvme_swap_folder = os.path.join(offload_optimizer_config.nvme_path,
-                                        'zero_stage_3')
+        nvme_swap_folder = os.path.join(offload_optimizer_config.nvme_path, 'zero_stage_3')
         os.makedirs(nvme_swap_folder, exist_ok=True)
         if dist.get_rank() == 0:
             logger.info(f'Tensor Swapping: Adding optimizer tensors')
 
         swapper_type = PipelinedOptimizerSwapper if offload_optimizer_config.pipeline else PartitionedOptimizerSwapper
 
-        self.optimizer_swapper = swapper_type(
-            swap_config=offload_optimizer_config,
-            aio_config=aio_config,
-            base_folder=nvme_swap_folder,
-            optimizer=self.optimizer,
-            largest_numel=max(self.fp16_partitioned_groups_flat_numel),
-            device=self.device,
-            dtype=torch.float32,
-            timers=self.timers)
+        self.optimizer_swapper = swapper_type(swap_config=offload_optimizer_config,
+                                              aio_config=aio_config,
+                                              base_folder=nvme_swap_folder,
+                                              optimizer=self.optimizer,
+                                              largest_numel=max(self.fp16_partitioned_groups_flat_numel),
+                                              device=self.device,
+                                              dtype=torch.float32,
+                                              timers=self.timers)
 
     @property
     def elements_in_ipg_bucket(self):
         return sum(p.ds_numel for p in self.__params_in_ipg_bucket)
 
     def _move_to_flat_buffer(self, param_list, flat_buffer, avoid_copy=False):
         '''If flat buffer is None then the parameters in the param_list are
@@ -514,16 +496,15 @@
         for param in param_list:
             src = param.ds_tensor
             dest = flat_buffer.narrow(0, start, src.ds_numel)
             start = start + src.ds_numel
             '''if the parameter was initialized in nvme then bring it to the destination buffer directly'''
             if src.status == PartitionedParamStatus.NOT_AVAILABLE:
                 print_rank_0(
-                    f"Swapping in {param.ds_id} with partition size {param.partition_numel()} permanently to CPU"
-                )
+                    f"Swapping in {param.ds_id} with partition size {param.partition_numel()} permanently to CPU")
                 param.nvme_swapper.swap_into_buffer(param, dest)
                 src.data = dest.data
                 src.status = PartitionedParamStatus.AVAILABLE
             else:
                 assert src.status == PartitionedParamStatus.AVAILABLE, "Partitioned Param must be available here"
                 if not avoid_copy:
                     dest.data.copy_(src.data)
@@ -540,66 +521,52 @@
             params_in_group = sum([p.partition_numel() for p in param_group['params']])
 
             flat_buffer_size = params_in_group
 
             if self.params_in_nvme_and_cpu and \
                 aggregate_params_count + params_in_group > self.max_params_in_cpu:
 
-                flat_buffer_size = max(0,
-                                       self.max_params_in_cpu - aggregate_params_count)
+                flat_buffer_size = max(0, self.max_params_in_cpu - aggregate_params_count)
 
             aggregate_params_count += params_in_group
 
             if flat_buffer_size > 0:
-                print_rank_0(f"group {j} flat buffer size {flat_buffer_size}",
-                             force=False)
-                self.param_groups_fp16_flat_cpu_memory.append(
-                    get_accelerator().pin_memory(
-                        torch.empty(int(flat_buffer_size),
-                                    dtype=self.dtype)))
+                print_rank_0(f"group {j} flat buffer size {flat_buffer_size}", force=False)
+                self.param_groups_fp16_flat_cpu_memory.append(get_accelerator().pin_memory(
+                    torch.empty(int(flat_buffer_size), dtype=self.dtype)))
             else:
-                print_rank_0(
-                    f"No flat buffer size. Param group size was  {params_in_group}",
-                    force=False)
+                print_rank_0(f"No flat buffer size. Param group size was  {params_in_group}", force=False)
 
-                self.param_groups_fp16_flat_cpu_memory.append(
-                    torch.empty(1,
-                                dtype=self.dtype))
+                self.param_groups_fp16_flat_cpu_memory.append(torch.empty(1, dtype=self.dtype))
 
     def _create_fp16_partitions_with_defragmentation(self, fp16_param_groups):
         dist.barrier()
 
         param_groups: List[List[Parameter]] = tuple(
-            self._create_fp16_sub_groups(param_group["params"])
-            for param_group in fp16_param_groups)
+            self._create_fp16_sub_groups(param_group["params"]) for param_group in fp16_param_groups)
 
         # bookkeeping related to param groups
         for param_group_idx, param_group in enumerate(param_groups):
             for sub_group in param_group:
                 sub_group_idx = len(self.fp16_groups)
 
                 # record sub group and partitions
                 self.fp16_groups.append(sub_group)
-                self.fp16_partitioned_groups.append(
-                    [param.ds_tensor for param in sub_group])
+                self.fp16_partitioned_groups.append([param.ds_tensor for param in sub_group])
 
                 # record sub group -> group mapping
                 self.sub_group_to_group_id[sub_group_idx] = param_group_idx
 
                 # record total elements of parameter partitions in sub group
-                self.fp16_partitioned_groups_flat_numel.append(
-                    sum(p.partition_numel() for p in sub_group))
+                self.fp16_partitioned_groups_flat_numel.append(sum(p.partition_numel() for p in sub_group))
 
                 # record padding required to align group to world size (only applies to last rank)
                 rank_requires_padding = dist.get_rank(
-                    self.dp_process_group) == dist.get_world_size(
-                        self.dp_process_group) - 1
-                self.groups_padding.append([
-                    p.padding_size() if rank_requires_padding else 0 for p in sub_group
-                ])
+                    self.dp_process_group) == dist.get_world_size(self.dp_process_group) - 1
+                self.groups_padding.append([p.padding_size() if rank_requires_padding else 0 for p in sub_group])
 
         # move parameters to flattened buffer
         if not self.offload_param:  # partitioned params remain in GPU during training
             # move parameter partitions into a single contiguous flat buffer
             parameter_partitions: List[Tensor] = []
             for sub_group in self.fp16_groups:
                 for param in sub_group:
@@ -607,115 +574,101 @@
             device_buffer = __class__.defragment(parameter_partitions)
 
             # setup flat buffers per subgroup, these are each just sections of the
             # contiguous flat buffer for all parameters that we created earlier
             offset = 0
             for sub_group in self.fp16_groups:
                 sub_group_numel = sum(param.partition_numel() for param in sub_group)
-                self.fp16_partitioned_groups_flat.append(
-                    device_buffer.narrow(0,
-                                         offset,
-                                         sub_group_numel))
+                self.fp16_partitioned_groups_flat.append(device_buffer.narrow(0, offset, sub_group_numel))
                 offset += sub_group_numel
         else:  # partitioned params offloaded to CPU when not in use
             # create a flat CPU memory allocation for each param group
             self._create_param_groups_fp16_flat_cpu_memory()
             for param_group_idx, param_group in enumerate(param_groups):
                 flat_offset = 0
                 for i, sub_group in enumerate(param_group):
                     total_elements = sum(p.partition_numel() for p in sub_group)
                     print_rank_0(f"Params in nvme and cpu {self.params_in_nvme_and_cpu}")
                     #Flat buffer may not be available for parameters that reside in NVME
                     if not self.params_in_nvme_and_cpu or flat_offset + total_elements <= self.param_groups_fp16_flat_cpu_memory[
                             param_group_idx].numel():
-                        fp16_partitioned_group_flat = self.param_groups_fp16_flat_cpu_memory[
-                            param_group_idx].narrow(0,
-                                                    flat_offset,
-                                                    total_elements)
+                        fp16_partitioned_group_flat = self.param_groups_fp16_flat_cpu_memory[param_group_idx].narrow(
+                            0, flat_offset, total_elements)
                         print_rank_0(
                             f"Creating a flat buffer for subgroup {i} requiring {total_elements} elements, and cumulative CPU elements {flat_offset + total_elements}",
                             force=False)
 
                     elif self.params_in_nvme_and_cpu:
                         fp16_partitioned_group_flat = None
-                        print_rank_0(
-                            f"No flat buffer for sub group {i} of {total_elements} elements",
-                            force=False)
+                        print_rank_0(f"No flat buffer for sub group {i} of {total_elements} elements", force=False)
                     else:
                         assert False, "Either params are in nvme, or they are in CPU memory. This code path should not be triggered. Please see you max_params_in_cpu and params_in_nvme configs"
 
                     self.fp16_partitioned_groups_flat.append(fp16_partitioned_group_flat)
                     flat_offset += total_elements
 
                     self._move_to_flat_buffer(sub_group,
                                               fp16_partitioned_group_flat,
                                               avoid_copy=not self.offload_param)
 
         # if necessary, create a pinned memory buffer to be used for swapping out
         # params to NVME after optimizer step
-        should_create_fp16_flat_reuse_buffer = any(
-            flattened_partition_group is None
-            for flattened_partition_group in self.fp16_partitioned_groups_flat)
+        should_create_fp16_flat_reuse_buffer = any(flattened_partition_group is None
+                                                   for flattened_partition_group in self.fp16_partitioned_groups_flat)
         if should_create_fp16_flat_reuse_buffer:
             max_partition_numel, largest_partition_numel = 0, None
             for sub_group in self.fp16_groups:
                 total_elements = sum(t.partition_numel() for t in sub_group)
                 if total_elements > max_partition_numel:
                     largest_partition_numel = [t.ds_numel for t in sub_group]
                     max_partition_numel = total_elements
 
             assert len(largest_partition_numel) > 0, f'Unexpected that largest partition is empty'
-            self.fp16_groups[0][0].nvme_swapper.reserve_partitioned_swap_space(
-                largest_partition_numel)
+            self.fp16_groups[0][0].nvme_swapper.reserve_partitioned_swap_space(largest_partition_numel)
 
     def _swap_in_sub_group_to_flat_buffer(self, flat_buffer, sub_group_id):
         offset = 0
-        elements_in_sub_group = sum(
-            [t.ds_numel for t in self.fp16_partitioned_groups[sub_group_id]])
+        elements_in_sub_group = sum([t.ds_numel for t in self.fp16_partitioned_groups[sub_group_id]])
         assert (flat_buffer.numel() == elements_in_sub_group)
-        for param, partitioned_param in zip(self.fp16_groups[sub_group_id], self.fp16_partitioned_groups[sub_group_id]):
+        for param, partitioned_param in zip(self.fp16_groups[sub_group_id],
+                                            self.fp16_partitioned_groups[sub_group_id]):
             dest = flat_buffer.narrow(0, offset, partitioned_param.ds_numel)
             if partitioned_param.status == PartitionedParamStatus.NOT_AVAILABLE:
                 print_rank_0(
                     f"Swapping in {param.ds_id} with elements {param.ds_numel} and partition {param.partition_numel()}"
                 )
                 param.nvme_swapper.swap_in([param], async_op=False)
                 dest.data.copy_(partitioned_param.data)
                 param.nvme_swapper.remove_partition_and_release_buffers([param])
                 print_rank_0(f"Swapping in {param.ds_id} done")
             else:
                 dest.data.copy_(partitioned_param.data)
             offset += partitioned_param.ds_numel
 
     def _create_next_swappable_fp32_groups(self):
-        reverse_order_indices = [
-            i for i in range(len(self.fp32_partitioned_groups_flat))
-        ]
+        reverse_order_indices = [i for i in range(len(self.fp32_partitioned_groups_flat))]
         reverse_order_indices.reverse()
 
         next_group = None
         for i in reverse_order_indices:
             self.next_swappable_fp32_partitioned_groups.append(next_group)
             if self._swappable_optimizer_subgroup(i):
                 next_group = self.fp32_partitioned_groups_flat[i]
 
         self.next_swappable_fp32_partitioned_groups.reverse()
 
     def _get_sub_group_partitions(self, sub_group_id):
         sub_group_partitions = []
-        for param, partitioned_param in zip(self.fp16_groups[sub_group_id], self.fp16_partitioned_groups[sub_group_id]):
+        for param, partitioned_param in zip(self.fp16_groups[sub_group_id],
+                                            self.fp16_partitioned_groups[sub_group_id]):
             if partitioned_param.status == PartitionedParamStatus.NOT_AVAILABLE:
                 swap_path = param.nvme_swapper.get_path(param, True)
-                sub_group_partitions.append((partitioned_param,
-                                             param.partition_numel(),
-                                             swap_path))
+                sub_group_partitions.append((partitioned_param, param.partition_numel(), swap_path))
             else:
-                sub_group_partitions.append((partitioned_param,
-                                             partitioned_param.ds_numel,
-                                             None))
+                sub_group_partitions.append((partitioned_param, partitioned_param.ds_numel, None))
 
         return sub_group_partitions
 
     def _create_fp32_partitions(self):
         cpu_memory_usage = 0
         cpu_memory_sub_groups = 0
         nvme_memory_usage = 0
@@ -745,80 +698,66 @@
                 if self.params_in_nvme_and_cpu and tensor is None:
                     num_swap_from_nvme_partitions += 1
                     swap_from_nvme_memory_usage += (fp32_element_size * num_elements)
                     if self.offload_optimizer_fast_init:
                         sub_group_partitions = self._get_sub_group_partitions(i)
                         nvme_fp16_partitions_info.append(sub_group_partitions)
                         nvme_fp16_num_elems.append(num_elements)
-                        nvme_fp32_dest_tensors.append(
-                            self.fp32_partitioned_groups_flat[i])
+                        nvme_fp32_dest_tensors.append(self.fp32_partitioned_groups_flat[i])
                     else:
-                        unpinned_fp32_buffer = torch.empty(num_elements,
-                                                           device=self.device,
-                                                           dtype=torch.float)
+                        unpinned_fp32_buffer = torch.empty(num_elements, device=self.device, dtype=torch.float)
                         self._swap_in_sub_group_to_flat_buffer(unpinned_fp32_buffer, i)
-                        self.optimizer_swapper.initialize_parameters(
-                            parameters=[self.fp32_partitioned_groups_flat[i]],
-                            src_tensors=[unpinned_fp32_buffer])
+                        self.optimizer_swapper.initialize_parameters(parameters=[self.fp32_partitioned_groups_flat[i]],
+                                                                     src_tensors=[unpinned_fp32_buffer])
                 else:
                     num_swap_from_cpu_partitions += 1
                     swap_from_cpu_memory_usage += (fp32_element_size * num_elements)
                     swappable_fp32_tensors.append(self.fp32_partitioned_groups_flat[i])
-                    swappable_fp16_src_tensors.append(
-                        self.fp16_partitioned_groups_flat[i])
+                    swappable_fp16_src_tensors.append(self.fp16_partitioned_groups_flat[i])
             else:
                 cpu_memory_usage += (fp32_element_size * num_elements)
                 cpu_memory_sub_groups += 1
 
                 if self.params_in_nvme_and_cpu and tensor is None:
-                    unpinned_fp32_buffer = torch.empty(num_elements,
-                                                       device=self.device,
-                                                       dtype=torch.float)
+                    unpinned_fp32_buffer = torch.empty(num_elements, device=self.device, dtype=torch.float)
                     self._swap_in_sub_group_to_flat_buffer(unpinned_fp32_buffer, i)
                     self.fp32_partitioned_groups_flat.append(unpinned_fp32_buffer)
                 else:
-                    self.fp32_partitioned_groups_flat.append(
-                        self.fp16_partitioned_groups_flat[i].to(
-                            self.device).clone().float().detach())
+                    self.fp32_partitioned_groups_flat.append(self.fp16_partitioned_groups_flat[i].to(
+                        self.device).clone().float().detach())
 
-            self.fp32_partitioned_groups_flat[
-                i].requires_grad = True  # keep this in case internal optimizer uses it
+            self.fp32_partitioned_groups_flat[i].requires_grad = True  # keep this in case internal optimizer uses it
 
         if len(swappable_fp32_tensors) > 0:
-            self.optimizer_swapper.initialize_parameters(
-                parameters=swappable_fp32_tensors,
-                src_tensors=swappable_fp16_src_tensors)
+            self.optimizer_swapper.initialize_parameters(parameters=swappable_fp32_tensors,
+                                                         src_tensors=swappable_fp16_src_tensors)
 
         if len(nvme_fp32_dest_tensors) > 0:
-            fp16_pinned_buffers = self.fp16_groups[0][
-                0].nvme_swapper.reserve_available_buffers()
+            fp16_pinned_buffers = self.fp16_groups[0][0].nvme_swapper.reserve_available_buffers()
             assert len(fp16_pinned_buffers) > 0
-            self.optimizer_swapper.initialize_from_swapped_fp16_params(
-                fp16_partitions_info=nvme_fp16_partitions_info,
-                fp16_num_elems=nvme_fp16_num_elems,
-                fp16_pinned_buffers=fp16_pinned_buffers,
-                fp32_parameters=nvme_fp32_dest_tensors)
+            self.optimizer_swapper.initialize_from_swapped_fp16_params(fp16_partitions_info=nvme_fp16_partitions_info,
+                                                                       fp16_num_elems=nvme_fp16_num_elems,
+                                                                       fp16_pinned_buffers=fp16_pinned_buffers,
+                                                                       fp32_parameters=nvme_fp32_dest_tensors)
             self.fp16_groups[0][0].nvme_swapper.release_reserved_buffers()
 
         nvme_gigabytes = nvme_memory_usage / GIGA_BYTES
-        print_rank_0(
-            f'Swappable FP32 Partitions: count={num_swappable_partitions} size={nvme_gigabytes:5.2f} GB',
-            force=False)
+        print_rank_0(f'Swappable FP32 Partitions: count={num_swappable_partitions} size={nvme_gigabytes:5.2f} GB',
+                     force=False)
         if self.params_in_nvme_and_cpu:
             print_rank_0(
                 f'Swap from NVMe Partitions: count = {num_swap_from_nvme_partitions}, size = {swap_from_nvme_memory_usage/GIGA_BYTES:5.2f}GB',
                 force=False)
             print_rank_0(
                 f'Swap from CPU Partitions: count = {num_swap_from_cpu_partitions}, size = {swap_from_cpu_memory_usage/GIGA_BYTES:5.2f}GB',
                 force=False)
 
         cpu_memory_gigabytes = cpu_memory_usage / GIGA_BYTES
-        print_rank_0(
-            f'In-Memory FP32 Partitions: count={cpu_memory_sub_groups} size={cpu_memory_gigabytes:5.2f} GB',
-            force=False)
+        print_rank_0(f'In-Memory FP32 Partitions: count={cpu_memory_sub_groups} size={cpu_memory_gigabytes:5.2f} GB',
+                     force=False)
 
         # Clear for on-the-fly population before the optimizer step
         for param_group in self.optimizer.param_groups:
             param_group['params'] = []
 
     def _create_fp16_sub_groups(self, params_group):
 
@@ -832,16 +771,15 @@
         sub_group = []
         local_sub_group_size = 0
         for param in params_group:
 
             sub_group.append(param)
             local_sub_group_size += param.partition_numel()
 
-            if local_sub_group_size >= sub_group_size or id(param) == id(
-                    params_group[-1]):
+            if local_sub_group_size >= sub_group_size or id(param) == id(params_group[-1]):
 
                 sub_groups.append(sub_group)
 
                 sub_group = []
                 local_sub_group_size = 0
 
         return sub_groups
@@ -858,17 +796,16 @@
         self.optimizer.step()
         self.optimizer.param_groups[param_group_id]['params'] = []
 
     def _swappable_optimizer_subgroup(self, sub_group_id):
         if not self.swap_optimizer:
             return False
 
-        return self.optimizer_swapper.swappable_tensor(
-            None,
-            numel=self.fp16_partitioned_groups_flat_numel[sub_group_id])
+        return self.optimizer_swapper.swappable_tensor(None,
+                                                       numel=self.fp16_partitioned_groups_flat_numel[sub_group_id])
 
     def _partitioned_params_swap_out(self, i):
         offset = 0
         fp32_param = self.fp32_partitioned_groups_flat[i]
         assert fp32_param is not None, \
         f'fp32 parameters of sub_group {i} is None'
 
@@ -880,27 +817,23 @@
                 partitioned_param.data.copy_(src.data)
             else:
                 swap_fp32_params.append(src)
                 swap_fp16_params.append(param)
             offset += partitioned_param.ds_numel
 
         if len(swap_fp16_params):
-            swap_fp16_params[0].nvme_swapper.swap_out_partitioned_params(
-                dst_fp16_params=swap_fp16_params,
-                src_fp32_params=swap_fp32_params)
+            swap_fp16_params[0].nvme_swapper.swap_out_partitioned_params(dst_fp16_params=swap_fp16_params,
+                                                                         src_fp32_params=swap_fp32_params)
 
     def initialize_optimizer_states(self):
         num_subgroups = len(self.fp16_groups)
 
-        largest_numel = max(
-            [sum([p.ds_numel for p in psg]) for psg in self.fp16_partitioned_groups])
+        largest_numel = max([sum([p.ds_numel for p in psg]) for psg in self.fp16_partitioned_groups])
         gradient_dtype = self.fp32_partitioned_groups_flat[0].dtype
-        gradient_buffer = torch.zeros(int(largest_numel),
-                                      dtype=gradient_dtype,
-                                      device=self.device)
+        gradient_buffer = torch.zeros(int(largest_numel), dtype=gradient_dtype, device=self.device)
 
         timer_names = set()
 
         if self.swap_optimizer:
             self.optimizer_swapper.init_timers()
 
         INIT_OPTIMIZER_TIMER = 'init_optimizer_state'
@@ -917,27 +850,21 @@
                 f'[Begin] Initialize optimizer states {i} / {num_subgroups} subgroups, num_elems: {num_elements}, swappable opt/param:{swappable_optimizer_subgroup}/{swappable_param_subgroup}',
                 force=False)
 
             if swappable_optimizer_subgroup:
                 self._optimizer_states_and_gradient_swap_in(i, timer_names)
 
             if self.offload_optimizer and not swappable_optimizer_subgroup:
-                subgroup_gradient_buffer = torch.zeros(num_elements,
-                                                       dtype=gradient_dtype,
-                                                       device=self.device)
+                subgroup_gradient_buffer = torch.zeros(num_elements, dtype=gradient_dtype, device=self.device)
                 if self.offload_optimizer_pin_memory:
-                    subgroup_gradient_buffer = get_accelerator().pin_memory(
-                        subgroup_gradient_buffer)
+                    subgroup_gradient_buffer = get_accelerator().pin_memory(subgroup_gradient_buffer)
 
                 self.fp32_partitioned_groups_flat[i].grad = subgroup_gradient_buffer
             else:
-                self.fp32_partitioned_groups_flat[i].grad = gradient_buffer.narrow(
-                    0,
-                    0,
-                    num_elements)
+                self.fp32_partitioned_groups_flat[i].grad = gradient_buffer.narrow(0, 0, num_elements)
 
             self._optimizer_step(i)
 
             if swappable_param_subgroup:
                 self._partitioned_params_swap_out(i)
 
             if swappable_optimizer_subgroup:
@@ -988,19 +915,16 @@
 
             for partition_id in range(total_partitions):
                 self.is_grad_computed[i][partition_id] = {}
                 self.grad_partition_insertion_offset[i][partition_id] = {}
                 self.grad_start_offset[i][partition_id] = {}
                 self.initialize_gradient_partition(i, param_group, partition_id)
                 self.is_partition_reduced[i][partition_id] = False
-                self.first_param_index_in_partition[i][
-                    partition_id] = self.get_first_param_index(
-                        i,
-                        param_group,
-                        partition_id)
+                self.first_param_index_in_partition[i][partition_id] = self.get_first_param_index(
+                    i, param_group, partition_id)
 
     @instrument_w_nvtx
     def independent_gradient_partition_epilogue(self):
         self.report_ipg_memory_usage(f"In ipg_epilogue before reduce_ipg_grads", 0)
         self.__reduce_and_partition_ipg_grads()
         self.report_ipg_memory_usage(f"In ipg_epilogue after reduce_ipg_grads", 0)
 
@@ -1013,16 +937,15 @@
 
         #in case of cpu offload, averaged gradients are already in fp32_partitioned_groups_flat.grad
         #TODO: use a similar code path for both cpu_offload and non-cpu offload
         if not self.offload_optimizer:
             for i, sub_group in enumerate(self.fp16_groups):
                 self.averaged_gradients[i] = [
                     self.__param_id_to_grad_partition[param.ds_id]
-                    if param.requires_grad else torch.zeros_like(param.ds_tensor)
-                    for param in sub_group
+                    if param.requires_grad else torch.zeros_like(param.ds_tensor) for param in sub_group
                 ]
                 # self.averaged_gradients[i] = self.get_flat_partition(
                 #     self.fp16_groups[i],
                 #     0,
                 #     self.fp32_partitioned_groups_flat[i].numel(),
                 #     return_tensor_list=True)
 
@@ -1083,74 +1006,67 @@
 
         # Because the ipg bucket is initialized with a random place holder tensor, we must
         # explicitly check that the bucket has any real data in it (self.elements_in_ipg_bucket >
         # 0). Otherwise if the incoming param.ds_numel is large, this branch may get triggered on a
         # garbage data and `self.average_tensor()` will crash because its params_to_reduce will be
         # empty, while reduction_list will have that garbage data.
         if self.elements_in_ipg_bucket > 0 and self.elements_in_ipg_bucket + param.ds_numel > self.reduce_bucket_size:
-            self.report_ipg_memory_usage("In ipg_remove_grads before reduce_ipg_grads",
-                                         param.ds_numel)
+            self.report_ipg_memory_usage("In ipg_remove_grads before reduce_ipg_grads", param.ds_numel)
 
             self.__reduce_and_partition_ipg_grads()
 
         param_id = self.get_param_id(param)
+
         assert self.params_already_reduced[param_id] == False, \
             f"The parameter {param_id} has already been reduced. \
             Gradient computed twice for this partition. \
             Multiple gradient reduction is currently not supported"
 
         self.__add_grad_to_ipg_bucket(param)
 
     @instrument_w_nvtx
     @torch.no_grad()
     def __add_grad_to_ipg_bucket(self, param: Parameter) -> None:
-        self.__reduce_and_partition_stream.wait_stream(
-            get_accelerator().default_stream())
+        self.__reduce_and_partition_stream.wait_stream(get_accelerator().default_stream())
 
-        if self.contiguous_gradients and self.elements_in_ipg_bucket + param.grad.numel(
-        ) < self.reduce_bucket_size:
+        if self.contiguous_gradients and self.elements_in_ipg_bucket + param.grad.numel() < self.reduce_bucket_size:
             # move the gradient to a contiguous buffer
             with get_accelerator().stream(self.__reduce_and_partition_stream):
                 # move the parameter's gradient to the contiguous flat buffer
-                new_grad_tensor = self.__ipg_bucket_flat_buffer.narrow(
-                    0,
-                    self.elements_in_ipg_bucket,
-                    param.grad.numel()).view_as(param.grad)
+                new_grad_tensor = self.__ipg_bucket_flat_buffer.narrow(0, self.elements_in_ipg_bucket,
+                                                                       param.grad.numel()).view_as(param.grad)
                 new_grad_tensor.copy_(param.grad, non_blocking=True)
                 param.grad.record_stream(get_accelerator().current_stream())
                 param.grad.data = new_grad_tensor
 
         self.__params_in_ipg_bucket.append(param)
 
     @instrument_w_nvtx
     @torch.no_grad()
     def __reduce_and_partition_ipg_grads(self, safe_mode: bool = False) -> None:
         if not self.__params_in_ipg_bucket:
             return
 
         for param in self.__params_in_ipg_bucket:
             if param.grad.numel() != param.ds_numel:
-                raise RuntimeError(
-                    f"{param.grad.numel()} != {param.ds_numel} Cannot reduce scatter "
-                    f"gradients whose size is not same as the params")
+                raise RuntimeError(f"{param.grad.numel()} != {param.ds_numel} Cannot reduce scatter "
+                                   f"gradients whose size is not same as the params")
 
         self.__params_in_ipg_bucket.sort(key=lambda p: p.ds_id)
 
-        assert len(set(p.ds_id for p in self.__params_in_ipg_bucket)) == len(
-            self.__params_in_ipg_bucket)
+        assert len(set(p.ds_id for p in self.__params_in_ipg_bucket)) == len(self.__params_in_ipg_bucket)
 
         while self.__param_reduce_events and self.__param_reduce_events[0].query():
             self.__param_reduce_events.popleft()
         if len(self.__param_reduce_events) > self.__max_param_reduce_events:
             self.__param_reduce_events.popleft().synchronize()
 
         with get_accelerator().stream(self.__reduce_and_partition_stream):
             if safe_mode:
-                assert_ints_same_as_other_ranks(
-                    [p.ds_id for p in self.__params_in_ipg_bucket])
+                assert_ints_same_as_other_ranks([p.ds_id for p in self.__params_in_ipg_bucket])
 
             grad_partitions = self.__avg_scatter_grads(self.__params_in_ipg_bucket)
             self.__partition_grads(self.__params_in_ipg_bucket, grad_partitions)
 
             self.__params_in_ipg_bucket.clear()
 
             event = get_accelerator().Event()
@@ -1159,51 +1075,37 @@
 
     @instrument_w_nvtx
     def __avg_scatter_grads(self, params_to_reduce: List[Parameter]) -> List[Tensor]:
         """average gradients and scatter partitions across ranks"""
 
         full_grads_for_rank = [p.grad for p in params_to_reduce]
         if self.communication_data_type != self.dtype:
-            full_grads_for_rank = [
-                g.to(self.communication_data_type) for g in full_grads_for_rank
-            ]
+            full_grads_for_rank = [g.to(self.communication_data_type) for g in full_grads_for_rank]
 
         if self.postscale_gradients and self.gradient_predivide_factor != 1.0:
-            full_grads_for_rank = [
-                g.div(self.gradient_predivide_factor) for g in full_grads_for_rank
-            ]
-
-        grad_partitions_for_rank = reduce_scatter_coalesced(full_grads_for_rank,
-                                                            self.dp_process_group)
-
-        if self.postscale_gradients and self.gradient_predivide_factor != dist.get_world_size(
-                self.dp_process_group):
-            grad_partitions_for_rank = [
-                g.mul(self.gradient_predivide_factor) for g in grad_partitions_for_rank
-            ]
+            full_grads_for_rank = [g.div(self.gradient_predivide_factor) for g in full_grads_for_rank]
+
+        grad_partitions_for_rank = reduce_scatter_coalesced(full_grads_for_rank, self.dp_process_group)
+
+        if self.postscale_gradients and self.gradient_predivide_factor != dist.get_world_size(self.dp_process_group):
+            grad_partitions_for_rank = [g.mul(self.gradient_predivide_factor) for g in grad_partitions_for_rank]
 
         if self.communication_data_type != self.dtype:
-            grad_partitions_for_rank = [
-                g.to(self.dtype) for g in grad_partitions_for_rank
-            ]
+            grad_partitions_for_rank = [g.to(self.dtype) for g in grad_partitions_for_rank]
 
         return grad_partitions_for_rank
 
     def set_grad_positions(self):
         for i, group in enumerate(self.fp16_groups):
             current_offset = 0
             for param in group:
                 param_id = self.get_param_id(param)
                 num_elements = param.partition_numel()
 
-                self.grad_position[param_id] = [
-                    int(i),
-                    int(current_offset),
-                    int(num_elements)
-                ]
+                self.grad_position[param_id] = [int(i), int(current_offset), int(num_elements)]
                 #print(f"param id {param_id} i:{i}, ds_tensor {num_elements} numel {param.numel()}")
                 current_offset += num_elements
         see_memory_usage(f"After Set Grad positions", force=False)
 
     def _constant_buffered_norm2(self, input, buffer_size=250000000):
         norm = None
         for part in input.view(-1).split(buffer_size):
@@ -1236,60 +1138,50 @@
                 if param_id in self.norm_for_param_grads.keys():
                     param_norm = self.norm_for_param_grads[param_id]
                     total_norm += param_norm.item()**2
 
         # Sum across all model parallel GPUs.
         total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
 
-        dist.all_reduce(total_norm_cuda,
-                        op=dist.ReduceOp.SUM,
-                        group=self.dp_process_group)
+        dist.all_reduce(total_norm_cuda, op=dist.ReduceOp.SUM, group=self.dp_process_group)
 
         self._model_parallel_all_reduce(tensor=total_norm_cuda, op=dist.ReduceOp.SUM)
 
         total_norm = total_norm_cuda[0].item()**(1. / norm_type)
 
-        if total_norm == float(
-                'inf') or total_norm == -float('inf') or total_norm != total_norm:
+        if total_norm == float('inf') or total_norm == -float('inf') or total_norm != total_norm:
             total_norm = -1
 
         return total_norm
 
     @instrument_w_nvtx
-    def __partition_grads(self,
-                          params_to_release: List[Parameter],
-                          grad_partitions: List[Tensor]) -> None:
+    def __partition_grads(self, params_to_release: List[Parameter], grad_partitions: List[Tensor]) -> None:
         offload_fp32_gradients = {}
         offload_fp32_offsets = {}
         for param, grad_partition in zip(params_to_release, grad_partitions):
 
-            contains_real_data = param.partition_numel() * dist.get_rank(
-                self.dp_process_group) < param.ds_numel
+            contains_real_data = param.partition_numel() * dist.get_rank(self.dp_process_group) < param.ds_numel
             if not contains_real_data:
                 # this grad partition is empty - don't need to do anything
                 param.grad = None
                 continue
 
             # move or accumulate gradient partition to target buffer
-            grad_buffer = self.__param_id_to_grad_partition[param.ds_id].narrow(
-                0,
-                0,
-                grad_partition.numel())
+            grad_buffer = self.__param_id_to_grad_partition[param.ds_id].narrow(0, 0, grad_partition.numel())
             if self.micro_step_id == 0:  # don't accumulate
                 grad_buffer.copy_(grad_partition, non_blocking=True)
                 # ensure grad buffer is a CUDA buffer to speed up the next few
                 # operations and so it can be used asynchronously
                 grad_buffer = grad_buffer.to(grad_partition.device, non_blocking=True)
             elif get_accelerator().on_accelerator(grad_buffer):
                 grad_buffer.add_(grad_partition)
             else:
                 # if dst is CPU, copy first to src device, do the addition
                 # there, then move back to dst. adding directly to cpu is very slow
-                cuda_grad_buffer = grad_buffer.to(grad_partition.device,
-                                                  non_blocking=True)
+                cuda_grad_buffer = grad_buffer.to(grad_partition.device, non_blocking=True)
                 cuda_grad_buffer.add_(grad_partition)
                 grad_buffer.copy_(cuda_grad_buffer, non_blocking=True)
                 # ensure grad buffer is a CUDA buffer to speed up the next few
                 # operations and so it can be used asynchronously
                 grad_buffer = cuda_grad_buffer
 
             if hasattr(self.__inf_or_nan_tracker, "logical_or_"):
@@ -1302,47 +1194,44 @@
                 self.__inf_or_nan_tracker = self.__inf_or_nan_tracker > 0
 
             # offload the gradient partition if applicable
             if self.offload_optimizer:
                 i, dest_offset, _ = self.grad_position[self.get_param_id(param)]
 
                 if self.is_gradient_accumulation_boundary:
-                    self.norm_for_param_grads[self.get_param_id(
-                        param)] = self._constant_buffered_norm2(grad_buffer)
+                    self.norm_for_param_grads[self.get_param_id(param)] = self._constant_buffered_norm2(grad_buffer)
 
                     if self._swappable_optimizer_subgroup(i):
                         if not i in offload_fp32_gradients.keys():
                             offload_fp32_gradients[i] = []
                             offload_fp32_offsets[i] = []
 
                         offload_fp32_gradients[i].append(grad_buffer.float())
                         offload_fp32_offsets[i].append(dest_offset)
                     else:
-                        fp32_grad_tensor = self.fp32_partitioned_groups_flat[
-                            i].grad.narrow(0,
-                                           dest_offset,
-                                           grad_buffer.numel())
+                        fp32_grad_tensor = self.fp32_partitioned_groups_flat[i].grad.narrow(
+                            0, dest_offset, grad_buffer.numel())
                         fp32_grad_tensor.copy_(grad_buffer)
 
             # free the gradient
             param.grad.record_stream(get_accelerator().current_stream())
             param.grad = None
 
         if self.offload_optimizer and self.swap_optimizer:
             for i in offload_fp32_gradients.keys():
-                self.optimizer_swapper.swap_out_gradients(
-                    parameter=self.fp32_partitioned_groups_flat[i],
-                    gradient_offsets=offload_fp32_offsets[i],
-                    gradient_tensors=offload_fp32_gradients[i])
+                self.optimizer_swapper.swap_out_gradients(parameter=self.fp32_partitioned_groups_flat[i],
+                                                          gradient_offsets=offload_fp32_offsets[i],
+                                                          gradient_tensors=offload_fp32_gradients[i])
 
     def reduce_ready_partitions_and_remove_grads(self, param, i):
         #print_rank_0(f"Backward {debug_param2name_id_shape(param)}", force=True)
         self.reduce_independent_p_g_buckets_and_remove_grads(param, i)
 
     def zero_reduced_gradients(self, partition_id, i):
+
         def are_all_related_partitions_reduced(params_id):
             for partition_id in self.param_to_partition_ids[i][params_id]:
                 if not self.is_partition_reduced[i][partition_id]:
                     return False
             return True
 
         for params_id in self.is_grad_computed[i][partition_id]:
@@ -1354,37 +1243,31 @@
 
         def print_func():
             logger.info(flatten_tensor.contiguous().view(-1).narrow(0, start, n))
 
         self.sequential_execution(print_func, message)
 
     def get_grads_to_reduce(self, i, partition_id):
+
         def get_reducible_portion(key):
             grad = self.param_dict[key].grad
             total_elements = grad.numel()
             start = self.grad_start_offset[i][partition_id][key]
-            num_elements = min(
-                total_elements - start,
-                self.partition_size[i] -
-                self.grad_partition_insertion_offset[i][partition_id][key])
+            num_elements = min(total_elements - start,
+                               self.partition_size[i] - self.grad_partition_insertion_offset[i][partition_id][key])
             if not pg_correctness_test:
                 if num_elements == total_elements:
                     return grad
                 else:
-                    return grad.contiguous().view(-1).narrow(0,
-                                                             int(start),
-                                                             int(num_elements))
+                    return grad.contiguous().view(-1).narrow(0, int(start), int(num_elements))
             else:
                 if num_elements == total_elements:
                     return grad.clone()
                 else:
-                    return grad.clone().contiguous().view(-1).narrow(
-                        0,
-                        int(start),
-                        int(num_elements))
+                    return grad.clone().contiguous().view(-1).narrow(0, int(start), int(num_elements))
 
         grads_to_reduce = []
         for key in self.is_grad_computed[i][partition_id]:
             grad = get_reducible_portion(key)
             grads_to_reduce.append(grad)
         return grads_to_reduce
 
@@ -1439,19 +1322,15 @@
     def allreduce_and_copy(self, small_bucket, rank=None, log=None):
         with get_accelerator().stream(self.reduction_stream):
             allreduced = self.allreduce_bucket(small_bucket, rank=rank, log=log)
             if rank is None or rank == dist.get_rank(group=self.dp_process_group):
                 for buf, synced in zip(small_bucket, self.unflatten(allreduced, small_bucket)):
                     buf.copy_(synced)
 
-    def allreduce_no_retain(self,
-                            bucket,
-                            numel_per_bucket=500000000,
-                            rank=None,
-                            log=None):
+    def allreduce_no_retain(self, bucket, numel_per_bucket=500000000, rank=None, log=None):
         small_bucket = []
         numel = 0
         for tensor in bucket:
             small_bucket.append(tensor)
             numel = numel + tensor.numel()
             if numel > numel_per_bucket:
                 self.allreduce_and_copy(small_bucket, rank=rank, log=None)
@@ -1498,19 +1377,19 @@
         for tensor in tensor_list:
 
             tensor_size = tensor.numel()
 
             if (current_index >= start_index and current_index < end_index):
                 params_in_partition.append(tensor)
 
-            elif start_index > current_index and start_index < (current_index +
-                                                                tensor_size):
+            elif start_index > current_index and start_index < (current_index + tensor_size):
                 params_in_partition.append(tensor)
 
-                assert (first_offset == 0), "This can happen either zero or only once as this must be the first tensor in the partition"
+                assert (first_offset == 0
+                        ), "This can happen either zero or only once as this must be the first tensor in the partition"
                 first_offset = start_index - current_index
 
             else:
                 params_not_in_partition.append(tensor)
 
             current_index = current_index + tensor_size
 
@@ -1562,56 +1441,45 @@
         Returns:
             Total norm of the parameters (viewed as a single vector).
         """
         norm_type = float(norm_type)
         if norm_type == inf:
             total_norm = max(g.data.abs().max() for g in gradients)
             total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
-            dist.all_reduce(total_norm_cuda,
-                            op=dist.ReduceOp.MAX,
-                            group=self.dp_process_group)
+            dist.all_reduce(total_norm_cuda, op=dist.ReduceOp.MAX, group=self.dp_process_group)
 
             # Take max across all GPUs.
             self._model_parallel_all_reduce(tensor=total_norm_cuda, op=dist.ReduceOp.MAX)
             total_norm = total_norm_cuda[0].item()
         else:
             # if dist.get_rank() == 0:
             #    logger.info(f"Total Norm beginning {total_norm}")
             grad_norms = []
             for g, p in zip(gradients, params):
                 if is_model_parallel_parameter(p) or (self.model_parallel_rank == 0):
-                    grad_norms.append(
-                        g.to(get_accelerator().device_name(),
-                             non_blocking=True).double().norm(2))
+                    grad_norms.append(g.to(get_accelerator().device_name(), non_blocking=True).double().norm(2))
 
             # Sum across all model parallel GPUs.
             total_norm_cuda = torch.sum(torch.pow(torch.stack(grad_norms), 2))
 
-            dist.all_reduce(total_norm_cuda,
-                            op=dist.ReduceOp.SUM,
-                            group=self.dp_process_group)
+            dist.all_reduce(total_norm_cuda, op=dist.ReduceOp.SUM, group=self.dp_process_group)
 
             self._model_parallel_all_reduce(tensor=total_norm_cuda, op=dist.ReduceOp.SUM)
 
             total_norm = total_norm_cuda.item()**(1. / norm_type)
 
-        if total_norm == float(
-                'inf') or total_norm == -float('inf') or total_norm != total_norm:
+        if total_norm == float('inf') or total_norm == -float('inf') or total_norm != total_norm:
             total_norm = -1
 
         return total_norm
 
     # creates a flat fused tensor from the tensor list starting at the first_offset
     # in the first tensor of the list. If there are not enough elements in the tensor
     # list then the flat tensor will be padded with zeros
-    def get_flat_partition(self,
-                           tensor_list,
-                           first_offset,
-                           partition_size,
-                           return_tensor_list=False):
+    def get_flat_partition(self, tensor_list, first_offset, partition_size, return_tensor_list=False):
         flat_tensor_list = []
         current_size = 0
         for i, tensor in enumerate(tensor_list):
             if tensor.grad is None:
                 tensor.grad = torch.zeros_like(tensor)
 
             tensor = tensor.grad
@@ -1626,18 +1494,15 @@
             # we dont need all elements of the tensor
             if num_elements > (partition_size - current_size):
                 num_elements = partition_size - current_size
 
             # we need a narrow view of the tensor based on the tensor offset and number of elements that
             # we need from this tensor
             if tensor_offset > 0 or num_elements < tensor.numel():
-                flat_tensor_list.append(tensor.contiguous().view(-1).narrow(
-                    0,
-                    int(tensor_offset),
-                    int(num_elements)))
+                flat_tensor_list.append(tensor.contiguous().view(-1).narrow(0, int(tensor_offset), int(num_elements)))
             else:
                 flat_tensor_list.append(tensor)
 
             current_size = current_size + num_elements
 
         # this means its the last partition and does not align with the dp boundary. We need to pad before flattening
         if current_size < partition_size:
@@ -1691,21 +1556,17 @@
         print_rank_0("Finished Tracing at Beginning of Step")
 
     @instrument_w_nvtx
     def _get_norm_groups(self):
         norm_groups = []
         for i, group in enumerate(self.fp16_groups):
             if self.offload_optimizer:
-                norm_groups.append(
-                    self.complete_grad_norm_calculation_for_cpu_offload(
-                        self.fp16_groups[i]))
+                norm_groups.append(self.complete_grad_norm_calculation_for_cpu_offload(self.fp16_groups[i]))
             else:
-                norm_groups.append(
-                    self.get_grad_norm_direct(self.averaged_gradients[i],
-                                              self.fp16_groups[i]))
+                norm_groups.append(self.get_grad_norm_direct(self.averaged_gradients[i], self.fp16_groups[i]))
         return norm_groups
 
     @instrument_w_nvtx
     def _prepare_fp32_grad_for_sub_group(self, sub_group_id):
         partition_id = dist.get_rank(group=self.dp_process_group)
 
         single_grad_partition = self.flatten(self.averaged_gradients[sub_group_id]).to(
@@ -1716,78 +1577,69 @@
                 single_grad_partition.numel(), self.fp32_partitioned_groups_flat[sub_group_id].numel(), sub_group_id, partition_id)
 
         self.fp32_partitioned_groups_flat[sub_group_id].grad = single_grad_partition
 
         # release all the gradient since we have already created a necessary copy in dp_grad_partition
         self.zero_grad(set_to_none=True)
 
-        for grad in filter(lambda g: get_accelerator().on_accelerator(g),
-                           self.averaged_gradients[sub_group_id]):
+        for grad in filter(lambda g: get_accelerator().on_accelerator(g), self.averaged_gradients[sub_group_id]):
             grad.record_stream(get_accelerator().current_stream())
 
         self.averaged_gradients[sub_group_id] = None
 
     @instrument_w_nvtx
     def _prepare_sub_group(self, sub_group_id, timer_names=set()):
-        see_memory_usage(f'Before prepare optimizer sub group {sub_group_id}',
-                         force=False)
+        see_memory_usage(f'Before prepare optimizer sub group {sub_group_id}', force=False)
         if self._swappable_optimizer_subgroup(sub_group_id):
             self._optimizer_states_and_gradient_swap_in(sub_group_id, timer_names)
         elif not self.offload_optimizer:
             self._prepare_fp32_grad_for_sub_group(sub_group_id)
-        see_memory_usage(f'After prepare optimizer sub group {sub_group_id}',
-                         force=False)
+        see_memory_usage(f'After prepare optimizer sub group {sub_group_id}', force=False)
 
     def _optimizer_states_and_gradient_swap_in(self, sub_group_id, timer_names=set()):
         param_length = self.fp16_partitioned_groups_flat_numel[sub_group_id]
         fp32_param_id = id(self.fp32_partitioned_groups_flat[sub_group_id])
         assert self._swappable_optimizer_subgroup(sub_group_id), \
             f'Parameter {fp32_param_id} of numel={param_length} is not swappable'
 
         OPTIMIZER_SWAP_IN_STATE = 'optimizer_swap_in_state'
-        see_memory_usage(f'pre-step Before swapping in optimizer tensors {sub_group_id}',
-                         force=False)
+        see_memory_usage(f'pre-step Before swapping in optimizer tensors {sub_group_id}', force=False)
         self.start_timers([OPTIMIZER_SWAP_IN_STATE])
 
         self.optimizer_swapper.swap_in_optimizer_state(
             parameter=self.fp32_partitioned_groups_flat[sub_group_id],
             async_parameter=self.next_swappable_fp32_partitioned_groups[sub_group_id])
 
         self.stop_timers([OPTIMIZER_SWAP_IN_STATE])
         timer_names.add(OPTIMIZER_SWAP_IN_STATE)
-        see_memory_usage(f'pre-step After swapping in optimizer tensors {sub_group_id}',
-                         force=False)
+        see_memory_usage(f'pre-step After swapping in optimizer tensors {sub_group_id}', force=False)
 
     @instrument_w_nvtx
     def _release_sub_group(self, sub_group_id, timer_names=set()):
-        see_memory_usage(f'Before release optimizer sub group {sub_group_id}',
-                         force=False)
+        see_memory_usage(f'Before release optimizer sub group {sub_group_id}', force=False)
         # get rid of the fp32 gradients. Not needed anymore
         if not self.offload_optimizer:
             self.fp32_partitioned_groups_flat[sub_group_id].grad = None
 
         if self._swappable_optimizer_subgroup(sub_group_id):
             self._optimizer_states_and_gradient_swap_out(sub_group_id, timer_names)
-        see_memory_usage(f'After release optimizer sub group {sub_group_id}',
-                         force=False)
+        see_memory_usage(f'After release optimizer sub group {sub_group_id}', force=False)
 
     # create a flat tensor aligned at the alignment boundary
     @instrument_w_nvtx
     def flatten_dense_tensors_aligned(self, tensor_list, alignment):
         num_elements = 0
         for tens in tensor_list:
             num_elements = num_elements + tens.numel()
 
         remaining = num_elements % alignment
 
         if remaining:
             elements_to_add = alignment - remaining
-            pad_tensor = torch.zeros(elements_to_add,
-                                     device=tensor_list[0].device,
-                                     dtype=tensor_list[0].dtype)
+            pad_tensor = torch.zeros(elements_to_add, device=tensor_list[0].device, dtype=tensor_list[0].dtype)
             padded_tensor_list = tensor_list + [pad_tensor]
 
             num_elements = num_elements + elements_to_add
         else:
             padded_tensor_list = tensor_list
 
         return self.flatten(padded_tensor_list)
@@ -1795,28 +1647,23 @@
     def _optimizer_states_and_gradient_swap_out(self, sub_group_id, timer_names=set()):
         param_length = self.fp16_partitioned_groups_flat_numel[sub_group_id]
         fp32_param_id = id(self.fp32_partitioned_groups_flat[sub_group_id])
         assert self._swappable_optimizer_subgroup(sub_group_id), \
             f'Parameter {fp32_param_id} of numel={param_length} is not swappable'
 
         OPTIMIZER_SWAP_OUT_STATE = 'optimizer_swap_out_state'
-        see_memory_usage(
-            f'post-step Before swapping out optimizer tensors {sub_group_id}',
-            force=False)
+        see_memory_usage(f'post-step Before swapping out optimizer tensors {sub_group_id}', force=False)
         self.start_timers([OPTIMIZER_SWAP_OUT_STATE])
 
         self.optimizer_swapper.swap_out_optimizer_state(
             parameter=self.fp32_partitioned_groups_flat[sub_group_id],
-            async_swap=self.next_swappable_fp32_partitioned_groups[sub_group_id]
-            is not None)
+            async_swap=self.next_swappable_fp32_partitioned_groups[sub_group_id] is not None)
 
         self.stop_timers([OPTIMIZER_SWAP_OUT_STATE])
-        see_memory_usage(
-            f'post-step After swapping out optimizer tensors {sub_group_id}',
-            force=False)
+        see_memory_usage(f'post-step After swapping out optimizer tensors {sub_group_id}', force=False)
         timer_names.add(OPTIMIZER_SWAP_OUT_STATE)
 
         # get rid of the fp32 gradients. Not needed anymore
         self.fp32_partitioned_groups_flat[sub_group_id].grad = None
 
     def _unflatten_partitioned_parameters(self, sub_group_id):
         updated_params = self.unflatten(self.fp16_partitioned_groups_flat[sub_group_id],
@@ -1877,17 +1724,15 @@
             #unflatten fp16 parameter subgroup
             self._unflatten_partitioned_parameters(sub_group_id)
         else:
             self._partitioned_params_swap_out(sub_group_id)
 
     def override_loss_scale(self, loss_scale):
         if loss_scale != self.external_loss_scale:
-            logger.info(
-                f'[deepspeed] setting loss scale from {self.external_loss_scale} -> {loss_scale}'
-            )
+            logger.info(f'[deepspeed] setting loss scale from {self.external_loss_scale} -> {loss_scale}')
         self.custom_loss_scaler = True
         self.external_loss_scale = loss_scale
 
     @instrument_w_nvtx
     def step(self, closure=None):
         """
             Not supporting closure.
@@ -1961,29 +1806,23 @@
                 fp32_grad_norm = [float(t.data.float().norm(2)) for t in fp32_grad]
                 norm_list = [fp16_grad_norm, fp32_grad_norm]
                 print(f'Pre-Step Norms {i} {param_id} = {norm_list}')
 
     def dump_post_step_gradients(self):
         # Dump gradient norms for debugging
         for i, group in enumerate(self.fp16_groups):
-            print(
-                f'Post-Step Dump Norms for Group {i} FP16P, FP16DS, FP16FLAT, FP32FLAT')
+            print(f'Post-Step Dump Norms for Group {i} FP16P, FP16DS, FP16FLAT, FP32FLAT')
             unflat_fp16 = self.unflatten(self.fp16_groups_flat[i], self.fp16_groups[i])
-            unflat_fp32 = self.unflatten(self.fp32_partitioned_groups_flat[i],
-                                         self.fp16_groups[i])
+            unflat_fp32 = self.unflatten(self.fp32_partitioned_groups_flat[i], self.fp16_groups[i])
             for j, p in enumerate(self.fp16_groups[i]):
                 param_id = self.get_param_id(p)
                 param_norm = float(p.data.float().norm(2))
                 ds_norm = float(p.ds_tensor.data.float().norm(2))
 
-                unflat_norm = [
-                    float(t.data.float().norm(2))
-                    for t in [unflat_fp16[j],
-                              unflat_fp32[j]]
-                ]
+                unflat_norm = [float(t.data.float().norm(2)) for t in [unflat_fp16[j], unflat_fp32[j]]]
                 norm_list = [param_norm, ds_norm] + unflat_norm
                 print(f'Post-Step Norms {i} {param_id} = {norm_list}')
 
     @instrument_w_nvtx
     def unscale_and_clip_grads(self, sub_group_id, total_norm):
         # compute combined scale factor for this group
         combined_scale = self.loss_scale
@@ -2019,17 +1858,15 @@
             with get_accelerator().stream(self.__reduce_and_partition_stream):
                 self.local_overflow = bool(self.__inf_or_nan_tracker.item())
                 self.__inf_or_nan_tracker.zero_()
 
             overflow = self.local_overflow
             #overflow = self.has_overflow_partitioned_grads_serial()
             overflow_gpu = get_accelerator().ByteTensor([overflow])
-            dist.all_reduce(overflow_gpu,
-                            op=dist.ReduceOp.MAX,
-                            group=self.dp_process_group)
+            dist.all_reduce(overflow_gpu, op=dist.ReduceOp.MAX, group=self.dp_process_group)
 
         else:
             params = []
             for group in self.fp16_groups:
                 for param in group:
                     params.append(param)
 
@@ -2096,18 +1933,15 @@
         """
         self.__reduce_and_partition_stream.synchronize()
         grad_dict = collections.defaultdict(dict)
         if self.offload_optimizer:
             for group in self.fp16_groups:
                 for param_idx, param in enumerate(group):
                     group_idx, dest_offset, num_elements = self.grad_position[self.get_param_id(param)]
-                    fp32_grad = self.fp32_partitioned_groups_flat[group_idx].grad.narrow(
-                        0,
-                        dest_offset,
-                        num_elements)
+                    fp32_grad = self.fp32_partitioned_groups_flat[group_idx].grad.narrow(0, dest_offset, num_elements)
                     grad_dict[group_idx][param_idx] = fp32_grad
         else:
             for group_idx, group in self.averaged_gradients.items():
                 for param_idx, gradient in enumerate(group):
                     grad_dict[group_idx][param_idx] = gradient.float()
 
         return grad_dict
@@ -2115,16 +1949,15 @@
     def _fp32_state_allgather(self, param, fp32_state):
         reduce_buffer = torch.zeros(self.partition_count * fp32_state.numel(),
                                     dtype=torch.float32,
                                     device=param.device).flatten()
         my_rank = dist.get_rank(group=self.dp_process_group)
         partitions = [
             reduce_buffer.narrow(0,
-                                 fp32_state.numel() * i,
-                                 fp32_state.numel()) for i in range(self.partition_count)
+                                 fp32_state.numel() * i, fp32_state.numel()) for i in range(self.partition_count)
         ]
         partitions[my_rank].data.copy_(fp32_state.data, non_blocking=False)
 
         dist.all_gather(partitions, partitions[my_rank], group=self.dp_process_group)
 
         return reduce_buffer.narrow(0, 0, param.ds_numel).view(param.ds_shape)
 
@@ -2132,18 +1965,16 @@
         if not param.requires_grad:
             return None
 
         self.__reduce_and_partition_stream.synchronize()
 
         if self.offload_optimizer:
             group_idx, dest_offset, num_elements = self.grad_position[self.get_param_id(param)]
-            fp32_grad = self.fp32_partitioned_groups_flat[group_idx].grad.narrow(
-                0,
-                dest_offset,
-                num_elements).to(device=param.device)
+            fp32_grad = self.fp32_partitioned_groups_flat[group_idx].grad.narrow(0, dest_offset,
+                                                                                 num_elements).to(device=param.device)
         else:
             fp32_grad = self.__param_id_to_grad_partition[param.ds_id].float()
 
         return self._fp32_state_allgather(param, fp32_grad)
 
     def get_full_hp_param(self, param, optim_state_key=None) -> Tensor:
         if not param.requires_grad:
@@ -2153,22 +1984,18 @@
         group_idx, dest_offset, num_elements = self.grad_position[self.get_param_id(param)]
 
         if self._swappable_optimizer_subgroup(group_idx):
             self._optimizer_states_and_gradient_swap_in(group_idx)
 
         fp32_param = self.fp32_partitioned_groups_flat[group_idx]
         if optim_state_key is None:
-            fp32_opt_state = fp32_param.narrow(0,
-                                               dest_offset,
-                                               num_elements).to(device=param.device)
+            fp32_opt_state = fp32_param.narrow(0, dest_offset, num_elements).to(device=param.device)
         else:
             fp32_opt_state = self.optimizer.state[fp32_param][optim_state_key].narrow(
-                0,
-                dest_offset,
-                num_elements).to(device=param.device)
+                0, dest_offset, num_elements).to(device=param.device)
 
         hp_param = self._fp32_state_allgather(param, fp32_opt_state)
         if self._swappable_optimizer_subgroup(group_idx):
             self._optimizer_states_and_gradient_swap_out(group_idx)
         return hp_param
 
     @instrument_w_nvtx
@@ -2230,33 +2057,29 @@
 
         for i, group in enumerate(self.optimizer.param_groups):
             p = group['params'][0]
             lean_state = {}
             for key, value in self.optimizer.state[p].items():
                 if torch.is_tensor(value):
                     padded_lens = [t.numel() for t in self.fp16_partitioned_groups[i]]
-                    lean_state[key] = self._get_lean_tensors(
-                        value,
-                        self.fp16_partitioned_groups[i],
-                        self.groups_padding[i])
+                    lean_state[key] = self._get_lean_tensors(value, self.fp16_partitioned_groups[i],
+                                                             self.groups_padding[i])
                     lean_flat_len = sum([t.numel() for t in lean_state[key]])
                 else:
                     lean_state[key] = value
 
             optimizer_groups_state.append(lean_state)
 
         return optimizer_groups_state
 
     def get_groups_without_padding(self, groups_with_padding):
         # Return group tensor after removing paddings added for alignment to DP world size.
         groups_without_padding = []
         for i, group in enumerate(groups_with_padding):
-            lean_group = self._get_lean_tensors(group,
-                                                self.fp16_partitioned_groups[i],
-                                                self.groups_padding[i])
+            lean_group = self._get_lean_tensors(group, self.fp16_partitioned_groups[i], self.groups_padding[i])
             groups_without_padding.append(lean_group)
 
         return groups_without_padding
 
     def _set_fp32_optimizer_param_groups(self):
         for sub_group_id, _ in enumerate(self.fp16_groups):
             param_group_id = self.sub_group_to_group_id[sub_group_id]
@@ -2290,22 +2113,19 @@
         Example::
             checkpoint = {}
             checkpoint['model'] = model.state_dict()
             checkpoint['optimizer'] = optimizer.state_dict()
             torch.save(checkpoint, "saved.pth")
         """
         if self.elastic_checkpoint:
-            raise NotImplementedError(
-                "ZeRO-3 does not yet support elastic checkpointing, please disable for now."
-            )
+            raise NotImplementedError("ZeRO-3 does not yet support elastic checkpointing, please disable for now.")
 
         if self.swap_optimizer or self.params_in_nvme_and_cpu:
             raise NotImplementedError(
-                "ZeRO-3 does not yet support checkpointing with NVMe offloading, please disable for now."
-            )
+                "ZeRO-3 does not yet support checkpointing with NVMe offloading, please disable for now.")
 
         return self._rigid_state_dict()
 
 
 # Restore base optimizer fp32 weights from checkpoint by:
 # 1) Merging fp32 weights from checkpoints of all partitions
 # 2) Extracting fp32 weights for current partition from merged weights
@@ -2319,15 +2139,16 @@
             flat_local_partition.append(self._get_flattened_partition(merged_partitions))
 
         for current, saved in zip(self.fp32_partitioned_groups_flat, flat_local_partition):
             current.data.copy_(saved.data)
 
     # Restore base optimizer fp32 weights from ZeRO fp16 weights
     def _restore_from_bit16_weights(self):
-        for fp16_partitions, fp32_partition in zip(self.fp16_partitioned_groups_flat, self.fp32_partitioned_groups_flat):
+        for fp16_partitions, fp32_partition in zip(self.fp16_partitioned_groups_flat,
+                                                   self.fp32_partitioned_groups_flat):
             fp32_partition.data.copy_(fp16_partitions.data)
 
     # Refresh the fp32 master params from the fp16 copies.
     def refresh_fp32_params(self):
         self._restore_from_bit16_weights()
 
     # Extract flattened partition for current rank from all partitions
@@ -2338,17 +2159,15 @@
         param_partitions = [[] for _ in range(len(all_partition_states[0]))]
         for i, partition in enumerate(all_partition_states):
             for j, param in enumerate(partition):
                 param_partitions[j].append(param)
 
         local_state_partitions = []
         for param_index, param_slices in enumerate(param_partitions):
-            flattened_merged_tensor = self.flatten_dense_tensors_aligned(
-                param_slices,
-                alignment)
+            flattened_merged_tensor = self.flatten_dense_tensors_aligned(param_slices, alignment)
             new_partitions = self.get_data_parallel_partitions(flattened_merged_tensor)
             local_state_partitions.append(new_partitions[partition_id])
 
         if torch.is_tensor(local_state_partitions[0]):
             return self.flatten_dense_tensors_aligned(local_state_partitions, alignment)
 
         # Assume non-tensor states are not partitioned and equal across ranks, so return first one
@@ -2358,23 +2177,18 @@
     # 1) Merging optimizer state from checkpoints of all partitions
     # 2) Extracting optimizer state for current partition from the merged state
     # 3) Using the extracted value to directly update the base optimizer.
     def _restore_base_optimizer_state(self, all_state_dict):
         base_optimizer_group_states = []
         for i in range(len(self.optimizer.param_groups)):
             partition_states = {}
-            all_partition_group_states = [
-                sd['base_optimizer_state'][i] for sd in all_state_dict
-            ]
+            all_partition_group_states = [sd['base_optimizer_state'][i] for sd in all_state_dict]
             for key in all_partition_group_states[0].keys():
-                all_partition_states = [
-                    all_states[key] for all_states in all_partition_group_states
-                ]
-                partition_states[key] = self._get_flattened_partition(
-                    all_partition_states)
+                all_partition_states = [all_states[key] for all_states in all_partition_group_states]
+                partition_states[key] = self._get_flattened_partition(all_partition_states)
             base_optimizer_group_states.append(partition_states)
 
         for i, group in enumerate(self.optimizer.param_groups):
             p = group['params'][0]
             for key, saved in base_optimizer_group_states[i].items():
                 if torch.is_tensor(self.optimizer.state[p][key]):
                     self.optimizer.state[p][key].data.copy_(saved.data)
@@ -2400,17 +2214,16 @@
         for sub_group_id in range(len(self.fp32_partitioned_groups_flat)):
             fp32_param = self.fp32_partitioned_groups_flat[sub_group_id]
             fp16_param = self.fp16_partitioned_groups_flat[sub_group_id]
             fp16_param.data.copy_(fp32_param.data)
 
         # update fp16 unflattened params
         for sub_group_id in range(len(self.fp16_partitioned_groups_flat)):
-            updated_params = self.unflatten(
-                self.fp16_partitioned_groups_flat[sub_group_id],
-                self.fp16_partitioned_groups[sub_group_id])
+            updated_params = self.unflatten(self.fp16_partitioned_groups_flat[sub_group_id],
+                                            self.fp16_partitioned_groups[sub_group_id])
 
             for partitioned_param, q in zip(self.fp16_partitioned_groups[sub_group_id], updated_params):
                 partitioned_param.data = q.data
 
     # TODO: Support different/changing load/save DP degree.
     def load_state_dict(self,
                         state_dict_list,
@@ -2439,51 +2252,48 @@
             ...
             checkpoint = torch.load("saved.pth")
             model.load_state_dict(checkpoint['model'])
             optimizer.load_state_dict(checkpoint['optimizer'])
         """
 
         if self.elastic_checkpoint:
-            raise NotImplementedError(
-                "ZeRO-3 does not yet support elastic checkpointing, please disable for now."
-            )
+            raise NotImplementedError("ZeRO-3 does not yet support elastic checkpointing, please disable for now.")
 
         if self.swap_optimizer or self.params_in_nvme_and_cpu:
             raise NotImplementedError(
-                "ZeRO-3 does not yet support checkpointing with NVMe offloading, please disable for now."
-            )
+                "ZeRO-3 does not yet support checkpointing with NVMe offloading, please disable for now.")
 
-        self._rigid_load_state_dict(
-            state_dict_list[dist.get_rank(group=self.dp_process_group)],
-            load_optimizer_states=load_optimizer_states)
+        self._rigid_load_state_dict(state_dict_list[dist.get_rank(group=self.dp_process_group)],
+                                    load_optimizer_states=load_optimizer_states)
 
         if len(self.persistent_parameters) > 0:
             self.persistent_parameters[0].partition(self.persistent_parameters)
             self.persistent_parameters[0].all_gather(self.persistent_parameters)
 
     def checkpoint_event_prologue(self):
         self._partition_all_parameters()
 
     def checkpoint_event_epilogue(self):
         if len(self.persistent_parameters) > 0:
             self.persistent_parameters[0].all_gather(self.persistent_parameters)
 
+    def empty_partition_cache(self):
+        self.parameter_offload.empty_partition_cache()
+
 
 def _handle_overflow(cpu_sum, x, i):
     import math
     rank = dist.get_rank()
     if rank == 0:
         t_i = -1
         for v_i, v in enumerate(x.data.contiguous().view(-1)):
             if not math.isfinite(float(v)):
                 t_i = v_i
                 break
-        logger.info(
-            f"rank {rank} detected overflow {cpu_sum} in tensor {i}:{t_i} shape {x.shape}"
-        )
+        logger.info(f"rank {rank} detected overflow {cpu_sum} in tensor {i}:{t_i} shape {x.shape}")
 
 
 def estimate_zero3_model_states_mem_needs(total_params,
                                           largest_layer_params,
                                           num_gpus_per_node=1,
                                           num_nodes=1,
                                           cpu_offload=True,
@@ -2498,40 +2308,36 @@
     if cpu_offload:
         if cpu_offload_params:
             gpu_mem = largest_layer_memory
 
             if zero_init:
                 cpu_mem = total_params * 18 * gpus_factor * additional_buffer_factor
             else:
-                cpu_mem = total_params * max(4 * num_gpus_per_node,
-                                             18 * gpus_factor) * additional_buffer_factor
+                cpu_mem = total_params * max(4 * num_gpus_per_node, 18 * gpus_factor) * additional_buffer_factor
 
         else:
             gpu_mem = largest_layer_memory + int(2 * total_params / total_gpus)
 
             if zero_init:
                 cpu_mem = total_params * 16 * gpus_factor * additional_buffer_factor
             else:
-                cpu_mem = total_params * max(4 * num_gpus_per_node,
-                                             16 * gpus_factor) * additional_buffer_factor
+                cpu_mem = total_params * max(4 * num_gpus_per_node, 16 * gpus_factor) * additional_buffer_factor
     else:
         gpu_mem = largest_layer_memory + int(18 * total_params / total_gpus)
         if zero_init:
             cpu_mem = largest_layer_params * 4 * num_gpus_per_node * additional_buffer_factor
         else:
             cpu_mem = total_params * 4 * num_gpus_per_node * additional_buffer_factor
 
     return int(cpu_mem), int(gpu_mem), largest_layer_memory
 
 
 def model_to_params(model):
     # shared params calculated only once
-    total_params = sum(
-        dict((p.data_ptr(),
-              p.numel()) for p in model.parameters()).values())
+    total_params = sum(dict((p.data_ptr(), p.numel()) for p in model.parameters()).values())
 
     largest_layer_params = 0
     for m in model.modules():
         # assuming no shared params within a single layer
         layer_params = sum(p.numel() for p in m.parameters(recurse=False))
         largest_layer_params = max(largest_layer_params, layer_params)
 
@@ -2558,20 +2364,19 @@
         - ``num_nodes``: how many nodes (defaults to 1),
         - ``additional_buffer_factor``: estimation factor (defaults to 1.5):
 
     """
 
     total_params, largest_layer_params = model_to_params(model)
 
-    estimate_zero3_model_states_mem_needs_all_cold(
-        total_params=total_params,
-        largest_layer_params=largest_layer_params,
-        num_gpus_per_node=num_gpus_per_node,
-        num_nodes=num_nodes,
-        additional_buffer_factor=additional_buffer_factor)
+    estimate_zero3_model_states_mem_needs_all_cold(total_params=total_params,
+                                                   largest_layer_params=largest_layer_params,
+                                                   num_gpus_per_node=num_gpus_per_node,
+                                                   num_nodes=num_nodes,
+                                                   additional_buffer_factor=additional_buffer_factor)
 
 
 def estimate_zero3_model_states_mem_needs_all_cold(total_params,
                                                    largest_layer_params,
                                                    num_gpus_per_node=1,
                                                    num_nodes=1,
                                                    additional_buffer_factor=1.5):
@@ -2589,14 +2394,15 @@
         - ``total_params``: total  model params
         - ``largest_layer_params``: largest layer's params
         - ``num_gpus_per_node``: how many gpus per node (defaults to 1)
         - ``num_nodes``: how many nodes (defaults to 1),
         - ``additional_buffer_factor``: estimation factor (defaults to 1.5):
 
     """
+
     def format_options(cpu_offload, cpu_offload_params, zero_init):
         enabled = []
         padded_cpu_str = f'{OffloadDeviceEnum.cpu:4}'
         param_device = padded_cpu_str if cpu_offload_params else "none"
         enabled.append(f"offload_param={param_device}")
         optimizer_device = padded_cpu_str if cpu_offload else "none"
         enabled.append(f"offload_optimizer={optimizer_device}")
@@ -2620,15 +2426,13 @@
                     total_params=total_params,
                     largest_layer_params=largest_layer_params,
                     num_gpus_per_node=num_gpus_per_node,
                     num_nodes=num_nodes,
                     cpu_offload=cpu_offload,
                     cpu_offload_params=cpu_offload_params,
                     zero_init=zero_init,
-                    additional_buffer_factor=additional_buffer_factor
-                )
+                    additional_buffer_factor=additional_buffer_factor)
 
                 options_str = format_options(cpu_offload=cpu_offload,
                                              cpu_offload_params=cpu_offload_params,
                                              zero_init=zero_init)
-                print(
-                    f" {cpu_mem/2**30:7.2f}GB | {gpu_mem/2**30:6.2f}GB | {options_str}")
+                print(f" {cpu_mem/2**30:7.2f}GB | {gpu_mem/2**30:6.2f}GB | {options_str}")
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/zero/stage_1_and_2.py` & `deepspeed-0.9.0/deepspeed/runtime/zero/stage_1_and_2.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,47 +1,37 @@
-'''
-Copyright 2019 The Microsoft DeepSpeed Team
-'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import torch
 import os
 from deepspeed import comm as dist
 from packaging import version as pkg_version
 from collections import OrderedDict
 
 from deepspeed.runtime import ZeROOptimizer
 from deepspeed.runtime.fp16.loss_scaler import CreateLossScaler
-from deepspeed.runtime.utils import (bwc_tensor_model_parallel_rank,
-                                     get_global_norm,
-                                     empty_cache,
-                                     see_memory_usage,
-                                     inf,
-                                     is_model_parallel_parameter,
-                                     align_dense_tensors,
-                                     all_gather_dp_groups)
+from deepspeed.runtime.utils import (bwc_tensor_model_parallel_rank, get_global_norm, empty_cache, see_memory_usage,
+                                     inf, is_model_parallel_parameter, align_dense_tensors, all_gather_dp_groups)
 
 from deepspeed.runtime.zero.config import ZeroStageEnum
 from deepspeed.runtime.zero.offload_config import OffloadDeviceEnum
 from deepspeed.ops.adam import DeepSpeedCPUAdam
 from deepspeed.utils import logger
 from deepspeed.moe.utils import is_moe_param
 from deepspeed.git_version_info import version
 
 from deepspeed.runtime.constants import PIPE_REPLICATED
 from deepspeed.accelerator import get_accelerator
 from deepspeed.ops.op_builder import UtilsBuilder
 
-from deepspeed.checkpoint.constants import (DS_VERSION,
-                                            GROUP_PADDINGS,
-                                            PARTITION_COUNT,
-                                            SINGLE_PARTITION_OF_FP32_GROUPS,
-                                            BASE_OPTIMIZER_STATE,
-                                            CLIP_GRAD,
-                                            ZERO_STAGE,
-                                            PARAM_SLICE_MAPPINGS)
+from deepspeed.checkpoint.constants import (DS_VERSION, GROUP_PADDINGS, PARTITION_COUNT,
+                                            SINGLE_PARTITION_OF_FP32_GROUPS, BASE_OPTIMIZER_STATE, CLIP_GRAD,
+                                            ZERO_STAGE, PARAM_SLICE_MAPPINGS)
 from deepspeed.utils import link_hp_params
 from deepspeed.checkpoint import enable_universal_checkpoint
 
 # Toggle this to true to enable correctness test
 # with gradient partitioning and without
 pg_correctness_test = False
 
@@ -49,18 +39,16 @@
 def input(msg):
     return
 
 
 def split_half_float_double(tensors):
     device_type = get_accelerator().device_name()
     dtypes = [
-        "torch.{}.HalfTensor".format(device_type),
-        "torch.{}.FloatTensor".format(device_type),
-        "torch.{}.DoubleTensor".format(device_type),
-        "torch.{}.BFloat16Tensor".format(device_type)
+        "torch.{}.HalfTensor".format(device_type), "torch.{}.FloatTensor".format(device_type),
+        "torch.{}.DoubleTensor".format(device_type), "torch.{}.BFloat16Tensor".format(device_type)
     ]
     buckets = []
     for i, dtype in enumerate(dtypes):
         bucket = [t for t in tensors if t.type() == dtype]
         if bucket:
             buckets.append(bucket)
     return buckets
@@ -106,14 +94,15 @@
 
     For more details please see ZeRO: Memory Optimization Towards Training A Trillion Parameter Models
     https://arxiv.org/abs/1910.02054
 
     For usage examples, refer to TODO: DeepSpeed Tutorial
 
     """
+
     def __init__(self,
                  init_optimizer,
                  param_names,
                  timers,
                  static_loss_scale=1.0,
                  dynamic_loss_scale=False,
                  dynamic_loss_args=None,
@@ -164,44 +153,42 @@
         # Load pre-built or JIT compile (un)flatten ops
         util_ops = UtilsBuilder().load()
         self.flatten = util_ops.flatten
         self.unflatten = util_ops.unflatten
 
         # ZeRO stage 1 (False) or 2 (True)
         self.partition_gradients = partition_grads
+        self.zero_stage_string = "ZeRO-2" if partition_grads else "ZeRO-1"
 
         self.timers = timers
 
         self.reduce_scatter = reduce_scatter
 
         self.overlap_comm = overlap_comm
 
         self.cpu_offload = cpu_offload
 
         self.deepspeed_adam_offload = cpu_offload
 
-        self.device = get_accelerator().current_device_name(
-        ) if not self.cpu_offload else 'cpu'
+        self.device = get_accelerator().current_device_name() if not self.cpu_offload else 'cpu'
 
         self.dp_process_group = dp_process_group
 
         #expert parallel group
         self.ep_process_group = expert_parallel_group
 
         #data parallel group for experts
         self.expert_dp_process_group = expert_data_parallel_group
 
         #data parallel size for non-experts
         dp_size = dist.get_world_size(group=self.dp_process_group)
 
         #For MoE models this maybe different for different param group
         #It will be modified during MoE setup later in the init
-        self.real_dp_process_group = [
-            dp_process_group for i in range(len(self.optimizer.param_groups))
-        ]
+        self.real_dp_process_group = [dp_process_group for i in range(len(self.optimizer.param_groups))]
         self.partition_count = [dp_size for i in range(len(self.optimizer.param_groups))]
 
         self.is_gradient_accumulation_boundary = True
 
         # CPU-Offload requires contiguous gradients
         self.contiguous_gradients = contiguous_gradients or cpu_offload
 
@@ -229,20 +216,24 @@
         self.ignore_unused_parameters = ignore_unused_parameters
         self.round_robin_gradients = round_robin_gradients
 
         self.extra_large_param_to_reduce = None
         self.fp16_master_weights_and_gradients = fp16_master_weights_and_gradients
 
         if self.fp16_master_weights_and_gradients:
-            assert self.cpu_offload and type(self.optimizer) in [DeepSpeedCPUAdam], f"fp16_master_and_gradients requires optimizer to support keeping fp16 master and gradients while keeping the optimizer states in fp32. Currently only supported using ZeRO-Offload with DeepSpeedCPUAdam. But current setting is ZeRO-Offload:{self.cpu_offload} and optimizer type {type(self.optimizer)}. Either disable fp16_master_weights_and_gradients or enable ZeRO-2 Offload with DeepSpeedCPUAdam"
+            assert self.cpu_offload and type(self.optimizer) in [DeepSpeedCPUAdam], \
+            f"fp16_master_and_gradients requires optimizer to support keeping fp16 master and gradients while keeping the optimizer states in fp32."\
+            f"Currently only supported using ZeRO-Offload with DeepSpeedCPUAdam. But current setting is ZeRO-Offload:{self.cpu_offload} and optimizer type {type(self.optimizer)}." \
+            f"Either disable fp16_master_weights_and_gradients or enable {self.zero_stage_string} Offload with DeepSpeedCPUAdam."
 
         if self.reduce_scatter:
-            assert self.communication_data_type in (torch.float16, torch.bfloat16), f"ZeRO-2 supports only float16 or bfloat16 communication_data_type with reduce scatter enabled. Got: '{self.communication_data_type}'"
-            assert self.gradient_predivide_factor == 1.0, "gradient_predivide_factor != 1.0 is not yet supported with ZeRO-2 with reduce scatter enabled"
-            assert self.postscale_gradients, "pre-scale gradients is not yet supported with ZeRO-2 with reduce scatter enabled"
+            valid_reduce_scatter_dtypes = (torch.float16, torch.bfloat16, torch.float32)
+            assert self.communication_data_type in valid_reduce_scatter_dtypes, f"{self.zero_stage_string} supports {valid_reduce_scatter_dtypes} communication_data_type with reduce scatter enabled. Got: '{self.communication_data_type}'"
+            assert self.gradient_predivide_factor == 1.0, "gradient_predivide_factor != 1.0 is not yet supported with {self.zero_stage_string} with reduce scatter enabled"
+            assert self.postscale_gradients, "pre-scale gradients is not yet supported with {self.zero_stage_string} with reduce scatter enabled"
 
         # param flattened by groups
         self.bit16_groups = []
         self.bit16_groups_flat = []
 
         # param partitioned by data parallel degree
         # this will contain a list of equal sized tensors
@@ -268,15 +259,17 @@
 
         # number of elements per partition in each group
         self.partition_size = []
 
         # align nccl all-gather send buffers to 4-byte boundary
         self.nccl_start_alignment_factor = 2  # 4-byte alignment/sizeof(fp16) = 2
 
-        assert (allgather_bucket_size % self.nccl_start_alignment_factor == 0), f"allgather_bucket_size must be a multiple of nccl_start_alignment_factor, {self.nccl_start_alignment_factor} "
+        assert (
+            allgather_bucket_size % self.nccl_start_alignment_factor == 0
+        ), f"allgather_bucket_size must be a multiple of nccl_start_alignment_factor, {self.nccl_start_alignment_factor} "
 
         self.all_reduce_print = False
         self.dtype = self.optimizer.param_groups[0]['params'][0].dtype
 
         self.round_robin_bit16_groups = []
         self.round_robin_bit16_indices = []
 
@@ -285,17 +278,15 @@
         self.groups_padding = []
         # loop to deal with groups
         for i, param_group in enumerate(self.optimizer.param_groups):
             partition_id = dist.get_rank(group=self.real_dp_process_group[i])
 
             # push this group to list before modify
             # TODO: Explore simplification that avoids the extra book-keeping by pushing the reordered group
-            trainable_parameters = [
-                param for param in param_group['params'] if param.requires_grad
-            ]
+            trainable_parameters = [param for param in param_group['params'] if param.requires_grad]
             self.bit16_groups.append(trainable_parameters)
 
             # not sure why apex was cloning the weights before flattening
             # removing cloning here
 
             see_memory_usage(f"Before moving param group {i} to CPU")
             # move all the parameters to cpu to free up GPU space for creating flat buffer
@@ -305,88 +296,73 @@
 
             # Reorder group parameters for load balancing of gradient partitioning during backward among ranks.
             # This ensures that gradients are reduced in a fashion such that ownership round robins among the ranks.
             # For example, rather than 3 gradients (g_n+2, g_n+1, g_n) that are reduced consecutively belonging
             # to the same rank, instead they will belong to 3 ranks (r_m+2, r_m+1, r_m).
             if self.round_robin_gradients:
                 round_robin_tensors, round_robin_indices = self._round_robin_reorder(
-                    self.bit16_groups[i],
-                    dist.get_world_size(group=self.real_dp_process_group[i])
-                )
+                    self.bit16_groups[i], dist.get_world_size(group=self.real_dp_process_group[i]))
             else:
                 round_robin_tensors = self.bit16_groups[i]
                 round_robin_indices = list(range(len(self.bit16_groups[i])))
 
             self.round_robin_bit16_groups.append(round_robin_tensors)
             self.round_robin_bit16_indices.append(round_robin_indices)
 
             # create flat buffer in CPU and move to GPU
             self.bit16_groups_flat.append(
                 self.flatten_dense_tensors_aligned(
                     self.round_robin_bit16_groups[i],
-                    self.nccl_start_alignment_factor *
-                    dist.get_world_size(group=self.real_dp_process_group[i])).to(
+                    self.nccl_start_alignment_factor * dist.get_world_size(group=self.real_dp_process_group[i])).to(
                         get_accelerator().current_device_name()))
-            see_memory_usage(f"After flattening and moving param group {i} to GPU",
-                             force=False)
+            see_memory_usage(f"After flattening and moving param group {i} to GPU", force=False)
 
             # Record padding required for alignment
-            if partition_id == dist.get_world_size(
-                    group=self.real_dp_process_group[i]) - 1:
+            if partition_id == dist.get_world_size(group=self.real_dp_process_group[i]) - 1:
                 padding = self.bit16_groups_flat[i].numel() - sum(
                     [t.numel() for t in self.round_robin_bit16_groups[i]])
             else:
                 padding = 0
             self.groups_padding.append(padding)
 
             if dist.get_rank(group=self.real_dp_process_group[i]) == 0:
-                see_memory_usage(
-                    f"After Flattening and after emptying param group {i} cache",
-                    force=False)
+                see_memory_usage(f"After Flattening and after emptying param group {i} cache", force=False)
 
             # set model bit16 weight to slices of flattened buffer
             self._update_model_bit16_weights(i)
 
             # divide the flat weights into near equal partition equal to the data parallel degree
             # each process will compute on a different part of the partition
-            data_parallel_partitions = self.get_data_parallel_partitions(
-                self.bit16_groups_flat[i],
-                i)
+            data_parallel_partitions = self.get_data_parallel_partitions(self.bit16_groups_flat[i], i)
             self.parallel_partitioned_bit16_groups.append(data_parallel_partitions)
 
             # verify that data partition start locations are 4-byte aligned
             for partitioned_data in data_parallel_partitions:
-                assert (partitioned_data.data_ptr() %
-                        (2 * self.nccl_start_alignment_factor) == 0)
+                assert (partitioned_data.data_ptr() % (2 * self.nccl_start_alignment_factor) == 0)
 
             # A partition of the fp32 master weights that will be updated by this process.
             # Note that the params in single_partition_of_fp32_groups is cloned and detached
             # from the origin params of the model.
             if not fp16_master_weights_and_gradients:
-                self.single_partition_of_fp32_groups.append(
-                    self.parallel_partitioned_bit16_groups[i][partition_id].to(
-                        self.device).clone().float().detach())
+                self.single_partition_of_fp32_groups.append(self.parallel_partitioned_bit16_groups[i][partition_id].to(
+                    self.device).clone().float().detach())
             else:
-                self.single_partition_of_fp32_groups.append(
-                    self.parallel_partitioned_bit16_groups[i][partition_id].to(
-                        self.device).clone().half().detach())
+                self.single_partition_of_fp32_groups.append(self.parallel_partitioned_bit16_groups[i][partition_id].to(
+                    self.device).clone().half().detach())
 
             # Set local optimizer to have flat params of its own partition.
             # After this, the local optimizer will only contain its own partition of params.
             # In that case, the local optimizer only saves the states(momentum, variance, etc.) related to its partition's params(zero stage1).
             self.single_partition_of_fp32_groups[
                 i].requires_grad = True  # keep this in case internal optimizer uses it
             param_group['params'] = [self.single_partition_of_fp32_groups[i]]
 
-            partition_size = len(self.bit16_groups_flat[i]) / dist.get_world_size(
-                group=self.real_dp_process_group[i])
+            partition_size = len(self.bit16_groups_flat[i]) / dist.get_world_size(group=self.real_dp_process_group[i])
             params_in_partition, params_not_in_partition, first_offset = self.get_partition_info(
-                self.round_robin_bit16_groups[i],
-                partition_size,
-                partition_id)
+                self.round_robin_bit16_groups[i], partition_size, partition_id)
 
             self.partition_size.append(partition_size)
             self.params_in_partition.append(params_in_partition)
             self.params_not_in_partition.append(params_not_in_partition)
             self.first_offset.append(first_offset)
 
         for rank in range(dist.get_world_size()):
@@ -395,16 +371,15 @@
                     f"Rank: {rank} partition count {self.partition_count} and sizes{[(p.numel(), self.is_moe_param_group[i] if hasattr(self, 'is_moe_param_group') else False) for i,p in enumerate(self.single_partition_of_fp32_groups)]} "
                 )
                 dist.barrier()
 
         self.reduce_bucket_size = int(reduce_bucket_size)
         self.allgather_bucket_size = int(allgather_bucket_size)
 
-        self.reduction_event = get_accelerator().Event(enable_timing=False,
-                                                       blocking=False)
+        self.reduction_event = get_accelerator().Event(enable_timing=False, blocking=False)
         self.reduction_stream = get_accelerator().Stream()
         self.cpu_computation_stream = get_accelerator().Stream()
         self.copy_grad_stream = get_accelerator().Stream()
         self.callback_queued = False
 
         self.param_dict = {}
 
@@ -445,26 +420,20 @@
 
         if self.cpu_offload:
             self.accumulated_grads_in_cpu = {}
             self.norm_for_param_grads = {}
             self.local_overflow = False
             self.grad_position = {}
             self.temp_grad_buffer_for_cpu_offload = get_accelerator().pin_memory(
-                torch.zeros(largest_param_numel,
-                            device=self.device,
-                            dtype=self.dtype))
-            self.temp_grad_buffer_for_gpu_offload = torch.zeros(
-                largest_param_numel,
-                device=get_accelerator().current_device_name(),
-                dtype=self.dtype)
+                torch.zeros(largest_param_numel, device=self.device, dtype=self.dtype))
+            self.temp_grad_buffer_for_gpu_offload = torch.zeros(largest_param_numel,
+                                                                device=get_accelerator().current_device_name(),
+                                                                dtype=self.dtype)
             for i, params_group in enumerate(self.bit16_groups):
-                self.get_grad_position(i,
-                                       self.params_in_partition[i],
-                                       self.first_offset[i],
-                                       self.partition_size[i])
+                self.get_grad_position(i, self.params_in_partition[i], self.first_offset[i], self.partition_size[i])
 
         # mapping from parameter to partition that it belongs to
         self.param_to_partition_ids = {}
 
         # stores if a partition has been reduced in this step
         self.is_partition_reduced = {}
 
@@ -533,65 +502,63 @@
     def _create_param_mapping(self):
         param_mapping = []
         for i, _ in enumerate(self.optimizer.param_groups):
             param_mapping_per_group = OrderedDict()
             for lp in self.bit16_groups[i]:
                 if lp._hp_mapping is not None:
                     lp_name = self.param_names[lp]
-                    param_mapping_per_group[
-                        lp_name] = lp._hp_mapping.get_hp_fragment_address()
+                    param_mapping_per_group[lp_name] = lp._hp_mapping.get_hp_fragment_address()
             param_mapping.append(param_mapping_per_group)
 
         return param_mapping
 
     def _link_all_hp_params(self):
         dp_world_size = dist.get_world_size(group=self.dp_process_group)
         if self.cpu_offload:
             self._get_offload_gradient_dict()
 
         for i, _ in enumerate(self.optimizer.param_groups):
             # Link bit16 and fp32 params in partition
             partition_id = dist.get_rank(group=self.real_dp_process_group[i])
             partition_size = self.bit16_groups_flat[i].numel() // dp_world_size
             flat_hp_partition = self.single_partition_of_fp32_groups[i]
-            link_hp_params(
-                lp_param_list=self.bit16_groups[i],
-                flat_hp_partition=flat_hp_partition,
-                gradient_dict=self.averaged_gradients,
-                offload_gradient_dict=self.offload_gradient_dict,
-                use_offload=self.cpu_offload,
-                param_group_index=i,
-                partition_start=partition_id * partition_size,
-                partition_size=partition_size,
-                partition_optimizer_state=self.optimizer.state[flat_hp_partition],
-                dp_group=self.real_dp_process_group[i])
+            link_hp_params(lp_param_list=self.bit16_groups[i],
+                           flat_hp_partition=flat_hp_partition,
+                           gradient_dict=self.averaged_gradients,
+                           offload_gradient_dict=self.offload_gradient_dict,
+                           use_offload=self.cpu_offload,
+                           param_group_index=i,
+                           partition_start=partition_id * partition_size,
+                           partition_size=partition_size,
+                           partition_optimizer_state=self.optimizer.state[flat_hp_partition],
+                           dp_group=self.real_dp_process_group[i])
 
     def is_moe_group(self, group):
         return 'moe' in group and group['moe']
 
     def _configure_moe_settings(self):
         # if we're using ZeRO stage 2, ensure contiguous gradients are used
         if self.partition_gradients:
             assert self.contiguous_gradients, "Contiguous Gradients in ZeRO Stage 2 must be set to True for MoE. Other code paths are not tested with MoE"
         # NOTE: To run ZeRO stage 1 with MoE, we need to set self.contiguous_gradients to True or ignore the assertion
         if not self.partition_gradients and not self.contiguous_gradients:
             logger.warn(
-                "ZeRO Stage 1 has not been thoroughly tested with MoE. This configuration is still experimental."
-            )
+                "ZeRO Stage 1 has not been thoroughly tested with MoE. This configuration is still experimental.")
         assert self.reduce_scatter, "Reduce Scatter in ZeRO Stage 2 must be set to True for MoE. Other code paths are not tested with MoE"
 
-        assert any([self.is_moe_group(group) for group in self.optimizer.param_groups]), "The model has moe layers, but None of the param groups are marked as MoE. Create a param group with 'moe' key set to True before creating optimizer"
+        assert any(
+            [self.is_moe_group(group) for group in self.optimizer.param_groups]
+        ), "The model has moe layers, but None of the param groups are marked as MoE. Create a param group with 'moe' key set to True before creating optimizer"
         self.is_moe_param_group = []
         for i, group in enumerate(self.optimizer.param_groups):
             if self.is_moe_group(group):
-                assert all([is_moe_param(param) for param in group['params']]), "All params in MoE group must be MoE params"
-                self.real_dp_process_group[i] = self.expert_dp_process_group[
-                    group['name']]
-                self.partition_count[i] = dist.get_world_size(
-                    group=self.expert_dp_process_group[group['name']])
+                assert all([is_moe_param(param)
+                            for param in group['params']]), "All params in MoE group must be MoE params"
+                self.real_dp_process_group[i] = self.expert_dp_process_group[group['name']]
+                self.partition_count[i] = dist.get_world_size(group=self.expert_dp_process_group[group['name']])
                 self.is_moe_param_group.append(True)
             else:
                 self.is_moe_param_group.append(False)
 
         assert self.expert_dp_process_group is not None, "Expert data parallel group should be configured with MoE"
         assert self.ep_process_group is not None, "Expert parallel group should be configured with MoE"
 
@@ -634,18 +601,17 @@
             self.ipg_buffer = None
             self.grads_in_partition = None
             self.grads_in_partition_offset = 0
 
     def initialize_optimizer_states(self):
 
         for i, group in enumerate(self.bit16_groups):
-            single_grad_partition = torch.zeros(
-                int(self.partition_size[i]),
-                dtype=self.single_partition_of_fp32_groups[i].dtype,
-                device=self.device)
+            single_grad_partition = torch.zeros(int(self.partition_size[i]),
+                                                dtype=self.single_partition_of_fp32_groups[i].dtype,
+                                                device=self.device)
             self.single_partition_of_fp32_groups[i].grad = get_accelerator().pin_memory(
                 single_grad_partition) if self.cpu_offload else single_grad_partition
 
         self.optimizer.step()
 
         if not self.cpu_offload:
             for group in self.single_partition_of_fp32_groups:
@@ -705,19 +671,16 @@
             for partition_id in range(total_partitions):
                 self.is_grad_computed[i][partition_id] = {}
                 self.grad_partition_insertion_offset[i][partition_id] = {}
                 self.grad_start_offset[i][partition_id] = {}
                 self.total_grads_in_partition[i][partition_id] = 0
                 self.initialize_gradient_partition(i, param_group, partition_id)
                 self.is_partition_reduced[i][partition_id] = False
-                self.first_param_index_in_partition[i][
-                    partition_id] = self.get_first_param_index(
-                        i,
-                        param_group,
-                        partition_id)
+                self.first_param_index_in_partition[i][partition_id] = self.get_first_param_index(
+                    i, param_group, partition_id)
 
     def independent_gradient_partition_epilogue(self):
         self.report_ipg_memory_usage(f"In ipg_epilogue before reduce_ipg_grads", 0)
         self.reduce_ipg_grads()
         self.report_ipg_memory_usage(f"In ipg_epilogue after reduce_ipg_grads", 0)
 
         # if dist.get_rank() == 0:
@@ -738,21 +701,20 @@
                         self.params_in_partition[i],
                         self.first_offset[i],
                         self.partition_size[i],
                         dtype=self.dtype,
                         device=get_accelerator().current_device_name(),
                         return_tensor_list=True)
                 else:
-                    avg_new = self.get_flat_partition(
-                        self.params_in_partition[i],
-                        self.first_offset[i],
-                        self.partition_size[i],
-                        dtype=self.dtype,
-                        device=get_accelerator().current_device_name(),
-                        return_tensor_list=True)
+                    avg_new = self.get_flat_partition(self.params_in_partition[i],
+                                                      self.first_offset[i],
+                                                      self.partition_size[i],
+                                                      dtype=self.dtype,
+                                                      device=get_accelerator().current_device_name(),
+                                                      return_tensor_list=True)
 
                     for accumulated_grad, new_avg_grad in zip(self.averaged_gradients[i], avg_new):
                         accumulated_grad.add_(new_avg_grad)
 
         self._release_ipg_buffers()
 
         # No need to keep the gradients anymore.
@@ -765,21 +727,21 @@
     # sets remaining grads to the total number of grads in each partition
     # set is grad computed to false for all grads in partition
     def reset_partition_gradient_structures(self):
         for i, _ in enumerate(self.bit16_groups):
             total_partitions = dist.get_world_size(group=self.real_dp_process_group[i])
             for partition_id in range(total_partitions):
                 self.is_partition_reduced[i][partition_id] = False
-                self.remaining_grads_in_partition[i][
-                    partition_id] = self.total_grads_in_partition[i][partition_id]
+                self.remaining_grads_in_partition[i][partition_id] = self.total_grads_in_partition[i][partition_id]
 
                 for param_id in self.is_grad_computed[i][partition_id]:
                     self.is_grad_computed[i][partition_id][param_id] = False
 
     def initialize_gradient_partition(self, i, param_group, partition_id):
+
         def set_key_value_list(dictionary, key, value):
             if key in dictionary:
                 dictionary[key].append(value)
             else:
                 dictionary[key] = [value]
 
         def increment_value(dictionary, key):
@@ -798,33 +760,28 @@
 
         for param in param_group:
 
             param_size = param.numel()
             param_id = self.get_param_id(param)
 
             if (current_index >= start_index and current_index < end_index):
-                set_key_value_list(self.param_to_partition_ids[i],
-                                   param_id,
-                                   partition_id)
+                set_key_value_list(self.param_to_partition_ids[i], param_id, partition_id)
                 increment_value(self.total_grads_in_partition[i], partition_id)
 
                 self.is_grad_computed[i][partition_id][param_id] = False
 
-                self.grad_partition_insertion_offset[i][partition_id][
-                    param_id] = current_index - start_index
+                self.grad_partition_insertion_offset[i][partition_id][param_id] = current_index - start_index
                 self.grad_start_offset[i][partition_id][param_id] = 0
 
-            elif start_index > current_index and start_index < (current_index +
-                                                                param_size):
-                assert (first_offset == 0), "This can happen either zero or only once as this must be the first tensor in the partition"
+            elif start_index > current_index and start_index < (current_index + param_size):
+                assert (first_offset == 0
+                        ), "This can happen either zero or only once as this must be the first tensor in the partition"
                 first_offset = start_index - current_index
 
-                set_key_value_list(self.param_to_partition_ids[i],
-                                   param_id,
-                                   partition_id)
+                set_key_value_list(self.param_to_partition_ids[i], param_id, partition_id)
                 increment_value(self.total_grads_in_partition[i], partition_id)
 
                 self.is_grad_computed[i][partition_id][param_id] = False
 
                 self.grad_partition_insertion_offset[i][partition_id][param_id] = 0
                 self.grad_start_offset[i][partition_id][param_id] = first_offset
 
@@ -865,38 +822,33 @@
     # create a flat tensor aligned at the alignment boundary
     def flatten_dense_tensors_aligned(self, tensor_list, alignment):
         return self.flatten(align_dense_tensors(tensor_list, alignment))
 
     ############### Independent Partition Gradient ########################
     def reduce_independent_p_g_buckets_and_remove_grads(self, param, i):
         if self.elements_in_ipg_bucket + param.numel() > self.reduce_bucket_size:
-            self.report_ipg_memory_usage("In ipg_remove_grads before reduce_ipg_grads",
-                                         param.numel())
+            self.report_ipg_memory_usage("In ipg_remove_grads before reduce_ipg_grads", param.numel())
             self.reduce_ipg_grads()
             if self.contiguous_gradients and self.overlap_comm:
                 # Swap ipg_index between 0 and 1
                 self.ipg_index = 1 - self.ipg_index
-            self.report_ipg_memory_usage("In ipg_remove_grads after reduce_ipg_grads",
-                                         param.numel())
+            self.report_ipg_memory_usage("In ipg_remove_grads after reduce_ipg_grads", param.numel())
 
         param_id = self.get_param_id(param)
         assert self.params_already_reduced[param_id] == False, \
             f"The parameter {param_id} has already been reduced. \
             Gradient computed twice for this partition. \
             Multiple gradient reduction is currently not supported"
 
         if param.numel() > self.reduce_bucket_size:
             self.extra_large_param_to_reduce = param
 
         elif self.contiguous_gradients:
             # keeping the gradients contiguous to prevent memory fragmentation, and avoid flattening
-            new_grad_tensor = self.ipg_buffer[self.ipg_index].narrow(
-                0,
-                self.elements_in_ipg_bucket,
-                param.numel())
+            new_grad_tensor = self.ipg_buffer[self.ipg_index].narrow(0, self.elements_in_ipg_bucket, param.numel())
             new_grad_tensor.copy_(param.grad.view(-1))
             param.grad.data = new_grad_tensor.data.view_as(param.grad)
 
         self.elements_in_ipg_bucket += param.numel()
 
         assert param.grad is not None, f"rank {dist.get_rank()} - Invalid to reduce Param {param_id} with None gradient"
 
@@ -965,21 +917,21 @@
             for i, param, param_id in self.params_in_ipg_bucket:
 
                 process_group = self.dp_process_group
                 #Averages gradients at parameter level if ipg has a moe param
                 #Otherwise averaging is done at the entire buffer level at the end of the loop
                 # MoE param have different groups
                 if self.ipg_bucket_has_moe_params:
-                    process_group = self.expert_dp_process_group[
-                        param.group_name] if is_moe_param(
-                            param) else self.dp_process_group
+                    process_group = self.expert_dp_process_group[param.group_name] if is_moe_param(
+                        param) else self.dp_process_group
                     param.grad.data.div_(dist.get_world_size(group=process_group))
 
                 partition_ids = self.param_to_partition_ids[i][param_id]
-                assert all([p_id < dist.get_world_size(group=process_group) for p_id in partition_ids]), f"world size {dist.get_world_size(group=process_group)} and p_ids: {partition_ids}"
+                assert all([p_id < dist.get_world_size(group=process_group) for p_id in partition_ids
+                            ]), f"world size {dist.get_world_size(group=process_group)} and p_ids: {partition_ids}"
                 partition_size = self.partition_size[i]
                 # Get all partition ids + their offsets
                 partition_ids_w_offsets = []
                 for partition_id in partition_ids:
                     offset = self.grad_start_offset[i][partition_id][param_id]
                     partition_ids_w_offsets.append((partition_id, offset))
                 partition_ids_w_offsets.sort(key=lambda t: t[1])
@@ -1021,18 +973,15 @@
             for i, (dst, bucket_offset, numel) in enumerate(rank_and_offsets):
                 grad_slice = tensor_to_reduce.narrow(0, int(bucket_offset), int(numel))
                 # if dist.get_rank() == 0:
                 #     print(f"Rank {dist.get_rank()} rank offset id {i} real dp size {dist.get_world_size(group=real_dp_process_group[i])} and dst: {dst}")
                 # dist.barrier()
                 #dist.barrier()
                 dst_rank = dist.get_global_rank(real_dp_process_group[i], dst)
-                async_handle = dist.reduce(grad_slice,
-                                           dst=dst_rank,
-                                           group=real_dp_process_group[i],
-                                           async_op=True)
+                async_handle = dist.reduce(grad_slice, dst=dst_rank, group=real_dp_process_group[i], async_op=True)
                 async_handles.append(async_handle)
 
             for handle in async_handles:
                 handle.wait()
 
             if self.communication_data_type != tensor.dtype:
                 tensor.copy_(tensor_to_reduce)
@@ -1056,92 +1005,68 @@
                 param_start_offset = first_offset
 
             # we dont need all elements of the tensor
             if num_elements > (partition_size - current_offset):
                 num_elements = partition_size - current_offset
 
             self.grad_position[param_id] = [
-                int(group_id),
-                int(param_start_offset),
-                int(current_offset),
-                int(num_elements)
+                int(group_id), int(param_start_offset),
+                int(current_offset), int(num_elements)
             ]
             current_offset += num_elements
 
     def update_overflow_tracker_for_param_grad(self, param):
         if param.grad is not None and self._has_inf_or_nan(param.grad.data):
             self.local_overflow = True
 
     def _get_offload_gradient_dict(self):
         for param_group_index, _ in enumerate(self.optimizer.param_groups):
             self.offload_gradient_dict[param_group_index] = []
             for lp_param in self.params_in_partition[param_group_index]:
                 param_id = self.get_param_id(lp_param)
                 [_, _, dest_offset, num_elements] = self.grad_position[param_id]
-                dest_tensor = self.single_partition_of_fp32_groups[
-                    param_group_index].grad.view(-1).narrow(0,
-                                                            dest_offset,
-                                                            num_elements)
+                dest_tensor = self.single_partition_of_fp32_groups[param_group_index].grad.view(-1).narrow(
+                    0, dest_offset, num_elements)
                 self.offload_gradient_dict[param_group_index].append(dest_tensor)
 
     def async_accumulate_grad_in_cpu_via_gpu(self, param):
         param_id = self.get_param_id(param)
 
         [i, source_offset, dest_offset, num_elements] = self.grad_position[param_id]
 
         # copy to a preexisiting buffer to avoid memory allocation penalty
-        dest_buffer = self.temp_grad_buffer_for_gpu_offload.view(-1).narrow(
-            0,
-            0,
-            param.numel())
+        dest_buffer = self.temp_grad_buffer_for_gpu_offload.view(-1).narrow(0, 0, param.numel())
 
         #buffer for storing gradients for this parameter in CPU
         def buffer_to_accumulate_to_in_cpu():
             if not self.fp16_master_weights_and_gradients:
-                return get_accelerator().pin_memory(
-                    torch.zeros(param.numel(),
-                                dtype=param.dtype,
-                                device=self.device))
+                return get_accelerator().pin_memory(torch.zeros(param.numel(), dtype=param.dtype, device=self.device))
             else:
-                return self.single_partition_of_fp32_groups[i].grad.view(-1).narrow(
-                    0,
-                    dest_offset,
-                    num_elements)
+                return self.single_partition_of_fp32_groups[i].grad.view(-1).narrow(0, dest_offset, num_elements)
 
         #accumulate gradients into param.grad or parts of it that belongs to this partition
         def accumulate_gradients():
             if not self.fp16_master_weights_and_gradients:
-                dest_buffer.copy_(self.accumulated_grads_in_cpu[param_id].view(-1),
-                                  non_blocking=True)
+                dest_buffer.copy_(self.accumulated_grads_in_cpu[param_id].view(-1), non_blocking=True)
                 param.grad.data.view(-1).add_(dest_buffer)
             else:
-                dest_buffer.narrow(0,
-                                   source_offset,
-                                   num_elements).copy_(
-                                       self.accumulated_grads_in_cpu[param_id].view(-1),
-                                       non_blocking=True)
-                param.grad.data.view(-1).narrow(
-                    0,
-                    source_offset,
-                    num_elements).add_(dest_buffer.narrow(0,
-                                                          source_offset,
-                                                          num_elements))
+                dest_buffer.narrow(0, source_offset,
+                                   num_elements).copy_(self.accumulated_grads_in_cpu[param_id].view(-1),
+                                                       non_blocking=True)
+                param.grad.data.view(-1).narrow(0, source_offset,
+                                                num_elements).add_(dest_buffer.narrow(0, source_offset, num_elements))
 
         #move accumulated gradients back to CPU
         def copy_gradients_to_cpu():
             if not self.fp16_master_weights_and_gradients:
-                self.accumulated_grads_in_cpu[param_id].data.copy_(
-                    param.grad.data.view(-1),
-                    non_blocking=True)
+                self.accumulated_grads_in_cpu[param_id].data.copy_(param.grad.data.view(-1), non_blocking=True)
             else:
-                self.accumulated_grads_in_cpu[param_id].data.copy_(
-                    param.grad.data.view(-1).narrow(0,
-                                                    source_offset,
-                                                    num_elements),
-                    non_blocking=True)
+                self.accumulated_grads_in_cpu[param_id].data.copy_(param.grad.data.view(-1).narrow(
+                    0, source_offset, num_elements),
+                                                                   non_blocking=True)
 
         if param_id not in self.accumulated_grads_in_cpu:
             self.accumulated_grads_in_cpu[param_id] = buffer_to_accumulate_to_in_cpu()
 
         if self.micro_step_id > 0:
             accumulate_gradients()
 
@@ -1173,18 +1098,15 @@
         self.norm_for_param_grads[param_id] = accumulated_grad.data.double().norm(2)
 
     def async_inplace_copy_grad_to_fp32_buffer_from_gpu(self, param):
         param_id = self.get_param_id(param)
 
         [i, source_offset, dest_offset, num_elements] = self.grad_position[param_id]
 
-        dest_tensor = self.single_partition_of_fp32_groups[i].grad.view(-1).narrow(
-            0,
-            dest_offset,
-            num_elements)
+        dest_tensor = self.single_partition_of_fp32_groups[i].grad.view(-1).narrow(0, dest_offset, num_elements)
 
         src_tensor = param.grad.view(-1).narrow(0, source_offset, num_elements)
         if not self.fp16_master_weights_and_gradients:
             src_tensor = src_tensor.float()
 
         dest_tensor.copy_(src_tensor, non_blocking=True)
         param.grad = None  #offload only
@@ -1216,24 +1138,21 @@
                         (1) enable ignore_unused_parameters option in zero_optimization config;
                         (2) making sure all trainable parameters and `forward` function
                             outputs participate in calculating loss.
                     """
 
         # Sum across all model parallel GPUs.
         total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
-        dist.all_reduce(total_norm_cuda,
-                        op=dist.ReduceOp.SUM,
-                        group=self.dp_process_group)
+        dist.all_reduce(total_norm_cuda, op=dist.ReduceOp.SUM, group=self.dp_process_group)
 
         self._model_parallel_all_reduce(tensor=total_norm_cuda, op=dist.ReduceOp.SUM)
 
         total_norm = total_norm_cuda[0].item()**(1. / norm_type)
 
-        if total_norm == float(
-                'inf') or total_norm == -float('inf') or total_norm != total_norm:
+        if total_norm == float('inf') or total_norm == -float('inf') or total_norm != total_norm:
             total_norm = -1
 
         return total_norm
 
     ############################################################################################
     def copy_grads_in_partition(self, param):
         if self.cpu_offload:
@@ -1254,46 +1173,41 @@
             self.grads_in_partition_offset = 0
             total_size = 0
             for group in self.params_in_partition:
                 for param_in_partition in group:
                     total_size += param_in_partition.numel()
 
             see_memory_usage(f"before copying {total_size} gradients into partition")
-            self.grads_in_partition = torch.empty(
-                int(total_size),
-                dtype=self.dtype,
-                device=get_accelerator().current_device_name())
+            self.grads_in_partition = torch.empty(int(total_size),
+                                                  dtype=self.dtype,
+                                                  device=get_accelerator().current_device_name())
             see_memory_usage(f"after copying {total_size} gradients into partition")
 
         # The allreduce buffer will be rewritten. Copy the gradients in partition to a new buffer
-        new_grad_tensor = self.grads_in_partition.view(-1).narrow(
-            0,
-            self.grads_in_partition_offset,
-            param.numel())
+        new_grad_tensor = self.grads_in_partition.view(-1).narrow(0, self.grads_in_partition_offset, param.numel())
         new_grad_tensor.copy_(param.grad.view(-1))
         param.grad.data = new_grad_tensor.data.view_as(param.grad)
         #print(f"Grad norm after copy to contiguous_buffer {param.grad.data.norm()}")
         self.grads_in_partition_offset += param.numel()
 
     def reduce_ipg_grads(self):
         if self.contiguous_gradients:
             if self.extra_large_param_to_reduce is not None:
                 assert len(self.params_in_ipg_bucket) == 1, "more than 1 param in ipg bucket, this shouldn't happen"
                 _, _, param_id = self.params_in_ipg_bucket[0]
-                assert self.get_param_id(
-                    self.extra_large_param_to_reduce) == param_id, "param in ipg bucket does not match extra-large param"
+                assert self.get_param_id(self.extra_large_param_to_reduce
+                                         ) == param_id, "param in ipg bucket does not match extra-large param"
                 self.average_tensor(self.extra_large_param_to_reduce.grad.view(-1))
                 self.extra_large_param_to_reduce = None
             else:
                 self.average_tensor(self.ipg_buffer[self.ipg_index])
         else:
-            self.buffered_reduce_fallback(
-                None,
-                self.grads_in_ipg_bucket,
-                elements_per_buffer=self.elements_in_ipg_bucket)
+            self.buffered_reduce_fallback(None,
+                                          self.grads_in_ipg_bucket,
+                                          elements_per_buffer=self.elements_in_ipg_bucket)
 
         if self.overlap_comm:
             stream = self.reduction_stream
         elif self.cpu_offload:
             # TODO: copy_grad_stream is disabled because of race with reduce. This hurts perf and should be fixed.
             #            get_accelerator().synchronize()
             #            stream = self.copy_grad_stream
@@ -1320,29 +1234,29 @@
                                 self.previous_reduced_grads = []
                             self.previous_reduced_grads.append(param)
                         else:
                             param.grad = None  #only if self.partition_gradients
                     elif self.contiguous_gradients:
                         self.copy_grads_in_partition(param)
                 else:  # zero stage 1 - partition only optimizer state
-                    if self.contiguous_gradients and self.is_param_in_current_partition[
-                            param_id]:
+                    if self.contiguous_gradients and self.is_param_in_current_partition[param_id]:
                         self.copy_grads_in_partition(param)
 
         self.grads_in_ipg_bucket = []
         self.params_in_ipg_bucket = []
         self.ipg_bucket_has_moe_params = False
         self.elements_in_ipg_bucket = 0
         #####################################################################
 
     def reduce_ready_partitions_and_remove_grads(self, param, i):
         if self.partition_gradients or self.is_gradient_accumulation_boundary:
             self.reduce_independent_p_g_buckets_and_remove_grads(param, i)
 
     def zero_reduced_gradients(self, partition_id, i):
+
         def are_all_related_partitions_reduced(params_id):
             for partition_id in self.param_to_partition_ids[i][params_id]:
                 if not self.is_partition_reduced[i][partition_id]:
                     return False
             return True
 
         for params_id in self.is_grad_computed[i][partition_id]:
@@ -1354,37 +1268,31 @@
 
         def print_func():
             logger.info(flatten_tensor.contiguous().view(-1).narrow(0, start, n))
 
         self.sequential_execution(print_func, message)
 
     def get_grads_to_reduce(self, i, partition_id):
+
         def get_reducible_portion(key):
             grad = self.param_dict[key].grad
             total_elements = grad.numel()
             start = self.grad_start_offset[i][partition_id][key]
-            num_elements = min(
-                total_elements - start,
-                self.partition_size[i] -
-                self.grad_partition_insertion_offset[i][partition_id][key])
+            num_elements = min(total_elements - start,
+                               self.partition_size[i] - self.grad_partition_insertion_offset[i][partition_id][key])
             if not pg_correctness_test:
                 if num_elements == total_elements:
                     return grad
                 else:
-                    return grad.contiguous().view(-1).narrow(0,
-                                                             int(start),
-                                                             int(num_elements))
+                    return grad.contiguous().view(-1).narrow(0, int(start), int(num_elements))
             else:
                 if num_elements == total_elements:
                     return grad.clone()
                 else:
-                    return grad.clone().contiguous().view(-1).narrow(
-                        0,
-                        int(start),
-                        int(num_elements))
+                    return grad.clone().contiguous().view(-1).narrow(0, int(start), int(num_elements))
 
         grads_to_reduce = []
         for key in self.is_grad_computed[i][partition_id]:
             grad = get_reducible_portion(key)
             grads_to_reduce.append(grad)
         return grads_to_reduce
 
@@ -1452,45 +1360,34 @@
 
         with get_accelerator().stream(stream):
             allreduced = self.allreduce_bucket(small_bucket, rank=rank, log=log)
             if rank is None or rank == dist.get_rank(group=self.dp_process_group):
                 for buf, synced in zip(small_bucket, self.unflatten(allreduced, small_bucket)):
                     buf.copy_(synced)
 
-    def allreduce_no_retain(self,
-                            bucket,
-                            numel_per_bucket=500000000,
-                            rank=None,
-                            log=None):
+    def allreduce_no_retain(self, bucket, numel_per_bucket=500000000, rank=None, log=None):
         small_bucket = []
         numel = 0
         for tensor in bucket:
             small_bucket.append(tensor)
             numel = numel + tensor.numel()
             if numel > numel_per_bucket:
                 self.allreduce_and_copy(small_bucket, rank=rank, log=None)
                 small_bucket = []
 
         if len(small_bucket) > 0:
             self.allreduce_and_copy(small_bucket, rank=rank, log=log)
 
     # allows using reduction of gradients instead of using all_reduce
 
-    def buffered_reduce_fallback(self,
-                                 rank,
-                                 grads,
-                                 elements_per_buffer=500000000,
-                                 log=None):
+    def buffered_reduce_fallback(self, rank, grads, elements_per_buffer=500000000, log=None):
         split_buckets = split_half_float_double(grads)
 
         for i, bucket in enumerate(split_buckets):
-            self.allreduce_no_retain(bucket,
-                                     numel_per_bucket=elements_per_buffer,
-                                     rank=rank,
-                                     log=log)
+            self.allreduce_no_retain(bucket, numel_per_bucket=elements_per_buffer, rank=rank, log=log)
 
     #############################################################################
     #############################################################################
     #############################################################################
 
     # views the tensor as multiple partitions and returns
     # those partitions
@@ -1527,19 +1424,19 @@
         for tensor in tensor_list:
 
             tensor_size = tensor.numel()
 
             if (current_index >= start_index and current_index < end_index):
                 params_in_partition.append(tensor)
 
-            elif start_index > current_index and start_index < (current_index +
-                                                                tensor_size):
+            elif start_index > current_index and start_index < (current_index + tensor_size):
                 params_in_partition.append(tensor)
 
-                assert (first_offset == 0), "This can happen either zero or only once as this must be the first tensor in the partition"
+                assert (first_offset == 0
+                        ), "This can happen either zero or only once as this must be the first tensor in the partition"
                 first_offset = start_index - current_index
 
             else:
                 params_not_in_partition.append(tensor)
 
             current_index = current_index + tensor_size
 
@@ -1585,17 +1482,15 @@
         Returns:
             Total norm of the parameters (viewed as a single vector).
         """
         norm_type = float(norm_type)
         if norm_type == inf:
             total_norm = max(g.data.abs().max() for g in gradients)
             total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
-            dist.all_reduce(total_norm_cuda,
-                            op=dist.ReduceOp.MAX,
-                            group=self.dp_process_group)
+            dist.all_reduce(total_norm_cuda, op=dist.ReduceOp.MAX, group=self.dp_process_group)
 
             # Take max across all GPUs.
             self._model_parallel_all_reduce(tensor=total_norm_cuda, op=dist.ReduceOp.MAX)
             total_norm = total_norm_cuda[0].item()
         else:
             total_norm = 0.0
             # if dist.get_rank() == 0:
@@ -1605,38 +1500,29 @@
                 if hasattr(p, PIPE_REPLICATED) and p.ds_pipe_replicated:
                     continue
                 if is_model_parallel_parameter(p) or (self.model_parallel_rank == 0):
                     param_norm = g.data.double().norm(2)
                     total_norm += param_norm.item()**2
             # Sum across all model parallel GPUs.
             total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
-            dist.all_reduce(total_norm_cuda,
-                            op=dist.ReduceOp.SUM,
-                            group=self.dp_process_group)
+            dist.all_reduce(total_norm_cuda, op=dist.ReduceOp.SUM, group=self.dp_process_group)
 
             self._model_parallel_all_reduce(tensor=total_norm_cuda, op=dist.ReduceOp.SUM)
 
             total_norm = total_norm_cuda[0].item()**(1. / norm_type)
 
-        if total_norm == float(
-                'inf') or total_norm == -float('inf') or total_norm != total_norm:
+        if total_norm == float('inf') or total_norm == -float('inf') or total_norm != total_norm:
             total_norm = -1
 
         return total_norm
 
     # creates a flat fused tensor from the tensor list starting at the first_offset
     # in the first tensor of the list. If there are not enough elements in the tensor
     # list then the flat tensor will be padded with zeros
-    def get_flat_partition(self,
-                           tensor_list,
-                           first_offset,
-                           partition_size,
-                           dtype,
-                           device,
-                           return_tensor_list=False):
+    def get_flat_partition(self, tensor_list, first_offset, partition_size, dtype, device, return_tensor_list=False):
         flat_tensor_list = []
         current_size = 0
         for i, tensor in enumerate(tensor_list):
             if tensor.grad is None:
                 tensor.grad = torch.zeros_like(tensor)
 
             tensor = tensor.grad
@@ -1651,29 +1537,23 @@
             # we dont need all elements of the tensor
             if num_elements > (partition_size - current_size):
                 num_elements = partition_size - current_size
 
             # we need a narrow view of the tensor based on the tensor offset and number of elements that
             # we need from this tensor
             if tensor_offset > 0 or num_elements < tensor.numel():
-                flat_tensor_list.append(tensor.contiguous().view(-1).narrow(
-                    0,
-                    int(tensor_offset),
-                    int(num_elements)))
+                flat_tensor_list.append(tensor.contiguous().view(-1).narrow(0, int(tensor_offset), int(num_elements)))
             else:
                 flat_tensor_list.append(tensor)
 
             current_size = current_size + num_elements
 
         # this means its the last partition and does not align with the dp boundary. We need to pad before flattening
         if current_size < partition_size:
-            flat_tensor_list.append(
-                torch.zeros(int(partition_size - current_size),
-                            dtype=dtype,
-                            device=device))
+            flat_tensor_list.append(torch.zeros(int(partition_size - current_size), dtype=dtype, device=device))
 
         if return_tensor_list:
             return flat_tensor_list
 
         return self.flatten(flat_tensor_list)
 
     def free_grad_in_param_list(self, param_list):
@@ -1711,56 +1591,50 @@
 
     def get_lr(self):
         """Return the current learning rate."""
         return self.optimizer.param_groups[0]["lr"]
 
     def override_loss_scale(self, loss_scale):
         if loss_scale != self.external_loss_scale:
-            logger.info(
-                f'[deepspeed] setting loss scale from {self.external_loss_scale} -> {loss_scale}'
-            )
+            logger.info(f'[deepspeed] setting loss scale from {self.external_loss_scale} -> {loss_scale}')
         self.custom_loss_scaler = True
         self.external_loss_scale = loss_scale
 
     def scaled_global_norm(self, norm_type=2):
         assert norm_type == 2, "only L2 norm supported"
         norm_groups = []
         for i, group in enumerate(self.bit16_groups):
             partition_id = dist.get_rank(group=self.real_dp_process_group[i])
             if self.cpu_offload:
-                norm_groups.append(
-                    self.complete_grad_norm_calculation_for_cpu_offload(
-                        self.params_in_partition[i]))
+                norm_groups.append(self.complete_grad_norm_calculation_for_cpu_offload(self.params_in_partition[i]))
                 single_grad_partition = self.single_partition_of_fp32_groups[i].grad
             else:
-                norm_groups.append(
-                    self.get_grad_norm_direct(self.averaged_gradients[i],
-                                              self.params_in_partition[i]))
+                norm_groups.append(self.get_grad_norm_direct(self.averaged_gradients[i], self.params_in_partition[i]))
 
         if self.has_moe_layers:
             self._average_expert_grad_norms(norm_groups)
 
         # note that the get_global_norm function only supports l2 norm
         return get_global_norm(norm_list=norm_groups)
 
     def get_bit16_param_group(self, group_no):
         bit16_partitions = self.parallel_partitioned_bit16_groups[group_no]
         partition_id = dist.get_rank(group=self.real_dp_process_group[group_no])
-        return [
-            bit16_partitions[dist.get_rank(group=self.real_dp_process_group[group_no])]
-        ]
+        return [bit16_partitions[dist.get_rank(group=self.real_dp_process_group[group_no])]]
 
     def _optimizer_step(self, group_no):
         original_param_groups = self.optimizer.param_groups
         self.optimizer.param_groups = [original_param_groups[group_no]]
-        from deepspeed.ops.adam import DeepSpeedCPUAdam
-        if type(self.optimizer) == DeepSpeedCPUAdam and self.dtype == torch.half:
-            self.optimizer.step(fp16_param_groups=[self.get_bit16_param_group(group_no)])
-        else:
-            self.optimizer.step()
+        # Disabling this as the C++ side copy & synchornize is not working correctly
+        #from deepspeed.ops.adam import DeepSpeedCPUAdam
+        #if type(self.optimizer) == DeepSpeedCPUAdam and self.dtype == torch.half:
+        #    self.optimizer.step(fp16_param_groups=[self.get_bit16_param_group(group_no)])
+        #else:
+        #    self.optimizer.step()
+        self.optimizer.step()
         self.optimizer.param_groups = original_param_groups
 
     def step(self, closure=None):
         """
         Not supporting closure.
         """
         self.micro_step_id = -1
@@ -1798,55 +1672,54 @@
         see_memory_usage('After norm before optimizer')
         # Step 2:- run optimizer and upscaling simultaneously
         for i, group in enumerate(self.bit16_groups):
             self.start_timers([OPTIMIZER_GRADIENTS])
             partition_id = dist.get_rank(group=self.real_dp_process_group[i])
             if self.cpu_offload:
                 single_grad_partition = self.single_partition_of_fp32_groups[i].grad
-                self.unscale_and_clip_grads([single_grad_partition],
-                                            scaled_global_grad_norm)
+                self.unscale_and_clip_grads([single_grad_partition], scaled_global_grad_norm)
                 self.stop_timers([OPTIMIZER_GRADIENTS])
                 self.start_timers([OPTIMIZER_STEP])
                 self._optimizer_step(i)
 
-                from deepspeed.ops.adam import DeepSpeedCPUAdam
-                if not (type(self.optimizer) == DeepSpeedCPUAdam
-                        and self.dtype == torch.half):
-                    bit16_partitions = self.parallel_partitioned_bit16_groups[i]
-                    fp32_partition = self.single_partition_of_fp32_groups[i]
-                    bit16_partitions[partition_id].data.copy_(fp32_partition.data)
+                # Disabled, this is not currently working
+                #from deepspeed.ops.adam import DeepSpeedCPUAdam
+                #if not (type(self.optimizer) == DeepSpeedCPUAdam and self.dtype == torch.half):
+                #    bit16_partitions = self.parallel_partitioned_bit16_groups[i]
+                #    fp32_partition = self.single_partition_of_fp32_groups[i]
+                #    bit16_partitions[partition_id].data.copy_(fp32_partition.data)
+                bit16_partitions = self.parallel_partitioned_bit16_groups[i]
+                fp32_partition = self.single_partition_of_fp32_groups[i]
+                bit16_partitions[partition_id].data.copy_(fp32_partition.data)
 
                 self.stop_timers([OPTIMIZER_STEP])
             else:
                 # free gradients for all the parameters that are not updated by this process(ZeRO stage2)
                 self.free_grad_in_param_list(self.params_not_in_partition[i])
 
                 # create a flat gradients for parameters updated by this process
                 # If we are last partition, ensure we have same size grads and partition size, if not pad with zero tensors
-                if partition_id == dist.get_world_size(
-                        group=self.real_dp_process_group[i]) - 1:
+                if partition_id == dist.get_world_size(group=self.real_dp_process_group[i]) - 1:
                     single_grad_partition = self.flatten_dense_tensors_aligned(
                         self.averaged_gradients[i],
-                        int(self.partition_size[i])).to(
-                            self.single_partition_of_fp32_groups[i].dtype)
+                        int(self.partition_size[i])).to(self.single_partition_of_fp32_groups[i].dtype)
                 else:
                     single_grad_partition = self.flatten(self.averaged_gradients[i]).to(
                         self.single_partition_of_fp32_groups[i].dtype)
                 assert single_grad_partition.numel() == self.partition_size[i], \
                     "averaged gradients have different number of elements that partition size {} {} {} {}".format(
                         single_grad_partition.numel(), self.partition_size[i], i, partition_id)
 
                 self.single_partition_of_fp32_groups[i].grad = single_grad_partition
                 # release all the gradient since we have already created a necessary copy in dp_grad_partition(ZeRO stage2)
                 self.free_grad_in_param_list(self.params_in_partition[i])
 
                 self.averaged_gradients[i] = None
 
-                self.unscale_and_clip_grads([single_grad_partition],
-                                            scaled_global_grad_norm)
+                self.unscale_and_clip_grads([single_grad_partition], scaled_global_grad_norm)
                 self.stop_timers([OPTIMIZER_GRADIENTS])
 
                 # Step 3:- run the optimizer if no offloading
                 self.start_timers([OPTIMIZER_STEP])
                 self._optimizer_step(i)
                 # Step 4:- get rid of the fp32 gradients. Not needed anymore
                 self.single_partition_of_fp32_groups[i].grad = None
@@ -1859,51 +1732,49 @@
         see_memory_usage('After optimizer before all-gather')
         if self.cpu_offload:
             self.reset_cpu_buffers()
 
         self.start_timers([OPTIMIZER_ALLGATHER])
         # Gather the updated weights from everyone.
         # Then all partitions of the model parameters are updated and ready for next round forward.
-        all_gather_dp_groups(
-            partitioned_param_groups=self.parallel_partitioned_bit16_groups,
-            dp_process_group=self.real_dp_process_group,
-            start_alignment_factor=self.nccl_start_alignment_factor,
-            allgather_bucket_size=self.allgather_bucket_size)
+        all_gather_dp_groups(partitioned_param_groups=self.parallel_partitioned_bit16_groups,
+                             dp_process_group=self.real_dp_process_group,
+                             start_alignment_factor=self.nccl_start_alignment_factor,
+                             allgather_bucket_size=self.allgather_bucket_size)
 
         self.stop_timers([OPTIMIZER_ALLGATHER])
 
         # TODO: we probably don't need this? just to be safe
         for i in range(len(self.bit16_groups)):
             self._update_model_bit16_weights(i)
 
         self.log_timers(timer_names)
         see_memory_usage('After zero_optimizer step')
 
         return
 
     @torch.no_grad()
     def update_lp_params(self):
-        for i, (bit16_partitions, fp32_partition) in enumerate(zip(self.parallel_partitioned_bit16_groups, self.single_partition_of_fp32_groups)):
+        for i, (bit16_partitions, fp32_partition) in enumerate(
+                zip(self.parallel_partitioned_bit16_groups, self.single_partition_of_fp32_groups)):
             partition_id = dist.get_rank(group=self.real_dp_process_group[i])
             bit16_partitions[partition_id].data.copy_(fp32_partition.data)
             # print_rank_0(f'update_lp_params {i=} {partition_id=}', force=True)
             # if i == 0:
             #     print_rank_0(f'{fp32_partition[:10]=}', force=True)
 
-        all_gather_dp_groups(
-            partitioned_param_groups=self.parallel_partitioned_bit16_groups,
-            dp_process_group=self.real_dp_process_group,
-            start_alignment_factor=self.nccl_start_alignment_factor,
-            allgather_bucket_size=self.allgather_bucket_size)
+        all_gather_dp_groups(partitioned_param_groups=self.parallel_partitioned_bit16_groups,
+                             dp_process_group=self.real_dp_process_group,
+                             start_alignment_factor=self.nccl_start_alignment_factor,
+                             allgather_bucket_size=self.allgather_bucket_size)
 
     def _average_expert_grad_norms(self, norm_groups):
         for i, norm in enumerate(norm_groups):
             if self.is_moe_param_group[i]:
-                scaled_norm = norm * 1.0 / float(
-                    dist.get_world_size(group=self.real_dp_process_group[i]))
+                scaled_norm = norm * 1.0 / float(dist.get_world_size(group=self.real_dp_process_group[i]))
                 scaled_norm_tensor = torch.tensor(scaled_norm,
                                                   device=get_accelerator().device_name(),
                                                   dtype=torch.float)
                 dist.all_reduce(scaled_norm_tensor, group=self.real_dp_process_group[i])
                 norm_groups[i] = scaled_norm_tensor.item()
 
     def unscale_and_clip_grads(self, grad_groups_flat, total_norm):
@@ -1939,22 +1810,19 @@
             for j, grad in enumerate(self.averaged_gradients[i]):
                 if grad is not None and self._has_inf_or_nan(grad.data, j):
                     return True
         return False
 
     def has_overflow(self, partition_gradients=True):
         if partition_gradients:
-            overflow = self.local_overflow if self.cpu_offload else self.has_overflow_partitioned_grads_serial(
-            )
+            overflow = self.local_overflow if self.cpu_offload else self.has_overflow_partitioned_grads_serial()
             overflow_gpu = get_accelerator().ByteTensor([overflow])
             '''This will capture overflow across all data parallel and expert parallel process
             Since expert parallel process are a subset of data parallel process'''
-            dist.all_reduce(overflow_gpu,
-                            op=dist.ReduceOp.MAX,
-                            group=self.dp_process_group)
+            dist.all_reduce(overflow_gpu, op=dist.ReduceOp.MAX, group=self.dp_process_group)
 
         else:
             params = []
             for group in self.bit16_groups:
                 for param in group:
                     params.append(param)
 
@@ -2083,17 +1951,15 @@
 
     # Return base optimizer states.
     # This method assumes that each param group contains a single flattened tensor.
     def _get_base_optimizer_state(self):
         optimizer_groups_state = []
         for i, group in enumerate(self.optimizer.param_groups):
             p = group['params'][0]
-            lean_optimizer_state = self._get_state_without_padding(
-                self.optimizer.state[p],
-                self.groups_padding[i])
+            lean_optimizer_state = self._get_state_without_padding(self.optimizer.state[p], self.groups_padding[i])
             optimizer_groups_state.append(lean_optimizer_state)
 
         return optimizer_groups_state
 
     def state_dict(self):
         """
         Returns a dict containing the current state of this :class:`FP16_Optimizer` instance.
@@ -2113,16 +1979,15 @@
 
         if self.elastic_checkpoint:
             state_dict[BASE_OPTIMIZER_STATE] = self._get_base_optimizer_state()
         else:
             state_dict[BASE_OPTIMIZER_STATE] = self.optimizer.state_dict()
 
         # Remove paddings for DP alignment to enable loading for other alignment values
-        fp32_groups_without_padding = self._get_groups_without_padding(
-            self.single_partition_of_fp32_groups)
+        fp32_groups_without_padding = self._get_groups_without_padding(self.single_partition_of_fp32_groups)
         state_dict[SINGLE_PARTITION_OF_FP32_GROUPS] = fp32_groups_without_padding
 
         state_dict[
             ZERO_STAGE] = ZeroStageEnum.gradients if self.partition_gradients else ZeroStageEnum.optimizer_states
         state_dict[GROUP_PADDINGS] = self.groups_padding
         state_dict[PARTITION_COUNT] = self.partition_count
 
@@ -2136,51 +2001,45 @@
     # 2) Extracting fp32 weights for current partition from merged weights
     # 3) Using extracted weights to update base optimizer weights directly.
     def _restore_from_elastic_fp32_weights(self, all_state_dict):
         merged_single_partition_of_fp32_groups = []
 
         for i in range(len(self.single_partition_of_fp32_groups)):
             partition_id = dist.get_rank(group=self.real_dp_process_group[i])
-            merged_partitions = [
-                sd[SINGLE_PARTITION_OF_FP32_GROUPS][i] for sd in all_state_dict
-            ]
+            merged_partitions = [sd[SINGLE_PARTITION_OF_FP32_GROUPS][i] for sd in all_state_dict]
             if self.is_moe_group(self.optimizer.param_groups[i]):
-                ranks = self.get_ep_ranks(
-                    group_name=self.optimizer.param_groups[i]['name'])
+                ranks = self.get_ep_ranks(group_name=self.optimizer.param_groups[i]['name'])
                 merged_partitions = [merged_partitions[i] for i in ranks]
             flat_merged_partitions = self.flatten_dense_tensors_aligned(
                 merged_partitions,
-                self.nccl_start_alignment_factor *
-                dist.get_world_size(group=self.real_dp_process_group[i]))
+                self.nccl_start_alignment_factor * dist.get_world_size(group=self.real_dp_process_group[i]))
             dp_partitions = self.get_data_parallel_partitions(flat_merged_partitions, i)
             merged_single_partition_of_fp32_groups.append(dp_partitions[partition_id])
 
         for current, saved in zip(self.single_partition_of_fp32_groups, merged_single_partition_of_fp32_groups):
             current.data.copy_(saved.data)
 
     # Restore base optimizer fp32 weights from ZeRO fp16 or bfloat16 weights
     def _restore_from_bit16_weights(self):
-        for group_id, (bit16_partitions, fp32_partition) in enumerate(zip(self.parallel_partitioned_bit16_groups, self.single_partition_of_fp32_groups)):
+        for group_id, (bit16_partitions, fp32_partition) in enumerate(
+                zip(self.parallel_partitioned_bit16_groups, self.single_partition_of_fp32_groups)):
             partition_id = dist.get_rank(group=self.real_dp_process_group[group_id])
             fp32_partition.data.copy_(bit16_partitions[partition_id].data)
 
     # Refresh the fp32 master params from the fp16 or bfloat16 copies.
     def refresh_fp32_params(self):
         self._restore_from_bit16_weights()
 
     # Extract optimizer state for current partition from merged states of all partitions
     def _partition_base_optimizer_state(self, state_key, all_partition_states, group_id):
         partition_id = dist.get_rank(group=self.real_dp_process_group[group_id])
         alignment = dist.get_world_size(group=self.real_dp_process_group[group_id])
         if torch.is_tensor(all_partition_states[0]):
-            flat_merged_partitions = self.flatten_dense_tensors_aligned(
-                all_partition_states,
-                alignment)
-            dp_partitions = self.get_data_parallel_partitions(flat_merged_partitions,
-                                                              group_id)
+            flat_merged_partitions = self.flatten_dense_tensors_aligned(all_partition_states, alignment)
+            dp_partitions = self.get_data_parallel_partitions(flat_merged_partitions, group_id)
             return dp_partitions[partition_id]
         else:
             # Assume non-tensor states are not partitioned and equal across ranks, so return first one
             return all_partition_states[0]
 
     def _restore_base_optimizer_state(self, base_optimizer_group_states):
         if type(base_optimizer_group_states) == dict:
@@ -2207,55 +2066,38 @@
     # 1) Merging optimizer state from checkpoints of all partitions
     # 2) Extracting optimizer state for current partition from the merged state
     # 3) Using the extracted value to directly update the base optimizer.
     def _restore_elastic_base_optimizer_state(self, all_state_dict):
         base_optimizer_group_states = []
         for i in range(len(self.optimizer.param_groups)):
             partition_states = {}
-            all_partition_group_states = [
-                sd[BASE_OPTIMIZER_STATE][i] for sd in all_state_dict
-            ]
+            all_partition_group_states = [sd[BASE_OPTIMIZER_STATE][i] for sd in all_state_dict]
 
             if self.is_moe_group(self.optimizer.param_groups[i]):
-                ranks = self.get_ep_ranks(
-                    group_name=self.optimizer.param_groups[i]['name'])
-                all_partition_group_states = [
-                    all_partition_group_states[i] for i in ranks
-                ]
+                ranks = self.get_ep_ranks(group_name=self.optimizer.param_groups[i]['name'])
+                all_partition_group_states = [all_partition_group_states[i] for i in ranks]
 
             for key in all_partition_group_states[0].keys():
-                all_partition_states = [
-                    all_states[key] for all_states in all_partition_group_states
-                ]
-                partition_states[key] = self._partition_base_optimizer_state(
-                    key,
-                    all_partition_states,
-                    i)
+                all_partition_states = [all_states[key] for all_states in all_partition_group_states]
+                partition_states[key] = self._partition_base_optimizer_state(key, all_partition_states, i)
             base_optimizer_group_states.append(partition_states)
 
         self._restore_base_optimizer_state(base_optimizer_group_states)
 
     def load_state_dict(self,
                         state_dict_list,
                         load_optimizer_states=True,
                         load_from_fp32_weights=False,
                         checkpoint_folder=None):
         if checkpoint_folder:
-            self._load_universal_checkpoint(checkpoint_folder,
-                                            load_optimizer_states,
-                                            load_from_fp32_weights)
+            self._load_universal_checkpoint(checkpoint_folder, load_optimizer_states, load_from_fp32_weights)
         else:
-            self._load_legacy_checkpoint(state_dict_list,
-                                         load_optimizer_states,
-                                         load_from_fp32_weights)
-
-    def _load_universal_checkpoint(self,
-                                   checkpoint_folder,
-                                   load_optimizer_states,
-                                   load_from_fp32_weights):
+            self._load_legacy_checkpoint(state_dict_list, load_optimizer_states, load_from_fp32_weights)
+
+    def _load_universal_checkpoint(self, checkpoint_folder, load_optimizer_states, load_from_fp32_weights):
         self._load_hp_checkpoint_state(checkpoint_folder)
 
     @property
     def param_groups(self):
         """Forward the wrapped optimizer's parameters."""
         return self.optimizer.param_groups
 
@@ -2264,24 +2106,18 @@
         tp_rank = bwc_tensor_model_parallel_rank(mpu=self.mpu)
         tp_world_size = self.mpu.get_slice_parallel_world_size()
 
         for i, _ in enumerate(self.optimizer.param_groups):
             for lp in self.bit16_groups[i]:
                 if lp._hp_mapping is not None:
                     #print(f"Loading {self.param_names[lp]} {tp_rank=} {tp_world_size=}")
-                    lp.load_hp_checkpoint_state(
-                        os.path.join(checkpoint_dir,
-                                     self.param_names[lp]),
-                        tp_rank,
-                        tp_world_size)
-
-    def _load_legacy_checkpoint(self,
-                                state_dict_list,
-                                load_optimizer_states=True,
-                                load_from_fp32_weights=False):
+                    lp.load_hp_checkpoint_state(os.path.join(checkpoint_dir, self.param_names[lp]), tp_rank,
+                                                tp_world_size)
+
+    def _load_legacy_checkpoint(self, state_dict_list, load_optimizer_states=True, load_from_fp32_weights=False):
         r"""Loading ZeRO checkpoint
 
         Arguments:
             state_dict_list: List of all saved ZeRO checkpoints, one for each saved partition.
                 Note that the number of saved partitions may differ from number of loading partitions to support
                 changing GPU count, specifically DP world size, between saving and loading checkpoints.
             load_optimizer_states: Boolean indicating whether or not to load base optimizer states
@@ -2304,16 +2140,15 @@
             optimizer.load_state_dict(checkpoint['optimizer'])
         """
 
         # I think it should actually be ok to reload the optimizer before the model.
         dp_rank = dist.get_rank(group=self.dp_process_group)
         current_rank_sd = state_dict_list[dp_rank]
         self.loss_scaler = current_rank_sd.get('loss_scaler', self.loss_scaler)
-        self.dynamic_loss_scale = current_rank_sd.get('dynamic_loss_scale',
-                                                      self.dynamic_loss_scale)
+        self.dynamic_loss_scale = current_rank_sd.get('dynamic_loss_scale', self.dynamic_loss_scale)
         self.overflow = current_rank_sd.get('overflow', self.overflow)
         self.clip_grad = current_rank_sd.get(CLIP_GRAD, self.clip_grad)
 
         ckpt_version = current_rank_sd.get(DS_VERSION, False)
         assert ckpt_version, f"Empty ds_version in checkpoint, not clear how to proceed"
         ckpt_version = pkg_version.parse(ckpt_version)
 
@@ -2343,16 +2178,15 @@
                 self.optimizer.load_state_dict(current_rank_sd[BASE_OPTIMIZER_STATE])
             else:
                 if self.elastic_checkpoint:
                     # loading elastic into elastic exec
                     self._restore_elastic_base_optimizer_state(state_dict_list)
                 else:
                     # loading an elastic checkpoint into rigid exec
-                    self._restore_base_optimizer_state(
-                        current_rank_sd[BASE_OPTIMIZER_STATE])
+                    self._restore_base_optimizer_state(current_rank_sd[BASE_OPTIMIZER_STATE])
 
         # At this point, the optimizer's references to the model's fp32 parameters are up to date.
         # The optimizer's hyperparameters and internal buffers are also up to date.
         # However, the fp32 master copies of the model's fp16 params stored by the optimizer are still
         # out of date.  There are two options.
         # 1:  Refresh the master params from the model's fp16 params.
         # This requires less storage but incurs precision loss.
@@ -2367,15 +2201,16 @@
 
         if load_from_fp32_weights:
             # option 2 from above
             if self.elastic_checkpoint and not ckpt_is_rigid:
                 self._restore_from_elastic_fp32_weights(state_dict_list)
             else:
                 # For non-elastic checkpoint, simply copying from saved weights of current rank is sufficient.
-                for current, saved in zip(self.single_partition_of_fp32_groups, current_rank_sd[SINGLE_PARTITION_OF_FP32_GROUPS]):
+                for current, saved in zip(self.single_partition_of_fp32_groups,
+                                          current_rank_sd[SINGLE_PARTITION_OF_FP32_GROUPS]):
                     src_tensor = _get_padded_tensor(saved, current.numel())
                     current.data.copy_(src_tensor.data)
         else:
             # option 1 from above
             self._restore_from_bit16_weights()
 
         if load_optimizer_states:
@@ -2387,17 +2222,15 @@
     rank = dist.get_rank()
     if rank == 0:
         t_i = -1
         for v_i, v in enumerate(x.data.contiguous().view(-1)):
             if not math.isfinite(float(v)):
                 t_i = v_i
                 break
-        logger.info(
-            f"rank {rank} detected overflow {cpu_sum} in tensor {i}:{t_i} shape {x.shape}"
-        )
+        logger.info(f"rank {rank} detected overflow {cpu_sum} in tensor {i}:{t_i} shape {x.shape}")
 
 
 def estimate_zero2_model_states_mem_needs(total_params,
                                           num_gpus_per_node=1,
                                           num_nodes=1,
                                           cpu_offload=True,
                                           additional_buffer_factor=1.5):
@@ -2412,17 +2245,15 @@
         cpu_mem = total_params * 4 * num_gpus_per_node * additional_buffer_factor
 
     return int(cpu_mem), int(gpu_mem)
 
 
 def model_to_params(model):
     # shared params calculated only once
-    total_params = sum(
-        dict((p.data_ptr(),
-              p.numel()) for p in model.parameters()).values())
+    total_params = sum(dict((p.data_ptr(), p.numel()) for p in model.parameters()).values())
     return total_params
 
 
 def estimate_zero2_model_states_mem_needs_all_live(model,
                                                    num_gpus_per_node=1,
                                                    num_nodes=1,
                                                    additional_buffer_factor=1.5):
@@ -2442,19 +2273,18 @@
         - ``num_nodes``: how many nodes (defaults to 1),
         - ``additional_buffer_factor``: estimation factor (defaults to 1.5):
 
     """
 
     total_params = model_to_params(model)
 
-    estimate_zero2_model_states_mem_needs_all_cold(
-        total_params=total_params,
-        num_gpus_per_node=num_gpus_per_node,
-        num_nodes=num_nodes,
-        additional_buffer_factor=additional_buffer_factor)
+    estimate_zero2_model_states_mem_needs_all_cold(total_params=total_params,
+                                                   num_gpus_per_node=num_gpus_per_node,
+                                                   num_nodes=num_nodes,
+                                                   additional_buffer_factor=additional_buffer_factor)
 
 
 def estimate_zero2_model_states_mem_needs_all_cold(total_params,
                                                    num_gpus_per_node=1,
                                                    num_nodes=1,
                                                    additional_buffer_factor=1.5):
     """
@@ -2470,31 +2300,29 @@
     Args:
         - ``total_params``: total  model params
         - ``num_gpus_per_node``: how many gpus per node (defaults to 1)
         - ``num_nodes``: how many nodes (defaults to 1),
         - ``additional_buffer_factor``: estimation factor (defaults to 1.5):
 
     """
+
     def format_options(cpu_offload):
         enabled = []
         device = f'{OffloadDeviceEnum.cpu:4}' if cpu_offload else "none"
         enabled.append(f"offload_optimizer={device}")
         return ", ".join(enabled)
 
     nodes_str = "nodes" if num_nodes > 1 else "node"
     gpus_str = "GPUs" if num_gpus_per_node > 1 else "GPU"
-    print(
-        "Estimated memory needed for params, optim states and gradients for a:\n"
-        f"HW: Setup with {num_nodes} {nodes_str}, {num_gpus_per_node} {gpus_str} per node.\n"
-        f"SW: Model with {int(total_params/1e6)}M total params.")
+    print("Estimated memory needed for params, optim states and gradients for a:\n"
+          f"HW: Setup with {num_nodes} {nodes_str}, {num_gpus_per_node} {gpus_str} per node.\n"
+          f"SW: Model with {int(total_params/1e6)}M total params.")
     print("  per CPU  |  per GPU |   Options")
     for cpu_offload in [True, False]:
-        cpu_mem, gpu_mem = estimate_zero2_model_states_mem_needs(
-            total_params=total_params,
-            num_gpus_per_node=num_gpus_per_node,
-            num_nodes=num_nodes,
-            cpu_offload=cpu_offload,
-            additional_buffer_factor=additional_buffer_factor
-        )
+        cpu_mem, gpu_mem = estimate_zero2_model_states_mem_needs(total_params=total_params,
+                                                                 num_gpus_per_node=num_gpus_per_node,
+                                                                 num_nodes=num_nodes,
+                                                                 cpu_offload=cpu_offload,
+                                                                 additional_buffer_factor=additional_buffer_factor)
 
         options_str = format_options(cpu_offload=cpu_offload)
         print(f" {cpu_mem/2**30:7.2f}GB | {gpu_mem/2**30:6.2f}GB | {options_str}")
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/zero/test.py` & `deepspeed-0.9.0/deepspeed/runtime/zero/test.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,11 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import torch
 from deepspeed.runtime.zero.contiguous_memory_allocator import ContiguousMemoryAllocator
 
 
 def test1():
     mem = ContiguousMemoryAllocator(1024, torch.half, 'cpu')
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/zero/tiling.py` & `deepspeed-0.9.0/deepspeed/runtime/zero/tiling.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,11 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import torch
 import deepspeed
 from deepspeed.runtime.utils import partition_uniform as partition
 
 
 def split_tensor_along_last_dim(tensor, partitions, contiguous_split_chunks=False):
@@ -23,14 +26,15 @@
     if contiguous_split_chunks:
         return tuple(chunk.contiguous() for chunk in tensor_list)
 
     return tensor_list
 
 
 class TiledLinear(torch.nn.Module):
+
     def __init__(self,
                  in_features,
                  out_features,
                  bias=True,
                  in_splits=1,
                  out_splits=1,
                  input_is_already_split=False,
@@ -110,34 +114,30 @@
             local_out_dim = self.out_parts[out_id + 1] - self.out_parts[out_id]
 
             for in_id in range(in_splits):
                 #if input_size is split, we only need one bias
                 local_bias = bias if in_id == (in_splits - 1) else False
 
                 local_in_dim = self.in_parts[in_id + 1] - self.in_parts[in_id]
-                local = linear_cls(local_in_dim,
-                                   local_out_dim,
-                                   bias=local_bias,
-                                   **kwargs)
+                local = linear_cls(local_in_dim, local_out_dim, bias=local_bias, **kwargs)
                 self.linears[out_id].append(local)
 
         # Optionally initialize with a known tensor
         if init_linear is not None:
             self.copy_params_from(init_linear)
 
     def forward(self, input_):
         if self.in_splits > 1 and not self.input_is_already_split:
             input_parts = partition(input_.shape[-1], self.in_splits)
-            split_sizes = [
-                input_parts[p + 1] - input_parts[p] for p in range(self.in_splits)
-            ]
+            split_sizes = [input_parts[p + 1] - input_parts[p] for p in range(self.in_splits)]
             inputs = self._split_global_input(input_, split_sizes)
         elif self.in_splits > 1:
             inputs = input_
-            assert len(inputs) == self.in_splits, f"Col splits {self.in_splits} does not match input splits {len(inputs)}"
+            assert len(
+                inputs) == self.in_splits, f"Col splits {self.in_splits} does not match input splits {len(inputs)}"
         else:
             # no splits
             inputs = [input_]
 
         outputs = [None] * self.out_splits
         for out_id in range(self.out_splits):
             for in_id in range(self.in_splits):
@@ -256,31 +256,29 @@
                     local.bias.data.copy_(other.bias[rstart:rstop].data)
 
 
 class TiledLinearReturnBias(TiledLinear):
     """Wrapper for a Linear class that returns its own bias parameter, such as
     used by Megatron-LM.
     """
+
     def _reduce_local_output(self, in_id, out_id, current_out, new_out):
         """Reduces output tensors, but not the returned bias. """
         if current_out is not None:
             old_tensor, old_bias = current_out
         else:
             old_tensor, old_bias = None, None
 
         assert isinstance(new_out, tuple)
         assert len(new_out) == 2
 
         tensor, bias = new_out
         assert tensor is not None
 
-        tensor = super()._reduce_local_output(in_id=in_id,
-                                              out_id=out_id,
-                                              current_out=old_tensor,
-                                              new_out=tensor)
+        tensor = super()._reduce_local_output(in_id=in_id, out_id=out_id, current_out=old_tensor, new_out=tensor)
 
         if bias is None:
             bias = old_bias
 
         return tensor, bias
 
     def _combine_output_splits(self, outputs):
```

### Comparing `deepspeed-0.8.3/deepspeed/runtime/zero/utils.py` & `deepspeed-0.9.0/deepspeed/runtime/zero/utils.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,8 +1,11 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import os
 from typing import List
 
 import torch
 from deepspeed import comm as dist
 from deepspeed.utils import logger
@@ -11,17 +14,15 @@
 from deepspeed.utils.nvtx import instrument_w_nvtx
 from deepspeed.accelerator import get_accelerator
 
 
 def _initialize_parameter_parallel_groups(parameter_parallel_size=None):
     data_parallel_size = int(dist.get_world_size())
     parameter_parallel_size = parameter_parallel_size or data_parallel_size
-    logger.info("data_parallel_size: %s, parameter_parallel_size: %s",
-                data_parallel_size,
-                parameter_parallel_size)
+    logger.info("data_parallel_size: %s, parameter_parallel_size: %s", data_parallel_size, parameter_parallel_size)
     assert data_parallel_size % parameter_parallel_size == 0, \
         'world size should be divisible by parameter parallel size'
     rank = dist.get_rank()
     my_group = None
     for i in range(data_parallel_size // parameter_parallel_size):
         ranks = range(i * parameter_parallel_size, (i + 1) * parameter_parallel_size)
         group = dist.new_group(ranks)
@@ -30,35 +31,28 @@
     return my_group
 
 
 class ZeRORuntimeException(Exception):
     pass
 
 
-ZERO_SUPPORTED_OPTIMIZERS = [
-    torch.optim.Adam,
-    torch.optim.AdamW,
-    FusedAdam,
-    DeepSpeedCPUAdam
-]
+ZERO_SUPPORTED_OPTIMIZERS = [torch.optim.Adam, torch.optim.AdamW, FusedAdam, DeepSpeedCPUAdam]
 
 # Add apex FusedAdam to supported list if apex is installed
 try:
     import apex
     if hasattr(apex, 'optimizers') and hasattr(apex.optimizers, 'FusedAdam'):
         ZERO_SUPPORTED_OPTIMIZERS.append(apex.optimizers.FusedAdam)
 except ImportError:
     pass
 
 
 def is_zero_supported_optimizer(optimizer):
     if dist.get_rank() == 0:
-        logger.info(
-            f'Checking ZeRO support for optimizer={optimizer.__class__.__name__} type={type(optimizer)}'
-        )
+        logger.info(f'Checking ZeRO support for optimizer={optimizer.__class__.__name__} type={type(optimizer)}')
     return type(optimizer) in ZERO_SUPPORTED_OPTIMIZERS
 
 
 def get_lst_from_rank0(lst: List[int]) -> None:
     """
     NOTE: creates both communication and synchronization overhead so should be used
     sparingly
```

### Comparing `deepspeed-0.8.3/deepspeed/utils/comms_logging.py` & `deepspeed-0.9.0/deepspeed/utils/comms_logging.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,8 +1,11 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import math
 from deepspeed.utils import log_dist
 
 
 def get_caller_func(frame=3):
     import sys
@@ -27,15 +30,15 @@
 
     n = dist.get_world_size()
     tput = 0
     busbw = 0
     if comm_op == "all_to_all_single":
         tput = (size / duration)
         busbw = (size / duration) * ((n - 1) / n)
-    elif comm_op == "all_gather" or comm_op == "all_gather_base" or comm_op == "reduce_scatter" or comm_op == "reduce_scatter_base":
+    elif comm_op == "all_gather" or comm_op == "all_gather_into_tensor" or comm_op == "reduce_scatter" or comm_op == "reduce_scatter_tensor":
         size *= n
         tput = (size / duration)
         busbw = (size / duration) * ((n - 1) / n)
     elif comm_op == "all_reduce":
         tput = (size * 2 / duration)
         busbw = (size / duration) * (2 * (n - 1) / n)
     elif comm_op == "send" or comm_op == "recv" or comm_op == "isend" or comm_op == "irecv" or comm_op == "broadcast" or comm_op == "reduce" or comm_op == "gather" or comm_op == "scatter" or comm_op == "barrier":
@@ -52,14 +55,15 @@
     tput /= 1e6
     busbw /= 1e6
 
     return tput, busbw
 
 
 class CommsLogger:
+
     def __init__(self):
         from deepspeed.comm.constants import COMMS_LOGGER_VERBOSE_DEFAULT, COMMS_LOGGER_DEBUG_DEFAULT, COMMS_LOGGER_PROF_OPS_DEFAULT, COMMS_LOGGER_PROF_ALL_DEFAULT, COMMS_LOGGER_ENABLED_DEFAULT
         self.comms_dict = {}
         self.verbose = COMMS_LOGGER_VERBOSE_DEFAULT
         self.debug = COMMS_LOGGER_DEBUG_DEFAULT
         self.prof_ops = COMMS_LOGGER_PROF_OPS_DEFAULT
         self.prof_all = COMMS_LOGGER_PROF_ALL_DEFAULT
@@ -107,16 +111,15 @@
         else:
             # Create entirely new record
             self.comms_dict[record_name] = {msg_size: [1, [latency], [algbw], [busbw]]}
         # If verbose, print every comm op
         # TODO: Add to tensorboard
         if self.verbose:
             n = dist.get_world_size()
-            log_str = f"rank={dist.get_rank()} | comm op: " + record_name + " | time (ms): {:.2f}".format(
-                latency)
+            log_str = f"rank={dist.get_rank()} | comm op: " + record_name + " | time (ms): {:.2f}".format(latency)
             log_str += " | msg size: " + convert_size(msg_size)
             log_str += " | algbw (Gbps): {:.2f} ".format(algbw)
             log_str += " | busbw (Gbps): {:.2f} ".format(busbw)
             log_dist(log_str, [0])
 
     # Print summary at end of iteration, epoch, or training
     def log_all(self):
```

### Comparing `deepspeed-0.8.3/deepspeed/utils/debug.py` & `deepspeed-0.9.0/deepspeed/utils/debug.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,11 @@
-'''Copyright The Microsoft DeepSpeed Team'''
-""" debug utils """
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 # For lazy import with printflock()
 fcntl = None
 
 # for debug purposes map module and param objects to their fully qualified names
 module_names = {}
 param_names = {}
@@ -124,22 +126,21 @@
         fh = open(f"log_rank_{rank}.txt", "w")
     for m in msgs:
         fh.write(f"{m}\n")
     fh.flush()
 
 
 def print_backward_tensors(tensor):
+
     def _print_bwd_tensors(grad_fn):
         print(f"Backward tensors in {grad_fn}")
         for funcs in grad_fn.next_functions:
             if funcs[0]:
                 try:
                     tensor = getattr(funcs[0], 'variable')
                     print(funcs[0])
-                    print(
-                        f"Tensor - id: {id(tensor)}, shape: {tensor.shape}, data: {tensor}, grad: {tensor.grad}"
-                    )
+                    print(f"Tensor - id: {id(tensor)}, shape: {tensor.shape}, data: {tensor}, grad: {tensor.grad}")
                 except AttributeError as e:
                     _print_bwd_tensors(funcs[0])
 
     if hasattr(tensor, 'grad_fn'):
         _print_bwd_tensors(tensor.grad_fn)
```

### Comparing `deepspeed-0.8.3/deepspeed/utils/groups.py` & `deepspeed-0.9.0/deepspeed/utils/groups.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-'''
-Copyright 2021 The Microsoft DeepSpeed Team
-'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 # The file has been adapted from https://github.com/NVIDIA/Megatron-LM and retains the following license from the original file
 
 # Copyright (c) 2019, NVIDIA CORPORATION.  All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
@@ -47,16 +48,15 @@
     raise DeprecatedException(
         "Please do not use the groups.initialize() API as it is deprecated. Instead, pass the desired ep_size to deepspeed.moe.layer.MoE(..,ep_size,..)"
     )
 
 
 def _ensure_divisibility(numerator, denominator):
     """Ensure that numerator is divisible by the denominator."""
-    assert numerator % denominator == 0, '{} is not divisible by {}'.format(
-        numerator, denominator)
+    assert numerator % denominator == 0, '{} is not divisible by {}'.format(numerator, denominator)
 
 
 # Not currently used. Helper function to create a model (tensor) parallel group.
 def _create_model_parallel(model_parallel_size_):
     """
     Initialize model data parallel groups.
 
@@ -74,16 +74,15 @@
         2 data parallel groups:
             [g0, g2, g4, g6], [g1, g3, g5, g7]
     Note that for efficiency, the caller should make sure adjacent ranks
     are on the same DGX box. For example if we are using 2 DGX-1 boxes
     with a total of 16 GPUs, rank 0 to 7 belong to the first box and
     ranks 8 to 15 belong to the second box.
     """
-    log_dist(f'Creating model parallel group with size {model_parallel_size_}',
-             ranks=[0])
+    log_dist(f'Creating model parallel group with size {model_parallel_size_}', ranks=[0])
     # Get world size and rank. Ensure some consistencies.
     assert dist.is_initialized()
     world_size = dist.get_world_size()
     model_parallel_size = min(model_parallel_size_, world_size)
     _ensure_divisibility(world_size, model_parallel_size)
     rank = dist.get_rank()
 
@@ -117,17 +116,15 @@
         expert_parallel_size = 2 # number of experts in same group
         expert_data_parallel_group = [0,2,4,6,8,10,12,14], [1,3,5,7,9,11,13,15] - all reduce is only on MoE params
         expert_parallel_group = [0, 1], [2,3], [4,5], [6,7], [8,9] - no all reduce, but all to all
         data_parallel_group = [0,1,...,15] - all reduce is only on non-MoE
     """
     assert dist.is_initialized()
 
-    log_dist(
-        f'Creating expert and data parallel groups with size {expert_parallel_size_}',
-        ranks=[0])
+    log_dist(f'Creating expert and data parallel groups with size {expert_parallel_size_}', ranks=[0])
     world_size = dist.get_world_size()
     rank = dist.get_rank()
 
     _ensure_divisibility(world_size, expert_parallel_size_)
 
     group_name = f"ep_size_{expert_parallel_size_}"
 
@@ -135,31 +132,27 @@
     global _EXPERT_DATA_PARALLEL_GROUP
 
     # Only create group if it does not already exist
     if group_name not in _EXPERT_DATA_PARALLEL_GROUP:
         for i in range(expert_parallel_size_):
             ranks = range(i, world_size, expert_parallel_size_)
             group = dist.new_group(ranks)
-            log_dist(
-                f'Creating expert data parallel process group named {group_name} with ranks: {list(ranks)}',
-                [0])
+            log_dist(f'Creating expert data parallel process group named {group_name} with ranks: {list(ranks)}', [0])
             if i == (rank % expert_parallel_size_):
                 _EXPERT_DATA_PARALLEL_GROUP[group_name] = group
 
     # Build the expert parallel groups.
     global _EXPERT_PARALLEL_GROUP
 
     # Only create group if it does not already exist
     if group_name not in _EXPERT_PARALLEL_GROUP:
         for i in range(world_size // expert_parallel_size_):
             ranks = range(i * expert_parallel_size_, (i + 1) * expert_parallel_size_)
             group = dist.new_group(ranks)
-            log_dist(
-                f'creating expert parallel process group named {group_name} with ranks: {list(ranks)}',
-                [0])
+            log_dist(f'creating expert parallel process group named {group_name} with ranks: {list(ranks)}', [0])
             if i == (rank // expert_parallel_size_):
                 _EXPERT_PARALLEL_GROUP[group_name] = group
 
 
 def _get_expert_parallel_ranks(world_size, model_parallel_size_, expert_parallel_size_):
     """Generate expert parallel and expert data parallel group ranks list.
```

### Comparing `deepspeed-0.8.3/deepspeed/utils/init_on_device.py` & `deepspeed-0.9.0/deepspeed/utils/init_on_device.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,12 @@
-'''
-Copyright 2020 The Microsoft DeepSpeed Team
-'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+
 import torch
 from typing import Callable
 from torch import Tensor
 from packaging import version as pkg_version
 
 
 class OnDevice(object):
@@ -31,32 +33,30 @@
     def __init__(self, dtype, device="meta", enabled=True):
         self.dtype = dtype
         self.enabled = enabled
         self.device = device
 
         if device == "meta":
             if pkg_version.parse('1.10') > pkg_version.parse(torch.__version__):
-                raise NotImplementedError(
-                    "Meta tensor support is not available, please upgrade to torch 1.10+"
-                )
-
-    def fp_tensor_constructor(self,
-                              fn: Callable,
-                              target_fp_dtype: torch.dtype) -> Callable:
+                raise NotImplementedError("Meta tensor support is not available, please upgrade to torch 1.10+")
+
+    def fp_tensor_constructor(self, fn: Callable, target_fp_dtype: torch.dtype) -> Callable:
+
         def wrapped_fn(*args, **kwargs) -> Tensor:
             if kwargs.get("device", None) is None:
                 kwargs['device'] = self.device
             tensor: Tensor = fn(*args, **kwargs)
             if tensor.is_floating_point():
                 tensor = tensor.to(target_fp_dtype)
             return tensor
 
         return wrapped_fn
 
     def get_new_tensor_fn_for_dtype(self, dtype: torch.dtype) -> Callable:
+
         def new_tensor(cls, *args) -> Tensor:
             tensor = OnDevice._orig_torch_empty(0, device=self.device).new_empty(*args)
             if tensor.is_floating_point():
                 tensor = tensor.to(dtype)
             return tensor
 
         return new_tensor
```

### Comparing `deepspeed-0.8.3/deepspeed/utils/logging.py` & `deepspeed-0.9.0/deepspeed/utils/logging.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,11 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import functools
 import logging
 import sys
 import os
 
 log_levels = {
@@ -11,14 +14,15 @@
     "warning": logging.WARNING,
     "error": logging.ERROR,
     "critical": logging.CRITICAL,
 }
 
 
 class LoggerFactory:
+
     @staticmethod
     def create_logger(name=None, level=logging.INFO):
         """create a logger
 
         Args:
             name (str): name of the logger
             level: level of logger
@@ -26,17 +30,16 @@
         Raises:
             ValueError is name is None
         """
 
         if name is None:
             raise ValueError("name for logger cannot be None")
 
-        formatter = logging.Formatter(
-            "[%(asctime)s] [%(levelname)s] "
-            "[%(filename)s:%(lineno)d:%(funcName)s] %(message)s")
+        formatter = logging.Formatter("[%(asctime)s] [%(levelname)s] "
+                                      "[%(filename)s:%(lineno)d:%(funcName)s] %(message)s")
 
         logger_ = logging.getLogger(name)
         logger_.setLevel(level)
         logger_.propagate = False
         ch = logging.StreamHandler(stream=sys.stdout)
         ch.setLevel(level)
         ch.setFormatter(formatter)
```

### Comparing `deepspeed-0.8.3/deepspeed/utils/tensor_fragment.py` & `deepspeed-0.9.0/deepspeed/utils/tensor_fragment.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-"""
-Copyright 2022 The Microsoft DeepSpeed Team
-"""
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import torch
 from dataclasses import dataclass
 from deepspeed import comm as dist
 
 
 @dataclass
@@ -44,18 +45,15 @@
         return list(self.optim_fragment.keys())
 
 
 def get_full_hp_param(self, optim_state_key=None):
     reduce_buffer = torch.zeros_like(self, dtype=torch.float32).flatten()
     if self._hp_mapping is not None:
         lp_frag_address = self._hp_mapping.lp_fragment_address
-        reduce_fragment = torch.narrow(reduce_buffer,
-                                       0,
-                                       lp_frag_address.start,
-                                       lp_frag_address.numel)
+        reduce_fragment = torch.narrow(reduce_buffer, 0, lp_frag_address.start, lp_frag_address.numel)
         if optim_state_key is None:
             hp_fragment = self._hp_mapping.hp_fragment
         else:
             hp_fragment = self._hp_mapping.get_optim_state_fragment(optim_state_key)
 
         reduce_fragment.data.copy_(hp_fragment.data)
     dist.all_reduce(reduce_buffer, group=self._dp_group)
@@ -68,29 +66,22 @@
         hp_mapping = self._hp_mapping
 
         if hp_mapping.use_offload:
             gradient_dict = hp_mapping.offload_gradient_dict
         else:
             gradient_dict = hp_mapping.gradient_dict
 
-        if hp_mapping.param_group_index not in gradient_dict or gradient_dict[
-                hp_mapping.param_group_index] is None:
-            raise ValueError(
-                "Gradients are only available immediately after backward and before engine step"
-            )
+        if hp_mapping.param_group_index not in gradient_dict or gradient_dict[hp_mapping.param_group_index] is None:
+            raise ValueError("Gradients are only available immediately after backward and before engine step")
 
-        lp_grad_fragment = gradient_dict[hp_mapping.param_group_index][
-            self._index_in_param_group]
+        lp_grad_fragment = gradient_dict[hp_mapping.param_group_index][self._index_in_param_group]
         hp_grad_fragment = lp_grad_fragment.to(torch.float32).flatten()
 
         lp_frag_address = self._hp_mapping.lp_fragment_address
-        reduce_fragment = torch.narrow(reduce_buffer,
-                                       0,
-                                       lp_frag_address.start,
-                                       lp_frag_address.numel)
+        reduce_fragment = torch.narrow(reduce_buffer, 0, lp_frag_address.start, lp_frag_address.numel)
 
         if self.view(-1).shape == hp_grad_fragment.shape:
             reduce_buffer.data.copy_(hp_grad_fragment.data)
         else:
             reduce_fragment.data.copy_(hp_grad_fragment.data)
 
     dist.all_reduce(reduce_buffer, group=self._dp_group)
@@ -146,53 +137,36 @@
     # ZeRO stage 1, 2, and bf16_optimizer params
     if hasattr(param, '_hp_mapping'):
         return param.get_full_hp_grad()
 
     return None
 
 
-def get_hp_fragment_mapping(lp_param,
-                            lp_start,
-                            flat_hp_partition,
-                            gradient_dict,
-                            offload_gradient_dict,
-                            use_offload,
-                            param_group_index,
-                            partition_start,
-                            partition_size,
-                            optimizer_state_dict):
+def get_hp_fragment_mapping(lp_param, lp_start, flat_hp_partition, gradient_dict, offload_gradient_dict, use_offload,
+                            param_group_index, partition_start, partition_size, optimizer_state_dict):
     lp_end = lp_param.numel() + lp_start
     hp_start = partition_start
     hp_end = partition_start + partition_size
 
     fragment_start = max(lp_start, hp_start)
     fragment_end = min(lp_end, hp_end)
     assert fragment_start < fragment_end, \
         f'fragment start {fragment_start} should be < fragment_end {fragment_end}'
 
     fragment_numel = fragment_end - fragment_start
-    hp_frag_address = fragment_address(start=fragment_start - hp_start,
-                                       numel=fragment_numel)
-    hp_fragment_tensor = flat_hp_partition.narrow(0,
-                                                  hp_frag_address.start,
-                                                  hp_frag_address.numel)
+    hp_frag_address = fragment_address(start=fragment_start - hp_start, numel=fragment_numel)
+    hp_fragment_tensor = flat_hp_partition.narrow(0, hp_frag_address.start, hp_frag_address.numel)
     optim_fragment = {
-        key: value.narrow(0,
-                          hp_frag_address.start,
-                          hp_frag_address.numel)
-        for key,
-        value in optimizer_state_dict.items()
+        key: value.narrow(0, hp_frag_address.start, hp_frag_address.numel)
+        for key, value in optimizer_state_dict.items()
         if torch.is_tensor(value) and value.shape == flat_hp_partition.shape
     }
 
-    lp_frag_address = fragment_address(start=fragment_start - lp_start,
-                                       numel=fragment_numel)
-    lp_fragment_tensor = lp_param.flatten().narrow(0,
-                                                   lp_frag_address.start,
-                                                   lp_frag_address.numel)
+    lp_frag_address = fragment_address(start=fragment_start - lp_start, numel=fragment_numel)
+    lp_fragment_tensor = lp_param.flatten().narrow(0, lp_frag_address.start, lp_frag_address.numel)
 
     return tensor_fragment(lp_fragment=lp_fragment_tensor,
                            lp_fragment_address=lp_frag_address,
                            hp_fragment=hp_fragment_tensor,
                            hp_fragment_address=hp_frag_address,
                            optim_fragment=optim_fragment,
                            gradient_dict=gradient_dict,
```

### Comparing `deepspeed-0.8.3/deepspeed/utils/timer.py` & `deepspeed-0.9.0/deepspeed/utils/timer.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-"""
-Copyright 2019 The Microsoft DeepSpeed Team
-"""
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 import time
 from numpy import mean
 from deepspeed.utils.logging import log_dist
 from deepspeed.accelerator import get_accelerator
 from deepspeed import comm as dist
 
@@ -14,30 +15,31 @@
     PSUTILS_INSTALLED = True
 except ImportError:
     PSUTILS_INSTALLED = False
     pass
 
 
 class CudaEventTimer(object):
-    def __init__(self,
-                 start_event: get_accelerator().Event,
-                 end_event: get_accelerator().Event):
+
+    def __init__(self, start_event: get_accelerator().Event, end_event: get_accelerator().Event):
         self.start_event = start_event
         self.end_event = end_event
 
     def get_elapsed_msec(self):
         get_accelerator().current_stream().wait_event(self.end_event)
         self.end_event.synchronize()
         return self.start_event.elapsed_time(self.end_event)
 
 
 class SynchronizedWallClockTimer:
     """Group of timers. Borrowed from Nvidia Megatron code"""
+
     class Timer:
         """Timer."""
+
         def __init__(self, name):
             self.name_ = name
             self.started_ = False
             self.event_timers = []
             self.start_event = None
             self.elapsed_records = None
 
@@ -98,22 +100,20 @@
     def __call__(self, name):
         if name not in self.timers:
             self.timers[name] = self.Timer(name)
         return self.timers[name]
 
     @staticmethod
     def memory_usage():
-        alloc = "mem_allocated: {:.4f} GB".format(get_accelerator().memory_allocated() /
-                                                  (1024 * 1024 * 1024))
-        max_alloc = "max_mem_allocated: {:.4f} GB".format(
-            get_accelerator().max_memory_allocated() / (1024 * 1024 * 1024))
-        cache = "cache_allocated: {:.4f} GB".format(get_accelerator().memory_cached() /
-                                                    (1024 * 1024 * 1024))
-        max_cache = "max_cache_allocated: {:.4f} GB".format(
-            get_accelerator().max_memory_cached() / (1024 * 1024 * 1024))
+        alloc = "mem_allocated: {:.4f} GB".format(get_accelerator().memory_allocated() / (1024 * 1024 * 1024))
+        max_alloc = "max_mem_allocated: {:.4f} GB".format(get_accelerator().max_memory_allocated() /
+                                                          (1024 * 1024 * 1024))
+        cache = "cache_allocated: {:.4f} GB".format(get_accelerator().memory_cached() / (1024 * 1024 * 1024))
+        max_cache = "max_cache_allocated: {:.4f} GB".format(get_accelerator().max_memory_cached() /
+                                                            (1024 * 1024 * 1024))
         return " | {} | {} | {} | {}".format(alloc, max_alloc, cache, max_cache)
 
     def log(self, names, normalizer=1.0, reset=True, memory_breakdown=False, ranks=None):
         """Log a group of timers."""
         assert normalizer > 0.0
         string = f"rank={dist.get_rank()} time (ms)"
         for name in names:
@@ -131,14 +131,15 @@
             if name in self.timers:
                 elapsed_time = (self.timers[name].mean() * 1000.0 / normalizer)
                 means[name] = elapsed_time
         return means
 
 
 class ThroughputTimer:
+
     def __init__(
         self,
         batch_size,
         start_step=2,
         steps_per_output=50,
         monitor_memory=False,
         logging_fn=None,
@@ -199,31 +200,27 @@
                         "epoch={}/micro_step={}/global_step={}, RunningAvgSamplesPerSec={}, CurrSamplesPerSec={}, "
                         "MemAllocated={}GB, MaxMemAllocated={}GB".format(
                             self.epoch_count,
                             self.micro_step_count,
                             self.global_step_count,
                             self.avg_samples_per_sec(),
                             self.batch_size / self.step_elapsed_time,
-                            round(get_accelerator().memory_allocated() / 1024**3,
-                                  2),
-                            round(get_accelerator().max_memory_allocated() / 1024**3,
-                                  2),
+                            round(get_accelerator().memory_allocated() / 1024**3, 2),
+                            round(get_accelerator().max_memory_allocated() / 1024**3, 2),
                         ))
                     if self.monitor_memory:
                         virt_mem = psutil.virtual_memory()
                         swap = psutil.swap_memory()
-                        self.logging(
-                            "epoch={}/micro_step={}/global_step={}, vm %: {}, swap %: {}"
-                            .format(
-                                self.epoch_count,
-                                self.micro_step_count,
-                                self.global_step_count,
-                                virt_mem.percent,
-                                swap.percent,
-                            ))
+                        self.logging("epoch={}/micro_step={}/global_step={}, vm %: {}, swap %: {}".format(
+                            self.epoch_count,
+                            self.micro_step_count,
+                            self.global_step_count,
+                            virt_mem.percent,
+                            swap.percent,
+                        ))
                 self.step_elapsed_time = 0
 
     def avg_samples_per_sec(self):
         if self.global_step_count > 0:
             total_step_offset = self.global_step_count - self.start_step
             avg_time_per_step = self.total_elapsed_time / total_step_offset
             # training samples per second
```

### Comparing `deepspeed-0.8.3/deepspeed/utils/zero_to_fp32.py` & `deepspeed-0.9.0/deepspeed/utils/zero_to_fp32.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,9 +1,13 @@
 #!/usr/bin/env python
-'''Copyright The Microsoft DeepSpeed Team'''
+
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 # This script extracts fp32 consolidated weights from a zero 2 and 3 DeepSpeed checkpoints. It gets
 # copied into the top level checkpoint dir, so the user can easily do the conversion at any point in
 # the future. Once extracted, the weights don't require DeepSpeed and can be used in any
 # application.
 #
 # example: python zero_to_fp32.py . pytorch_model.bin
@@ -15,22 +19,16 @@
 import os
 import re
 from collections import OrderedDict
 
 # while this script doesn't use deepspeed to recover data, since the checkpoints are pickled with
 # DeepSpeed data structures it has to be available in the current python environment.
 from deepspeed.utils import logger
-from deepspeed.checkpoint.constants import (DS_VERSION,
-                                            OPTIMIZER_STATE_DICT,
-                                            SINGLE_PARTITION_OF_FP32_GROUPS,
-                                            FP32_FLAT_GROUPS,
-                                            ZERO_STAGE,
-                                            PARTITION_COUNT,
-                                            PARAM_SHAPES,
-                                            BUFFER_NAMES)
+from deepspeed.checkpoint.constants import (DS_VERSION, OPTIMIZER_STATE_DICT, SINGLE_PARTITION_OF_FP32_GROUPS,
+                                            FP32_FLAT_GROUPS, ZERO_STAGE, PARTITION_COUNT, PARAM_SHAPES, BUFFER_NAMES)
 
 debug = 0
 
 # load to cpu
 device = torch.device('cpu')
 
 
@@ -61,45 +59,55 @@
         raise FileNotFoundError(f"can't find model states file at '{file}'")
 
     return file
 
 
 def get_optim_files(checkpoint_dir):
     # XXX: need to test that this simple glob rule works for multi-node setup too
-    optim_files = sorted(glob.glob(os.path.join(checkpoint_dir,
-                                                "*_optim_states.pt")),
-                         key=natural_keys)
+    optim_files = sorted(glob.glob(os.path.join(checkpoint_dir, "*_optim_states.pt")), key=natural_keys)
 
     if len(optim_files) == 0:
-        raise FileNotFoundError(
-            f"can't find '*_optim_states.pt' files in directory '{checkpoint_dir}'")
+        raise FileNotFoundError(f"can't find '*_optim_states.pt' files in directory '{checkpoint_dir}'")
 
     return optim_files
 
 
 def parse_model_state(file):
     state_dict = torch.load(file, map_location=device)
 
     if BUFFER_NAMES not in state_dict:
         raise ValueError(f"{file} is not a model state checkpoint")
     buffer_names = state_dict[BUFFER_NAMES]
     if debug:
         print("Found buffers:", buffer_names)
 
     # recover just the buffers while restoring them to fp32 if they were saved in fp16
-    buffers = {
-        k: v.float()
-        for k,
-        v in state_dict["module"].items() if k in buffer_names
-    }
+    buffers = {k: v.float() for k, v in state_dict["module"].items() if k in buffer_names}
     param_shapes = state_dict[PARAM_SHAPES]
 
+    # collect parameters that are included in param_shapes
+    param_names = []
+    for s in param_shapes:
+        for name in s.keys():
+            param_names.append(name)
+
+    # record shared parameters so that they can be recovered based on partners
+    # this is because such parameters holding reference only are not saved by optimizer
+    shared_params = []
+    for param in state_dict["module"]:
+        if param not in [*param_names, *buffer_names]:
+            for share_param in state_dict["module"]:
+                if (state_dict["module"][share_param].data_ptr() == state_dict["module"][param].data_ptr()
+                        and share_param != param):
+                    shared_params.append([param, share_param])
+                    break
+
     ds_version = state_dict.get(DS_VERSION, None)
 
-    return buffers, param_shapes, ds_version
+    return buffers, param_shapes, shared_params, ds_version
 
 
 def parse_optim_states(files, ds_checkpoint_dir):
 
     total_files = len(files)
     state_dicts = []
     for f in files:
@@ -128,28 +136,24 @@
         fp32_groups_key = SINGLE_PARTITION_OF_FP32_GROUPS
     elif zero_stage == 3:
         fp32_groups_key = FP32_FLAT_GROUPS
     else:
         raise ValueError(f"unknown zero stage {zero_stage}")
 
     if zero_stage == 2:
-        fp32_flat_groups = [
-            state_dicts[i][OPTIMIZER_STATE_DICT][fp32_groups_key]
-            for i in range(len(state_dicts))
-        ]
+        fp32_flat_groups = [state_dicts[i][OPTIMIZER_STATE_DICT][fp32_groups_key] for i in range(len(state_dicts))]
     elif zero_stage == 3:
         # if there is more than one param group, there will be multiple flattened tensors - one
         # flattened tensor per group - for simplicity merge them into a single tensor
         #
         # XXX: could make the script more memory efficient for when there are multiple groups - it
         # will require matching the sub-lists of param_shapes for each param group flattened tensor
 
         fp32_flat_groups = [
-            torch.cat(state_dicts[i][OPTIMIZER_STATE_DICT][fp32_groups_key],
-                      0) for i in range(len(state_dicts))
+            torch.cat(state_dicts[i][OPTIMIZER_STATE_DICT][fp32_groups_key], 0) for i in range(len(state_dicts))
         ]
 
     return zero_stage, world_size, fp32_flat_groups
 
 
 def _get_fp32_state_dict_from_zero_checkpoint(ds_checkpoint_dir):
     """
@@ -159,64 +163,52 @@
         - ``ds_checkpoint_dir``: path to the deepspeed checkpoint folder (where the optimizer files are)
 
     """
     print(f"Processing zero checkpoint '{ds_checkpoint_dir}'")
 
     optim_files = get_optim_files(ds_checkpoint_dir)
     zero_stage, world_size, fp32_flat_groups = parse_optim_states(optim_files, ds_checkpoint_dir)
-    print(
-        f"Detected checkpoint of type zero stage {zero_stage}, world_size: {world_size}")
+    print(f"Detected checkpoint of type zero stage {zero_stage}, world_size: {world_size}")
 
     model_file = get_model_state_file(ds_checkpoint_dir, zero_stage)
-    buffers, param_shapes, ds_version = parse_model_state(model_file)
+    buffers, param_shapes, shared_params, ds_version = parse_model_state(model_file)
     print(f'Parsing checkpoint created by deepspeed=={ds_version}')
 
     if zero_stage == 2:
-        return _get_fp32_state_dict_from_zero2_checkpoint(world_size,
-                                                          param_shapes,
-                                                          fp32_flat_groups,
-                                                          buffers)
+        return _get_fp32_state_dict_from_zero2_checkpoint(world_size, param_shapes, fp32_flat_groups, buffers,
+                                                          shared_params)
     elif zero_stage == 3:
-        return _get_fp32_state_dict_from_zero3_checkpoint(world_size,
-                                                          param_shapes,
-                                                          fp32_flat_groups,
-                                                          buffers)
+        return _get_fp32_state_dict_from_zero3_checkpoint(world_size, param_shapes, fp32_flat_groups, buffers,
+                                                          shared_params)
 
 
-def _get_fp32_state_dict_from_zero2_checkpoint(world_size,
-                                               param_shapes,
-                                               fp32_flat_groups,
-                                               buffers):
+def _get_fp32_state_dict_from_zero2_checkpoint(world_size, param_shapes, fp32_flat_groups, buffers, shared_params):
 
     # Reconstruction protocol:
     #
     # XXX: document this
 
     if debug:
         for i in range(world_size):
             for j in range(len(fp32_flat_groups[0])):
-                print(
-                    f"{FP32_FLAT_GROUPS}[{i}][{j}].shape={fp32_flat_groups[i][j].shape}")
+                print(f"{FP32_FLAT_GROUPS}[{i}][{j}].shape={fp32_flat_groups[i][j].shape}")
 
     # XXX: memory usage doubles here (zero2)
     num_param_groups = len(fp32_flat_groups[0])
     merged_single_partition_of_fp32_groups = []
     for i in range(num_param_groups):
         merged_partitions = [sd[i] for sd in fp32_flat_groups]
         full_single_fp32_vector = torch.cat(merged_partitions, 0)
         merged_single_partition_of_fp32_groups.append(full_single_fp32_vector)
-    avail_numel = sum([
-        full_single_fp32_vector.numel()
-        for full_single_fp32_vector in merged_single_partition_of_fp32_groups
-    ])
+    avail_numel = sum(
+        [full_single_fp32_vector.numel() for full_single_fp32_vector in merged_single_partition_of_fp32_groups])
 
     if debug:
         wanted_params = sum([len(shapes) for shapes in param_shapes])
-        wanted_numel = sum(
-            [sum(shape.numel() for shape in shapes.values()) for shapes in param_shapes])
+        wanted_numel = sum([sum(shape.numel() for shape in shapes.values()) for shapes in param_shapes])
         # not asserting if there is a mismatch due to possible padding
         print(f"Have {avail_numel} numels to process.")
         print(f"Need {wanted_numel} numels in {wanted_params} params.")
 
     state_dict = OrderedDict()
 
     # buffers
@@ -235,21 +227,16 @@
         for name, shape in shapes.items():
 
             unpartitioned_numel = shape.numel()
             total_numel += unpartitioned_numel
             total_params += 1
 
             if debug:
-                print(
-                    f"{name} full shape: {shape} unpartitioned numel {unpartitioned_numel} "
-                )
-            state_dict[name] = full_single_fp32_vector.narrow(
-                0,
-                offset,
-                unpartitioned_numel).view(shape)
+                print(f"{name} full shape: {shape} unpartitioned numel {unpartitioned_numel} ")
+            state_dict[name] = full_single_fp32_vector.narrow(0, offset, unpartitioned_numel).view(shape)
             offset += unpartitioned_numel
 
         # Z2 started to align to 2*world_size to improve nccl performance. Therefore both offset and
         # avail_numel can differ by anywhere between 0..2*world_size. Due to two unrelated complex
         # paddings performed in the code it's almost impossible to predict the exact numbers w/o the
         # live optimizer object, so we are checking that the numbers are within the right range
         align_to = 2 * world_size
@@ -264,35 +251,33 @@
         avail_numel = zero2_align(avail_numel)
 
         if debug:
             print(f"aligned  offset={offset}, avail_numel={avail_numel}")
 
         # Sanity check
         if offset != avail_numel:
-            raise ValueError(
-                f"consumed {offset} numels out of {avail_numel} - something is wrong")
+            raise ValueError(f"consumed {offset} numels out of {avail_numel} - something is wrong")
 
-    print(
-        f"Reconstructed fp32 state dict with {total_params} params {total_numel} elements"
-    )
+    # recover shared parameters
+    for pair in shared_params:
+        state_dict[pair[0]] = state_dict[pair[1]]
+
+    print(f"Reconstructed fp32 state dict with {total_params} params {total_numel} elements")
 
     return state_dict
 
 
 def zero3_partitioned_param_info(unpartitioned_numel, world_size):
     remainder = unpartitioned_numel % world_size
     padding_numel = (world_size - remainder) if remainder else 0
     partitioned_numel = math.ceil(unpartitioned_numel / world_size)
     return partitioned_numel, padding_numel
 
 
-def _get_fp32_state_dict_from_zero3_checkpoint(world_size,
-                                               param_shapes,
-                                               fp32_flat_groups,
-                                               buffers):
+def _get_fp32_state_dict_from_zero3_checkpoint(world_size, param_shapes, fp32_flat_groups, buffers, shared_params):
 
     # Reconstruction protocol: For zero3 we need to zip the partitions together at boundary of each
     # param, re-consolidating each param, while dealing with padding if any
 
     avail_numel = fp32_flat_groups[0].numel() * world_size
     # merge list of dicts, preserving order
     param_shapes = {k: v for d in param_shapes for k, v in d.items()}
@@ -331,33 +316,29 @@
         if debug:
             print(
                 f"{total_params} {name} full shape: {shape} partition0 numel={partitioned_numel} partitioned_padding_numel={partitioned_padding_numel}"
             )
 
         # XXX: memory usage doubles here
         state_dict[name] = torch.cat(
-            tuple(fp32_flat_groups[i].narrow(0,
-                                             offset,
-                                             partitioned_numel)
-                  for i in range(world_size)),
-            0).narrow(0,
-                      0,
-                      unpartitioned_numel).view(shape)
+            tuple(fp32_flat_groups[i].narrow(0, offset, partitioned_numel) for i in range(world_size)),
+            0).narrow(0, 0, unpartitioned_numel).view(shape)
         offset += partitioned_numel
 
     offset *= world_size
 
     # Sanity check
     if offset != avail_numel:
-        raise ValueError(
-            f"consumed {offset} numels out of {avail_numel} - something is wrong")
+        raise ValueError(f"consumed {offset} numels out of {avail_numel} - something is wrong")
+
+    # recover shared parameters
+    for pair in shared_params:
+        state_dict[pair[0]] = state_dict[pair[1]]
 
-    print(
-        f"Reconstructed fp32 state dict with {total_params} params {total_numel} elements"
-    )
+    print(f"Reconstructed fp32 state dict with {total_params} params {total_numel} elements")
 
     return state_dict
 
 
 def get_fp32_state_dict_from_zero_checkpoint(checkpoint_dir, tag=None):
     """
     Convert ZeRO 2 or 3 checkpoint into a single fp32 consolidated state_dict that can be loaded with
@@ -461,23 +442,20 @@
 
     return model
 
 
 if __name__ == "__main__":
 
     parser = argparse.ArgumentParser()
-    parser.add_argument(
-        "checkpoint_dir",
-        type=str,
-        help="path to the desired checkpoint folder, e.g., path/checkpoint-12")
+    parser.add_argument("checkpoint_dir",
+                        type=str,
+                        help="path to the desired checkpoint folder, e.g., path/checkpoint-12")
     parser.add_argument(
         "output_file",
         type=str,
-        help=
-        "path to the pytorch fp32 state_dict output file (e.g. path/checkpoint-12/pytorch_model.bin)"
-    )
+        help="path to the pytorch fp32 state_dict output file (e.g. path/checkpoint-12/pytorch_model.bin)")
     parser.add_argument("-d", "--debug", action='store_true', help="enable debug")
     args = parser.parse_args()
 
     debug = args.debug
 
     convert_zero_checkpoint_to_fp32_state_dict(args.checkpoint_dir, args.output_file)
```

### Comparing `deepspeed-0.8.3/deepspeed.egg-info/PKG-INFO` & `deepspeed-0.9.0/deepspeed.egg-info/PKG-INFO`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: deepspeed
-Version: 0.8.3
+Version: 0.9.0
 Summary: DeepSpeed library
 Home-page: http://deepspeed.ai
 Author: DeepSpeed Team
 Author-email: deepspeed-info@microsoft.com
 License: MIT
 Project-URL: Documentation, https://deepspeed.readthedocs.io
 Project-URL: Source, https://github.com/microsoft/DeepSpeed
@@ -23,40 +23,42 @@
 Provides-Extra: autotuning_ml
 Provides-Extra: sparse_attn
 Provides-Extra: inf
 Provides-Extra: sd
 Provides-Extra: all
 License-File: LICENSE
 
-[![License MIT](https://badgen.net/badge/license/MIT/blue)](https://github.com/Microsoft/DeepSpeed/blob/master/LICENSE)
+[![License Apache 2.0](https://badgen.net/badge/license/apache2.0/blue)](https://github.com/Microsoft/DeepSpeed/blob/master/LICENSE)
 [![PyPI version](https://badge.fury.io/py/deepspeed.svg)](https://pypi.org/project/deepspeed/)
 [![Downloads](https://pepy.tech/badge/deepspeed)](https://pepy.tech/project/deepspeed)
 [![Build](https://badgen.net/badge/build/check-status/blue)](#build-pipeline-status)
 
 
 <div align="center">
  <img src="docs/assets/images/DeepSpeed_light.svg#gh-light-mode-only" width="400px">
  <img src="docs/assets/images/DeepSpeed_dark_transparent.svg#gh-dark-mode-only" width="400px">
 </div>
 
 ## Latest News
-<b> DeepSpeed trained the world's most powerful language models ([MT-530B](https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/), [BLOOM](https://huggingface.co/blog/bloom-megatron-deepspeed)); [learn how](https://www.deepspeed.ai/tutorials/large-models-w-deepspeed/).</b>
+<b> <span style="color:orange" > DeepSpeed empowers ChatGPT-like model training with a single click, offering 15x speedup over SOTA RLHF systems with unprecedented cost reduction at all scales; [learn how](https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-chat)</span>.</b>
 
-* [2023/02] [Automatic Tensor Parallelism: Enables tensor parallelism by default without providing an injection policy](https://www.deepspeed.ai/tutorials/automatic-tensor-parallelism/)
+* ***[2023/04] 🚀 [DeepSpeed Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales](https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-chat)*** [[English](https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-chat/README.md)] [[中文](https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-chat/chinese/README.md)] [[日本語](https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-chat/japanese/README.md)]🚀
+* [2023/03] [Scaling Large-Scale Generative Mixture-of-Expert Multimodal Model With VL-MoE](https://www.deepspeed.ai/2023/03/30/multi-modal.html)
+* [2023/02] [Automatic Tensor Parallelism: Enables tensor parallelism by default without an injection policy](https://www.deepspeed.ai/tutorials/automatic-tensor-parallelism/)
 * [2022/12] [DeepSpeed Data Efficiency: A composable library that makes better use of data, increases training efficiency, and improves model quality](https://www.deepspeed.ai/2022/12/11/data-efficiency.html)
 * [2022/11] [Stable Diffusion Image Generation under 1 second w. DeepSpeed MII](https://github.com/microsoft/DeepSpeed-MII/tree/main/examples/benchmark/txt2img)
 * [2022/10] [DeepSpeed-MII: instant speedup on 24,000+ open-source DL models with up to 40x cheaper inference](https://www.deepspeed.ai/2022/10/10/mii.html)
 * [2022/09] [ZeRO-Inference: Democratizing massive model inference](https://www.deepspeed.ai/2022/09/09/zero-inference.html)
 * [2022/07] [Azure and DeepSpeed empower easy-to-use and high-performance model training](https://azure.microsoft.com/en-us/blog/azure-empowers-easytouse-highperformance-and-hyperscale-model-training-using-deepspeed/)
 
 ---
 
 # Extreme Speed and Scale for DL Training and Inference
 
-[DeepSpeed](https://www.deepspeed.ai/) is an easy-to-use deep learning optimization software suite that enables unprecedented scale and speed for Deep Learning Training and Inference. With DeepSpeed you can:
+***[DeepSpeed](https://www.deepspeed.ai/) enables world's most powerful language models like [MT-530B](https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/) and [BLOOM](https://huggingface.co/blog/bloom-megatron-deepspeed)***. It is an easy-to-use deep learning optimization software suite that powers unprecedented scale and speed for both training and inference. With DeepSpeed you can:
 
 * Train/Inference dense or sparse models with billions or trillions of parameters
 * Achieve excellent system throughput and efficiently scale to thousands of GPUs
 * Train/Inference on resource constrained GPU systems
 * Achieve unprecedented low latency and high throughput for inference
 * Achieve extreme compression for an unparalleled inference latency and model size reduction with low costs
 
@@ -129,33 +131,34 @@
 
 ---
 
 # Build Pipeline Status
 
 | Description | Status |
 | ----------- | ------ |
-| NVIDIA | [![nv-torch12-p40](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch12-p40.yml/badge.svg)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch12-p40.yml) [![nv-torch18-v100](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch18-v100.yml/badge.svg)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch18-v100.yml) [![nv-torch-latest-v100](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch-latest-v100.yml/badge.svg)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch-latest-v100.yml) [![nv-inference](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-inference.yml/badge.svg)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-inference.yml) [![nv-nightly](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-nightly.yml/badge.svg)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-nightly.yml) |
-| AMD | [![amd](https://github.com/microsoft/DeepSpeed/actions/workflows/amd.yml/badge.svg)](https://github.com/microsoft/DeepSpeed/actions/workflows/amd.yml) |
-| PyTorch Nightly | [![nv-torch-nightly-v100](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch-nightly-v100.yml/badge.svg)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch-nightly-v100.yml) |
-| Integrations | [![nv-transformers-v100](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-transformers-v100.yml/badge.svg)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-transformers-v100.yml) [![nv-lightning-v100](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-lightning-v100.yml/badge.svg)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-lightning-v100.yml) [![nv-accelerate-v100](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-accelerate-v100.yml/badge.svg)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-accelerate-v100.yml) |
-| Misc | [![Formatting](https://github.com/microsoft/DeepSpeed/actions/workflows/formatting.yml/badge.svg)](https://github.com/microsoft/DeepSpeed/actions/workflows/formatting.yml) [![pages-build-deployment](https://github.com/microsoft/DeepSpeed/actions/workflows/pages/pages-build-deployment/badge.svg)](https://github.com/microsoft/DeepSpeed/actions/workflows/pages/pages-build-deployment) [![Documentation Status](https://readthedocs.org/projects/deepspeed/badge/?version=latest)](https://deepspeed.readthedocs.io/en/latest/?badge=latest)|
+| NVIDIA | [![nv-torch19-p40](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch19-p40.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch19-p40.yml) [![nv-torch19-v100](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch19-v100.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch19-v100.yml) [![nv-torch-latest-v100](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch-latest-v100.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch-latest-v100.yml) [![nv-inference](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-inference.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-inference.yml) [![nv-nightly](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-nightly.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-nightly.yml) |
+| AMD | [![amd-mi100](https://github.com/microsoft/DeepSpeed/actions/workflows/amd-mi100.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/amd-mi100.yml) [![amd-mi200](https://github.com/microsoft/DeepSpeed/actions/workflows/amd-mi200.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/amd-mi200.yml) |
+| CPU | [![nv-torch-latest-cpu](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch-latest-cpu.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch-latest-cpu.yml) |
+| PyTorch Nightly | [![nv-torch-nightly-v100](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch-nightly-v100.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch-nightly-v100.yml) |
+| Integrations | [![nv-transformers-v100](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-transformers-v100.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-transformers-v100.yml) [![nv-lightning-v100](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-lightning-v100.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-lightning-v100.yml) [![nv-accelerate-v100](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-accelerate-v100.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-accelerate-v100.yml)[![nv-megatron](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-megatron.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-megatron.yml)[![nv-mii](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-mii.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-mii.yml) |
+| Misc | [![Formatting](https://github.com/microsoft/DeepSpeed/actions/workflows/formatting.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/formatting.yml) [![pages-build-deployment](https://github.com/microsoft/DeepSpeed/actions/workflows/pages/pages-build-deployment/badge.svg)](https://github.com/microsoft/DeepSpeed/actions/workflows/pages/pages-build-deployment) [![Documentation Status](https://readthedocs.org/projects/deepspeed/badge/?version=latest)](https://deepspeed.readthedocs.io/en/latest/?badge=latest)[![python](https://github.com/microsoft/DeepSpeed/actions/workflows/python.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/python.yml) |
 
 # Installation
 
 The quickest way to get started with DeepSpeed is via pip, this will install
 the latest release of DeepSpeed which is not tied to specific PyTorch or CUDA
 versions. DeepSpeed includes several C++/CUDA extensions that we commonly refer
 to as our 'ops'.  By default, all of these extensions/ops will be built
 just-in-time (JIT) using [torch's JIT C++ extension loader that relies on
 ninja](https://pytorch.org/docs/stable/cpp_extension.html) to build and
 dynamically link them at runtime.
 
 ## Requirements
 * [PyTorch](https://pytorch.org/) must be installed _before_ installing DeepSpeed.
-* For full feature support we recommend a version of PyTorch that is >= 1.8 and ideally the latest PyTorch stable release.
+* For full feature support we recommend a version of PyTorch that is >= 1.9 and ideally the latest PyTorch stable release.
 * A CUDA or ROCm compiler such as [nvcc](https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/#introduction) or [hipcc](https://github.com/ROCm-Developer-Tools/HIPCC) used to compile C++/CUDA/HIP extensions.
 * Specific GPUs we develop and test against are listed below, this doesn't mean your GPU will not work if it doesn't fall into this category it's just DeepSpeed is most well tested on the following:
   * NVIDIA: Pascal, Volta, Ampere, and Hopper architectures
   * AMD: MI100 and MI200
 
 ## PyPI
 We regularly push releases to [PyPI](https://pypi.org/project/deepspeed/) and encourage users to install from there in most cases.
@@ -238,14 +241,15 @@
 13. Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, Yuxiong He. (2022) ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers. [arXiv:2206.01861](https://arxiv.org/abs/2206.01861) and [NeurIPS 2022](https://openreview.net/forum?id=f-fVCElZ-G1).
 14. Reza Yazdani Aminabadi, Samyam Rajbhandari, Minjia Zhang, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Jeff Rasley, Shaden Smith, Olatunji Ruwase, Yuxiong He. (2022) DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale. [arXiv:2207.00032](https://arxiv.org/abs/2207.00032) and [SC 2022](https://dl.acm.org/doi/abs/10.5555/3571885.3571946).
 15. Zhewei Yao, Xiaoxia Wu, Conglong Li, Connor Holmes, Minjia Zhang, Cheng Li, Yuxiong He. (2022) Random-LTD: Random and Layerwise Token Dropping Brings Efficient Training for Large-scale Transformers. [arXiv:2211.11586](https://arxiv.org/abs/2211.11586).
 16. Conglong Li, Zhewei Yao, Xiaoxia Wu, Minjia Zhang, Yuxiong He. (2022) DeepSpeed Data Efficiency: Improving Deep Learning Model Quality and Training Efficiency via Efficient Data Sampling and Routing. [arXiv:2212.03597](https://arxiv.org/abs/2212.03597).
 17. Xiaoxia Wu, Cheng Li, Reza Yazdani Aminabadi, Zhewei Yao, Yuxiong He. (2023) Understanding INT4 Quantization for Transformer Models: Latency Speedup, Composability, and Failure Cases. [arXiv:2301.12017](https://arxiv.org/abs/2301.12017).
 18. Syed Zawad, Cheng Li, Zhewei Yao, Elton Zheng, Yuxiong He, Feng Yan. (2023) DySR: Adaptive Super-Resolution via Algorithm and System Co-design. [ICLR:2023](https://openreview.net/forum?id=Pgtn4l6eKjv).
 19. Sheng Shen, Zhewei Yao, Chunyuan Li, Trevor Darrell, Kurt Keutzer, Yuxiong He. (2023) Scaling Vision-Language Models with Sparse Mixture of Experts. [arXiv:2303.07226](https://arxiv.org/abs/2303.07226).
+20. Quentin Anthony, Ammar Ahmad Awan, Jeff Rasley, Yuxiong He, Aamir Shafi, Mustafa Abduljabbar, Hari Subramoni, Dhabaleswar Panda. (2023) MCR-DL: Mix-and-Match Communication Runtime for Deep Learning [arXiv:2303.08374](https://arxiv.org/abs/2303.08374) and will appear at IPDPS 2023.
 
 
 # Videos
 1. DeepSpeed KDD 2020 Tutorial
     1. [Overview](https://www.youtube.com/watch?v=CaseqC45DNc&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=29)
     2. [ZeRO + large model training](https://www.youtube.com/watch?v=y4_bCiAsIAk&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=28)
     3. [17B T-NLG demo](https://www.youtube.com/watch?v=9V-ZbP92drg&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=27)
```

### Comparing `deepspeed-0.8.3/deepspeed.egg-info/SOURCES.txt` & `deepspeed-0.9.0/deepspeed.egg-info/SOURCES.txt`

 * *Files 2% similar despite different names*

```diff
@@ -5,27 +5,14 @@
 setup.cfg
 setup.py
 version.txt
 accelerator/__init__.py
 accelerator/abstract_accelerator.py
 accelerator/cuda_accelerator.py
 accelerator/real_accelerator.py
-benchmarks/__init__.py
-benchmarks/communication/__init__.py
-benchmarks/communication/all_gather.py
-benchmarks/communication/all_reduce.py
-benchmarks/communication/all_to_all.py
-benchmarks/communication/broadcast.py
-benchmarks/communication/constants.py
-benchmarks/communication/pt2pt.py
-benchmarks/communication/run_all.py
-benchmarks/communication/utils.py
-benchmarks/inference/bert-bench.py
-benchmarks/inference/collect_results.py
-benchmarks/inference/gpt-bench.py
 bin/deepspeed
 bin/deepspeed.pt
 bin/ds
 bin/ds_bench
 bin/ds_elastic
 bin/ds_report
 bin/ds_ssh
@@ -401,14 +388,15 @@
 deepspeed/runtime/bf16_optimizer.py
 deepspeed/runtime/config.py
 deepspeed/runtime/config_utils.py
 deepspeed/runtime/constants.py
 deepspeed/runtime/dataloader.py
 deepspeed/runtime/eigenvalue.py
 deepspeed/runtime/engine.py
+deepspeed/runtime/hybrid_engine.py
 deepspeed/runtime/lr_schedules.py
 deepspeed/runtime/progressive_layer_drop.py
 deepspeed/runtime/quantize.py
 deepspeed/runtime/sparse_tensor.py
 deepspeed/runtime/state_dict_factory.py
 deepspeed/runtime/utils.py
 deepspeed/runtime/weight_quantizer.py
```

### Comparing `deepspeed-0.8.3/deepspeed.egg-info/requires.txt` & `deepspeed-0.9.0/deepspeed.egg-info/requires.txt`

 * *Files 5% similar despite different names*

```diff
@@ -10,49 +10,49 @@
 
 [1bit]
 
 [1bit_mpi]
 mpi4py
 
 [all]
-packaging
-transformers
 mpi4py
-tensorboard
-py-cpuinfo
-pytest
-future
-diffusers
 tqdm
-docutils<0.18
-pytest-randomly
-recommonmark
-pre-commit>=2.20.0
-transformers[sentencepiece]
 sphinx-rtd-theme
-psutil
-lm-eval==0.3.0
 pytest-forked
-wandb
-pydantic
 google
-protobuf
-pytest-xdist
-xgboost
-megatron-lm==1.1.5
-tabulate
 torchvision
+diffusers
 torch
-autodoc_pydantic
-sphinx
-triton==2.0.0.dev20221005
+packaging
+importlib-metadata>=4
+transformers[sentencepiece]
 hjson
 clang-format>=14.0.6
-importlib-metadata>=4
+pre-commit>=2.20.0
 triton==1.0.0
+recommonmark
+docutils<0.18
+psutil
+xgboost
+pytest-randomly
+tensorboard
+pytest
+pytest-xdist
+pydantic
+megatron-lm==1.1.5
+protobuf
+lm-eval==0.3.0
+py-cpuinfo
+future
+wandb
+autodoc_pydantic
+sphinx
+tabulate
+triton==2.0.0.dev20221202
+transformers
 
 [autotuning]
 tabulate
 
 [autotuning_ml]
 hjson
 tabulate
@@ -93,11 +93,11 @@
 py-cpuinfo
 pydantic
 torch
 tqdm
 
 [sd]
 diffusers
-triton==2.0.0.dev20221005
+triton==2.0.0.dev20221202
 
 [sparse_attn]
 triton==1.0.0
```

### Comparing `deepspeed-0.8.3/op_builder/__init__.py` & `deepspeed-0.9.0/op_builder/__init__.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,12 @@
-"""
-Copyright 2020 The Microsoft DeepSpeed Team
-"""
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+
 import sys
 import os
 import pkgutil
 import importlib
 
 from .builder import get_default_compute_capabilities, OpBuilder
 
@@ -41,13 +43,11 @@
 
 
 # reflect builder names and add builder closure, such as 'TransformerBuilder()' creates op builder wrt current accelerator
 for _, module_name, _ in pkgutil.iter_modules([os.path.dirname(this_module.__file__)]):
     if module_name != 'all_ops' and module_name != 'builder':
         module = importlib.import_module(f".{module_name}", package=op_builder_dir)
         for member_name in module.__dir__():
-            if member_name.endswith(
-                    'Builder'
-            ) and member_name != "OpBuilder" and member_name != "CUDAOpBuilder":
+            if member_name.endswith('Builder') and member_name != "OpBuilder" and member_name != "CUDAOpBuilder":
                 # assign builder name to variable with same name
                 # the following is equivalent to i.e. TransformerBuilder = "TransformerBuilder"
                 this_module.__dict__[member_name] = builder_closure(member_name)
```

### Comparing `deepspeed-0.8.3/op_builder/all_ops.py` & `deepspeed-0.9.0/op_builder/all_ops.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,10 +1,12 @@
-"""
-Copyright 2020 The Microsoft DeepSpeed Team
-"""
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+
 import os
 import pkgutil
 import importlib
 try:
     # during installation time accelerator is visible, otherwise return deepspeed.accelerator
     from accelerator import get_accelerator
 except ImportError:
```

### Comparing `deepspeed-0.8.3/op_builder/async_io.py` & `deepspeed-0.9.0/op_builder/async_io.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,10 +1,12 @@
-"""
-Copyright 2020 The Microsoft DeepSpeed Team
-"""
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+
 import distutils.spawn
 import subprocess
 
 from .builder import OpBuilder
 
 
 class AsyncIOBuilder(OpBuilder):
@@ -15,22 +17,18 @@
         super().__init__(name=self.NAME)
 
     def absolute_name(self):
         return f'deepspeed.ops.aio.{self.NAME}_op'
 
     def sources(self):
         return [
-            'csrc/aio/py_lib/deepspeed_py_copy.cpp',
-            'csrc/aio/py_lib/py_ds_aio.cpp',
-            'csrc/aio/py_lib/deepspeed_py_aio.cpp',
-            'csrc/aio/py_lib/deepspeed_py_aio_handle.cpp',
-            'csrc/aio/py_lib/deepspeed_aio_thread.cpp',
-            'csrc/aio/common/deepspeed_aio_utils.cpp',
-            'csrc/aio/common/deepspeed_aio_common.cpp',
-            'csrc/aio/common/deepspeed_aio_types.cpp',
+            'csrc/aio/py_lib/deepspeed_py_copy.cpp', 'csrc/aio/py_lib/py_ds_aio.cpp',
+            'csrc/aio/py_lib/deepspeed_py_aio.cpp', 'csrc/aio/py_lib/deepspeed_py_aio_handle.cpp',
+            'csrc/aio/py_lib/deepspeed_aio_thread.cpp', 'csrc/aio/common/deepspeed_aio_utils.cpp',
+            'csrc/aio/common/deepspeed_aio_common.cpp', 'csrc/aio/common/deepspeed_aio_types.cpp',
             'csrc/aio/py_lib/deepspeed_pin_tensor.cpp'
         ]
 
     def include_paths(self):
         return ['csrc/aio/py_lib', 'csrc/aio/common']
 
     def cxx_args(self):
@@ -52,54 +50,42 @@
         ]
 
     def extra_ldflags(self):
         return ['-laio']
 
     def check_for_libaio_pkg(self):
         libs = dict(
-            dpkg=["-l",
-                  "libaio-dev",
-                  "apt"],
-            pacman=["-Q",
-                    "libaio",
-                    "pacman"],
-            rpm=["-q",
-                 "libaio-devel",
-                 "yum"],
+            dpkg=["-l", "libaio-dev", "apt"],
+            pacman=["-Q", "libaio", "pacman"],
+            rpm=["-q", "libaio-devel", "yum"],
         )
 
         found = False
         for pkgmgr, data in libs.items():
             flag, lib, tool = data
             path = distutils.spawn.find_executable(pkgmgr)
             if path is not None:
                 cmd = f"{pkgmgr} {flag} {lib}"
-                result = subprocess.Popen(cmd,
-                                          stdout=subprocess.PIPE,
-                                          stderr=subprocess.PIPE,
-                                          shell=True)
+                result = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)
                 if result.wait() == 0:
                     found = True
                 else:
-                    self.warning(
-                        f"{self.NAME}: please install the {lib} package with {tool}")
+                    self.warning(f"{self.NAME}: please install the {lib} package with {tool}")
                 break
         return found
 
     def is_compatible(self, verbose=True):
         # Check for the existence of libaio by using distutils
         # to compile and link a test program that calls io_submit,
         # which is a function provided by libaio that is used in the async_io op.
         # If needed, one can define -I and -L entries in CFLAGS and LDFLAGS
         # respectively to specify the directories for libaio.h and libaio.so.
         aio_compatible = self.has_function('io_submit', ('aio', ))
         if verbose and not aio_compatible:
-            self.warning(
-                f"{self.NAME} requires the dev libaio .so object and headers but these were not found."
-            )
+            self.warning(f"{self.NAME} requires the dev libaio .so object and headers but these were not found.")
 
             # Check for the libaio package via known package managers
             # to print suggestions on which package to install.
             self.check_for_libaio_pkg()
 
             self.warning(
                 "If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found."
```

### Comparing `deepspeed-0.8.3/op_builder/builder.py` & `deepspeed-0.9.0/op_builder/builder.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,12 @@
-"""
-Copyright 2020 The Microsoft DeepSpeed Team
-"""
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+
 import os
 import sys
 import time
 import importlib
 from pathlib import Path
 import subprocess
 import shlex
@@ -23,46 +25,41 @@
 
 DEFAULT_TORCH_EXTENSION_PATH = "/tmp/torch_extensions"
 DEFAULT_COMPUTE_CAPABILITIES = "6.0;6.1;7.0"
 
 try:
     import torch
 except ImportError:
-    print(
-        f"{WARNING} unable to import torch, please install it if you want to pre-compile any deepspeed ops."
-    )
+    print(f"{WARNING} unable to import torch, please install it if you want to pre-compile any deepspeed ops.")
 else:
     TORCH_MAJOR = int(torch.__version__.split('.')[0])
     TORCH_MINOR = int(torch.__version__.split('.')[1])
 
 
 def installed_cuda_version(name=""):
     import torch.cuda
     if not torch.cuda.is_available():
         return 0, 0
     import torch.utils.cpp_extension
     cuda_home = torch.utils.cpp_extension.CUDA_HOME
     assert cuda_home is not None, "CUDA_HOME does not exist, unable to compile CUDA op(s)"
     # Ensure there is not a cuda version mismatch between torch and nvcc compiler
-    output = subprocess.check_output([cuda_home + "/bin/nvcc",
-                                      "-V"],
-                                     universal_newlines=True)
+    output = subprocess.check_output([cuda_home + "/bin/nvcc", "-V"], universal_newlines=True)
     output_split = output.split()
     release_idx = output_split.index("release")
     release = output_split[release_idx + 1].replace(',', '').split(".")
     # Ignore patch versions, only look at major + minor
     cuda_major, cuda_minor = release[:2]
     return int(cuda_major), int(cuda_minor)
 
 
 def get_default_compute_capabilities():
     compute_caps = DEFAULT_COMPUTE_CAPABILITIES
     import torch.utils.cpp_extension
-    if torch.utils.cpp_extension.CUDA_HOME is not None and installed_cuda_version(
-    )[0] >= 11:
+    if torch.utils.cpp_extension.CUDA_HOME is not None and installed_cuda_version()[0] >= 11:
         if installed_cuda_version()[0] == 11 and installed_cuda_version()[1] == 0:
             # Special treatment of CUDA 11.0 because compute_86 is not supported.
             compute_caps += ";8.0"
         else:
             compute_caps += ";8.0;8.6"
     return compute_caps
 
@@ -71,45 +68,35 @@
 # to build deepspeed and system-wide installed cuda 11.2
 cuda_minor_mismatch_ok = {
     10: [
         "10.0",
         "10.1",
         "10.2",
     ],
-    11: ["11.0",
-         "11.1",
-         "11.2",
-         "11.3",
-         "11.4",
-         "11.5",
-         "11.6",
-         "11.7",
-         "11.8"],
+    11: ["11.0", "11.1", "11.2", "11.3", "11.4", "11.5", "11.6", "11.7", "11.8"],
 }
 
 
 def assert_no_cuda_mismatch(name=""):
     cuda_major, cuda_minor = installed_cuda_version(name)
     if cuda_minor == 0 and cuda_major == 0:
         return False
     sys_cuda_version = f'{cuda_major}.{cuda_minor}'
     torch_cuda_version = ".".join(torch.version.cuda.split('.')[:2])
     # This is a show-stopping error, should probably not proceed past this
     if sys_cuda_version != torch_cuda_version:
-        if (cuda_major in cuda_minor_mismatch_ok
-                and sys_cuda_version in cuda_minor_mismatch_ok[cuda_major]
+        if (cuda_major in cuda_minor_mismatch_ok and sys_cuda_version in cuda_minor_mismatch_ok[cuda_major]
                 and torch_cuda_version in cuda_minor_mismatch_ok[cuda_major]):
             print(f"Installed CUDA version {sys_cuda_version} does not match the "
                   f"version torch was compiled with {torch.version.cuda} "
                   "but since the APIs are compatible, accepting this combination")
             return True
-        raise Exception(
-            f">- DeepSpeed Op Builder: Installed CUDA version {sys_cuda_version} does not match the "
-            f"version torch was compiled with {torch.version.cuda}, unable to compile "
-            "cuda/cpp extensions without a matching cuda version.")
+        raise Exception(f">- DeepSpeed Op Builder: Installed CUDA version {sys_cuda_version} does not match the "
+                        f"version torch was compiled with {torch.version.cuda}, unable to compile "
+                        "cuda/cpp extensions without a matching cuda version.")
     return True
 
 
 class OpBuilder(ABC):
     _rocm_version = None
     _is_rocm_pytorch = None
 
@@ -138,58 +125,54 @@
         pass
 
     @staticmethod
     def validate_torch_version(torch_info):
         install_torch_version = torch_info['version']
         current_torch_version = ".".join(torch.__version__.split('.')[:2])
         if install_torch_version != current_torch_version:
-            raise RuntimeError(
-                "PyTorch version mismatch! DeepSpeed ops were compiled and installed "
-                "with a different version than what is being used at runtime. "
-                f"Please re-install DeepSpeed or switch torch versions. "
-                f"Install torch version={install_torch_version}, "
-                f"Runtime torch version={current_torch_version}")
+            raise RuntimeError("PyTorch version mismatch! DeepSpeed ops were compiled and installed "
+                               "with a different version than what is being used at runtime. "
+                               f"Please re-install DeepSpeed or switch torch versions. "
+                               f"Install torch version={install_torch_version}, "
+                               f"Runtime torch version={current_torch_version}")
 
     @staticmethod
     def validate_torch_op_version(torch_info):
         if not OpBuilder.is_rocm_pytorch():
             current_cuda_version = ".".join(torch.version.cuda.split('.')[:2])
             install_cuda_version = torch_info['cuda_version']
             if install_cuda_version != current_cuda_version:
-                raise RuntimeError(
-                    "CUDA version mismatch! DeepSpeed ops were compiled and installed "
-                    "with a different version than what is being used at runtime. "
-                    f"Please re-install DeepSpeed or switch torch versions. "
-                    f"Install CUDA version={install_cuda_version}, "
-                    f"Runtime CUDA version={current_cuda_version}")
+                raise RuntimeError("CUDA version mismatch! DeepSpeed ops were compiled and installed "
+                                   "with a different version than what is being used at runtime. "
+                                   f"Please re-install DeepSpeed or switch torch versions. "
+                                   f"Install CUDA version={install_cuda_version}, "
+                                   f"Runtime CUDA version={current_cuda_version}")
         else:
             current_hip_version = ".".join(torch.version.hip.split('.')[:2])
             install_hip_version = torch_info['hip_version']
             if install_hip_version != current_hip_version:
-                raise RuntimeError(
-                    "HIP version mismatch! DeepSpeed ops were compiled and installed "
-                    "with a different version than what is being used at runtime. "
-                    f"Please re-install DeepSpeed or switch torch versions. "
-                    f"Install HIP version={install_hip_version}, "
-                    f"Runtime HIP version={current_hip_version}")
+                raise RuntimeError("HIP version mismatch! DeepSpeed ops were compiled and installed "
+                                   "with a different version than what is being used at runtime. "
+                                   f"Please re-install DeepSpeed or switch torch versions. "
+                                   f"Install HIP version={install_hip_version}, "
+                                   f"Runtime HIP version={current_hip_version}")
 
     @staticmethod
     def is_rocm_pytorch():
         if OpBuilder._is_rocm_pytorch is not None:
             return OpBuilder._is_rocm_pytorch
 
         _is_rocm_pytorch = False
         try:
             import torch
         except ImportError:
             pass
         else:
             if TORCH_MAJOR > 1 or (TORCH_MAJOR == 1 and TORCH_MINOR >= 5):
-                _is_rocm_pytorch = hasattr(torch.version,
-                                           'hip') and torch.version.hip is not None
+                _is_rocm_pytorch = hasattr(torch.version, 'hip') and torch.version.hip is not None
                 if _is_rocm_pytorch:
                     from torch.utils.cpp_extension import ROCM_HOME
                     _is_rocm_pytorch = ROCM_HOME is not None
         OpBuilder._is_rocm_pytorch = _is_rocm_pytorch
         return OpBuilder._is_rocm_pytorch
 
     @staticmethod
@@ -242,18 +225,15 @@
     def extra_ldflags(self):
         return []
 
     def libraries_installed(self, libraries):
         valid = False
         check_cmd = 'dpkg -l'
         for lib in libraries:
-            result = subprocess.Popen(f'dpkg -l {lib}',
-                                      stdout=subprocess.PIPE,
-                                      stderr=subprocess.PIPE,
-                                      shell=True)
+            result = subprocess.Popen(f'dpkg -l {lib}', stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)
             valid = valid or result.wait() == 0
         return valid
 
     def has_function(self, funcname, libraries, verbose=False):
         '''
         Test for existence of a function within a tuple of libraries.
 
@@ -276,17 +256,15 @@
             # Configure compiler and linker to build according to Python install.
             distutils.sysconfig.customize_compiler(compiler)
 
             # Create a temporary directory to hold test files.
             tempdir = tempfile.mkdtemp()
 
             # Define a simple C program that calls the function in question
-            prog = "void %s(void); int main(int argc, char** argv) { %s(); return 0; }" % (
-                funcname,
-                funcname)
+            prog = "void %s(void); int main(int argc, char** argv) { %s(); return 0; }" % (funcname, funcname)
 
             # Write the test program to a file.
             filename = os.path.join(tempdir, 'test.c')
             with open(filename, 'w') as f:
                 f.write(prog)
 
             # Redirect stderr file descriptor to a file to silence compile/link warnings.
@@ -299,24 +277,21 @@
             # Otherwise, a local directory will be used instead of tempdir
             drive, driveless_filename = os.path.splitdrive(filename)
             root_dir = driveless_filename[0] if os.path.isabs(driveless_filename) else ''
             output_dir = os.path.join(drive, root_dir)
 
             # Attempt to compile the C program into an object file.
             cflags = shlex.split(os.environ.get('CFLAGS', ""))
-            objs = compiler.compile([filename],
-                                    output_dir=output_dir,
-                                    extra_preargs=self.strip_empty_entries(cflags))
+            objs = compiler.compile([filename], output_dir=output_dir, extra_preargs=self.strip_empty_entries(cflags))
 
             # Attempt to link the object file into an executable.
             # Be sure to tack on any libraries that have been specified.
             ldflags = shlex.split(os.environ.get('LDFLAGS', ""))
             compiler.link_executable(objs,
-                                     os.path.join(tempdir,
-                                                  'a.out'),
+                                     os.path.join(tempdir, 'a.out'),
                                      extra_preargs=self.strip_empty_entries(ldflags),
                                      libraries=libraries)
 
             # Compile and link succeeded
             return True
 
         except CompileError:
@@ -352,45 +327,40 @@
             cpu_info = self._backup_cpuinfo()
             if cpu_info is None:
                 return "-march=native"
 
         try:
             cpu_info = get_cpu_info()
         except Exception as e:
-            self.warning(
-                f"{self.name} attempted to use `py-cpuinfo` but failed (exception type: {type(e)}, {e}), "
-                "falling back to `lscpu` to get this information.")
+            self.warning(f"{self.name} attempted to use `py-cpuinfo` but failed (exception type: {type(e)}, {e}), "
+                         "falling back to `lscpu` to get this information.")
             cpu_info = self._backup_cpuinfo()
             if cpu_info is None:
                 return "-march=native"
 
         if cpu_info['arch'].startswith('PPC_'):
             # gcc does not provide -march on PowerPC, use -mcpu instead
             return '-mcpu=native'
         return '-march=native'
 
     def is_cuda_enable(self):
         try:
             if torch.cuda.is_available():
                 return '-D__ENABLE_CUDA__'
         except:
-            print(
-                f"{WARNING} {self.name} torch.cuda is missing, only cpu ops can be compiled!"
-            )
+            print(f"{WARNING} {self.name} torch.cuda is missing, only cpu ops can be compiled!")
             return '-D__DISABLE_CUDA__'
         return '-D__DISABLE_CUDA__'
 
     def _backup_cpuinfo(self):
         # Construct cpu_info dict from lscpu that is similar to what py-cpuinfo provides
         if not self.command_exists('lscpu'):
-            self.warning(
-                f"{self.name} attempted to query 'lscpu' after failing to use py-cpuinfo "
-                "to detect the CPU architecture. 'lscpu' does not appear to exist on "
-                "your system, will fall back to use -march=native and non-vectorized execution."
-            )
+            self.warning(f"{self.name} attempted to query 'lscpu' after failing to use py-cpuinfo "
+                         "to detect the CPU architecture. 'lscpu' does not appear to exist on "
+                         "your system, will fall back to use -march=native and non-vectorized execution.")
             return None
         result = subprocess.check_output('lscpu', shell=True)
         result = result.decode('utf-8').strip().lower()
 
         cpu_info = {}
         cpu_info['arch'] = None
         cpu_info['flags'] = ""
@@ -414,17 +384,16 @@
             cpu_info = self._backup_cpuinfo()
             if cpu_info is None:
                 return '-D__SCALAR__'
 
         try:
             cpu_info = get_cpu_info()
         except Exception as e:
-            self.warning(
-                f"{self.name} attempted to use `py-cpuinfo` but failed (exception type: {type(e)}, {e}), "
-                "falling back to `lscpu` to get this information.")
+            self.warning(f"{self.name} attempted to use `py-cpuinfo` but failed (exception type: {type(e)}, {e}), "
+                         "falling back to `lscpu` to get this information.")
             cpu_info = self._backup_cpuinfo()
             if cpu_info is None:
                 return '-D__SCALAR__'
 
         if cpu_info['arch'] == 'X86_64':
             if 'avx512' in cpu_info['flags'] or 'avx512f' in cpu_info['flags']:
                 return '-D__AVX512__'
@@ -439,41 +408,36 @@
             cmds = [cmd]
         valid = False
         for cmd in cmds:
             result = subprocess.Popen(f'type {cmd}', stdout=subprocess.PIPE, shell=True)
             valid = valid or result.wait() == 0
 
         if not valid and len(cmds) > 1:
-            print(
-                f"{WARNING} {self.name} requires one of the following commands '{cmds}', but it does not exist!"
-            )
+            print(f"{WARNING} {self.name} requires one of the following commands '{cmds}', but it does not exist!")
         elif not valid and len(cmds) == 1:
-            print(
-                f"{WARNING} {self.name} requires the '{cmd}' command, but it does not exist!"
-            )
+            print(f"{WARNING} {self.name} requires the '{cmd}' command, but it does not exist!")
         return valid
 
     def warning(self, msg):
         self.error_log = f"{msg}"
         print(f"{WARNING} {msg}")
 
     def deepspeed_src_path(self, code_path):
         if os.path.isabs(code_path):
             return code_path
         else:
             return os.path.join(Path(__file__).parent.parent.absolute(), code_path)
 
     def builder(self):
         from torch.utils.cpp_extension import CppExtension
-        return CppExtension(
-            name=self.absolute_name(),
-            sources=self.strip_empty_entries(self.sources()),
-            include_dirs=self.strip_empty_entries(self.include_paths()),
-            extra_compile_args={'cxx': self.strip_empty_entries(self.cxx_args())},
-            extra_link_args=self.strip_empty_entries(self.extra_ldflags()))
+        return CppExtension(name=self.absolute_name(),
+                            sources=self.strip_empty_entries(self.sources()),
+                            include_dirs=self.strip_empty_entries(self.include_paths()),
+                            extra_compile_args={'cxx': self.strip_empty_entries(self.cxx_args())},
+                            extra_link_args=self.strip_empty_entries(self.extra_ldflags()))
 
     def load(self, verbose=True):
         from deepspeed.git_version_info import installed_ops, torch_info
         if installed_ops[self.name]:
             # Ensure the op we're about to load was compiled with the same
             # torch/cuda versions we are currently using at runtime.
             self.validate_torch_version(torch_info)
@@ -488,60 +452,56 @@
         if not self.is_compatible(verbose):
             raise RuntimeError(
                 f"Unable to JIT load the {self.name} op due to it not being compatible due to hardware/software issue. {self.error_log}"
             )
         try:
             import ninja  # noqa: F401
         except ImportError:
-            raise RuntimeError(
-                f"Unable to JIT load the {self.name} op due to ninja not being installed."
-            )
+            raise RuntimeError(f"Unable to JIT load the {self.name} op due to ninja not being installed.")
 
         if isinstance(self, CUDAOpBuilder) and not self.is_rocm_pytorch():
             self.build_for_cpu = not assert_no_cuda_mismatch(self.name)
 
         self.jit_mode = True
         from torch.utils.cpp_extension import load
 
         start_build = time.time()
         sources = [self.deepspeed_src_path(path) for path in self.sources()]
-        extra_include_paths = [
-            self.deepspeed_src_path(path) for path in self.include_paths()
-        ]
+        extra_include_paths = [self.deepspeed_src_path(path) for path in self.include_paths()]
 
         # Torch will try and apply whatever CCs are in the arch list at compile time,
         # we have already set the intended targets ourselves we know that will be
         # needed at runtime. This prevents CC collisions such as multiple __half
         # implementations. Stash arch list to reset after build.
         torch_arch_list = None
         if "TORCH_CUDA_ARCH_LIST" in os.environ:
             torch_arch_list = os.environ.get("TORCH_CUDA_ARCH_LIST")
             os.environ["TORCH_CUDA_ARCH_LIST"] = ""
 
-        op_module = load(
-            name=self.name,
-            sources=self.strip_empty_entries(sources),
-            extra_include_paths=self.strip_empty_entries(extra_include_paths),
-            extra_cflags=self.strip_empty_entries(self.cxx_args()),
-            extra_cuda_cflags=self.strip_empty_entries(self.nvcc_args()),
-            extra_ldflags=self.strip_empty_entries(self.extra_ldflags()),
-            verbose=verbose)
+        op_module = load(name=self.name,
+                         sources=self.strip_empty_entries(sources),
+                         extra_include_paths=self.strip_empty_entries(extra_include_paths),
+                         extra_cflags=self.strip_empty_entries(self.cxx_args()),
+                         extra_cuda_cflags=self.strip_empty_entries(self.nvcc_args()),
+                         extra_ldflags=self.strip_empty_entries(self.extra_ldflags()),
+                         verbose=verbose)
 
         build_duration = time.time() - start_build
         if verbose:
             print(f"Time to load {self.name} op: {build_duration} seconds")
 
         # Reset arch list so we are not silently removing it for other possible use cases
         if torch_arch_list:
             os.environ["TORCH_CUDA_ARCH_LIST"] = torch_arch_list
 
         return op_module
 
 
 class CUDAOpBuilder(OpBuilder):
+
     def compute_capability_args(self, cross_compile_archs=None):
         """
         Returns nvcc compute capability compile flags.
 
         1. `TORCH_CUDA_ARCH_LIST` takes priority over `cross_compile_archs`.
         2. If neither is set default compute capabilities will be used
         3. Under `jit_mode` compute capabilities of all visible cards will be used plus PTX
@@ -580,16 +540,15 @@
                 if cross_compile_archs is None:
                     cross_compile_archs = get_default_compute_capabilities()
             ccs = cross_compile_archs.split(';')
 
         ccs = self.filter_ccs(ccs)
         if len(ccs) == 0:
             raise RuntimeError(
-                f"Unable to load {self.name} op due to no compute capabilities remaining after filtering"
-            )
+                f"Unable to load {self.name} op due to no compute capabilities remaining after filtering")
 
         args = []
         for cc in ccs:
             num = cc[0] + cc[2]
             args.append(f'-gencode=arch=compute_{num},code=sm_{num}')
             if cc.endswith('+PTX'):
                 args.append(f'-gencode=arch=compute_{num},code=compute_{num}')
@@ -626,40 +585,41 @@
         else:
             from torch.utils.cpp_extension import CUDAExtension as ExtensionBuilder
 
         compile_args = {'cxx': self.strip_empty_entries(self.cxx_args())} if self.build_for_cpu else \
                        {'cxx': self.strip_empty_entries(self.cxx_args()), \
                            'nvcc': self.strip_empty_entries(self.nvcc_args())}
 
-        cuda_ext = ExtensionBuilder(
-            name=self.absolute_name(),
-            sources=self.strip_empty_entries(self.sources()),
-            include_dirs=self.strip_empty_entries(self.include_paths()),
-            libraries=self.strip_empty_entries(self.libraries_args()),
-            extra_compile_args=compile_args)
+        cuda_ext = ExtensionBuilder(name=self.absolute_name(),
+                                    sources=self.strip_empty_entries(self.sources()),
+                                    include_dirs=self.strip_empty_entries(self.include_paths()),
+                                    libraries=self.strip_empty_entries(self.libraries_args()),
+                                    extra_compile_args=compile_args)
 
         if self.is_rocm_pytorch():
             # hip converts paths to absolute, this converts back to relative
             sources = cuda_ext.sources
             curr_file = Path(__file__).parent.parent  # ds root
             for i in range(len(sources)):
                 src = Path(sources[i])
-                sources[i] = str(src.relative_to(curr_file))
+                if src.is_absolute():
+                    sources[i] = str(src.relative_to(curr_file))
+                else:
+                    sources[i] = str(src)
             cuda_ext.sources = sources
         return cuda_ext
 
     def hipify_extension(self):
         if self.is_rocm_pytorch():
             from torch.utils.hipify import hipify_python
             hipify_python.hipify(
                 project_directory=os.getcwd(),
                 output_directory=os.getcwd(),
                 header_include_dirs=self.include_paths(),
-                includes=[os.path.join(os.getcwd(),
-                                       '*')],
+                includes=[os.path.join(os.getcwd(), '*')],
                 extra_files=[os.path.abspath(s) for s in self.sources()],
                 show_detailed=True,
                 is_pytorch_extension=True,
                 hipify_extra_files_only=True,
             )
 
     def cxx_args(self):
@@ -671,31 +631,25 @@
     def nvcc_args(self):
         if self.build_for_cpu:
             return []
         args = ['-O3']
         if self.is_rocm_pytorch():
             ROCM_MAJOR, ROCM_MINOR = self.installed_rocm_version()
             args += [
-                '-std=c++14',
-                '-U__HIP_NO_HALF_OPERATORS__',
-                '-U__HIP_NO_HALF_CONVERSIONS__',
+                '-std=c++14', '-U__HIP_NO_HALF_OPERATORS__', '-U__HIP_NO_HALF_CONVERSIONS__',
                 '-U__HIP_NO_HALF2_OPERATORS__',
                 '-DROCM_VERSION_MAJOR=%s' % ROCM_MAJOR,
                 '-DROCM_VERSION_MINOR=%s' % ROCM_MINOR
             ]
         else:
             cuda_major, _ = installed_cuda_version()
             args += [
-                '-allow-unsupported-compiler' if sys.platform == "win32" else '',
-                '--use_fast_math',
-                '-std=c++17'
-                if sys.platform == "win32" and cuda_major > 10 else '-std=c++14',
-                '-U__CUDA_NO_HALF_OPERATORS__',
-                '-U__CUDA_NO_HALF_CONVERSIONS__',
-                '-U__CUDA_NO_HALF2_OPERATORS__'
+                '-allow-unsupported-compiler' if sys.platform == "win32" else '', '--use_fast_math',
+                '-std=c++17' if sys.platform == "win32" and cuda_major > 10 else '-std=c++14',
+                '-U__CUDA_NO_HALF_OPERATORS__', '-U__CUDA_NO_HALF_CONVERSIONS__', '-U__CUDA_NO_HALF2_OPERATORS__'
             ]
             if os.environ.get('DS_DEBUG_CUDA_BUILD', '0') == '1':
                 args.append('--ptxas-options=-v')
             args += self.compute_capability_args()
         return args
 
     def libraries_args(self):
@@ -705,14 +659,15 @@
         if sys.platform == "win32":
             return ['cublas', 'curand']
         else:
             return []
 
 
 class TorchCPUOpBuilder(CUDAOpBuilder):
+
     def extra_ldflags(self):
         if self.build_for_cpu:
             return ['-fopenmp']
 
         if not self.is_rocm_pytorch():
             return ['-lcurand']
```

### Comparing `deepspeed-0.8.3/op_builder/cpu_adagrad.py` & `deepspeed-0.9.0/op_builder/cpu_adagrad.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,10 +1,12 @@
-"""
-Copyright 2020 The Microsoft DeepSpeed Team
-"""
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+
 import os
 from .builder import TorchCPUOpBuilder
 
 
 class CPUAdagradBuilder(TorchCPUOpBuilder):
     BUILD_VAR = "DS_BUILD_CPU_ADAGRAD"
     NAME = "cpu_adagrad"
@@ -34,17 +36,12 @@
         import torch
         if self.build_for_cpu:
             CUDA_INCLUDE = []
         elif not self.is_rocm_pytorch():
             CUDA_INCLUDE = [os.path.join(torch.utils.cpp_extension.CUDA_HOME, "include")]
         else:
             CUDA_INCLUDE = [
-                os.path.join(torch.utils.cpp_extension.ROCM_HOME,
-                             "include"),
-                os.path.join(torch.utils.cpp_extension.ROCM_HOME,
-                             "include",
-                             "rocrand"),
-                os.path.join(torch.utils.cpp_extension.ROCM_HOME,
-                             "include",
-                             "hiprand"),
+                os.path.join(torch.utils.cpp_extension.ROCM_HOME, "include"),
+                os.path.join(torch.utils.cpp_extension.ROCM_HOME, "include", "rocrand"),
+                os.path.join(torch.utils.cpp_extension.ROCM_HOME, "include", "hiprand"),
             ]
         return ['csrc/includes'] + CUDA_INCLUDE
```

### Comparing `deepspeed-0.8.3/op_builder/cpu_adam.py` & `deepspeed-0.9.0/op_builder/cpu_adam.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,12 @@
-"""
-Copyright 2020 The Microsoft DeepSpeed Team
-"""
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+
 import os
 from .builder import TorchCPUOpBuilder
 
 
 class CPUAdamBuilder(TorchCPUOpBuilder):
     BUILD_VAR = "DS_BUILD_CPU_ADAM"
     NAME = "cpu_adam"
@@ -35,17 +37,12 @@
         import torch
         if self.build_for_cpu:
             CUDA_INCLUDE = []
         elif not self.is_rocm_pytorch():
             CUDA_INCLUDE = [os.path.join(torch.utils.cpp_extension.CUDA_HOME, "include")]
         else:
             CUDA_INCLUDE = [
-                os.path.join(torch.utils.cpp_extension.ROCM_HOME,
-                             "include"),
-                os.path.join(torch.utils.cpp_extension.ROCM_HOME,
-                             "include",
-                             "rocrand"),
-                os.path.join(torch.utils.cpp_extension.ROCM_HOME,
-                             "include",
-                             "hiprand"),
+                os.path.join(torch.utils.cpp_extension.ROCM_HOME, "include"),
+                os.path.join(torch.utils.cpp_extension.ROCM_HOME, "include", "rocrand"),
+                os.path.join(torch.utils.cpp_extension.ROCM_HOME, "include", "hiprand"),
             ]
         return ['csrc/includes'] + CUDA_INCLUDE
```

### Comparing `deepspeed-0.8.3/op_builder/fused_adam.py` & `deepspeed-0.9.0/op_builder/fused_adam.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,10 +1,12 @@
-"""
-Copyright 2020 The Microsoft DeepSpeed Team
-"""
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+
 from .builder import CUDAOpBuilder
 
 import sys
 
 
 class FusedAdamBuilder(CUDAOpBuilder):
     BUILD_VAR = "DS_BUILD_FUSED_ADAM"
@@ -25,13 +27,11 @@
     def cxx_args(self):
         args = super().cxx_args()
         return args + self.version_dependent_macros()
 
     def nvcc_args(self):
         nvcc_flags = ['-O3'] + self.version_dependent_macros()
         if not self.is_rocm_pytorch():
-            nvcc_flags.extend([
-                '-allow-unsupported-compiler' if sys.platform == "win32" else '',
-                '-lineinfo',
-                '--use_fast_math'
-            ] + self.compute_capability_args())
+            nvcc_flags.extend(
+                ['-allow-unsupported-compiler' if sys.platform == "win32" else '', '-lineinfo', '--use_fast_math'] +
+                self.compute_capability_args())
         return nvcc_flags
```

### Comparing `deepspeed-0.8.3/op_builder/fused_lamb.py` & `deepspeed-0.9.0/op_builder/fused_lamb.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,10 +1,12 @@
-"""
-Copyright 2020 The Microsoft DeepSpeed Team
-"""
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+
 from .builder import CUDAOpBuilder
 
 import sys
 
 
 class FusedLambBuilder(CUDAOpBuilder):
     BUILD_VAR = 'DS_BUILD_FUSED_LAMB'
@@ -26,18 +28,13 @@
         args = super().cxx_args()
         return args + self.version_dependent_macros()
 
     def nvcc_args(self):
         nvcc_flags = ['-O3'] + self.version_dependent_macros()
         if self.is_rocm_pytorch():
             ROCM_MAJOR, ROCM_MINOR = self.installed_rocm_version()
-            nvcc_flags += [
-                '-DROCM_VERSION_MAJOR=%s' % ROCM_MAJOR,
-                '-DROCM_VERSION_MINOR=%s' % ROCM_MINOR
-            ]
+            nvcc_flags += ['-DROCM_VERSION_MAJOR=%s' % ROCM_MAJOR, '-DROCM_VERSION_MINOR=%s' % ROCM_MINOR]
         else:
-            nvcc_flags.extend([
-                '-allow-unsupported-compiler' if sys.platform == "win32" else '',
-                '-lineinfo',
-                '--use_fast_math'
-            ] + self.compute_capability_args())
+            nvcc_flags.extend(
+                ['-allow-unsupported-compiler' if sys.platform == "win32" else '', '-lineinfo', '--use_fast_math'] +
+                self.compute_capability_args())
         return nvcc_flags
```

### Comparing `deepspeed-0.8.3/op_builder/quantizer.py` & `deepspeed-0.9.0/op_builder/quantizer.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,8 +1,11 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 from .builder import CUDAOpBuilder
 
 
 class QuantizerBuilder(CUDAOpBuilder):
     BUILD_VAR = "DS_BUILD_QUANTIZER"
     NAME = "quantizer"
```

### Comparing `deepspeed-0.8.3/op_builder/random_ltd.py` & `deepspeed-0.9.0/op_builder/random_ltd.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,10 +1,12 @@
-"""
-Copyright 2022 The Microsoft DeepSpeed Team
-"""
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+
 from .builder import CUDAOpBuilder
 
 
 class RandomLTDBuilder(CUDAOpBuilder):
     BUILD_VAR = "DS_BUILD_RANDOM_LTD"
     NAME = "random_ltd"
 
@@ -19,22 +21,17 @@
         if not self.is_rocm_pytorch():
             return ['-lcurand']
         else:
             return []
 
     def sources(self):
         return [
-            'csrc/random_ltd/pt_binding.cpp',
-            'csrc/random_ltd/gather_scatter.cu',
-            'csrc/random_ltd/slice_attn_masks.cu',
-            'csrc/random_ltd/token_sort.cu'
+            'csrc/random_ltd/pt_binding.cpp', 'csrc/random_ltd/gather_scatter.cu',
+            'csrc/random_ltd/slice_attn_masks.cu', 'csrc/random_ltd/token_sort.cu'
         ]
 
     def include_paths(self):
         includes = ['csrc/includes']
         if self.is_rocm_pytorch():
             from torch.utils.cpp_extension import ROCM_HOME
-            includes += [
-                '{}/hiprand/include'.format(ROCM_HOME),
-                '{}/rocrand/include'.format(ROCM_HOME)
-            ]
+            includes += ['{}/hiprand/include'.format(ROCM_HOME), '{}/rocrand/include'.format(ROCM_HOME)]
         return includes
```

### Comparing `deepspeed-0.8.3/op_builder/sparse_attn.py` & `deepspeed-0.9.0/op_builder/sparse_attn.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,10 +1,12 @@
-"""
-Copyright 2020 The Microsoft DeepSpeed Team
-"""
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+
 from .builder import OpBuilder
 
 try:
     from packaging import version as pkg_version
 except ImportError:
     pkg_version = None
 
@@ -43,43 +45,37 @@
 
         # torch-cpu will not have a cuda version
         if torch.version.cuda is None:
             cuda_compatible = False
             self.warning(f"{self.NAME} cuda is not available from torch")
         else:
             major, minor = torch.version.cuda.split('.')[:2]
-            cuda_compatible = (int(major) == 10
-                               and int(minor) >= 1) or (int(major) >= 11)
+            cuda_compatible = (int(major) == 10 and int(minor) >= 1) or (int(major) >= 11)
             if not cuda_compatible:
                 self.warning(f"{self.NAME} requires CUDA version 10.1+")
 
         TORCH_MAJOR = int(torch.__version__.split('.')[0])
         TORCH_MINOR = int(torch.__version__.split('.')[1])
         torch_compatible = TORCH_MAJOR == 1 and TORCH_MINOR >= 5
         if not torch_compatible:
-            self.warning(
-                f'{self.NAME} requires a torch version >= 1.5 but detected {TORCH_MAJOR}.{TORCH_MINOR}'
-            )
+            self.warning(f'{self.NAME} requires a torch version >= 1.5 but detected {TORCH_MAJOR}.{TORCH_MINOR}')
 
         try:
             import triton
         except ImportError:
             # auto-install of triton is broken on some systems, reverting to manual install for now
             # see this issue: https://github.com/microsoft/DeepSpeed/issues/1710
-            self.warning(
-                f"please install triton==1.0.0 if you want to use sparse attention")
+            self.warning(f"please install triton==1.0.0 if you want to use sparse attention")
             return False
 
         if pkg_version:
             installed_triton = pkg_version.parse(triton.__version__)
             triton_mismatch = installed_triton != pkg_version.parse("1.0.0")
         else:
             installed_triton = triton.__version__
             triton_mismatch = installed_triton != "1.0.0"
 
         if triton_mismatch:
-            self.warning(
-                f"using untested triton version ({installed_triton}), only 1.0.0 is known to be compatible"
-            )
+            self.warning(f"using untested triton version ({installed_triton}), only 1.0.0 is known to be compatible")
             return False
 
         return super().is_compatible(verbose) and torch_compatible and cuda_compatible
```

### Comparing `deepspeed-0.8.3/op_builder/spatial_inference.py` & `deepspeed-0.9.0/op_builder/spatial_inference.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,10 +1,12 @@
-'''
-Copyright 2022 The Microsoft DeepSpeed Team
-'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+
 from .builder import CUDAOpBuilder, installed_cuda_version
 
 
 class SpatialInferenceBuilder(CUDAOpBuilder):
     BUILD_VAR = "DS_BUILD_SPATIAL_INFERENCE"
     NAME = "spatial_inference"
 
@@ -15,27 +17,25 @@
     def absolute_name(self):
         return f'deepspeed.ops.spatial.{self.NAME}_op'
 
     def is_compatible(self, verbose=True):
         try:
             import torch
         except ImportError:
-            self.warning(
-                "Please install torch if trying to pre-compile inference kernels")
+            self.warning("Please install torch if trying to pre-compile inference kernels")
             return False
 
         cuda_okay = True
         if not self.is_rocm_pytorch() and torch.cuda.is_available():
             sys_cuda_major, _ = installed_cuda_version()
             torch_cuda_major = int(torch.version.cuda.split('.')[0])
             cuda_capability = torch.cuda.get_device_properties(0).major
             if cuda_capability >= 8:
                 if torch_cuda_major < 11 or sys_cuda_major < 11:
-                    self.warning(
-                        "On Ampere and higher architectures please use CUDA 11+")
+                    self.warning("On Ampere and higher architectures please use CUDA 11+")
                     cuda_okay = False
         return super().is_compatible(verbose) and cuda_okay
 
     def sources(self):
         return [
             'csrc/spatial/csrc/opt_bias_add.cu',
             'csrc/spatial/csrc/pt_binding.cpp',
```

### Comparing `deepspeed-0.8.3/op_builder/stochastic_transformer.py` & `deepspeed-0.9.0/op_builder/stochastic_transformer.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,10 +1,12 @@
-"""
-Copyright 2020 The Microsoft DeepSpeed Team
-"""
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+
 from .transformer import TransformerBuilder
 
 
 class StochasticTransformerBuilder(TransformerBuilder):
     BUILD_VAR = "DS_BUILD_STOCHASTIC_TRANSFORMER"
     NAME = "stochastic_transformer"
```

### Comparing `deepspeed-0.8.3/op_builder/transformer.py` & `deepspeed-0.9.0/op_builder/transformer.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,10 +1,12 @@
-"""
-Copyright 2020 The Microsoft DeepSpeed Team
-"""
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+
 from .builder import CUDAOpBuilder
 
 
 class TransformerBuilder(CUDAOpBuilder):
     BUILD_VAR = "DS_BUILD_TRANSFORMER"
     NAME = "transformer"
 
@@ -19,26 +21,19 @@
         if not self.is_rocm_pytorch():
             return ['-lcurand']
         else:
             return []
 
     def sources(self):
         return [
-            'csrc/transformer/ds_transformer_cuda.cpp',
-            'csrc/transformer/cublas_wrappers.cu',
-            'csrc/transformer/transform_kernels.cu',
-            'csrc/transformer/gelu_kernels.cu',
-            'csrc/transformer/dropout_kernels.cu',
-            'csrc/transformer/normalize_kernels.cu',
-            'csrc/transformer/softmax_kernels.cu',
-            'csrc/transformer/general_kernels.cu'
+            'csrc/transformer/ds_transformer_cuda.cpp', 'csrc/transformer/cublas_wrappers.cu',
+            'csrc/transformer/transform_kernels.cu', 'csrc/transformer/gelu_kernels.cu',
+            'csrc/transformer/dropout_kernels.cu', 'csrc/transformer/normalize_kernels.cu',
+            'csrc/transformer/softmax_kernels.cu', 'csrc/transformer/general_kernels.cu'
         ]
 
     def include_paths(self):
         includes = ['csrc/includes']
         if self.is_rocm_pytorch():
             from torch.utils.cpp_extension import ROCM_HOME
-            includes += [
-                '{}/hiprand/include'.format(ROCM_HOME),
-                '{}/rocrand/include'.format(ROCM_HOME)
-            ]
+            includes += ['{}/hiprand/include'.format(ROCM_HOME), '{}/rocrand/include'.format(ROCM_HOME)]
         return includes
```

### Comparing `deepspeed-0.8.3/op_builder/transformer_inference.py` & `deepspeed-0.9.0/op_builder/transformer_inference.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,8 +1,11 @@
-'''Copyright The Microsoft DeepSpeed Team'''
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
 
 from .builder import CUDAOpBuilder, installed_cuda_version
 
 
 class InferenceBuilder(CUDAOpBuilder):
     BUILD_VAR = "DS_BUILD_TRANSFORMER_INFERENCE"
     NAME = "transformer_inference"
@@ -14,32 +17,28 @@
     def absolute_name(self):
         return f'deepspeed.ops.transformer.inference.{self.NAME}_op'
 
     def is_compatible(self, verbose=True):
         try:
             import torch
         except ImportError:
-            self.warning(
-                "Please install torch if trying to pre-compile inference kernels")
+            self.warning("Please install torch if trying to pre-compile inference kernels")
             return False
 
         cuda_okay = True
         if not self.is_rocm_pytorch() and torch.cuda.is_available():
             sys_cuda_major, _ = installed_cuda_version()
             torch_cuda_major = int(torch.version.cuda.split('.')[0])
             cuda_capability = torch.cuda.get_device_properties(0).major
             if cuda_capability < 6:
-                self.warning(
-                    "NVIDIA Inference is only supported on Pascal and newer architectures"
-                )
+                self.warning("NVIDIA Inference is only supported on Pascal and newer architectures")
                 cuda_okay = False
             if cuda_capability >= 8:
                 if torch_cuda_major < 11 or sys_cuda_major < 11:
-                    self.warning(
-                        "On Ampere and higher architectures please use CUDA 11+")
+                    self.warning("On Ampere and higher architectures please use CUDA 11+")
                     cuda_okay = False
         return super().is_compatible(verbose) and cuda_okay
 
     def filter_ccs(self, ccs):
         ccs_retained = []
         ccs_pruned = []
         for cc in ccs:
```

### Comparing `deepspeed-0.8.3/setup.py` & `deepspeed-0.9.0/setup.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,20 +1,23 @@
-"""
-Copyright 2020 The Microsoft DeepSpeed Team
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
 
+# DeepSpeed Team
+"""
 DeepSpeed library
 
 To build wheel on Windows:
-    1. Install pytorch, such as pytorch 1.12 + cuda 11.6
-    2. Install visual cpp build tool
-    3. Include cuda toolkit
-    4. Launch cmd console with Administrator privilege for creating required symlink folders
+1. Install pytorch, such as pytorch 1.12 + cuda 11.6.
+2. Install visual cpp build tool.
+3. Include cuda toolkit.
+4. Launch cmd console with Administrator privilege for creating required symlink folders.
+
 
 Create a new wheel via the following command:
-    build_win.bat
+build_win.bat
 
 The wheel will be located at: dist/*.whl
 """
 
 import os
 import sys
 import subprocess
@@ -30,15 +33,15 @@
     print('[WARNING] Unable to import torch, pre-compiling ops will be disabled. ' \
         'Please visit https://pytorch.org/ to see how to properly install torch on your system.')
 
 from op_builder import get_default_compute_capabilities, OpBuilder
 from op_builder.all_ops import ALL_OPS
 from op_builder.builder import installed_cuda_version
 
-# fetch rocm state
+# Fetch rocm state.
 is_rocm_pytorch = OpBuilder.is_rocm_pytorch()
 rocm_version = OpBuilder.installed_rocm_version()
 
 RED_START = '\033[31m'
 RED_END = '\033[0m'
 ERROR = f"{RED_START} [ERROR] {RED_END}"
 
@@ -51,67 +54,65 @@
 def fetch_requirements(path):
     with open(path, 'r') as fd:
         return [r.strip() for r in fd.readlines()]
 
 
 install_requires = fetch_requirements('requirements/requirements.txt')
 extras_require = {
-    '1bit': [], # add cupy based on cuda/rocm version
+    '1bit': [],  # add cupy based on cuda/rocm version
     '1bit_mpi': fetch_requirements('requirements/requirements-1bit-mpi.txt'),
     'readthedocs': fetch_requirements('requirements/requirements-readthedocs.txt'),
     'dev': fetch_requirements('requirements/requirements-dev.txt'),
     'autotuning': fetch_requirements('requirements/requirements-autotuning.txt'),
     'autotuning_ml': fetch_requirements('requirements/requirements-autotuning-ml.txt'),
     'sparse_attn': fetch_requirements('requirements/requirements-sparse_attn.txt'),
     'inf': fetch_requirements('requirements/requirements-inf.txt'),
     'sd': fetch_requirements('requirements/requirements-sd.txt')
 }
 
-# Add specific cupy version to both onebit extension variants
+# Add specific cupy version to both onebit extension variants.
 if torch_available and torch.cuda.is_available():
     cupy = None
     if is_rocm_pytorch:
         rocm_major, rocm_minor = rocm_version
-        # XXX cupy support for rocm 5 is not available yet
+        # XXX cupy support for rocm 5 is not available yet.
         if rocm_major <= 4:
             cupy = f"cupy-rocm-{rocm_major}-{rocm_minor}"
     else:
         cupy = f"cupy-cuda{''.join(map(str,installed_cuda_version()))}"
     if cupy:
         extras_require['1bit'].append(cupy)
         extras_require['1bit_mpi'].append(cupy)
 
-# Make an [all] extra that installs all needed dependencies
+# Make an [all] extra that installs all needed dependencies.
 all_extras = set()
 for extra in extras_require.items():
     for req in extra[1]:
         all_extras.add(req)
 extras_require['all'] = list(all_extras)
 
 cmdclass = {}
 
-# For any pre-installed ops force disable ninja
+# For any pre-installed ops force disable ninja.
 if torch_available:
     from accelerator import get_accelerator
-    cmdclass['build_ext'] = get_accelerator().build_extension().with_options(
-        use_ninja=False)
+    cmdclass['build_ext'] = get_accelerator().build_extension().with_options(use_ninja=False)
 
 if torch_available:
     TORCH_MAJOR = torch.__version__.split('.')[0]
     TORCH_MINOR = torch.__version__.split('.')[1]
 else:
     TORCH_MAJOR = "0"
     TORCH_MINOR = "0"
 
 if torch_available and not torch.cuda.is_available():
-    # Fix to allow docker builds, similar to https://github.com/NVIDIA/apex/issues/486
-    print(
-        "[WARNING] Torch did not find cuda available, if cross-compiling or running with cpu only "
-        "you can ignore this message. Adding compute capability for Pascal, Volta, and Turing "
-        "(compute capabilities 6.0, 6.1, 6.2)")
+    # Fix to allow docker builds, similar to https://github.com/NVIDIA/apex/issues/486.
+    print("[WARNING] Torch did not find cuda available, if cross-compiling or running with cpu only "
+          "you can ignore this message. Adding compute capability for Pascal, Volta, and Turing "
+          "(compute capabilities 6.0, 6.1, 6.2)")
     if os.environ.get("TORCH_CUDA_ARCH_LIST", None) is None:
         os.environ["TORCH_CUDA_ARCH_LIST"] = get_default_compute_capabilities()
 
 ext_modules = []
 
 # Default to pre-install kernels to false so we rely on JIT on Linux, opposite on Windows.
 BUILD_OP_PLATFORM = 1 if sys.platform == "win32" else 0
@@ -144,34 +145,34 @@
 
 compatible_ops = dict.fromkeys(ALL_OPS.keys(), False)
 install_ops = dict.fromkeys(ALL_OPS.keys(), False)
 for op_name, builder in ALL_OPS.items():
     op_compatible = builder.is_compatible()
     compatible_ops[op_name] = op_compatible
 
-    # If op is requested but not available, throw an error
+    # If op is requested but not available, throw an error.
     if op_enabled(op_name) and not op_compatible:
         env_var = op_envvar(op_name)
         if env_var not in os.environ:
             builder.warning(f"One can disable {op_name} with {env_var}=0")
         abort(f"Unable to pre-compile {op_name}")
 
-    # if op is compatible but install is not enabled (JIT mode)
+    # If op is compatible but install is not enabled (JIT mode).
     if is_rocm_pytorch and op_compatible and not op_enabled(op_name):
         builder.hipify_extension()
 
-    # If op install enabled, add builder to extensions
+    # If op install enabled, add builder to extensions.
     if op_enabled(op_name) and op_compatible:
         assert torch_available, f"Unable to pre-compile {op_name}, please first install torch"
         install_ops[op_name] = op_enabled(op_name)
         ext_modules.append(builder.builder())
 
 print(f'Install Ops={install_ops}')
 
-# Write out version/git info
+# Write out version/git info.
 git_hash_cmd = "git rev-parse --short HEAD"
 git_branch_cmd = "git rev-parse --abbrev-ref HEAD"
 if command_exists('git') and 'DS_BUILD_STRING' not in os.environ:
     try:
         result = subprocess.check_output(git_hash_cmd, shell=True)
         git_hash = result.decode('utf-8').strip()
         result = subprocess.check_output(git_branch_cmd, shell=True)
@@ -196,46 +197,46 @@
     # This creates a symbolic links on Windows.
     # It needs Administrator privilege to create symlinks on Windows.
     create_dir_symlink('..\\..\\csrc', '.\\deepspeed\\ops\\csrc')
     create_dir_symlink('..\\..\\op_builder', '.\\deepspeed\\ops\\op_builder')
     create_dir_symlink('..\\accelerator', '.\\deepspeed\\accelerator')
     egg_info.manifest_maker.template = 'MANIFEST_win.in'
 
-# Parse the DeepSpeed version string from version.txt
+# Parse the DeepSpeed version string from version.txt.
 version_str = open('version.txt', 'r').read().strip()
 
 # Build specifiers like .devX can be added at install time. Otherwise, add the git hash.
-# example: DS_BUILD_STRING=".dev20201022" python setup.py sdist bdist_wheel
+# Example: DS_BUILD_STRING=".dev20201022" python setup.py sdist bdist_wheel.
 
-# Building wheel for distribution, update version file
+# Building wheel for distribution, update version file.
 if 'DS_BUILD_STRING' in os.environ:
-    # Build string env specified, probably building for distribution
+    # Build string env specified, probably building for distribution.
     with open('build.txt', 'w') as fd:
         fd.write(os.environ.get('DS_BUILD_STRING'))
     version_str += os.environ.get('DS_BUILD_STRING')
 elif os.path.isfile('build.txt'):
-    # build.txt exists, probably installing from distribution
+    # build.txt exists, probably installing from distribution.
     with open('build.txt', 'r') as fd:
         version_str += fd.read().strip()
 else:
-    # None of the above, probably installing from source
+    # None of the above, probably installing from source.
     version_str += f'+{git_hash}'
 
 torch_version = ".".join([TORCH_MAJOR, TORCH_MINOR])
 bf16_support = False
-# Set cuda_version to 0.0 if cpu-only
+# Set cuda_version to 0.0 if cpu-only.
 cuda_version = "0.0"
 nccl_version = "0.0"
-# Set hip_version to 0.0 if cpu-only
+# Set hip_version to 0.0 if cpu-only.
 hip_version = "0.0"
 if torch_available and torch.version.cuda is not None:
     cuda_version = ".".join(torch.version.cuda.split('.')[:2])
     if sys.platform != "win32":
         if isinstance(torch.cuda.nccl.version(), int):
-            # This will break if minor version > 9
+            # This will break if minor version > 9.
             nccl_version = ".".join(str(torch.cuda.nccl.version())[:2])
         else:
             nccl_version = ".".join(map(str, torch.cuda.nccl.version()[:2]))
     if hasattr(torch.cuda, 'is_bf16_supported') and torch.cuda.is_available():
         bf16_support = torch.cuda.is_bf16_supported()
 if torch_available and hasattr(torch.version, 'hip') and torch.version.hip is not None:
     hip_version = ".".join(torch.version.hip.split('.')[:2])
@@ -277,32 +278,23 @@
       url='http://deepspeed.ai',
       project_urls={
           'Documentation': 'https://deepspeed.readthedocs.io',
           'Source': 'https://github.com/microsoft/DeepSpeed',
       },
       install_requires=install_requires,
       extras_require=extras_require,
-      packages=find_packages(include=['deepspeed',
-                                      'deepspeed.*']),
+      packages=find_packages(include=['deepspeed', 'deepspeed.*']),
       include_package_data=True,
       scripts=[
-          'bin/deepspeed',
-          'bin/deepspeed.pt',
-          'bin/ds',
-          'bin/ds_ssh',
-          'bin/ds_report',
-          'bin/ds_bench',
-          'bin/dsr',
+          'bin/deepspeed', 'bin/deepspeed.pt', 'bin/ds', 'bin/ds_ssh', 'bin/ds_report', 'bin/ds_bench', 'bin/dsr',
           'bin/ds_elastic'
       ],
       classifiers=[
-          'Programming Language :: Python :: 3.6',
-          'Programming Language :: Python :: 3.7',
-          'Programming Language :: Python :: 3.8',
-          'Programming Language :: Python :: 3.9',
+          'Programming Language :: Python :: 3.6', 'Programming Language :: Python :: 3.7',
+          'Programming Language :: Python :: 3.8', 'Programming Language :: Python :: 3.9',
           'Programming Language :: Python :: 3.10'
       ],
       license='MIT',
       ext_modules=ext_modules,
       cmdclass=cmdclass)
 
 end_time = time.time()
```


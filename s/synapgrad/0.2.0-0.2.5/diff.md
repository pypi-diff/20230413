# Comparing `tmp/synapgrad-0.2.0-py3-none-any.whl.zip` & `tmp/synapgrad-0.2.5-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,21 +1,21 @@
-Zip file size: 18906 bytes, number of entries: 19
+Zip file size: 22280 bytes, number of entries: 19
 -rw-rw-rw-  2.0 fat       72 b- defN 23-Apr-08 18:28 synapgrad/__init__.py
--rw-rw-rw-  2.0 fat    19226 b- defN 23-Apr-08 19:15 synapgrad/engine.py
--rw-rw-rw-  2.0 fat      200 b- defN 23-Apr-08 15:28 synapgrad/nn/__init__.py
--rw-rw-rw-  2.0 fat     1577 b- defN 23-Apr-08 16:01 synapgrad/nn/activations.py
--rw-rw-rw-  2.0 fat     1666 b- defN 23-Apr-08 00:44 synapgrad/nn/layers.py
--rw-rw-rw-  2.0 fat     2867 b- defN 23-Apr-08 16:17 synapgrad/nn/losses.py
--rw-rw-rw-  2.0 fat     2413 b- defN 23-Apr-07 23:47 synapgrad/nn/modules.py
--rw-rw-rw-  2.0 fat     2131 b- defN 23-Apr-08 00:26 synapgrad/nn/neurons.py
+-rw-rw-rw-  2.0 fat    21680 b- defN 23-Apr-12 10:56 synapgrad/engine.py
+-rw-rw-rw-  2.0 fat      306 b- defN 23-Apr-13 00:11 synapgrad/nn/__init__.py
+-rw-rw-rw-  2.0 fat     4883 b- defN 23-Apr-13 00:43 synapgrad/nn/activations.py
+-rw-rw-rw-  2.0 fat     1997 b- defN 23-Apr-11 18:05 synapgrad/nn/layers.py
+-rw-rw-rw-  2.0 fat     8087 b- defN 23-Apr-13 00:32 synapgrad/nn/losses.py
+-rw-rw-rw-  2.0 fat     2477 b- defN 23-Apr-11 18:04 synapgrad/nn/modules.py
+-rw-rw-rw-  2.0 fat     2150 b- defN 23-Apr-11 00:40 synapgrad/nn/neurons.py
 -rw-rw-rw-  2.0 fat       46 b- defN 23-Apr-08 14:30 synapgrad/optim/__init__.py
--rw-rw-rw-  2.0 fat     2226 b- defN 23-Apr-08 14:30 synapgrad/optim/optimizers.py
--rw-rw-rw-  2.0 fat      134 b- defN 23-Apr-08 18:50 synapgrad/utils/__init__.py
--rw-rw-rw-  2.0 fat     2965 b- defN 23-Apr-07 09:47 synapgrad/utils/data.py
--rw-rw-rw-  2.0 fat     1173 b- defN 23-Apr-08 19:37 synapgrad/utils/graph.py
--rw-rw-rw-  2.0 fat    10050 b- defN 23-Apr-07 12:39 synapgrad/utils/train.py
--rw-rw-rw-  2.0 fat     1088 b- defN 23-Apr-08 19:56 synapgrad-0.2.0.dist-info/LICENSE
--rw-rw-rw-  2.0 fat     4433 b- defN 23-Apr-08 19:56 synapgrad-0.2.0.dist-info/METADATA
--rw-rw-rw-  2.0 fat       92 b- defN 23-Apr-08 19:56 synapgrad-0.2.0.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       10 b- defN 23-Apr-08 19:56 synapgrad-0.2.0.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat     1525 b- defN 23-Apr-08 19:56 synapgrad-0.2.0.dist-info/RECORD
-19 files, 53894 bytes uncompressed, 16432 bytes compressed:  69.5%
+-rw-rw-rw-  2.0 fat     4004 b- defN 23-Apr-11 19:24 synapgrad/optim/optimizers.py
+-rw-rw-rw-  2.0 fat      154 b- defN 23-Apr-11 17:18 synapgrad/utils/__init__.py
+-rw-rw-rw-  2.0 fat     3403 b- defN 23-Apr-11 18:19 synapgrad/utils/data.py
+-rw-rw-rw-  2.0 fat     1272 b- defN 23-Apr-11 01:07 synapgrad/utils/graph.py
+-rw-rw-rw-  2.0 fat    10586 b- defN 23-Apr-12 12:20 synapgrad/utils/train.py
+-rw-rw-rw-  2.0 fat     1088 b- defN 23-Apr-13 00:59 synapgrad-0.2.5.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat     4762 b- defN 23-Apr-13 00:59 synapgrad-0.2.5.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 23-Apr-13 00:59 synapgrad-0.2.5.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       10 b- defN 23-Apr-13 00:59 synapgrad-0.2.5.dist-info/top_level.txt
+?rw-rw-r--  2.0 fat     1525 b- defN 23-Apr-13 00:59 synapgrad-0.2.5.dist-info/RECORD
+19 files, 68594 bytes uncompressed, 19806 bytes compressed:  71.1%
```

## zipnote {}

```diff
@@ -36,23 +36,23 @@
 
 Filename: synapgrad/utils/graph.py
 Comment: 
 
 Filename: synapgrad/utils/train.py
 Comment: 
 
-Filename: synapgrad-0.2.0.dist-info/LICENSE
+Filename: synapgrad-0.2.5.dist-info/LICENSE
 Comment: 
 
-Filename: synapgrad-0.2.0.dist-info/METADATA
+Filename: synapgrad-0.2.5.dist-info/METADATA
 Comment: 
 
-Filename: synapgrad-0.2.0.dist-info/WHEEL
+Filename: synapgrad-0.2.5.dist-info/WHEEL
 Comment: 
 
-Filename: synapgrad-0.2.0.dist-info/top_level.txt
+Filename: synapgrad-0.2.5.dist-info/top_level.txt
 Comment: 
 
-Filename: synapgrad-0.2.0.dist-info/RECORD
+Filename: synapgrad-0.2.5.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## synapgrad/engine.py

```diff
@@ -2,14 +2,16 @@
 import numpy as np
 from typing import Iterable, Union
 
 
 gradient__ = True
 retain_grads__ = False
 
+default_type__ = np.float32
+
 class no_grad:
     
     def __init__(self) -> None:
         self.prev = gradient__
     
     def __enter__(self):
         global gradient__
@@ -34,23 +36,24 @@
         
 
 def manual_seed(seed:int):
     np.random.seed(seed)
     random.seed(seed)
 
 
-def tensor(data, requires_grad=False) -> 'Tensor':
-    return Tensor(data, requires_grad=requires_grad)
+def tensor(data, requires_grad=False, dtype=None) -> 'Tensor':
+    return Tensor(data, requires_grad=requires_grad, dtype=dtype)
 
 
 class Tensor:
     
-    def __init__(self, data, _children=(), _operation=None, requires_grad=False) -> None:
+    def __init__(self, data, _children=(), _operation=None, requires_grad=False, dtype=None) -> None:
         if not isinstance(data, np.ndarray):
-            data = np.array(data, dtype=np.float64)
+            data = np.array(data, dtype=default_type__)
+        if dtype is not None and data.dtype != dtype: data = data.astype(dtype)
         assert isinstance(data, np.ndarray), "data must be a list or numpy array"
         
         self.data = data
         # Internal variables used for autograd graph construction
         self._grad = None
         self._grad_fn = None
         self._requires_grad = requires_grad and gradient__
@@ -89,15 +92,15 @@
         tensor_shape = tensor.data.shape; grad_shape = grad.data.shape
         diff_axis = len(grad_shape)-len(tensor_shape)
         sum_axis = None if diff_axis <= 0 else tuple(np.arange(abs(diff_axis), dtype=np.int8).tolist())
         
         if tensor.grad.matches_shape(grad) or sum_axis is None: 
             if sum_axis is None and not tensor.grad.matches_shape(grad):
                 not_compatible_dims = get_incompatible_dims(tensor_shape, grad_shape)
-                tensor += grad.sum(axis=not_compatible_dims)
+                tensor += grad.sum(axis=not_compatible_dims, keepdims=True)
             else:
                 tensor._grad += grad
         else:
             tensor._grad += grad.sum(axis=sum_axis)
     
     
     def __add__(self, summand:'Tensor') -> 'Tensor':
@@ -258,38 +261,58 @@
                 tensor._grad += grad
 
         out._backward = _backward
         
         return out
     
     
+    @staticmethod
+    def add(t1:'Tensor', t2:'Tensor') -> 'Tensor':
+        return t1 + t2
+    
+    
+    @staticmethod
+    def mul(t1:'Tensor', t2:'Tensor') -> 'Tensor':
+        return t1 * t2
+    
+    
+    @staticmethod
+    def matmul(t1:'Tensor', t2:'Tensor') -> 'Tensor':
+        return t1 @ t2
+    
+    
     def view(self, shape:tuple) -> 'Tensor':
         out = Tensor(self.data.reshape(shape), (self,), '<View>', requires_grad=self.requires_grad)
         
         def _backward():
             if self.requires_grad:
                 self._grad += out._grad.reshape(self.shape)
             
         out._backward = _backward
         
         return out
     
     
-    def squeeze(self, dim=None) -> 'Tensor':
-        data = np.squeeze(self.data, dim)
-        if dim is None:
-            dim = []
-            for i, d in enumerate(self.data.shape):
-                if d == 1: dim.append(i)
-                
+    def squeeze(self, dim:int=None) -> 'Tensor':
+        data = self.data
+        can_apply = len(self.shape) > 0 and (dim is None or self.shape[dim] == 1)
+        if can_apply:
+            data = np.squeeze(self.data, dim)
+            if dim is None:
+                dim = []
+                for i, d in enumerate(self.data.shape):
+                    if d == 1: dim.append(i) 
         out = Tensor(data, (self,), '<Squeeze>', requires_grad=self.requires_grad)
         
         def _backward():
             if self.requires_grad:
-                self._grad += np.expand_dims(out._grad, dim)
+                if can_apply:
+                    self._grad += np.expand_dims(out._grad, dim)
+                else:
+                    self._grad += out._grad
             
         out._backward = _backward
         
         return out
     
     
     def unsqueeze(self, dim) -> 'Tensor':
@@ -301,20 +324,20 @@
                 self._grad += np.squeeze(out._grad, dim)
             
         out._backward = _backward
         
         return out
     
     
-    def sum(self, dim:int=None) -> 'Tensor':
-        out = Tensor(self.data.sum(axis=dim), (self,), '<Sum>', requires_grad=self.requires_grad)
+    def sum(self, dim:int=None, keepdims=False) -> 'Tensor':
+        out = Tensor(self.data.sum(axis=dim, keepdims=keepdims), (self,), '<Sum>', requires_grad=self.requires_grad)
         
         def _backward():
             if self.requires_grad:
-                self._grad += np.ones(self.shape) * out._grad
+                self._grad += out._grad
             
         out._backward = _backward
         
         return out
     
     
     def mean(self, dim:int=None) -> 'Tensor':
@@ -338,14 +361,33 @@
                 self._grad += out._grad.swapaxes(dim0, dim1)
                 
         out._backward = _backward
         
         return out
     
     
+    def flatten(self, start_dim=0, end_dim=-1) -> 'Tensor':
+        shape = self.shape
+        start = start_dim if start_dim != -1 else len(shape)
+        end = end_dim if end_dim != -1 else len(shape)
+        if start > end:
+            raise RuntimeError("flatten() has invalid args: start_dim cannot come after end_dim")
+        if start < end:
+            shape = self.shape[:start] + (-1,) + self.shape[end+1:]
+        out = Tensor(self.data.reshape(shape), (self,), '<Flatten>', requires_grad=self.requires_grad)
+        
+        def _backward():
+            if self.requires_grad:
+                self._grad += out._grad.reshape(self.shape)
+                
+        out._backward = _backward
+        
+        return out
+    
+    
     def exp(self) -> 'Tensor':
         return np.e**self
     
     
     def log(self) -> 'Tensor':
         out = Tensor(np.log(self.data), (self,), '<Log>', requires_grad=self.requires_grad)
         
@@ -374,15 +416,15 @@
         if not self.requires_grad:
             raise RuntimeError("Trying to call backward on Tensor with requires_grad=False")
 
         if grad is None:
             if self.data.size > 1:
                 raise RuntimeError("grad must be specified for non-scalar tensors")
             else:
-                grad = np.array(1.0, dtype=np.float64)
+                grad = np.array(1.0, dtype=self.data.dtype)
                 
         if not isinstance(grad, np.ndarray):
             raise ValueError("Gradient parameter must be a numpy array")
                 
         self._is_leaf = True
         
         # Topological order all of the children in the graph 
@@ -426,14 +468,18 @@
         if len(squeezed.shape) > 0:
             raise ValueError("only one element tensors can be converted to Python scalars")
         
         return squeezed
         
     def detach(self) -> 'Tensor':
         return Tensor(self.data)
+    
+    def cpu(self) -> 'Tensor':
+        """ Returns itself, just added for compatibility with .cpu() pytorch call """
+        return self
         
     def matches_shape(self, tensor:Union['Tensor', np.ndarray]) -> bool:
         if len(self.shape) != len(tensor.shape):
             return False
         
         for n1, n2 in zip(self.shape, tensor.shape):
             if n1 != n2: return False
@@ -441,14 +487,18 @@
         return True
                 
     @property
     def shape(self) -> tuple:
         return self.data.shape
     
     @property
+    def dtype(self) -> np.dtype:
+        return self.data.dtype
+    
+    @property
     def size(self) -> int:
         return self.data.size
     
     @property
     def is_leaf(self) -> bool:
         return not self.requires_grad or self.grad_fn is None
     
@@ -515,23 +565,38 @@
         return self * other
 
     def __truediv__(self, other) -> 'Tensor': # self / other
         return self * other**-1
 
     def __rtruediv__(self, other) -> 'Tensor': # other / self
         return other * self**-1
+    
+    @staticmethod
+    def pretty_numpy(array:np.ndarray, decimals=5) -> str:
+        def truncate(f, n):
+            return np.floor(f * 10 ** n) / 10 ** n
+        rounded_data = array.copy().round(decimals=decimals)
+        #rounded_data += 0. # Remove -0.0 values (just 0.0)
+        str_to_rm = "array("
+        data_str = repr(rounded_data).replace(str_to_rm, "")
+        crop_index = (len(data_str)) - data_str[::-1].find("]")
+        cropped = data_str[:crop_index]
+        
+        braces_padd = (len(array.shape)-1)
+        no_padding = cropped.replace("\n" + " "*(braces_padd+len(str_to_rm)), "\n" + " "*braces_padd)
+        return no_padding
         
     def __repr__(self) -> str:
+        pretty_data_str = self.pretty_numpy(self.data)
         beggining = "Tensor("
         data_str = ""
-        for i, line in enumerate(str(self.data).splitlines(keepends=True)):
+        for i, line in enumerate(pretty_data_str.splitlines(keepends=True)):
             if i != 0:
                 line = " "*len(beggining) + line  
-            data_str += line 
-             
+            data_str += line
         string = f"{beggining}{data_str}, shape={self.shape}"
         
         if self.requires_grad:
             string += f", req_grad={self.requires_grad}"
             
         if self._operation is not None:
             string += f", op={self._operation})"
```

## synapgrad/nn/__init__.py

```diff
@@ -1,6 +1,6 @@
 
 from .modules import Module, Sequential
 from .neurons import Neuron
-from .layers import Linear
-from .losses import Loss, MSELoss, CrossEntropyLoss
-from .activations import ReLU, Sigmoid, Softmax
+from .layers import Linear, Flatten
+from .losses import Loss, MSELoss, CrossEntropyLoss, NLLLoss, BCELoss, BCEWithLogitsLoss
+from .activations import relu_fn, ReLU, Tanh, tanh_fn, sigmoid_fn, Sigmoid, softmax_fn, Softmax, LogSoftmax
```

## synapgrad/nn/activations.py

```diff
@@ -1,51 +1,142 @@
 import numpy as np
 from .. import Tensor, nn
 
 
+# ---------------------------- Functions ----------------------------
+# -------------------------------------------------------------------
+def relu_fn(data:np.ndarray) -> np.ndarray:
+    return np.maximum(0, data)
+
+
+def tanh_fn(data:np.ndarray) -> np.ndarray:
+    return np.tanh(data)
+
+
+def sigmoid_fn(data:np.ndarray) -> np.ndarray:
+    return 1/(1 + np.exp(-data))
+
+
+def softmax_fn(data:np.ndarray, dim:int) -> np.ndarray:
+    # Shift to make it numerically stable (with large values 'inf' appears)
+    shiftx = data - data.max(axis=dim, keepdims=True) 
+    exps = np.exp(shiftx)
+    exp_sums = exps.sum(axis=dim, keepdims=True)
+    return exps / exp_sums
+
+def log_softmax_fn(data:np.ndarray, dim:int) -> np.ndarray:
+    # Using log-sum-exp trick for numerical stability
+    max_val = data.max(axis=dim, keepdims=True)
+    substract = data - max_val
+    exp = np.exp(substract)
+    lse = max_val + np.log(exp.sum(axis=dim, keepdims=True))
+    log_softmax = data - lse
+    return log_softmax
+
+# ----------------------------- Modules -----------------------------
+# -------------------------------------------------------------------
 class ReLU(nn.Module):
     
     def forward(self, x:Tensor) -> Tensor:
-        assert isinstance(x, Tensor), "Input must be a Tensor"
-        relu = np.maximum(0, x.data)
+        assert isinstance(x, Tensor), "Input must be a Tensor" 
+        relu = relu_fn(x.data)
         out = Tensor(relu, (x,), '<ReLU>', requires_grad=x.requires_grad)
 
         def _backward():
             if x.requires_grad:
                 x._grad += (out.data > 0) * out._grad
         
         out._backward = _backward
 
         return out
 
 
+class Tanh(nn.Module):
+    """ np.sinh(x)/np.cosh(x) or -1j * np.tan(1j*x) """
+    
+    def forward(self, x:Tensor) -> Tensor:
+        assert isinstance(x, Tensor), "Input must be a Tensor"
+        tanh = tanh_fn(x.data)
+        out = Tensor(tanh, (x,), '<Tanh>', requires_grad=x.requires_grad)
+    
+        def _backward():
+            if x.requires_grad:
+                x_grad = 1 - tanh**2
+                x._grad += x_grad * out._grad
+            
+        out._backward = _backward
+        
+        return out
+
+
 class Sigmoid(nn.Module):
     
     def forward(self, x:Tensor) -> Tensor:
         # Returning (1/(1 + np.e**-x)) should be enough, but defining explicit 
         # grad function should be faster
         assert isinstance(x, Tensor), "Input must be a Tensor"
-        sigmoid = 1/(1 + np.exp(-x.data))
+        sigmoid = sigmoid_fn(x.data)
         out = Tensor(sigmoid, (x,), '<Sigmoid>', requires_grad=x.requires_grad)
     
         def _backward():
             if x.requires_grad:
-                f = 1/(1 + np.exp(-x.data))
-                x_grad = f * (1 - f)
-                
-                x._grad += (x_grad) * out._grad
+                x_grad = sigmoid * (1 - sigmoid)
+                x._grad += x_grad * out._grad
             
         out._backward = _backward
         
         return out
-    
+
     
 class Softmax(nn.Module):
-    """ Reference: https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/ """
+    """ References: 
+            https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/
+            https://aimatters.wordpress.com/2019/06/17/the-softmax-function-derivative/
+    """
     
     def __init__(self, dim) -> None:
         super().__init__()
         self.dim = dim
     
     def forward(self, x: Tensor) -> Tensor:
         assert isinstance(x, Tensor), "Input must be a Tensor"
-        return x.exp() / (x.exp().sum(-1)).unsqueeze(-1)
+        softmax = softmax_fn(x.data, self.dim)
+        out = Tensor(softmax, (x,), '<Softmax>', requires_grad=x.requires_grad)
+    
+        def _backward():
+            # Hand made derivation
+            if x.requires_grad:
+                jacobians = np.stack([np.diag(y) - np.outer(y, y) for y in softmax])
+                out_grad = np.expand_dims(out._grad, axis=self.dim)
+                x._grad += (out_grad @ jacobians).sum(axis=self.dim)
+            
+        out._backward = _backward
+        
+        return out
+
+    
+class LogSoftmax(nn.Module):
+    """ Same as Softmax(dim=self.dim)(x).log() but more numerically stable due to the log-sum-exp trick
+            Reference to log-sum-exp trick: https://en.wikipedia.org/wiki/LogSumExp 
+    """
+    
+    def __init__(self, dim) -> None:
+        super().__init__()
+        self.dim = dim
+    
+    def forward(self, x: Tensor) -> Tensor:
+        assert isinstance(x, Tensor), "Input must be a Tensor"
+        log_softmax = log_softmax_fn(x.data, self.dim)
+        out = Tensor(log_softmax, (x,), '<LogSoftmax>', requires_grad=x.requires_grad)
+    
+        def _backward():
+            if x.requires_grad:
+                softmax = np.exp(log_softmax)
+                jacobians = np.stack([np.diag(y) - np.outer(y, y) for y in softmax])
+                dlog_dsoftmax = (1/softmax) * out._grad
+                dlog_dsoftmax = np.expand_dims(dlog_dsoftmax, axis=self.dim)
+                x._grad += (dlog_dsoftmax @ jacobians).sum(axis=self.dim)
+            
+        out._backward = _backward
+        
+        return out
+
```

## synapgrad/nn/layers.py

```diff
@@ -6,31 +6,43 @@
 class Linear(nn.Module):
     
     def __init__(self, input_size:int, output_size:int, weight_init_method='he'):
         super().__init__()
         self.input_size = input_size
         self.output_size = output_size
         # Randomly initialize weights and biases
-        weight_values = [ init_weights(input_size, output_size, weight_init_method) for _ in range(output_size) ]
+        weight_values = [ init_weights(input_size, output_size, weight_init_method).astype(np.float32) for _ in range(output_size) ]
         self.weights = Tensor(weight_values, requires_grad=True)
-        self.biases = Tensor(np.zeros(output_size,), requires_grad=True)
+        self.biases = Tensor(np.zeros((output_size,), dtype=np.float32), requires_grad=True)
         
     def forward(self, x:Tensor) -> Tensor:
         assert x.shape[1] == self.input_size, f"Expected input size '{self.input_size}' but received '{x.shape[1]}'"
 
         out = (x @ self.weights.transpose(0,-1)) + self.biases
         
         return out
     
     def parameters(self) -> list[Tensor]:
         return [self.weights, self.biases]
     
     def __repr__(self) -> str:
         return f"Linear(input_size={self.input_size}, neurons={len(self.output_size)})"
-        
+
+
+class Flatten(nn.Module):
+    
+    def __init__(self, start_dim=1, end_dim=-1) -> None:
+        super().__init__()
+        self.start_dim = start_dim
+        self.end_dim = end_dim
+    
+    def forward(self, x: Tensor) -> Tensor:
+        return x.flatten(self.start_dim, self.end_dim)
+
+
 
 # class Conv2D(Layer):
     
 #     def __init__(self, filters, kernel_size, strides=None, padding=None) -> None:
 #         self.filters = filters
 #         self.kernel_size = kernel_size
 #         self.strides = strides
```

## synapgrad/nn/losses.py

```diff
@@ -1,80 +1,199 @@
 from typing import Any
 from abc import ABC, abstractmethod
 from .. import Tensor
+from .activations import softmax_fn, relu_fn
 import numpy as np
 
 
-epsilon = 1e-7
+epsilon = 1e-12
+
+# ---------------------------- Functions ----------------------------
+# -------------------------------------------------------------------
+def mse_loss(y_pred: np.ndarray, y_true: np.ndarray) -> np.ndarray:
+    loss = (y_pred - y_true)**2
+    return loss
+    
+    
+def nll_loss(y_pred: np.ndarray, y_true: np.ndarray) -> np.ndarray:
+    loss = -y_pred[range(len(y_pred)), y_true].reshape((-1, 1))
+    return loss
+
+
+def bce_loss(y_pred: np.ndarray, y_true: np.ndarray) -> np.ndarray:
+    assert y_pred.max() <= 1 and y_pred.min() >= 0, "BCELoss inputs must be between 0 and 1"
+    loss = - (y_true * np.log(y_pred + epsilon) + (1 - y_true) * np.log(1 - y_pred + epsilon))
+    # For compatibility with pytorch (returns 100 when y_pred=0 and y_true=1; vice versa)
+    loss = np.where(loss == -np.log(epsilon), 100, loss) 
+    return loss
+
+
+def bce_with_logits_loss(y_pred: np.ndarray, y_true: np.ndarray) -> np.ndarray:
+    tn = -relu_fn(y_pred)
+    loss = (1-y_true) * y_pred + tn + np.log(np.exp(-tn) + np.exp((-y_pred-tn)))
+    return loss
 
 
+def cross_entropy_loss(y_pred: np.ndarray, y_true: np.ndarray) -> np.ndarray:
+    softmax = np.log(softmax_fn(y_pred, 1) + epsilon)
+    log_likelihood = nll_loss(softmax, y_true)
+    return log_likelihood
+
+# ----------------------------- Modules -----------------------------
+# -------------------------------------------------------------------
 class Loss(ABC):
     
     def __init__(self, reduction='mean') -> None:
         self.reduction = reduction
         
     def __call__(self, y_pred:Tensor, y_true:Tensor) -> Any:
-        assert y_pred.matches_shape(y_true), f"Inputs shape don't match y_pred={y_pred.shape}, y_true={y_true.shape}"
+        assert isinstance(y_pred, Tensor) and isinstance(y_true, Tensor), "Inputs must be Tensors"
 
         loss = self.criterion(y_pred, y_true)
         
         # Reduction
         if self.reduction == 'sum':
-            loss = loss.sum()
+            reduction = loss.sum()
         elif self.reduction == 'mean':
-            loss = loss.mean()
+            reduction = loss.mean()
+        else:
+            reduction = loss    
+          
+        if self.reduction is not None:
+            reduction._operation = loss._operation.replace(">", reduction._operation.replace("<", ""))
             
-        return loss
+        return reduction
     
     @abstractmethod
     def criterion(self, y_pred:Tensor, y_true:Tensor) -> Tensor:
         pass
 
 
 class MSELoss(Loss):
     """ Mean Squared Error Loss: (y_pred - y_true)**2 """
     
     def criterion(self, y_pred:Tensor, y_true:Tensor) -> Tensor:
-        assert isinstance(y_pred, Tensor) and isinstance(y_true, Tensor), "Inputs must be Tensors"
+        assert y_pred.matches_shape(y_true), f"Inputs shape don't match y_pred={y_pred.shape}, y_true={y_true.shape}"
         req_grad = y_pred.requires_grad or y_true.requires_grad
-        loss = Tensor((y_pred.data - y_true.data)**2, (y_pred, y_true), '<MSELoss>', requires_grad=req_grad)
+        mse = mse_loss(y_pred.data, y_true.data)
+        loss = Tensor(mse, (y_pred, y_true), '<MSELoss>', requires_grad=req_grad)
         
         def _backward():
-            grad = 2*(y_pred.data - y_true.data) * loss._grad
-            if y_pred.requires_grad: y_pred._grad += grad # * loss._grad
-            if y_true.requires_grad: y_true._grad += grad # * loss._grad
+            if y_pred.requires_grad: 
+                grad = 2*(y_pred.data - y_true.data) 
+                y_pred._grad += grad * loss._grad
         
         loss._backward = _backward
         
         return loss
+
+
+class NLLLoss(Loss):
+    """ This class expects y_true to be the probabilities of a LogSoftmax function. Also, y_pred 
+        should be shape=(batch, num_classes). It expects to receive the probability of a sample to be of each class.
+        For binary classification problems, output should not be a scalar value (0 <= x <=1) but an array with 
+        the probability of each class [0.3, 0.7]
+        reference: https://towardsdatascience.com/cross-entropy-negative-log-likelihood-and-all-that-jazz-47a95bd2e81    
+    """
     
+    def criterion(self, y_pred: Tensor, y_true: Tensor) -> Tensor:
+        assert isinstance(y_pred, Tensor) and isinstance(y_true, Tensor), "Inputs must be Tensors"
+        req_grad = y_pred.requires_grad or y_true.requires_grad
+        log_likelihood = nll_loss(y_pred.data, y_true.data)
+        loss = Tensor(log_likelihood, (y_pred, y_true), '<NLLLoss>', requires_grad=req_grad)
+        
+        def _backward():
+            # Hand made derivation
+            if y_pred.requires_grad:
+                grad = np.zeros(y_pred.shape)
+                grad[range(len(y_pred.data)), y_true.data] = -1.0
+                # for i, true in enumerate(y_true.data):
+                #     grad[i][true] = -1.0
+                y_pred._grad += (grad * loss._grad)
+        
+        loss._backward = _backward
+        
+        return loss
 
-# class BCELoss(Loss):
-#     # TODO: Not working properly
+   
+class BCELoss(Loss):
     
-#     def __init__(self) -> None:
-#         pass
+    def criterion(self, y_pred: Tensor, y_true: Tensor) -> Tensor:
+        req_grad = y_pred.requires_grad or y_true.requires_grad
+        bce = bce_loss(y_pred.data, y_true.data)
+        loss = Tensor(bce, (y_pred, y_true), '<BCELoss>', requires_grad=req_grad)
+        
+        def _backward():
+            # Hand made derivation
+            if y_pred.requires_grad:
+                term_0 = -(1 - y_true.data + epsilon) / ((1 - y_pred.data) + epsilon)
+                term_1 = (y_true.data + epsilon) / (y_pred.data + epsilon)
+                y_pred._grad += -(term_0 + term_1) * loss._grad
+        
+        loss._backward = _backward
+        
+        return loss
+
+
+class BCEWithLogitsLoss(Loss):
+    """ This loss combines a Sigmoid layer and the BCELoss in one single class.
+    This version is more numerically stable than using a plain Sigmoid followed by a BCELoss as,
+    by combining the operations into one layer, we take advantage of the log-sum-exp
+    trick for numerical stability
+    
+    References:
+        https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html
+        https://stackoverflow.com/questions/66906884/how-is-pytorchs-class-bcewithlogitsloss-exactly-implemented
+    """
     
-#     def __call__(self, y_pred:np.ndarray, y_true:np.ndarray) -> float:
-#         super().__call__(y_pred, y_true)
-#         y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
-#         term_0 = (1-y_true) * np.log(1-y_pred + epsilon)
-#         term_1 = y_true * np.log(y_pred + epsilon)
+    def criterion(self, y_pred: Tensor, y_true: Tensor) -> Tensor:
+        req_grad = y_pred.requires_grad or y_true.requires_grad
+        bce = bce_with_logits_loss(y_pred.data, y_true.data)
+        loss = Tensor(bce, (y_pred, y_true), '<BCEWithLogitsLoss>', requires_grad=req_grad)
+        
+        def _backward():
+            # Hand made derivation
+            if y_pred.requires_grad:
+                tn = -relu_fn(y_pred.data)
+                dtn = np.where(tn == 0, 0, -1)
+                div1 = -dtn*np.exp(-tn) + (-1-dtn)*np.exp((-y_pred.data-tn))
+                div2 = np.exp(-tn) + np.exp((-y_pred.data-tn))
+                grad = (1 - y_true.data) + dtn + (div1/(div2 + epsilon))
+                
+                y_pred._grad += grad * loss._grad
         
-#         return -np.mean(term_0+term_1, axis=0)
+        loss._backward = _backward
+        
+        return loss
     
     
 class CrossEntropyLoss(Loss):
-    """ Reference: https://deepnotes.io/softmax-crossentropy#:~:text=Cross%20entropy%20indicates%20the%20distance,used%20alternative%20of%20squared%20error"""
-    # TODO: Not working properly
+    """ Same as:\n
+    log_softmax = LogSoftmax(dim=1)(y_pred)\n
+    loss = NLLLoss(reduction=None)(log_softmax, y_true)
+    
+    Reference: https://deepnotes.io/softmax-crossentropy#:~:text=Cross%20entropy%20indicates%20the%20distance,used%20alternative%20of%20squared%20error
+    """
     
     def criterion(self, y_pred: Tensor, y_true: Tensor) -> Tensor:
         """
-        X is the output from fully connected layer (num_examples x num_classes)
-        y is labels (num_examples x 1)
+        y_pred is the output (logits) from fully connected layer (num_examples x num_classes)
+        y_true is labels (num_examples x 1)
             Note that y is not one-hot encoded vector. 
             It can be computed as y.argmax(axis=1) from one-hot encoded vectors of labels if required.
         """
-        assert isinstance(y_pred, Tensor) and isinstance(y_true, Tensor), "Inputs must be Tensors"
-        return -y_pred[range(y_true.shape[0]), y_true.data.argmax(axis=1)].log().mean()
-                    
+        req_grad = y_pred.requires_grad or y_true.requires_grad
+        cross_entropy = cross_entropy_loss(y_pred.data, y_true.data)
+        loss = Tensor(cross_entropy, (y_pred, y_true), '<CrossEntropyLoss>', requires_grad=req_grad)
+        
+        def _backward():
+            # Hand made derivation
+            if y_pred.requires_grad:
+                dlogits = softmax_fn(y_pred.data, 1)
+                n = y_pred.shape[0]
+                dlogits[range(n), y_true.data] -= 1
+                y_pred._grad += (dlogits * loss._grad)
+        
+        loss._backward = _backward
+        
+        return loss
```

## synapgrad/nn/modules.py

```diff
@@ -2,22 +2,23 @@
 from .. import Tensor
     
 
 class Module(ABC):
     
     def __init__(self) -> None:
         self.modules = []
+        self.training = False
         
     def train(self):
         """ Set module to train mode"""
-        pass
+        self.training = True
         
     def eval(self):
         """ Set module to evaluation mode """
-        pass
+        self.training = False
         
     def __call__(self, batch:Tensor) -> Tensor:
         if len(batch.shape) <= 1:
             raise ValueError(f"Module '{self.__class__.__name__}' expects a batched input, Shape=(batch_size, *), but received {batch.shape}")
 
         return self.forward(batch)
```

## synapgrad/nn/neurons.py

```diff
@@ -7,15 +7,15 @@
 
 
 class Neuron(nn.Module):
     
     def __init__(self, inputs:int, weight_init_method='he') -> None:
         self.inputs = inputs
         # Randomly initialize weights and bias
-        weight_values = np.expand_dims(init_weights(inputs, 1, weight_init_method), 0)
+        weight_values = np.expand_dims(init_weights(inputs, 1, weight_init_method), 0).astype(np.float32)
         self.weights = Tensor(weight_values, requires_grad=True)
         self.bias = Tensor([0], requires_grad=True)
     
     def forward(self, x:Tensor) -> Tensor:
         assert x[0].matches_shape(self.weights[0]), f"Expected input size '{self.weights[0].shape}' but received '{x[0].shape}'"
 
         out = (x @ self.weights.transpose(0,-1)) + self.bias
```

## synapgrad/optim/optimizers.py

```diff
@@ -7,30 +7,74 @@
 
 class Optimizer(ABC):
     
     def __init__(self, parameters:list[Tensor], lr=0.001) -> None:
         super().__init__()
         self.parameters = parameters
         self.lr = lr
+        self.t = 0
         
     def zero_grad(self):
         for p in self.parameters:
             p.zero_()
         
     @abstractmethod
     def step(self):
-        pass
+        self.t += 1
 
 
 class SGD(Optimizer):
+    """Reference: 
+        https://pytorch.org/docs/stable/generated/torch.optim.SGD.html
+    """
+    
+    def __init__(self, parameters: list[Tensor], lr=0.001, momentum=0, dampening=0,
+                 weight_decay=0, nesterov=False, maximize=False) -> None:
+        super().__init__(parameters, lr)
+        self.momentum = momentum
+        self.acum_m = []
+        self.nesterov = nesterov
+        self.dampening = dampening
+        self.maximize = maximize
+        self.weight_decay = weight_decay
+        
+        if nesterov and (momentum <= 0 or dampening != 0):
+            raise ValueError("Nesterov momentum requires a momentum and zero dampening")
     
     def step(self):
+        super().step()
         with engine.no_grad():
-            for p in self.parameters:
-                p.data -= self.lr*p._grad
+            for i, p in enumerate(self.parameters):
+                grad = p._grad
+                
+                p.data -= self.lr*grad
+                
+                # Pytorch version with extra attrs (Not working yet)
+                # # Weight decay
+                # if self.weight_decay != 0:
+                #     grad += self.weight_decay*p.data
+                
+                # # Momentum
+                # if self.momentum != 0:
+                #     if self.t > 1:
+                #         self.acum_m[i] = self.momentum*self.acum_m[i] + (1-self.dampening)*grad
+                #     else:
+                #         self.acum_m.append(grad)
+                
+                #     # Nesterov
+                #     if self.nesterov:
+                #         grad += self.momentum*self.acum_m[i]
+                #     else:
+                #         grad = self.acum_m[i]
+                
+                # # Update Parameter
+                # if self.maximize:
+                #     p.data += self.lr*grad
+                # else:
+                #     p.data -= self.lr*grad
         
     
 class Adam(Optimizer):
     # TODO: Nos working properly
     
     def __init__(self, parameters: list[Tensor], lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8) -> None:
         """Simplified Adam optimizer -> https://pytorch.org/docs/stable/generated/torch.optim.Adam.html
@@ -45,15 +89,15 @@
         self.beta2 = beta2
         self.epsilon = epsilon
         
         self.mt = [0 for _ in range(len(parameters))]
         self.vt = [0 for _ in range(len(parameters))]
     
     def step(self):
-        
+        super().step()
         with engine.no_grad():
             for i, p in enumerate(self.parameters):
                 # Update the moving average of the gradient
                 m = self.beta1 * self.mt[i] + (1 - self.beta1) * p._grad
                 self.mt[i] = m
                 m_corrected = m / (1 - self.beta1)
```

## synapgrad/utils/__init__.py

```diff
@@ -1,4 +1,4 @@
 
 from . import data, graph, train
 from .train import Trainer, Evaluator
-from .data import split_dataset, one_hot_encode, DataLoader
+from .data import split_dataset, one_hot_encode, DataLoader, DataLoaderCallback
```

## synapgrad/utils/data.py

```diff
@@ -1,8 +1,9 @@
 
+from abc import ABC, abstractmethod 
 import numpy as np
 
 
 def split_dataset(X, y, test_split:float=0.2, val_split=None, shuffle:bool=False):
     
     def get_split_indices(data_size, test_split, val_split):
         # Creating data indices for training and validation splits:
@@ -49,24 +50,31 @@
     encoded = []
     for label in y:
         zeros = [0]*len(uniques)
         zeros[uniques.index(label)] = 1
         encoded.append(zeros)
         
     return np.array(encoded)
-        
+
+
+class DataLoaderCallback(ABC):
+    
+    @abstractmethod
+    def __call__(self, data_loader:'DataLoader', X_batch:np.ndarray, y_batch:np.ndarray):
+        pass
+    
         
 class DataLoader:
     
-    def __init__(self, X, y, batch_size, engine) -> None:
-        self.X = X
-        self.y = y
+    def __init__(self, X, y, batch_size, engine, transform:DataLoaderCallback=None) -> None:
+        self.X = X; self.y = y
         self.batach_size = batch_size
         self.engine = engine
         self.step = 0
+        self.transform = transform
     
     def __len__(self):
         return len(self.y) // self.batach_size
     
     def __iter__(self):
         self.step = 0
         return self
@@ -78,11 +86,17 @@
             return item
         
         raise StopIteration
         
     def __getitem__(self, idx) -> tuple:
         start = idx*self.batach_size
         end = (idx*self.batach_size) + self.batach_size
-        x = self.engine.tensor(self.X[start:end])
-        y = self.engine.tensor(self.y[start:end])
+        
+        X_batch = self.X[start:end]
+        y_batch = self.y[start:end]
+        if self.transform is not None:
+            return self.transform(self, X_batch, y_batch)
+        
+        x = self.engine.tensor(X_batch)
+        y = self.engine.tensor(y_batch)
         
         return x, y
```

## synapgrad/utils/graph.py

```diff
@@ -1,36 +1,37 @@
 
+from .. import Tensor
 from graphviz import Digraph
 
 
 def trace(root):
     nodes, edges = set(), set()
     def build(n):
         if n not in nodes:
             nodes.add(n)
             for child in n._children:
                 edges.add((child, n))
                 build(child)
     build(root)
     return nodes, edges
 
-def draw_dot(root, format='svg', rankdir='LR'):
+def draw(root, format='svg', rankdir='LR'):
     """
     format: png | svg | ...
     rankdir: TB (top to bottom graph) | LR (left to right)
     """
     assert rankdir in ['LR', 'TB']
     nodes, edges = trace(root)
     dot = Digraph(format=format, graph_attr={'rankdir': rankdir}) #, node_attr={'rankdir': 'TB'})
     
-    
     for n in nodes:
         nid = str(id(n))
-        grad = None if n._grad is None else n._grad.round(decimals=2)
-        dot.node(name=nid, label=f"tensor={n.data.round(decimals=2)} | req_grad={n.requires_grad}, is_leaf={n.is_leaf} | grad={grad}", shape='record')
+        data_str = Tensor.pretty_numpy(n.data, decimals=2)
+        grad_str = 'None' if n._grad is None else Tensor.pretty_numpy(n._grad, decimals=2)
+        dot.node(name=nid, label=f"<<b>Tensor</b> | data={data_str} | req_grad={n.requires_grad}   is_leaf={n.is_leaf} | grad={grad_str}>", shape='record')
         if n._operation:
             dot.node(name=nid + n._operation, label=n._operation)
             dot.edge(nid + n._operation, nid)
     
     for n1, n2 in edges:
         dot.edge(str(id(n1)), str(id(n2)) + n2._operation)
```

## synapgrad/utils/train.py

```diff
@@ -8,26 +8,33 @@
 )
 
 MIN_MODE = 'min'
 MAX_MODE = 'max'
 
 class Evaluator:
     
-    def __init__(self, epoch_callback=None, step_callback=None, accuracy=True) -> None:
+    # output of model is scalar as well as labels (0 or 1) ex: [0,1,0,1,0]
+    BINARY = 'binary' 
+    # output of model is vector but labels are scalars with values [0, n_class) ex: [0,3,9,2,1] (10 classes)
+    MULTI_CLASS = 'multi-class'
+    # output of model is vector and labels are one-hot encoded ex: [[0,0,1] [0,1,0]]
+    CATEGORICAL = 'categorical'
+    
+    
+    def __init__(self, epoch_callback=None, step_callback=None, accuracy=True, mode=MULTI_CLASS) -> None:
         self.accuracy_bool = accuracy
         self.epoch_callback = epoch_callback
         self.step_callback = step_callback
-        self.y_true = np.array([], dtype=np.uint0)
-        self.y_pred = np.array([], dtype=np.uint0)
-        self.binary_low_boundary = 0
-        self.binary_high_boundary = 1
+        self.y_true = np.array([], dtype=np.int16)
+        self.y_pred = np.array([], dtype=np.int16)
+        self.mode = mode
         
     def reset(self):
-        self.y_true = np.array([], dtype=np.uint0)
-        self.y_pred = np.array([], dtype=np.uint0)
+        self.y_true = np.array([], dtype=np.int16)
+        self.y_pred = np.array([], dtype=np.int16)
     
     def step(self, labels, outputs, prefix=None) -> list:
         """ Calculates custom metrics for each step
 
         Args:
             labels (torch.Tensor): tensor with labels (0 or 1) in each coordinate
             outputs (torch.Tensor): tensor with labels not rounded, as the model returned
@@ -36,29 +43,33 @@
         Returns:
             tuple: [('metric_name', metric_val), ...]
         """
         
         labels_numpy = labels.squeeze().detach().numpy()
         outputs_numpy = outputs.squeeze().detach().numpy()
         
-        if len(labels_numpy.shape) > 1:
-            # Multi-class
-            y_true = np.argmax(labels_numpy, axis=1)
+        
+        if self.mode == self.BINARY:
+            y_pred = np.where(outputs_numpy > 0.5, 1, 0)
+            y_true = labels_numpy
+        elif self.mode == self.MULTI_CLASS:
             y_pred = np.argmax(outputs_numpy, axis=1)
-        else:
-            # Binary-class
             y_true = labels_numpy
-            low = self.binary_low_boundary; high = self.binary_high_boundary
-            y_pred = np.where(outputs_numpy > (high+low)/2, high, low)
+        elif self.mode == self.CATEGORICAL:
+            y_pred = np.argmax(outputs_numpy, axis=1)
+            y_true = np.argmax(labels_numpy, axis=1)
+        else:
+            raise RuntimeError(f"Evaluator: mode '{self.mode}' is not valid")
             
-        y_true = y_true.astype(np.uint0)
-        y_pred = y_pred.astype(np.uint0)
+            
+        y_true = y_true.astype(np.int16)
+        y_pred = y_pred.astype(np.int16)
         
-        self.y_true = np.concatenate([self.y_true, y_true], axis=0, dtype=np.uint0)
-        self.y_pred = np.concatenate([self.y_pred, y_pred], axis=0, dtype=np.uint0)
+        self.y_true = np.concatenate([self.y_true, y_true], axis=0, dtype=np.int16)
+        self.y_pred = np.concatenate([self.y_pred, y_pred], axis=0, dtype=np.int16)
         
         metrics = self.__compute(y_true, y_pred, prefix, callback=self.step_callback)
         
         return metrics
     
     def basic_accuracy_callback(self, y_true:np.ndarray, y_pred:np.ndarray) -> list:
         train_accuracy = ((y_true == y_pred).sum() / len(y_true))
@@ -83,14 +94,34 @@
         if prefix is not None:
             metrics_with_prefix = []
             for (m, v) in metrics:
                 metrics_with_prefix.append((f"{prefix}_{m}", v))
             metrics = metrics_with_prefix
             
         return metrics
+    
+    def report(self, y_pred, y_true, classes=None):
+        
+        auc = None
+        if self.mode == self.BINARY:
+            y_pred = np.where(y_pred > 0.5, 1, 0)
+            # Compute AUC
+            auc = roc_auc_score(y_true, y_pred)
+        elif self.mode == self.MULTI_CLASS:
+            y_pred = np.argmax(y_pred, axis=1)
+        elif self.mode == self.CATEGORICAL:
+            y_pred = np.argmax(y_pred, axis=1)
+            y_true = np.argmax(y_true, axis=1)
+        else:
+            raise RuntimeError(f"Evaluator: mode '{self.mode}' is not valid")
+        
+        print("Accuracy:", accuracy_score(y_true, y_pred))
+        if auc is not None: print("ROC_AUC:", auc)
+        print(confusion_matrix(y_true, y_pred))
+        print(classification_report(y_true, y_pred, target_names=classes))
             
              
 class Trainer:
     
     def __init__(self, model, engine) -> None:
         self.model = model
         self.engine = engine
@@ -174,15 +205,15 @@
                 self.evaluator.step(labels, outputs, prefix='val') if self.evaluator != None else []
             
             val_loss = total_val_loss / (i + 1)
             val_metrics = self.evaluator.compute(prefix='val') if self.evaluator != None else []
         
         return [('val_loss', val_loss)] + val_metrics
         
-    def plot(self, metrics:list=['loss'], figsize=(15,5), style=False, ylim:tuple=None):
+    def plot(self, metrics:list=['loss'], figsize=(15,5), style=False, ylim:tuple=(0,1)):
         if style: plt.style.use("ggplot")
         
         plt.figure(figsize=figsize)
         def plot_history_metric(metric):
             plt.plot(self.history[f"{metric}"])
             val_metric = f"val_{metric}"
             if val_metric in self.history: plt.plot(self.history[val_metric])
@@ -194,44 +225,27 @@
             plt.legend(['train', 'validation'], loc='upper left')
         
         for i, m in enumerate(metrics):
             plt.subplot(1,len(metrics), i+1)
             plot_history_metric(m)
         plt.show()
     
-    # def test(self, test_loader, report=True, classes=None) -> np.ndarray:
-    #     # we can now evaluate the network on the test set
-    #     print("[INFO] Testing network...")
-    #     # set the model in evaluation mode
-    #     self.model.eval()
-    #     if self.model.training: raise Exception("Model is in training mode")
-    #     preds = []; y_test = []
-    #     with torch.no_grad():
-    #         for data in test_loader:
-    #             *inputs, labels = data
-    #             for l in labels.cpu().numpy():
-    #                 y_test.append(l)
-    #             inputs = self.__to_device(inputs)
-    #             pred = self.model(*inputs).squeeze(dim=1)
-    #             pred = pred.cpu()
-    #             for p in pred:
-    #                 preds.append(p.numpy())
+    def test(self, test_loader) -> tuple[np.ndarray, np.ndarray]:
+        print("[INFO] Testing network...")
+        # set the model in evaluation mode
+        self.model.eval()
+        if self.model.training: raise Exception("Model is in training mode")
+        preds = []; y_true = []
+        with self.engine.no_grad():
+            for data in test_loader:
+                *inputs, labels = data
+                for l in labels.numpy():
+                    y_true.append(l)
+                pred = self.model(*inputs).squeeze(dim=1)
+                for p in pred:
+                    preds.append(p.numpy())
 
-    #     y_test = np.array(y_test)
-    #     y_pred = np.array(preds)
-        
-    #     if len(y_test.shape) > 1:
-    #         # Multi-class
-    #         y_test = np.argmax(y_test, axis=1)
-    #         y_pred = np.argmax(y_pred, axis=1)
-    #     else:
-    #         # Binary-class
-    #         y_pred = np.where(y_pred > 0.5, 1, 0)
-        
-    #     if report:
-    #         print("Accuracy:", accuracy_score(y_test, y_pred))
-    #         print("ROC_AUC:", roc_auc_score(y_test, y_pred))
-    #         print(confusion_matrix(y_test, y_pred))
-    #         print(classification_report(y_test, y_pred, target_names=classes))
+        y_true = np.array(y_true)
+        y_pred = np.array(preds)
         
-    #     return y_pred, y_test
+        return y_pred, y_true
```

## Comparing `synapgrad-0.2.0.dist-info/LICENSE` & `synapgrad-0.2.5.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `synapgrad-0.2.0.dist-info/METADATA` & `synapgrad-0.2.5.dist-info/METADATA`

 * *Files 7% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: synapgrad
-Version: 0.2.0
+Version: 0.2.5
 Summary: An autograd Tensor-based engine with a deep learning library built on top of it made from scratch
 Home-page: https://github.com/pgmesa/synapgrad
 Author: Pablo García Mesa
 Author-email: pgmesa.sm@gmail.com
 License: UNKNOWN
 Platform: UNKNOWN
 Classifier: Programming Language :: Python :: 3.9
@@ -15,28 +15,31 @@
 License-File: LICENSE
 Requires-Dist: numpy (>=1.21.0)
 
 #  SynapGrad
 
 An autograd Tensor-based engine with a deep learning library built on top of it made from scratch
 
+> :warning: This project is not yet fully completed. Please review the following sections for updates on [next steps](#todo-list). 
+
 ## Technical Description
 This project implements a completely functional engine for tracking operations between Tensors, by dynamically building a Directed Acyclic Graph (DAG), and an automatic backpropagation algorithm (reverse-mode autodiff) over this DAG.
 
 Built on top of the engine, the deep learning library implements the most common functions, layers, losses and optimizers in order to create MLPs and CNNs able to solve AI problems
 
 This library tries to mimic Pytorch in a simplified way, but with similar functions and behaviour. 
 
 ## Aim of the project
 The aim of this project is to create a deep learning library from scratch, without using any existing framework (such as keras, pytorch, tensorflow, sklearn, etc) in order to fully understand the core aspects of how they work. Specifically, this time I have focused on pytorch.
 
-Some of the external frameworks mentioned before have been used for the following reasons:
+However, some of these external frameworks have been used for the following reasons:
 
-- (pytorch) to check gradient calculation is correct
-- (sklearn) (keras) to download the example datasets
+- (pytorch) to check gradient calculation is correct.
+- (keras) to download handwritten digits MNIST dataset
+- (sklearn) to create 'make_moons' dataset
 
 This project stems from the amazing educational project `micrograd` by `karpathy` (https://github.com/karpathy/micrograd)
 
 Note: Supporting GPU execution is out of the scope of this project
 
 ## Installation
 ```bash
@@ -68,34 +71,34 @@
 ```
 
 ## Training examples using synapgrad
 
 This project comes with 3 jupyter notebooks (in `examples/`) that solve 3 beginner's problems in AI:
 
 - [x] 1. Basic MLP for binary classification (sklearn 'make_moons' toy dataset)
-- [ ] 2. MLP for handwritten digits classification (MNIST dataset) 
+- [x] 2. MLP for handwritten digits classification (MNIST dataset) 
 - [ ] 3. CNN for handwritten digits classification (MNIST dataset)
 
-So far, only the first example has been implemented
+> :warning: The 3rd example has not been implemented yet
 
 Example 1 (synapgrad MLP solution)     |  Example 2 and 3
 :-------------------------:|:-------------------------:
-![Board Image](/assets/example1.png) | ![Check Image](/assets/example23.png) 
+![Board Image](/assets/example_moons.png) | ![Check Image](/assets/example_mnist.png) 
 
 ## Comparisons with other frameworks
 In order to see the efficiency of synapgrad, it is compared with other existing engines (in this case torch and micrograd).
 
 
 | Training Example | synapgrad | torch | micrograd |
 |     :---:        |  :---:  |  :---:  |   :---:   |  
-| 1  | 1.7 s | 1.5 s | 1 min y 43 s |
-| 2  | - | - | - |
-| 3  | - | - | - |
+| 1 | 1.7 s | 1.5 s | 1 min 43 s |
+| 2 | 52 s | 31 s | - |
+| 3 |  -  |  -  | - |
 
-As you can see, synapgrad is fast
+As it can be seen, synapgrad is quite fast
 
 ## Graph Visualization
 In the `examples/trace_graph.ipynb` notebook there is an example of how to display the graph that synapgrad creates in the background as operations are chained.
 
 ```python
 import synapgrad
 from synapgrad import nn, utils
@@ -104,18 +107,23 @@
     x = synapgrad.tensor([5.0, 3.0], requires_grad=True)
     x2 = synapgrad.tensor([6.0, 0.4], requires_grad=True)
     y = (x ** 3 + 3) 
     y2 = (x2 / x)
     z = y * y2 * x2
     z = z.sum()
     z.backward()
-utils.graph.draw_dot(z)
+utils.graph.draw(z)
 ```
 
 ![Board Image](/assets/graph_example.svg)
 
 ## Running tests
 To run the unit tests you will have to install PyTorch. In these tests, gradients calculation as well as losses, layers, etc, are assessed with pytorch to check everything is working fine. To run the tests:
 ```bash
 python -m pytest
 ```
 
+## ToDo list
+- Add extra parameters to SGD
+- Finish Adam optimizer
+- Implement training example 3 (Conv2D, MaxPool2D, BatchNorm2D)
+
```


# Comparing `tmp/bdpy-0.18-py2.py3-none-any.whl.zip` & `tmp/bdpy-0.19-py2.py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,65 +1,65 @@
-Zip file size: 85471 bytes, number of entries: 63
--rw-r--r--  2.0 unx      277 b- defN 22-Jan-14 04:50 bdpy/__init__.py
--rw-r--r--  2.0 unx      142 b- defN 22-Jan-14 04:50 bdpy/bdata/__init__.py
--rw-r--r--  2.0 unx    27841 b- defN 22-Apr-09 14:12 bdpy/bdata/bdata.py
--rw-r--r--  2.0 unx     3142 b- defN 22-Jan-14 04:50 bdpy/bdata/featureselector.py
--rw-r--r--  2.0 unx     3959 b- defN 22-Jan-14 04:50 bdpy/bdata/metadata.py
--rw-r--r--  2.0 unx     6016 b- defN 22-Jan-14 04:50 bdpy/bdata/utils.py
--rw-r--r--  2.0 unx      155 b- defN 22-Jan-14 04:50 bdpy/dataform/__init__.py
--rw-r--r--  2.0 unx     8031 b- defN 22-Mar-21 14:19 bdpy/dataform/datastore.py
--rw-r--r--  2.0 unx     5687 b- defN 22-Feb-01 08:32 bdpy/dataform/features.py
--rw-r--r--  2.0 unx      835 b- defN 22-Jan-14 04:50 bdpy/dataform/pd.py
--rw-r--r--  2.0 unx     3915 b- defN 22-Jan-14 04:50 bdpy/dataform/sparse.py
--rw-r--r--  2.0 unx       97 b- defN 22-Jan-14 04:50 bdpy/distcomp/__init__.py
--rw-r--r--  2.0 unx     4552 b- defN 22-Jan-14 04:50 bdpy/distcomp/distcomp.py
--rw-r--r--  2.0 unx        0 b- defN 22-Jan-14 04:50 bdpy/dl/__init__.py
--rw-r--r--  2.0 unx     4095 b- defN 22-Jan-14 04:50 bdpy/dl/caffe.py
--rw-r--r--  2.0 unx       50 b- defN 22-Jan-14 04:50 bdpy/dl/torch/__init__.py
--rw-r--r--  2.0 unx     9020 b- defN 22-Feb-28 14:40 bdpy/dl/torch/models.py
--rw-r--r--  2.0 unx     5602 b- defN 22-Mar-21 14:19 bdpy/dl/torch/torch.py
--rw-r--r--  2.0 unx        0 b- defN 22-Jan-14 04:50 bdpy/evals/__init__.py
--rw-r--r--  2.0 unx     3317 b- defN 22-Mar-21 14:19 bdpy/evals/metrics.py
--rw-r--r--  2.0 unx       74 b- defN 22-Jan-14 04:50 bdpy/feature/__init__.py
--rw-r--r--  2.0 unx     2436 b- defN 22-Jan-14 04:50 bdpy/feature/feature.py
--rw-r--r--  2.0 unx      200 b- defN 22-Feb-01 13:46 bdpy/fig/__init__.py
--rw-r--r--  2.0 unx     8007 b- defN 22-Apr-09 14:20 bdpy/fig/draw_group_image_set.py
--rw-r--r--  2.0 unx     4967 b- defN 22-Feb-01 08:32 bdpy/fig/fig.py
--rw-r--r--  2.0 unx    25224 b- defN 22-Apr-09 14:20 bdpy/fig/makeplots.py
--rw-r--r--  2.0 unx     6572 b- defN 22-Jan-14 04:50 bdpy/fig/tile_images.py
--rw-r--r--  2.0 unx      342 b- defN 22-Jan-14 04:50 bdpy/ml/__init__.py
--rw-r--r--  2.0 unx     3627 b- defN 22-Jan-14 04:50 bdpy/ml/crossvalidation.py
--rw-r--r--  2.0 unx      981 b- defN 22-Jan-14 04:50 bdpy/ml/ensemble.py
--rw-r--r--  2.0 unx    18935 b- defN 22-Jan-14 04:50 bdpy/ml/learning.py
--rw-r--r--  2.0 unx      695 b- defN 22-Jan-14 04:50 bdpy/ml/regress.py
--rw-r--r--  2.0 unx     1217 b- defN 22-Jan-14 04:50 bdpy/ml/searchlight.py
--rw-r--r--  2.0 unx      368 b- defN 22-Jan-14 04:50 bdpy/mri/__init__.py
--rw-r--r--  2.0 unx    37172 b- defN 22-Jan-14 04:50 bdpy/mri/fmriprep.py
--rw-r--r--  2.0 unx     3037 b- defN 22-Jan-14 04:50 bdpy/mri/glm.py
--rw-r--r--  2.0 unx     1516 b- defN 22-Jan-14 04:50 bdpy/mri/image.py
--rw-r--r--  2.0 unx     1890 b- defN 22-Jan-14 04:50 bdpy/mri/load_epi.py
--rw-r--r--  2.0 unx     1064 b- defN 22-Jan-14 04:50 bdpy/mri/load_mri.py
--rw-r--r--  2.0 unx    12800 b- defN 22-Feb-28 14:40 bdpy/mri/roi.py
--rw-r--r--  2.0 unx    11280 b- defN 22-Jan-14 04:50 bdpy/mri/spm.py
--rw-r--r--  2.0 unx       32 b- defN 22-Jan-14 04:50 bdpy/opendata/__init__.py
--rw-r--r--  2.0 unx    11226 b- defN 22-Jan-14 04:50 bdpy/opendata/openneuro.py
--rw-r--r--  2.0 unx      159 b- defN 22-Jan-14 04:50 bdpy/preproc/__init__.py
--rw-r--r--  2.0 unx     5050 b- defN 22-Jan-14 04:50 bdpy/preproc/interface.py
--rw-r--r--  2.0 unx     6217 b- defN 22-Jan-14 04:50 bdpy/preproc/preprocessor.py
--rw-r--r--  2.0 unx     1294 b- defN 22-Jan-14 04:50 bdpy/preproc/select_top.py
--rw-r--r--  2.0 unx      467 b- defN 22-Jan-14 04:50 bdpy/preproc/util.py
--rw-r--r--  2.0 unx        1 b- defN 22-Jan-14 04:50 bdpy/recon/__init__.py
--rw-r--r--  2.0 unx     5657 b- defN 22-Jan-14 04:50 bdpy/recon/utils.py
--rw-r--r--  2.0 unx       13 b- defN 22-Jan-14 04:50 bdpy/recon/torch/__init__.py
--rw-r--r--  2.0 unx    15319 b- defN 22-Feb-14 02:32 bdpy/recon/torch/icnn.py
--rw-r--r--  2.0 unx      229 b- defN 22-Jan-14 04:50 bdpy/stats/__init__.py
--rw-r--r--  2.0 unx     2548 b- defN 22-Jan-14 04:50 bdpy/stats/corr.py
--rw-r--r--  2.0 unx      147 b- defN 22-Jan-14 04:50 bdpy/util/__init__.py
--rw-r--r--  2.0 unx     2262 b- defN 22-Jan-14 04:50 bdpy/util/info.py
--rw-r--r--  2.0 unx      586 b- defN 22-Jan-14 04:50 bdpy/util/math.py
--rw-r--r--  2.0 unx     3375 b- defN 22-Jan-14 04:50 bdpy/util/utils.py
--rw-r--r--  2.0 unx     1074 b- defN 22-Apr-09 14:22 bdpy-0.18.dist-info/LICENSE
--rw-r--r--  2.0 unx     2769 b- defN 22-Apr-09 14:22 bdpy-0.18.dist-info/METADATA
--rw-r--r--  2.0 unx      110 b- defN 22-Apr-09 14:22 bdpy-0.18.dist-info/WHEEL
--rw-r--r--  2.0 unx        5 b- defN 22-Apr-09 14:22 bdpy-0.18.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     4893 b- defN 22-Apr-09 14:22 bdpy-0.18.dist-info/RECORD
-63 files, 292570 bytes uncompressed, 77891 bytes compressed:  73.4%
+Zip file size: 91299 bytes, number of entries: 63
+-rw-r--r--  2.0 unx      277 b- defN 23-Apr-13 10:55 bdpy/__init__.py
+-rw-r--r--  2.0 unx      142 b- defN 23-Apr-13 10:55 bdpy/bdata/__init__.py
+-rw-r--r--  2.0 unx    27829 b- defN 23-Apr-13 10:55 bdpy/bdata/bdata.py
+-rw-r--r--  2.0 unx     3142 b- defN 23-Apr-13 10:55 bdpy/bdata/featureselector.py
+-rw-r--r--  2.0 unx     3953 b- defN 23-Apr-13 10:55 bdpy/bdata/metadata.py
+-rw-r--r--  2.0 unx     8555 b- defN 23-Apr-13 10:55 bdpy/bdata/utils.py
+-rw-r--r--  2.0 unx      155 b- defN 23-Apr-13 10:55 bdpy/dataform/__init__.py
+-rw-r--r--  2.0 unx     7238 b- defN 23-Apr-13 10:55 bdpy/dataform/datastore.py
+-rw-r--r--  2.0 unx    15415 b- defN 23-Apr-13 10:55 bdpy/dataform/features.py
+-rw-r--r--  2.0 unx      835 b- defN 23-Apr-13 10:55 bdpy/dataform/pd.py
+-rw-r--r--  2.0 unx     3915 b- defN 23-Apr-13 10:55 bdpy/dataform/sparse.py
+-rw-r--r--  2.0 unx       97 b- defN 23-Apr-13 10:55 bdpy/distcomp/__init__.py
+-rw-r--r--  2.0 unx     4552 b- defN 23-Apr-13 10:55 bdpy/distcomp/distcomp.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Apr-13 10:55 bdpy/dl/__init__.py
+-rw-r--r--  2.0 unx     4095 b- defN 23-Apr-13 10:55 bdpy/dl/caffe.py
+-rw-r--r--  2.0 unx       50 b- defN 23-Apr-13 10:55 bdpy/dl/torch/__init__.py
+-rw-r--r--  2.0 unx    13557 b- defN 23-Apr-13 10:55 bdpy/dl/torch/models.py
+-rw-r--r--  2.0 unx     8112 b- defN 23-Apr-13 10:55 bdpy/dl/torch/torch.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Apr-13 10:55 bdpy/evals/__init__.py
+-rw-r--r--  2.0 unx     4016 b- defN 23-Apr-13 10:55 bdpy/evals/metrics.py
+-rw-r--r--  2.0 unx       74 b- defN 23-Apr-13 10:55 bdpy/feature/__init__.py
+-rw-r--r--  2.0 unx     2436 b- defN 23-Apr-13 10:55 bdpy/feature/feature.py
+-rw-r--r--  2.0 unx      200 b- defN 23-Apr-13 10:55 bdpy/fig/__init__.py
+-rw-r--r--  2.0 unx     8007 b- defN 23-Apr-13 10:55 bdpy/fig/draw_group_image_set.py
+-rw-r--r--  2.0 unx     4967 b- defN 23-Apr-13 10:55 bdpy/fig/fig.py
+-rw-r--r--  2.0 unx    23906 b- defN 23-Apr-13 10:55 bdpy/fig/makeplots.py
+-rw-r--r--  2.0 unx     6572 b- defN 23-Apr-13 10:55 bdpy/fig/tile_images.py
+-rw-r--r--  2.0 unx      366 b- defN 23-Apr-13 10:55 bdpy/ml/__init__.py
+-rw-r--r--  2.0 unx     5920 b- defN 23-Apr-13 10:55 bdpy/ml/crossvalidation.py
+-rw-r--r--  2.0 unx      981 b- defN 23-Apr-13 10:55 bdpy/ml/ensemble.py
+-rw-r--r--  2.0 unx    18935 b- defN 23-Apr-13 10:55 bdpy/ml/learning.py
+-rw-r--r--  2.0 unx      695 b- defN 23-Apr-13 10:55 bdpy/ml/regress.py
+-rw-r--r--  2.0 unx     1217 b- defN 23-Apr-13 10:55 bdpy/ml/searchlight.py
+-rw-r--r--  2.0 unx      405 b- defN 23-Apr-13 10:55 bdpy/mri/__init__.py
+-rw-r--r--  2.0 unx    35073 b- defN 23-Apr-13 10:55 bdpy/mri/fmriprep.py
+-rw-r--r--  2.0 unx     3037 b- defN 23-Apr-13 10:55 bdpy/mri/glm.py
+-rw-r--r--  2.0 unx     1516 b- defN 23-Apr-13 10:55 bdpy/mri/image.py
+-rw-r--r--  2.0 unx     1890 b- defN 23-Apr-13 10:55 bdpy/mri/load_epi.py
+-rw-r--r--  2.0 unx     1064 b- defN 23-Apr-13 10:55 bdpy/mri/load_mri.py
+-rw-r--r--  2.0 unx    18772 b- defN 23-Apr-13 10:55 bdpy/mri/roi.py
+-rw-r--r--  2.0 unx    11280 b- defN 23-Apr-13 10:55 bdpy/mri/spm.py
+-rw-r--r--  2.0 unx       32 b- defN 23-Apr-13 10:55 bdpy/opendata/__init__.py
+-rw-r--r--  2.0 unx    11226 b- defN 23-Apr-13 10:55 bdpy/opendata/openneuro.py
+-rw-r--r--  2.0 unx      159 b- defN 23-Apr-13 10:55 bdpy/preproc/__init__.py
+-rw-r--r--  2.0 unx     5050 b- defN 23-Apr-13 10:55 bdpy/preproc/interface.py
+-rw-r--r--  2.0 unx     6214 b- defN 23-Apr-13 10:55 bdpy/preproc/preprocessor.py
+-rw-r--r--  2.0 unx     1363 b- defN 23-Apr-13 10:55 bdpy/preproc/select_top.py
+-rw-r--r--  2.0 unx      467 b- defN 23-Apr-13 10:55 bdpy/preproc/util.py
+-rw-r--r--  2.0 unx        1 b- defN 23-Apr-13 10:55 bdpy/recon/__init__.py
+-rw-r--r--  2.0 unx     5657 b- defN 23-Apr-13 10:55 bdpy/recon/utils.py
+-rw-r--r--  2.0 unx       13 b- defN 23-Apr-13 10:55 bdpy/recon/torch/__init__.py
+-rw-r--r--  2.0 unx    15319 b- defN 23-Apr-13 10:55 bdpy/recon/torch/icnn.py
+-rw-r--r--  2.0 unx      229 b- defN 23-Apr-13 10:55 bdpy/stats/__init__.py
+-rw-r--r--  2.0 unx     2548 b- defN 23-Apr-13 10:55 bdpy/stats/corr.py
+-rw-r--r--  2.0 unx      147 b- defN 23-Apr-13 10:55 bdpy/util/__init__.py
+-rw-r--r--  2.0 unx     2289 b- defN 23-Apr-13 10:55 bdpy/util/info.py
+-rw-r--r--  2.0 unx      586 b- defN 23-Apr-13 10:55 bdpy/util/math.py
+-rw-r--r--  2.0 unx     3375 b- defN 23-Apr-13 10:55 bdpy/util/utils.py
+-rw-r--r--  2.0 unx     1074 b- defN 23-Apr-13 10:55 bdpy-0.19.dist-info/LICENSE
+-rw-r--r--  2.0 unx     2791 b- defN 23-Apr-13 10:55 bdpy-0.19.dist-info/METADATA
+-rw-r--r--  2.0 unx      110 b- defN 23-Apr-13 10:55 bdpy-0.19.dist-info/WHEEL
+-rw-r--r--  2.0 unx        5 b- defN 23-Apr-13 10:55 bdpy-0.19.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     4895 b- defN 23-Apr-13 10:55 bdpy-0.19.dist-info/RECORD
+63 files, 316798 bytes uncompressed, 83719 bytes compressed:  73.6%
```

## zipnote {}

```diff
@@ -168,23 +168,23 @@
 
 Filename: bdpy/util/math.py
 Comment: 
 
 Filename: bdpy/util/utils.py
 Comment: 
 
-Filename: bdpy-0.18.dist-info/LICENSE
+Filename: bdpy-0.19.dist-info/LICENSE
 Comment: 
 
-Filename: bdpy-0.18.dist-info/METADATA
+Filename: bdpy-0.19.dist-info/METADATA
 Comment: 
 
-Filename: bdpy-0.18.dist-info/WHEEL
+Filename: bdpy-0.19.dist-info/WHEEL
 Comment: 
 
-Filename: bdpy-0.18.dist-info/top_level.txt
+Filename: bdpy-0.19.dist-info/top_level.txt
 Comment: 
 
-Filename: bdpy-0.18.dist-info/RECORD
+Filename: bdpy-0.19.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## bdpy/bdata/bdata.py

```diff
@@ -857,24 +857,24 @@
         '''Load dataset and metadata from HDF5 file'''
 
         dat = h5py.File(load_filename, 'r')
 
         if 'metaData' in dat:
             md_keys = [self.__to_unicode(x) for x in dat["metaData"]['key'][:].tolist()]
             md_descs = [self.__to_unicode(x) for x in dat["metaData"]['description'][:].tolist()]
-            md_values = np.asarray(dat["metaData"]['value'], dtype=np.float)
+            md_values = np.asarray(dat["metaData"]['value'], dtype=float)
         else:
             md_keys = [self.__to_unicode(x) for x in dat["metadata"]['key'][:].tolist()]
             md_descs = [self.__to_unicode(x) for x in dat["metadata"]['description'][:].tolist()]
-            md_values = np.asarray(dat["metadata"]['value'], dtype=np.float)
+            md_values = np.asarray(dat["metadata"]['value'], dtype=float)
 
         if 'dataSet' in dat:
-            self.dataset = np.asarray(dat["dataSet"], dtype=np.float)
+            self.dataset = np.asarray(dat["dataSet"], dtype=float)
         else:
-            self.dataset = np.asarray(dat["dataset"], dtype=np.float)
+            self.dataset = np.asarray(dat["dataset"], dtype=float)
 
         if 'header' in dat:
             for k, v in dat['header'].items():
                 k = self.__to_unicode(k)
                 if isinstance(v[()], np.ndarray):
                     v = [self.__to_unicode(x) for x in v[()]]
                 else:
```

## bdpy/bdata/metadata.py

```diff
@@ -96,15 +96,15 @@
                 cols[:] = np.nan
 
                 self.__value = np.hstack([self.__value, cols])
 
             if updater is None:
                 self.__value[ind, :] = value
             else:
-                self.__value[ind, :] = np.array(updater(value, self.__value[ind, :]), dtype=np.float)
+                self.__value[ind, :] = np.array(updater(value, self.__value[ind, :]), dtype=float)
         else:
             # Add new metadata
             self.__key.append(key)
             self.__description.append(description)
 
             if value.shape[0] > self.get_value_len():
                 cols = np.empty((self.__value.shape[0], value.shape[0] - self.get_value_len()))
@@ -134,15 +134,15 @@
         """
         if key in self.__key:
             ind = self.__key.index(key)
         else:
             return None
 
         if field == 'value':
-            return self.__value[ind, :].astype(np.float)
+            return self.__value[ind, :].astype(float)
 
         if field == 'description':
             return self.__description[ind]
 
 
     def get_value_len(self):
         """Returns length of meta-data value"""
```

## bdpy/bdata/utils.py

```diff
@@ -111,14 +111,77 @@
         for s in successive:
             v = dat.select(s)
             suc_cols[s] = np.max(v)
 
     return dat
 
 
+def resolve_vmap(bdata_list):
+    """ Replace the conflicting vmaps for multiple bdata with non-conflicting vmaps.
+
+    Parameters
+    ----------
+    bdata_list : list of BData
+        Data to be concatenated
+        
+    Returns
+    -------        
+    bdata_list : list of BData 
+        The vmap is fixed to avoid a collision.
+    """
+    # Get the vmap key list.
+    vmap_keys = bdata_list[0].get_vmap_keys()
+    
+    # Check each vmap key.
+    for vmap_key in vmap_keys:
+        new_vmap = {}
+        # Check each bdata vmap.
+        for ds in bdata_list:
+            vmap = ds.get_vmap(vmap_key)
+            ds_values, selector = ds.select(vmap_key, return_index = True) # keep original dataset values
+            new_dsvalues = copy.deepcopy(ds_values)  # to update
+            
+            # Sanity check
+            if not vmap_key in ds.metadata.key:
+                raise ValueError('%s not found in metadata.' % vmap_key)
+            if type(vmap) is not dict:
+                raise TypeError('`vmap` should be a dictionary.')
+            for vk in vmap.keys():
+                if type(vk) is str:
+                    raise TypeError('Keys of `vmap` should be numerical.')
+            
+            # Check duplicate and create new vmap
+            for vk in vmap.keys():
+                if vk not in new_vmap.keys():
+                    # If find a novel key, add the new key and new value
+                    new_vmap[vk] = vmap[vk]
+                elif new_vmap[vk] != vmap[vk]:
+                    # If find the exisiting key and the values are different,
+                    # assign a new key by incrementing 1 to the maximum exisiting key.
+                    inflation_key_value = max(new_vmap.keys()) 
+                    new_vmap[inflation_key_value + 1] = vmap[vk]
+                    # Update dataset values
+                    new_dsvalues[ds_values == vk] = inflation_key_value + 1
+                else:
+                    # If the key and value is same, nothing to do.
+                    pass
+                
+            # Update dataset
+            ds.dataset[:, selector] = new_dsvalues
+
+        # Update each bdata vmap.
+        for ds in bdata_list:
+            vmap = ds.get_vmap(vmap_key)
+            if not np.array_equal(sorted(list(vmap.keys())), sorted(list(new_vmap.keys()))): 
+                # If the present vmap is different from new_vmap, update it.
+                ds._BData__vmap[vmap_key] = new_vmap # BDataクラスにvmapのsetterがあると良い
+        
+        return bdata_list   
+
+
 def concat_dataset(data_list, successive=[]):
     '''Concatenate datasets
 
     Currently, `concat_dataset` does not validate the consistency of meta-data
     among data.
 
     Parameters
```

### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

## bdpy/dataform/datastore.py

```diff
@@ -12,15 +12,15 @@
 
 import numpy as np
 import scipy.io as sio
 import h5py
 import hdf5storage
 
 
-__all__ = ['DataStore', 'DirStore', 'DecodedFeatures']
+__all__ = ['DataStore', 'DirStore']
 
 
 class DataStore(object):
     '''Data store class.
 
     Parameters
     ----------
@@ -259,29 +259,7 @@
         return dat
 
     def __load_feature(self, fpath):
         r = hdf5storage.loadmat(fpath)[self.__variable]
         if self.__squeeze:
             r = np.squeeze(r)
         return r
-
-
-##############################################################################
-# Interfaces
-##############################################################################
-
-class DecodedFeatures(DirStore):
-    def __init__(self, dpath, squeeze=False):
-        DirStore.__init__(self, dpath,
-                          dirs_pattern=['layer', 'subject', 'roi'],
-                          file_pattern='<image>.mat',
-                          variable='feat',
-                          squeeze=squeeze)
-
-    @property
-    def labels(self):
-        # FIXME: super dirty solution
-        labels = sorted(np.unique([
-            os.path.splitext(os.path.basename(f))[0]
-            for f in glob.glob(os.path.join(self._dpath, '*/*/*', '*.mat'))
-        ]))
-        return labels
```

## bdpy/dataform/features.py

```diff
@@ -4,19 +4,22 @@
 This file is a part of BdPy.
 '''
 
 
 from __future__ import print_function
 
 
-__all__ = ['Features']
+__all__ = ['Features', 'DecodedFeatures']
 
 
 import os
 import glob
+import sqlite3
+import pickle
+import warnings
 
 import numpy as np
 import scipy.io as sio
 import hdf5storage
 
 
 class Features(object):
@@ -57,14 +60,22 @@
         self.__feature_index = None   # Indexes of loaded features
 
         if self.__feat_index_table is not None:
             if not os.path.exists(self.__feat_index_table):
                 raise RuntimeError('%s do not exist' % self.__feat_index_table)
             self.__feat_index_table = hdf5storage.loadmat(self.__feat_index_table)['index']
 
+        self.__statistics = {}
+        for fdir in self.__dpath:
+            stat_file = os.path.join(fdir, 'statistics.pkl')
+            if os.path.exists(stat_file):
+                with open(stat_file, 'rb') as f:
+                    feat_stat = pickle.load(f)
+                self.__statistics.update(feat_stat)
+
     @property
     def labels(self):
         return self.__labels
 
     @property
     def index(self):
         return self.__index
@@ -73,29 +84,93 @@
     def layers(self):
         return self.__layers
 
     @property
     def feature_index(self):
         return self.__feature_index
 
-    def get(self, layer):
+    def get(self, layer=None, label=None):
         '''Return features in `layer`.
 
         Parameters
         ----------
         layer: str
             DNN layer
+        label: str or list
+            Sample label(s)
 
         Returns
         -------
         numpy.ndarray, shape=(n_samples, shape_layers)
             DNN features
         '''
 
-        return self.get_features(layer)
+        if layer is None:
+            raise ValueError('`layer` is required.')
+
+        if label is None:
+            return self.get_features(layer)
+
+        if isinstance(label, str):
+            labels = [label]
+        else:
+            labels = label
+
+        try:
+            features = np.vstack(
+                [sio.loadmat(self.__feature_file_table[layer][label])['feat']
+                 for label in labels]
+            )
+        except NotImplementedError:
+            features = np.vstack(
+                [hdf5storage.loadmat(self.__feature_file_table[layer][label])['feat']
+                 for label in labels]
+            )
+
+        if self.__feat_index_table is not None:
+            # Select features by index
+            self.__feature_index = self.__feat_index_table[layer]
+            n_sample = features.shape[0]
+            n_feat = np.array(features.shape[1:]).prod()
+
+            features = features.reshape([n_sample, n_feat], order='C')[:, self.__feature_index]
+
+        return features
+
+    def statistic(self, statistic='mean', layer=None):
+
+        if statistic == 'std':
+            statistic = 'std, ddof=1'
+
+        k = (statistic, layer)
+        if k in self.__statistics:
+            s = self.__statistics[k]
+        else:
+            f = self.get(layer)
+
+            if statistic == 'mean':
+                s = np.mean(f, axis=0)[np.newaxis, :]
+            elif statistic == 'std, ddof=1':
+                s = np.std(f, axis=0, ddof=1)[np.newaxis, :]
+            elif statistic == 'std, ddof=0':
+                s = np.std(f, axis=0, ddof=0)[np.newaxis, :]
+            else:
+                raise ValueError('Unknown statistics: {}'.format(statistic))
+
+            self.__statistics.update({k: s})
+
+        if self.__feat_index_table is not None:
+            # Select features by index
+            self.__feature_index = self.__feat_index_table[layer]
+            n_sample = self.__features.shape[0]
+            n_feat = np.array(self.__features.shape[1:]).prod()
+
+            s = s.reshape([n_sample, n_feat], order='C')[:, self.__feature_index]
+
+        return s
 
     def get_features(self, layer):
         '''Return features in `layer`.
 
         Parameters
         ----------
         layer: str
@@ -188,7 +263,228 @@
             labels_t = sorted([os.path.splitext(os.path.basename(f))[0] for f in files])
             if not labels:
                 labels = labels_t
             else:
                 if labels != labels_t:
                     raise RuntimeError('Invalid feature file in %s ' % dpath)
         return labels
+
+
+class DecodedFeatures(object):
+    '''Decoded features class.
+
+    Parameters
+    ----------
+    path: str
+       Path to the decoded feature directory
+    '''
+
+    def __init__(self, path=None, keys=None, file_ext='mat', file_key='feat', squeeze=False):
+
+        self.__path = path          # Path to decoded feature directory
+        self.__keys = keys          # Keys
+        self.__file_ext = file_ext  # Decoded feature file extension
+        self.__file_key = file_key  # Decoded feature data key (FIXME)
+        self.__squeeze = squeeze    # Whether squeeze the output array or not
+
+        if self.__path is not None:
+            self.__db = self.__parse_dir(self.__path, self.__keys)
+        else:
+            self.__db = self.__init_db(self.__keys)
+
+        stat_file = os.path.join(self.__path, 'statistics.pkl')
+
+        if os.path.exists(stat_file):
+            with open(stat_file, 'rb') as f:
+                self.__statistics = pickle.load(f)
+        else:
+            self.__statistics = {}
+
+    @property
+    def layers(self):
+        return self.__db.get_available_values('layer')
+
+    @property
+    def subjects(self):
+        return self.__db.get_available_values('subject')
+
+    @property
+    def rois(self):
+        return self.__db.get_available_values('roi')
+
+    @property
+    def folds(self):
+        return self.__db.get_available_values('fold')
+
+    @property
+    def labels(self):
+        return self.__db.get_available_values('label')
+
+    @property
+    def selected_layer(self):
+        return self.__db.get_selected_values('layer')
+
+    @property
+    def selected_subject(self):
+        return self.__db.get_selected_values('subject')
+
+    @property
+    def selected_roi(self):
+        return self.__db.get_selected_values('roi')
+
+    @property
+    def selected_fold(self):
+        return self.__db.get_selected_values('fold')
+
+    @property
+    def selected_label(self):
+        return self.__db.get_selected_values('label')
+
+    def get(self, layer=None, subject=None, roi=None, fold=None, label=None, image=None):
+        '''Returns decoded features as an array.'''
+
+        if image is not None:
+            if label is None:
+                warnings.warn('`image` will be deprecated.')
+                label = image
+            else:
+                warnings.warn('`image` will be deprecated and overwritten by `label`.')
+
+        files = self.__db.get_file(
+            layer=layer,
+            subject=subject,
+            roi=roi,
+            fold=fold,
+            label=label
+        )
+
+        if len(files) == 0:
+            raise RuntimeError('No decoded feature found')
+
+        y = np.vstack(
+            [hdf5storage.loadmat(f)[self.__file_key] for f in files]
+        )
+
+        if self.__squeeze:
+            y = np.squeeze(y)
+
+        return y
+
+    def statistic(self, statistic='mean', layer=None, subject=None, roi=None, fold=None):
+
+        if statistic == 'std':
+            statistic = 'std, ddof=1'
+
+        k = (statistic, layer, subject, roi, fold)
+        if k in self.__statistics:
+            s = self.__statistics[k]
+        else:
+            f = self.get(layer=layer, subject=subject, roi=roi, fold=fold)
+
+            if statistic == 'mean':
+                s = np.mean(f, axis=0)[np.newaxis, :]
+            elif statistic == 'std, ddof=1':
+                s = np.std(f, axis=0, ddof=1)[np.newaxis, :]
+            elif statistic == 'std, ddof=0':
+                s = np.std(f, axis=0, ddof=0)[np.newaxis, :]
+            else:
+                raise ValueError('Unknown statistics: {}'.format(statistic))
+
+            self.__statistics.update({k: s})
+
+        return s
+
+    def __parse_dir(self, path, keys):
+        # TODO: refactoring
+        if keys is None:
+            files = glob.glob(os.path.join(path, '*', '*', '*', '*', 'decoded_features', '*.' + self.__file_ext))
+            keys = ['layer', 'subject', 'roi', 'fold', 'label']
+            if len(files) == 0:
+                files = glob.glob(os.path.join(path, '*', '*', '*', 'decoded_features', '*.' + self.__file_ext))
+                keys = ['layer', 'subject', 'roi', 'label']
+            if len(files) == 0:
+                files = glob.glob(os.path.join(path, '*', '*', '*', '*.' + self.__file_ext))
+                keys = ['layer', 'subject', 'roi', 'label']
+        elif len(keys) == 4:
+            # <layer>/<subject>/<roi>/<label>
+            files = glob.glob(os.path.join(path, '*', '*', '*', 'decoded_features', '*.' + self.__file_ext))
+            if len(files) == 0:
+                files = glob.glob(os.path.join(path, '*', '*', '*', '*.' + self.__file_ext))
+            if len(files) == 0:
+                raise RuntimeError('Decoded features not found')
+        elif len(keys) == 5:
+            # <layer>/<subject>/<roi>/<fold>/<label>
+            files = glob.glob(os.path.join(path, '*', '*', '*', '*', 'decoded_features', '*.' + self.__file_ext))
+        else:
+            raise ValueError('Invalid keys')
+
+        if len(files) == 0:
+            raise RuntimeError('Decoded features not found')
+
+        print('Found {} decoded features in {}'.format(len(files), self.__path))
+
+        self.__keys = keys
+
+        db = FileDatabase(keys)
+
+        # TODO: performance improvement
+        for file in files:
+            # FXIME: "decoded_features"
+            k = {
+                k: os.path.splitext(file.replace('/decoded_features/', '/'))[0].split('/')[i - len(keys)]
+                for i, k in enumerate(keys)
+            }
+            db.add_file(file, **k)
+
+        return db
+
+    def __init_db(self, keys):
+        raise NotImplementedError
+
+
+class FileDatabase(object):
+    def __init__(self, keys):
+        self.__keys = keys
+
+        self.__res = None
+
+        self.__con = sqlite3.connect(':memory:')
+        self.__cursor = self.__con.cursor()
+
+        self.__cursor.execute(
+            '''
+            CREATE TABLE files (
+            {},
+            path TEXT,
+            UNIQUE ({})
+            )
+            '''.format(
+                (', ').join([s + ' TEXT' for s in keys]),
+                (', ').join(keys)
+            )
+        )
+
+    def add_file(self, path, **kargs):
+        key_list = ', '.join(kargs) + ', path'
+        val_list = ', '.join(['"{}"'.format(s) for s in kargs.values()]) + ', "{}"'.format(path)
+        self.__cursor.execute('INSERT INTO files({}) VALUES ({})'.format(key_list, val_list))
+
+    def get_file(self, **kargs):
+        where = ' AND '.join(['{} = "{}"'.format(k, v) for k, v in kargs.items() if k in self.__keys and v is not None])
+        self.__cursor.execute('SELECT * FROM files WHERE {}'.format(where))
+        self.__res = self.__cursor.fetchall()
+        return [a[-1] for a in self.__res]
+
+    def get_available_values(self, key):
+        if not key in self.__keys:
+            return None
+        self.__cursor.execute('SELECT DISTINCT {} FROM files'.format(key))
+        return [a[0] for a in self.__cursor.fetchall()]
+
+    def get_selected_values(self, key):
+        if not key in self.__keys:
+            return None
+        return [a[self.__keys.index(key)] for a in self.__res]
+
+    def show(self):
+        self.__cursor.execute('SELECT * FROM files')
+        print(self.__cursor.fetchall())
```

## bdpy/dl/torch/models.py

```diff
@@ -1,12 +1,30 @@
+from typing import Dict, Union, Optional, Sequence
+
+import re
+
 import torch
 import torch.nn as nn
 
 
-def layer_map(net):
+def layer_map(net: str) -> Dict[str, str]:
+    '''Get layer map for a given network.
+
+    Parameters
+    ----------
+    net : str
+        Network name. Currently, 'vgg19' and 'alexnet' are supported.
+
+    Returns
+    -------
+    Dict[str, str]
+        Layer map. Keys are human-readable layer names, and values are
+        corresponding layer names in the network.
+    '''
+
     maps = {
         'vgg19': {
             'conv1_1': 'features[0]',
             'conv1_2': 'features[2]',
             'conv2_1': 'features[5]',
             'conv2_2': 'features[7]',
             'conv3_1': 'features[10]',
@@ -35,19 +53,87 @@
             'conv4': 'features[10]',
             'conv5': 'features[12]',
             'fc6':   'classifier[0]',
             'relu6': 'classifier[1]',
             'fc7':   'classifier[2]',
             'relu7': 'classifier[3]',
             'fc8':   'classifier[4]',
+        },
+        
+        'reference_net': {
+            'conv1': 'features[0]',
+            'relu1': 'features[1]',
+            'conv2': 'features[4]',
+            'relu2': 'features[5]',
+            'conv3': 'features[8]',
+            'relu3': 'features[9]',
+            'conv4': 'features[10]',
+            'relu4': 'features[11]',
+            'conv5': 'features[12]',
+            'relu5': 'features[13]',
+            'fc6':   'classifier[1]',
+            'relu6': 'classifier[2]',
+            'fc7':   'classifier[4]',
+            'relu7': 'classifier[5]',
+            'fc8':   'classifier[6]',
         }
     }
     return maps[net]
 
 
+def _parse_layer_name(model: nn.Module, layer_name: str) -> nn.Module:
+    '''Parse layer name and return the corresponding layer object.
+
+    Parameters
+    ----------
+    model : nn.Module
+        Network model.
+    layer_name : str
+        Layer name. It accepts the following formats: 'layer_name',
+        'layer_name[index]', 'parent_name.child_name', and combinations of them.
+
+    Returns
+    -------
+    nn.Module
+        Layer object.
+
+    Examples
+    --------
+    >>> model = nn.Module()
+    >>> model.layer1 = nn.Linear(10, 10)
+    >>> model.layers = nn.Sequential(nn.Conv2d(3, 3, 3), nn.Conv2d(3, 3, 3))
+    >>> _parse_layer_name(model, 'layer1')
+    Linear(in_features=10, out_features=10, bias=True)
+    >>> _parse_layer_name(model, 'layers[0]')
+    Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))
+    '''
+
+    if hasattr(model, layer_name):
+        return getattr(model, layer_name)
+
+    # parse layer name having parent name (e.g., 'features.conv1')
+    if '.' in layer_name:
+        top_most_layer_name, child_layer_name = layer_name.split('.', 1)
+        model = _parse_layer_name(model, top_most_layer_name)
+        return _parse_layer_name(model, child_layer_name)
+
+    # parse layer name having index (e.g., 'features[0]')
+    pattern = re.compile(r'^(?P<layer_name>\w+)\[(?P<index>\d+)\]$')
+    m = pattern.match(layer_name)
+    if m is not None:
+        layer_name = m.group('layer_name')
+        index = int(m.group('index'))
+        if hasattr(model, layer_name):
+            return getattr(model, layer_name)[index]
+
+    raise ValueError(
+        f"Invalid layer name: '{layer_name}'. Either the syntax of '{layer_name}' is not supported, "
+        f"or {type(model).__name__} object has no attribute '{layer_name}'.")
+
+
 class VGG19(nn.Module):
     def __init__(self):
 
         super(VGG19, self).__init__()
 
         self.features = nn.Sequential(
             nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),
@@ -95,15 +181,15 @@
             nn.Dropout(p=0.5, inplace=False),
             nn.Linear(in_features=4096, out_features=4096, bias=True),
             nn.ReLU(inplace=False),
             nn.Dropout(p=0.5, inplace=False),
             nn.Linear(in_features=4096, out_features=1000, bias=True),
         )
 
-    def forward(self, x):
+    def forward(self, x: torch.Tensor) -> torch.Tensor:
         x = self.features(x)
         x = self.avgpool(x)
         x = torch.flatten(x, 1)
         x = self.classifier(x)
         return x
 
 
@@ -122,36 +208,77 @@
             nn.LocalResponseNorm(5, alpha=0.0001, beta=0.75),
             nn.MaxPool2d(kernel_size=3, stride=2),
             nn.Conv2d(256, 384, kernel_size=3, padding=1),
             nn.ReLU(inplace=False),
             nn.Conv2d(384, 384, kernel_size=3, padding=1, groups=2),
             nn.ReLU(inplace=False),
             nn.Conv2d(384, 256, kernel_size=3, padding=1, groups=2),
-            nn.ReLU(inplace=False), 
+            nn.ReLU(inplace=False),
             nn.MaxPool2d(kernel_size=3, stride=2),
         )
         self.avgpool = nn.AdaptiveAvgPool2d((6, 6))
         self.classifier = nn.Sequential(
             nn.Linear(256 * 6 * 6, 4096),
             nn.ReLU(inplace=False),
             nn.Linear(4096, 4096),
             nn.ReLU(inplace=False),
-            nn.Linear(4096, num_classes), 
+            nn.Linear(4096, num_classes),
         )
 
     def forward(self, x: torch.Tensor) -> torch.Tensor:
         x = self.features(x)
         x = self.avgpool(x)
         x = torch.flatten(x, 1)
         x = self.classifier(x)
         return x
+    
+class ReferenceNet(nn.Module):
+    def __init__(self, num_classes: int = 1000) -> None:
+            super(ReferenceNet, self).__init__()
+            self.features = nn.Sequential(
+                nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=0),
+                nn.ReLU(inplace=False),
+                nn.MaxPool2d(kernel_size=3, stride=2),
+                nn.LocalResponseNorm(5, alpha=0.0001, beta=0.75),
+                nn.Conv2d(96, 256, kernel_size=5, padding=2, groups=2),
+                nn.ReLU(inplace=False),
+                nn.MaxPool2d(kernel_size=3, stride=2),
+                nn.LocalResponseNorm(5, alpha=0.0001, beta=0.75),
+                nn.Conv2d(256, 384, kernel_size=3, padding=1),
+                nn.ReLU(inplace=False),
+                nn.Conv2d(384, 384, kernel_size=3, padding=1, groups=2),
+                nn.ReLU(inplace=False),
+                nn.Conv2d(384, 256, kernel_size=3, padding=1, groups=2),
+                nn.ReLU(inplace=False),
+                nn.MaxPool2d(kernel_size=3, stride=2),
+            )
+            self.avgpool = nn.AdaptiveAvgPool2d((6, 6))
+            self.classifier = nn.Sequential(
+                nn.Dropout(),
+                nn.Linear(256 * 6 * 6, 4096),
+                nn.ReLU(inplace=False),
+                nn.Dropout(),
+                nn.Linear(4096, 4096),
+                nn.ReLU(inplace=False),
+                nn.Linear(4096, num_classes),
+            )
+    
+    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        x = self.features(x)
+        x = self.avgpool(x)
+        x = torch.flatten(x, 1)
+        x = self.classifier(x)
+        return x
+
 
 class AlexNetGenerator(nn.Module):
 
-    def __init__(self, input_size=4096, n_out_channel=3, device=None):
+    def __init__(
+            self, input_size: int = 4096, n_out_channel: int = 3,
+            device: Optional[Union[str, Sequence[str]]] = None):
 
         super(AlexNetGenerator, self).__init__()
 
         if device is None:
             self.__device0 = 'cpu'
             self.__device1 = 'cpu'
         elif isinstance(device, str):
@@ -221,14 +348,14 @@
             self.deconv2,
             self.relu_deconv2,
             self.deconv1,
             self.relu_deconv1,
             self.deconv0,
         ).to(self.__device1)
 
-    def forward(self, z):
+    def forward(self, z: torch.Tensor) -> torch.Tensor:
 
         f = self.defc(z)
         f = f.view(-1, 256, 4, 4)
         g = self.deconv(f)
 
-        return g
+        return g
```

## bdpy/dl/torch/torch.py

```diff
@@ -1,96 +1,171 @@
 '''PyTorch module.'''
 
+from typing import Iterable, List, Dict, Union, Tuple, Any, Callable, Optional
 
 import os
 
 import numpy as np
 from PIL import Image
 import torch
+import torch.nn as nn
+
+from . import models
+
+_tensor_t = Union[np.ndarray, torch.Tensor]
 
 
 class FeatureExtractor(object):
-    def __init__(self, encoder, layers=None, layer_mapping=None, device='cpu', detach=True):
+    def __init__(
+            self, encoder: nn.Module, layers: Iterable[str],
+            layer_mapping: Optional[Dict[str, str]] = None,
+            device: str = 'cpu', detach: bool = True
+    ):
+        '''Feature extractor.
+
+        Parameters
+        ----------
+        encoder : torch.nn.Module
+            Network model we want to extract features from.
+        layers : Iterable[str]
+            List of layer names we want to extract features from.
+        layer_mapping : Dict[str, str], optional
+            Mapping from (human-readable) layer names to layer names in the model.
+            If None, layers will be directly used as layer names in the model.
+        device : str, optional
+            Device name (default: 'cpu').
+        detach : bool, optional
+            If True, detach the feature activations from the computation graph
+        '''
+
         self._encoder = encoder
         self.__layers = layers
         self.__layer_map = layer_mapping
         self.__detach = detach
         self.__device = device
 
-        self._extractor = FeatureExtractorHandle()
+        if detach:
+            self._extractor = FeatureExtractorHandleDetach()
+        else:
+            self._extractor = FeatureExtractorHandle()
 
         self._encoder.to(self.__device)
 
         for layer in self.__layers:
             if self.__layer_map is not None:
                 layer = self.__layer_map[layer]
-            eval('self._encoder.{}.register_forward_hook(self._extractor)'.format(layer))
+            layer_object = models._parse_layer_name(self._encoder, layer)
+            layer_object.register_forward_hook(self._extractor)
 
-    def __call__(self, x) -> dict:
+    def __call__(self, x: _tensor_t) -> Dict[str, _tensor_t]:
         return self.run(x)
 
-    def run(self, x) -> dict:
+    def run(self, x: _tensor_t) -> Dict[str, _tensor_t]:
+        '''Extract feature activations from the specified layers.
+
+        Parameters
+        ----------
+        x : numpy.ndarray or torch.Tensor
+            Input image (numpy.ndarray or torch.Tensor).
+
+        Returns
+        -------
+        features : Dict[str, Union[numpy.ndarray, torch.Tensor]]
+            Feature activations from the specified layers.
+            Each key is the layer name and each value is the feature activation.
+        '''
+
         self._extractor.clear()
         if not isinstance(x, torch.Tensor):
             xt = torch.tensor(x[np.newaxis], device=self.__device)
         else:
             xt = x
 
         self._encoder.forward(xt)
 
-        features = {
+        features: Dict[str, _tensor_t] = {
             layer: self._extractor.outputs[i]
             for i, layer in enumerate(self.__layers)
         }
         if self.__detach:
             features = {
                 k: v.cpu().detach().numpy()
                 for k, v in features.items()
             }
 
         return features
 
 
 class FeatureExtractorHandle(object):
     def __init__(self):
-        self.outputs = []
+        self.outputs: List[torch.Tensor] = []
 
-    def __call__(self, module, module_in, module_out):
+    def __call__(
+            self,
+            module: nn.Module, module_in: Any,
+            module_out: torch.Tensor
+    ) -> None:
         self.outputs.append(module_out)
 
     def clear(self):
         self.outputs = []
 
 
+class FeatureExtractorHandleDetach(object):
+    def __init__(self):
+        self.outputs: List[torch.Tensor] = []
+
+    def __call__(
+            self,
+            module: nn.Module, module_in: Any,
+            module_out: torch.Tensor
+    ) -> None:
+        self.outputs.append(module_out.detach().clone())
+
+    def clear(self):
+        self.outputs = []
+
+
 class ImageDataset(torch.utils.data.Dataset):
     '''Pytoch dataset for images.'''
 
-    def __init__(self, images, labels=None, label_dirname=False, resize=None, shape='chw', transform=None, scale=1, rgb_mean=None, preload=False, preload_limit=np.inf):
+    def __init__(
+            self, images: List[str],
+            labels: Optional[List[str]] = None,
+            label_dirname: bool = False,
+            resize: Optional[Tuple[int, int]] = None,
+            shape: str = 'chw',
+            transform: Optional[Callable[[_tensor_t], torch.Tensor]] = None,
+            scale: float = 1,
+            rgb_mean: Optional[List[float]] = None,
+            preload: bool = False,
+            preload_limit: float = np.inf
+    ):
         '''
         Parameters
         ----------
-        images : list
+        images : List[str]
             List of image file paths.
-        labels : list, optional
+        labels : List[str], optional
             List of image labels (default: image file names).
         label_dirname : bool, optional
             Use directory names as labels if True (default: False).
         resize : None or tuple, optional
             If not None, images will be resized by the specified size.
         shape : str ({'chw', 'hwc', ...}), optional
             Specify array shape (channel, hieght, and width).
-        transform : optional
+        transform : Callable[[Union[np.ndarray, torch.Tensor]], torch.Tensor], optional
             Transformers (applied after resizing, reshaping, ans scaling to [0, 1])
         scale : optional
             Image intensity is scaled to [0, scale] (default: 1).
         rgb_mean : list([r, g, b]), optional
             Image values are centered by the specified mean (after scaling) (default: None).
         preload : bool, optional
             Pre-load images (default: False).
-        preload_limit : int
+        preload_limit : float
             Memory size limit of preloading in GiB (default: unlimited).
 
         Note
         ----
         - Images are converted to RGB. Alpha channels in RGBA images are ignored.
         '''
 
@@ -122,33 +197,33 @@
         self.data_path = images
         if not labels is None:
             self.labels = labels
         else:
             self.labels = image_labels
         self.n_sample = len(images)
 
-    def __len__(self):
+    def __len__(self) -> int:
         return self.n_sample
 
-    def __getitem__(self, idx):
+    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, str]:
         if idx in self.__data:
             data = self.__data[idx]
         else:
             data = self.__load_image(self.data_path[idx])
 
-        if not self.transform is None:
+        if self.transform is not None:
             data = self.transform(data)
         else:
             data = torch.Tensor(data)
 
         label = self.labels[idx]
 
         return data, label
 
-    def __load_image(self, fpath):
+    def __load_image(self, fpath: str) -> np.ndarray:
         img = Image.open(fpath)
 
         # CMYK, RGBA --> RGB
         if img.mode == 'CMYK':
             img = img.convert('RGB')
         if img.mode == 'RGBA':
             bg = Image.new('RGB', img.size, (255, 255, 255))
```

## bdpy/evals/metrics.py

```diff
@@ -69,42 +69,57 @@
             for i in range(n_sample)
         ]
     )
 
     return r
 
 
-def pairwise_identification(pred, true, metric='correlation', remove_nan=True, remove_nan_dist=True):
+def pairwise_identification(pred, true, metric='correlation', remove_nan=True, remove_nan_dist=True, single_trial=False, pred_labels=None, true_labels=None):
     '''Pair-wise identification.'''
 
     p = pred.reshape(pred.shape[0], -1)
     t = true.reshape(true.shape[0], -1)
 
     if remove_nan:
         # Remove nan columns based on the decoded features
         nan_cols = np.isnan(p).any(axis=0) | np.isnan(t).any(axis=0)
         if nan_cols.any():
             warnings.warn('NaN column removed ({})'.format(np.sum(nan_cols)))
         p = p[:, ~nan_cols]
         t = t[:, ~nan_cols]
 
-    d = 1 - cdist(p, t, metric=metric)
-
-    if remove_nan_dist:
+    if single_trial:
         cr = []
-        for d_ind in range(d.shape[0]):
-            pef = d[d_ind, :] - d[d_ind, d_ind]
-            if np.isnan(pef).any():
-                warnings.warn('NaN value detected in the distance matrix ({}).'.format(np.sum(np.isnan(pef))))
-                pef = pef[~np.isnan(pef)] # Remove nan value from the comparison for identification
-            pef = np.sum(pef < 0) / (len(pef) - 1)
-            cr.append(pef)
+        for i in range(p.shape[0]):
+            d = 1 - cdist(p[i][np.newaxis], t, metric=metric)
+            # label の情報
+            ind = np.where(np.array(true_labels) == pred_labels[i])[0][0]
+
+            s = (d - d[0, ind]).flatten()
+            if remove_nan_dist and np.isnan(s).any():
+                warnings.warn('NaN value detected in the distance matrix ({}).'.format(np.sum(np.isnan(s))))
+                s = s[~np.isnan(s)]
+            ac = np.sum(s < 0) / (len(s) - 1)
+            cr.append(ac)
         cr = np.asarray(cr)
     else:
-        cr = np.sum(d - np.diag(d)[:, np.newaxis] < 0, axis=1) / (d.shape[1] - 1)
+        d = 1 - cdist(p, t, metric=metric)
+
+        if remove_nan_dist:
+            cr = []
+            for d_ind in range(d.shape[0]):
+                pef = d[d_ind, :] - d[d_ind, d_ind]
+                if np.isnan(pef).any():
+                    warnings.warn('NaN value detected in the distance matrix ({}).'.format(np.sum(np.isnan(pef))))
+                    pef = pef[~np.isnan(pef)] # Remove nan value from the comparison for identification
+                pef = np.sum(pef < 0) / (len(pef) - 1)
+                cr.append(pef)
+            cr = np.asarray(cr)
+        else:
+            cr = np.sum(d - np.diag(d)[:, np.newaxis] < 0, axis=1) / (d.shape[1] - 1)
 
     return cr
 
 
 def remove_nan_value(array, nan_flag=None, return_nan_flag=False):
     '''Helper function:
     Remove columns (units) which contain nan values
```

### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

## bdpy/fig/makeplots.py

```diff
@@ -32,15 +32,14 @@
         box_color='blue', box_width=0.5, box_linewidth=1,
         box_meanprops=dict(linestyle='-', linewidth=1.5, color='red'),
         box_medianprops={},
         removenan=True,
         verbose=False, colors=None, reverse_x=False
 ):
     '''Make plots.
-
     Parameters
     ----------
     df : pandas.core.frame.DataFrame
     x : str
     y : str
     x_list : list
     subplot : str
@@ -53,49 +52,48 @@
     y_lim : (y_min, y_max)
     y_ticks : array_like
     title, x_label, y_label : str
     fontsize : int
     tick_fontsize : int
     style : str
     verbose : bool
-
     Returns
     -------
     fig : matplotlib.figure.Figure or list of matplotlib.figure.Figure
     '''
 
     x_keys       = sorted(df[x].unique())
-    subplot_keys = sorted(df[subplot].unique())
+    subplot_keys = sorted(df[subplot].unique()) if subplot is not None else [None]
     figure_keys  = sorted(df[figure].unique()) if figure is not None else [None]
     group_keys   = sorted(df[group].unique()) if group is not None else [None]
 
     x_list       = x_keys       if x_list       is None else x_list
     subplot_list = subplot_keys if subplot_list is None else subplot_list
     figure_list  = figure_keys  if figure_list  is None else figure_list
     group_list   = group_keys   if group_list   is None else group_list
 
     if reverse_x:
         x_list = x_list[::-1]
-        group_list = group_list[::-1]
 
     grouping = group is not None
 
     if plot_type == 'paired violin':
         if not grouping:
             RuntimeError('plot type "paired violin" can be used only when grouping is enabled')
         comparison_pairs = list(__split_list(group_list, 2))
 
     if grouping:
         warnings.warn('"grouping mode" is still experimental and will not work correctly yet!')
 
     if verbose:
         print('X:       {}'.format(x_list))
-        print('Subplot: {}'.format(subplot_list))
         if grouping:
             print('Group by: {} ({})'.format(group_keys, group_list))
+        if subplot is not None:
+            print('Subplot: {}'.format(subplot_list))
         if figure is not None:
             print('Figures: {}'.format(figure_list))
 
     col_num = np.ceil(np.sqrt(len(subplot_list)))
     row_num = int(np.ceil(len(subplot_list) / col_num))
     col_num = int(col_num)
 
@@ -159,25 +157,39 @@
             row = i - col * row_num
             sbpos = (row_num - row - 1) * col_num + col + 1
 
             # Get data
             if plot_type == 'paired violin':
                 group_list = figure_instance['comparison pair']
 
-            data = __get_data(df, subplot, sp_label,
-                            x, x_list, figure, fig_label, y,
-                            group, group_list, grouping, removenan)
-
-            if not isinstance(sp_label, list):
-                if grouping:
-                    data_mean = [[np.nanmean(d) for d in data_t] for data_t in data]
-                else:
-                    data_mean = [np.nanmean(d) for d in data]
+            if plot_type == "swarm+box":
+                df_t = __strict_data(
+                    df, subplot, sp_label,
+                    figure, fig_label, y, removenan
+                )
+                weird_keys = []
+                for key_candidate in [figure, subplot, group, x]:
+                    if key_candidate is not None:
+                        weird_keys.append(key_candidate)
+                df_t = __weird_form_to_long(df_t, y, identify_cols = weird_keys)
+
             else:
-                data_mean = None
+                data = __get_data(
+                    df, subplot, sp_label,
+                    x, x_list, figure, fig_label, y,
+                    group, group_list, grouping, removenan
+                )
+
+                if not isinstance(sp_label, list):
+                    if grouping:
+                        data_mean = [[np.nanmean(d) for d in data_t] for data_t in data]
+                    else:
+                        data_mean = [np.nanmean(d) for d in data]
+                else:
+                    data_mean = None
 
             # Plot
             ax = plt.subplot(row_num, col_num, sbpos)
 
             if not style == 'ggplot':
                 if horizontal:
                     ax.grid(axis='x', color='k', linestyle='-', linewidth=0.5)
@@ -213,38 +225,34 @@
                     horizontal=horizontal, grouping=grouping,
                     dot_color=swarm_dot_color,
                     dot_size=swarm_dot_size,
                     dot_alpha=swarm_dot_alpha,
                     violin_color=swarm_violin_color,
                 )
             elif plot_type == 'swarm+box':
-                group_label_list = __plot_swarmbox(
-                    ax, x_list, data,
+                legend_handler = __plot_swarmbox(
+                    ax, x, y, x_list, df_t,
                     horizontal=horizontal, reverse_x=reverse_x,
-                    grouping=grouping, group_list=group_list, 
+                    grouping=grouping, group=group, group_list=group_list,
                     dot_color=swarm_dot_color,
                     dot_size=swarm_dot_size,
                     dot_alpha=swarm_dot_alpha,
-                    box_color=box_color, box_width=box_width, box_linewidth=box_linewidth,
+                    box_color=box_color, box_width=box_width,
+                    box_linewidth=box_linewidth,
                     box_meanprops=box_meanprops,
                     box_medianprops=box_medianprops
                 )
             else:
                 raise ValueError('Unknown plot_type: {}'.format(plot_type))
 
             if not horizontal:
                 # Vertical plot
-                if grouping and plot_type == 'swarm+box': # swarm+boxのgroupingは擬似的なgroupingになっているためxticksの修正が必要
-                    ax.set_xlim([ -1, len(x_list) * len(group_list) ])
-                    new_x_list = np.arange(len(x_list)) * len(group_list) + len(group_list) / 2. - 0.5
-                    ax.set_xticks(new_x_list)
-                else:
-                    ax.set_xlim([-1, len(x_list)])
-                    ax.set_xticks(range(len(x_list)))
-                    
+                ax.set_xlim([-1, len(x_list)])
+                ax.set_xticks(range(len(x_list)))
+
                 if row == 0:
                     ax.set_xticklabels(x_list, rotation=-45, ha='left', fontsize=tick_fontsize)
                 else:
                     ax.set_xticklabels([])
 
                 if y_lim is None:
                     pass
@@ -256,22 +264,17 @@
 
                 ax.tick_params(axis='y', labelsize=tick_fontsize, grid_color='gray', grid_linestyle='--', grid_linewidth=0.8)
 
                 if chance_level is not None:
                     plt.hlines(chance_level, xmin=plt.gca().get_xlim()[0], xmax=plt.gca().get_xlim()[1], **chance_level_style)
             else:
                 # Horizontal plot
-                if grouping and plot_type == 'swarm+box': # swarm+boxのgroupingは擬似的なgroupingになっているためyticksの修正が必要
-                    ax.set_ylim([ -1, len(x_list) * len(group_list) ])
-                    new_x_list = np.arange(len(x_list)) * len(group_list) + len(group_list) / 2. - 0.5
-                    ax.set_yticks(new_x_list)
-                else:
-                    ax.set_ylim([-1, len(x_list)])
-                    ax.set_yticks(range(len(x_list)))
-                                                            
+                ax.set_ylim([-1, len(x_list)])
+                ax.set_yticks(range(len(x_list)))
+
                 if col == 0:
                     ax.set_yticklabels(x_list, fontsize=tick_fontsize)
                 else:
                     ax.set_yticklabels([])
 
                 if y_lim is None:
                     pass
@@ -287,36 +290,48 @@
                     plt.vlines(chance_level, ymin=plt.gca().get_ylim()[0], ymax=plt.gca().get_ylim()[1], **chance_level_style)
 
             # Inset title
             x_range = plt.gca().get_xlim()[1] - plt.gca().get_xlim()[0]
             y_range = plt.gca().get_ylim()[1] - plt.gca().get_ylim()[0]
             tpos = (
                 plt.gca().get_xlim()[0] + 0.03 * x_range,
-                plt.gca().get_ylim()[1] - 0.03 * y_range
+                plt.gca().get_ylim()[1] + 0.01 * y_range
             )
 
             ax.text(tpos[0], tpos[1], sp_label, horizontalalignment='left', verticalalignment='top', fontsize=fontsize, bbox=dict(facecolor='white', edgecolor='none'))
 
             # Inset legend
             if grouping:
                 if 'violin' in plot_type:
                     if i == len(subplot_list) - 1:
                         group_label_list = group_label_list[::-1]
                         ax.legend(*zip(*group_label_list), loc='upper left', bbox_to_anchor=(1, 1))
                 elif plot_type == 'swarm+box':
-                    if i == len(subplot_list) - 1:
-                        ax.legend(*zip(*group_label_list), loc='upper left', bbox_to_anchor=(1, 1))
+                    pass
                 else:
                     plt.legend()
 
             box_off(ax)
 
             plt.tight_layout()
 
-        # Draw X/Y labels and title ------------------------------------------
+        # Draw legend, X/Y labels, and title ----------------------------
+
+        # Legend
+        if plot_type == 'swarm+box':
+            if legend_handler is not None:
+                if len(subplot_list) < col_num*row_num:
+                    ax = plt.subplot(row_num, col_num, col_num)
+                else:
+                    ax = fig.add_axes([1, 0.5, 1./col_num*0.6, 0.5])
+                ax.legend(legend_handler[0], legend_handler[1],
+                          loc='upper left', bbox_to_anchor=(0, 1.0), fontsize=tick_fontsize)
+                ax.set_axis_off()
+                plt.tight_layout()
+
         ax = fig.add_axes([0, 0, 1, 1])
         ax.patch.set_alpha(0.0)
         ax.set_axis_off()
 
         # X Label
         if x_label is not None:
             txt = y_label if horizontal else x_label
@@ -375,15 +390,15 @@
     if grouping:
         n_grp = len(group_list)
         w = bar_group_width / (n_grp + 1)
 
         group_label_list = []
         for grpi in range(n_grp):
             offset = grpi * w - (n_grp // 2) * w
-            xpos_grp = np.array(xpos) + offset #- bar_group_width / 2 + (bar_group_width / 2) * w + offset
+            xpos_grp = np.array(xpos) + offset  #- bar_group_width / 2 + (bar_group_width / 2) * w + offset
             ydata_grp = [a_data[grpi] for a_data in data]
             violinobj = ax.violinplot(
                 ydata_grp, xpos_grp,
                 vert=not horizontal,
                 showmeans=True, showextrema=False, showmedians=False, points=points,
                 widths=w * 0.8)
             color = violinobj["bodies"][0].get_facecolor().flatten()
@@ -459,133 +474,93 @@
         )
         ax.scatter(x=scatterx, y=scattery, marker=scattermark, c="red", linewidths=2, zorder=10)
 
         ax.set(xlabel=None, ylabel=None)
 
 
 def __plot_swarmbox(
-        ax, x_list, data,
+        ax, x, y, x_list, df_t,
         horizontal=False, reverse_x=False,
-        grouping=False, group_list=[], 
+        grouping=False, group=None, group_list=[],
         dot_color='#696969', dot_size=3, dot_alpha=0.7,
         box_color='blue', box_width=0.5, box_linewidth=1, box_props={'alpha': .3},
         box_meanprops=dict(linestyle='-', linewidth=1.5, color='red'),
         box_medianprops={}
 ):
-    group_label_list = []
-    
+    # warning
     if grouping:
-        # color settings
-        if isinstance(box_color, str): # grouping=Trueにも関わらず，カラーが単色で指定されている場合，強制的に複数色リストに変更
-            box_color = [[0.3, 0.3, 1], [1, 0.3, 0.3], [0.3, 1, 0.3], [1, 0.3, 1], [0.3, 1, 1], [1, 1, 0.3], [0.6, 0.6, 0.6]]
-            
-        # data arrangement
-        df_list = []
-        grp_x_list = []
-        for xi, x_lbl in enumerate(x_list):
-            for grpi, grp_lbl in enumerate(group_list):
-                a_df = pd.DataFrame.from_dict({'y': data[xi][grpi]})
-                grp_x = x_lbl + "_" + grp_lbl
-                a_df['x'] = grp_x
-                df_list.append(a_df)
-                grp_x_list.append(grp_x)
-        tmp_df = pd.concat(df_list)
-        
-        # plot
-        if horizontal:
-            plotx, ploty = 'y', 'x'
-        else:
-            plotx, ploty = 'x', 'y'
+        warnings.warn('When grouping is True, "box_width" is not working to make the layout consistent with the swarm plot.')
+
+    # prepare plot
+    box_color_palette = sns.color_palette("pastel") if grouping else None
+    dot_color_palette = sns.color_palette("bright") if grouping else None
+    if horizontal:
+        plotx, ploty = y, x
+    else:
+        plotx, ploty = x, y
+
+    # plot swarmplot
+    if grouping:
+        sns.swarmplot(
+            data=df_t, ax=ax,
+            x=plotx, y=ploty, order=x_list, hue=group, hue_order=group_list,
+            orient="h" if horizontal else "v",
+            palette=dot_color_palette,
+            dodge=True,
+            color=dot_color, size=dot_size, alpha=dot_alpha, zorder=10
+        )
+    else:
         sns.swarmplot(
-            x=plotx, y=ploty, order=grp_x_list, orient="h" if horizontal else "v",
-            data=tmp_df, ax=ax, color=dot_color, size=dot_size, alpha=dot_alpha, zorder=10
+            data=df_t, ax=ax,
+            x=plotx, y=ploty, order=x_list,
+            orient="h" if horizontal else "v",
+            color=dot_color, size=dot_size, alpha=dot_alpha, zorder=10
         )
-        ax = sns.boxplot(
-            x=plotx, y=ploty, order=grp_x_list, orient="h" if horizontal else "v",
-            data=tmp_df, ax=ax, color="#0000FF", # この色指定は一時的なもの，後で変更
-            width=box_width, linewidth=box_linewidth,
-            showfliers=False, 
+
+    # plot boxplot
+    if grouping:
+        boxax = sns.boxplot(
+            data=df_t, ax=ax,
+            x=plotx, y=ploty, order=x_list, hue=group, hue_order=group_list,
+            orient="h" if horizontal else "v",
+            palette=dot_color_palette,
+            linewidth=box_linewidth,
+            showfliers=False,
             showmeans=True, meanline=True, meanprops=box_meanprops,
             medianprops=box_medianprops,
-            boxprops=box_props, zorder=100 # <- This zorder is very important for visualization.
+            boxprops=box_props, zorder=100  # <- This zorder is very important for visualization.
         )
-        ax.set(xlabel=None, ylabel=None)
-        
-        # coloring facecolor
-        artists = ax.artists
-        if reverse_x:
-            artists = artists[::-1]
-        color_patch_list = []
-        for axi, patch in enumerate(artists):
-            grpi = axi % len(group_list)
-            if grpi < len(box_color):
-                colori = grpi
-            else:
-                colori = grpi % len(box_color)
-            a_color = box_color[colori][:] # get rgb color (0~1, 3dim)
-            a_color.append(0.3) # add alpha value
-            patch.set_facecolor(a_color)
-            if axi < len(group_list): # get color patch for legend
-                color_patch_list.append(mpatches.Patch(color=a_color))
-        if reverse_x:
-            group_label_list = list(zip(color_patch_list, group_list[::-1]))
-        else:
-            group_label_list = list(zip(color_patch_list, group_list))
-                
-    else:
-        # color settings
-        # box_colorとしてlistが与えられていた場合，中身が単色のRGBAリストか， 複数色のRGBを格納したリストかを判定する
-        # 単色の場合はそのままboxplotに与えてOK
-        box_color_list = []
-        if hasattr(box_color, '__iter__'): 
-            if np.asarray(box_color).ndim == 2:
-                # 二重リストになる場合， 複数色のRGBのリストと判定し， box_color_listとして保持
-                box_color_list = box_color
-                box_color = "#0000FF" # temporaliry color
-            
-        # data arrangement
-        df_list = []
-        for xi, x_lbl in enumerate(x_list):
-            a_df = pd.DataFrame.from_dict({'y': data[xi]})
-            a_df['x'] = x_lbl
-            df_list.append(a_df)
-        tmp_df = pd.concat(df_list)
-        
-        # plot
+        # prepare legend
+        handlers, labels = boxax.get_legend_handles_labels()
+        handlers = handlers[:len(group_list)]
+        labels = labels[:len(group_list)]
         if horizontal:
-            plotx, ploty = 'y', 'x'
+            legend_handler = [handlers[::-1], labels[::-1]]
         else:
-            plotx, ploty = 'x', 'y'
-        sns.swarmplot(
-            x=plotx, y=ploty, order=x_list, orient="h" if horizontal else "v",
-            data=tmp_df, ax=ax, color=dot_color, size=dot_size, alpha=dot_alpha, zorder=10
-        )
-        ax = sns.boxplot(
-            x=plotx, y=ploty, order=x_list, orient="h" if horizontal else "v",
-            data=tmp_df, ax=ax, color=box_color, 
-            width=box_width,  linewidth=box_linewidth, 
-            showfliers=False, 
+            legend_handler = [handlers, labels]
+        ax.get_legend().remove()
+    else:
+        sns.boxplot(
+            data=df_t, ax=ax,
+            x=plotx, y=ploty, order=x_list,
+            orient="h" if horizontal else "v",
+            color=box_color,
+            linewidth=box_linewidth,
+            width=box_width,
+            showfliers=False,
             showmeans=True, meanline=True, meanprops=box_meanprops,
             medianprops=box_medianprops,
-            boxprops=box_props, zorder=100 # <- This zorder is very important for visualization. 
+            boxprops=box_props, zorder=100  # <- This zorder is very important for visualization.
         )
-        ax.set(xlabel=None, ylabel=None)
-        
-        # coloring facecolor if `box_color` is specified
-        if not len(box_color_list) == 0:
-            artists = ax.artists
-            if reverse_x:
-                artists = artists[::-1]
-            for axi, patch in enumerate(artists):
-                colori = axi % len(box_color_list)
-                a_color = box_color_list[colori][:] # get rgb color (0~1, 3dim)
-                a_color.append(0.3) # add alpha value
-                patch.set_facecolor(a_color)
-                
-    return group_label_list
+        legend_handler = None
+
+    # remove label
+    ax.set(xlabel=None, ylabel=None)
+
+    return legend_handler
 
 
 def __split_list(l, n):
     for idx in range(0, len(l), n):
         yield l[idx:idx + n]
 
 
@@ -621,14 +596,42 @@
             # violinplot requires at least two elements in the dataset
 
         data.append(data_t)
 
     return data
 
 
+def __strict_data(
+        df, subplot, sp_label, figure, fig_label, y, removenan
+):
+    if fig_label is None and sp_label is None:
+        df_t = df
+    elif fig_label is None:
+        df_t = df.query('`{}` == "{}"'.format(subplot, sp_label))
+    else:
+        df_t = df.query('`{}` == "{}" & `{}` == "{}"'.format(subplot, sp_label, figure, fig_label))
+    df_t = df_t.reset_index(drop=True)
+
+    if removenan:
+        df_t[y] = df_t[y].apply(lambda x: np.delete(x, np.isnan(x)))
+
+    return df_t
+
+
+def __weird_form_to_long(df, target_col, identify_cols=[]):
+    df_result = pd.DataFrame()
+    for i, row in df.iterrows():
+        tmp = {}
+        for col in identify_cols:
+            tmp[col] = row[col]
+        tmp[target_col] = row[target_col]
+        df_result = pd.concat([df_result, pd.DataFrame(tmp)])
+    return df_result
+
+
 def __draw_half_violin(
         ax, data, points, positions,
         color=None, left=True, vert=True
 ):
     v = ax.violinplot(data, points=points, positions=positions, vert=vert,
                       showmeans=True, showextrema=False, showmedians=False)
     for i, b in enumerate(v['bodies']):
```

### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

## bdpy/ml/__init__.py

```diff
@@ -2,12 +2,12 @@
 BdPy machine learning package
 
 This package is a part of BdPy
 """
 
 
 from .learning import Classification, CrossValidation, ModelTraining, ModelTest
-from .crossvalidation import make_cvindex, make_crossvalidationindex
+from .crossvalidation import make_cvindex, make_crossvalidationindex, make_cvindex_generator
 from .crossvalidation import cvindex_groupwise
 from .ensemble import *
 from .regress import *
 from .searchlight import *
```

## bdpy/ml/crossvalidation.py

```diff
@@ -3,51 +3,65 @@
 This module provides utility functions for cross-validation.
 '''
 
 
 import numpy as np
 
 
-def cvindex_groupwise(group, nfolds=None, return_bool=False):
+def cvindex_groupwise(group, nfolds=None, return_bool=False, exclusive=None):
     '''Return k-fold iterator for group-wise cross-validation (e.g, run-wise, block-wise, ...)
 
     n_folds` specification are not supported yet.
 
     Parameters
     ----------
     group : array-like (shape =  (n_samples, )
         Group labels (e.g., run labels, block labels, ...)
     n_folds : int, optional
         Number of folds (default: the number of unique elements in `group`)
     return_bool : bool, optional
         Return boolean arrays if True (default: False)
+    exclusive : array_like, optional
+        If specified, training samples that have the same labels as the test
+        data are removed in each fold.
 
     Returns
     -------
     K-fold iterator
     '''
 
     # TODO: add size checking of group
 
     group_set = np.unique(group)  # Unique labels in `group`
 
     if nfolds is None:
         nfolds = len(group_set)
 
+    if exclusive is not None:
+        exclusive = exclusive.flatten()
+
     for gl in group_set:
         index_train_bool = (group != gl).flatten()
         index_test_bool = (group == gl).flatten()
 
         if return_bool:
             index_train = index_train_bool
             index_test = index_test_bool
         else:
             index_train = np.where(index_train_bool)[0]
             index_test = np.where(index_test_bool)[0]
 
+        if exclusive is not None:
+            test_labels  = exclusive[index_test]
+            train_labels = exclusive[index_train]
+
+            index_train = np.array([
+                ind for ind, lab in zip(index_train, train_labels) if not lab in test_labels
+            ])
+
         yield index_train, index_test
 
 
 def make_cvindex(group):
     """
     Make indexes of training and test samples for cross-validation
 
@@ -118,7 +132,65 @@
     """
     Make indexes of training and test samples
 
     See 'make_cvindex' for the details.
     """
 
     return make_cvindex(group)
+
+
+def make_cvindex_generator(group, folds=None, exclusive=None, return_bool=False):
+    '''Return cross-validation iterator.
+
+    n_folds` specification are not supported yet.
+
+    Parameters
+    ----------
+    group : array-like (shape =  (n_samples, )
+        Group labels (e.g., run labels, block labels, ...)
+    folds : list, optional
+        List of CV folds(training and test labels in each fold)
+    return_bool : bool, optional
+        Return boolean arrays if True (default: False)
+    exclusive : array_like, optional
+        If specified, training samples that have the same labels as the test
+        data are removed in each fold.
+
+    Returns
+    -------
+    K-fold iterator
+    '''
+
+
+    if folds is None:
+        group_set = np.unique(group)
+        folds = [{'train': np.delete(group_set, i), 'test': gl} for i, gl in enumerate(group_set)]
+
+    if exclusive is not None:
+        exclusive = exclusive.flatten()
+
+    for f in folds:
+        ind_train = f['train']
+        ind_test  = f['test']
+
+        #index_train_bool = (group != gl).flatten()
+        #index_test_bool = (group == gl).flatten()
+
+        index_train_bool = np.isin(group, ind_train)
+        index_test_bool  = np.isin(group, ind_test)
+
+        if return_bool:
+            index_train = index_train_bool
+            index_test = index_test_bool
+        else:
+            index_train = np.where(index_train_bool)[0]
+            index_test = np.where(index_test_bool)[0]
+
+        if exclusive is not None:
+            test_labels  = exclusive[index_test]
+            train_labels = exclusive[index_train]
+
+            index_train = np.array([
+                ind for ind, lab in zip(index_train, train_labels) if not lab in test_labels
+            ])
+
+        yield index_train, index_test
```

## bdpy/mri/__init__.py

```diff
@@ -2,12 +2,12 @@
 BdPy MRI package
 
 This package is a part of BdPy
 """
 
 from .load_epi import load_epi
 from .load_mri import load_mri
-from .roi import add_roimask, get_roiflag, add_roilabel, add_rois, merge_rois
+from .roi import add_roimask, get_roiflag, add_roilabel, add_rois, merge_rois, add_hcp_rois, add_hcp_visual_cortex
 from .fmriprep import create_bdata_fmriprep, FmriprepData
 from .spm import create_bdata_spm_domestic
 from .image import export_brain_image
 from .glm import make_paradigm
```

## bdpy/mri/fmriprep.py

```diff
@@ -656,15 +656,15 @@
                 label_vals = []
                 for p in cols:
                     if p in label_mapper:
                         v = act_label_map.get_value(p, row[p])
                         label_vals.append(v)
                     else:
                         label_vals.append(row[p])
-                label_vals = np.array([np.nan if x == 'n/a' else np.float(x)
+                label_vals = np.array([np.nan if x == 'n/a' else float(x)
                                        for x in label_vals])
                 label_mat = np.tile(label_vals, (nsmp, 1))
                 labels.append(label_mat)
 
             ses_label_list.append(np.ones((num_vol, 1)) * (i + 1))
             run_label_list.append(np.ones((num_vol, 1)) * (j + 1) + last_run)
             block_label_list.append(np.vstack(blocks) + last_block)
@@ -727,66 +727,44 @@
     if with_confounds:
         default_confounds_keys = [
             'global_signal',
             'white_matter',
             'csf',
             'dvars',
             'std_dvars',
-            'framewise_displacement',
-            'a_comp_cor',
-            'a_comp_cor_00',
-            'a_comp_cor_01',
-            'a_comp_cor_02',
-            'a_comp_cor_03',
-            'a_comp_cor_04',
-            'a_comp_cor_05',
-            't_comp_cor',
-            't_comp_cor_00',
-            't_comp_cor_01',
-            't_comp_cor_02',
-            't_comp_cor_03',
-            't_comp_cor_04',
-            't_comp_cor_05',
-            'cosine',
-            'cosine00',
-            'cosine01',
-            'cosine02',
-            'cosine03',
-            'cosine04',
-            'cosine05',
+            'framewise_displacement'
         ]
         confounds_key_desc = {
             'global_signal':          {'key': 'GlobalSignal',          'desc': 'Confounds: Average signal in brain mask'},
             'white_matter':           {'key': 'WhiteMatterSignal',     'desc': 'Confounds: Average signal in white matter'},
             'csf':                    {'key': 'CSFSignal',             'desc': 'Confounds: Average signal in CSF'},
             'dvars':                  {'key': 'DVARS',                 'desc': 'Confounds: Original DVARS'},
             'std_dvars':              {'key': 'STD_DVARS',             'desc': 'Confounds: Standardized DVARS'},
             'framewise_displacement': {'key': 'FramewiseDisplacement', 'desc': 'Confounds: Framewise displacement (bulk-head motion)'},
-            'a_comp_cor':             {'key': 'aCompCor',              'desc': 'Confounds: Anatomical CompCor'},
-            'a_comp_cor_00':          {'key': 'aCompCor_0',            'desc': 'Confounds: Anatomical CompCor'},
-            'a_comp_cor_01':          {'key': 'aCompCor_1',            'desc': 'Confounds: Anatomical CompCor'},
-            'a_comp_cor_02':          {'key': 'aCompCor_2',            'desc': 'Confounds: Anatomical CompCor'},
-            'a_comp_cor_03':          {'key': 'aCompCor_3',            'desc': 'Confounds: Anatomical CompCor'},
-            'a_comp_cor_04':          {'key': 'aCompCor_4',            'desc': 'Confounds: Anatomical CompCor'},
-            'a_comp_cor_05':          {'key': 'aCompCor_5',            'desc': 'Confounds: Anatomical CompCor'},
-            't_comp_cor':             {'key': 'tCompCor',              'desc': 'Confounds: Temporal CompCor'},
-            't_comp_cor_00':          {'key': 'tCompCor_0',            'desc': 'Confounds: Temporal CompCor'},
-            't_comp_cor_01':          {'key': 'tCompCor_1',            'desc': 'Confounds: Temporal CompCor'},
-            't_comp_cor_02':          {'key': 'tCompCor_2',            'desc': 'Confounds: Temporal CompCor'},
-            't_comp_cor_03':          {'key': 'tCompCor_3',            'desc': 'Confounds: Temporal CompCor'},
-            't_comp_cor_04':          {'key': 'tCompCor_4',            'desc': 'Confounds: Temporal CompCor'},
-            't_comp_cor_05':          {'key': 'tCompCor_5',            'desc': 'Confounds: Temporal CompCor'},
-            'cosine':                 {'key': 'Cosine',                'desc': 'Confounds: Discrete cosine-basis regressors'},
-            'cosine00':               {'key': 'Cosine_0',              'desc': 'Confounds: Discrete cosine-basis regressors'},
-            'cosine01':               {'key': 'Cosine_1',              'desc': 'Confounds: Discrete cosine-basis regressors'},
-            'cosine02':               {'key': 'Cosine_2',              'desc': 'Confounds: Discrete cosine-basis regressors'},
-            'cosine03':               {'key': 'Cosine_3',              'desc': 'Confounds: Discrete cosine-basis regressors'},
-            'cosine04':               {'key': 'Cosine_4',              'desc': 'Confounds: Discrete cosine-basis regressors'},
-            'cosine05':               {'key': 'Cosine_5',              'desc': 'Confounds: Discrete cosine-basis regressors'},
-        }
+            }
+
+        additional_confounds_series = [
+            ('a_comp_cor', 'aCompCor', 'Confounds: Anatomical CompCor'),
+            ('t_comp_cor', 'tCompCor', 'Confounds: Temporal CompCor'),
+            ('cosine',     'Cosine',   'Confounds: Discrete cosine-basis regressors'),
+        ]
+        for target_key, target_key_name, target_key_desc in additional_confounds_series:
+            default_confounds_keys.append(target_key)
+            confounds_key_desc[target_key] = {
+                'key':target_key_name, 
+                'desc':target_key_desc
+            }
+            ack_list = sorted([ack for ack in confounds.keys() if target_key in ack])
+            for ack_i, ack in enumerate(ack_list): 
+                default_confounds_keys.append(ack)
+                confounds_key_desc[ack] = {
+                    'key': '%s_%d' % (target_key_name, ack_i), 
+                    'desc':target_key_desc
+                }
+
         confounds_array = np.hstack([
             confounds[dck]
             for dck in default_confounds_keys if dck in confounds
             ])
 
         bdata.add(confounds_array, 'Confounds')
```

## bdpy/mri/roi.py

```diff
@@ -1,35 +1,38 @@
-"""
-Utilities for ROIs
-"""
+'''Utilities for ROIs'''
+
 
 import os
 import glob
 import re
 import hashlib
 
 import numpy as np
 import nibabel.freesurfer
 
 from bdpy.mri import load_mri
 
 
-def add_roimask(bdata, roi_mask, roi_prefix='',
-                brain_data='VoxelData', xyz=['voxel_x', 'voxel_y', 'voxel_z'],
-                return_roi_flag=False,
-                verbose=True,
-                round=None):
+def add_roimask(
+        bdata, roi_mask, roi_prefix='',
+        brain_data='VoxelData', xyz=['voxel_x', 'voxel_y', 'voxel_z'],
+        return_roi_flag=False,
+        verbose=True,
+        round=None
+):
     '''Add an ROI mask to `bdata`.
+
     Parameters
     ----------
     bdata : BData
     roi_mask : str or list
         ROI mask file(s).
-    round :  int
+    round : int
         Number of decimal places to round the voxel coordinate.
+
     Returns
     -------
     bdata : BData
     '''
 
     if isinstance(roi_mask, str):
         roi_mask = [roi_mask]
@@ -93,18 +96,16 @@
 
     if return_roi_flag:
         return bdata, roi_flag
     else:
         return bdata
 
 
-
 def get_roiflag(roi_xyz_list, epi_xyz_array, verbose=True):
-    """
-    Get ROI flags
+    '''Get ROI flags.
 
     Parameters
     ----------
     roi_xyz_list : list, len = n_rois
         List of arrays that contain XYZ coordinates of ROIs. Each element is an
         array of shape = (3, n_voxels_in_roi).
     epi_xyz_array : array, shape = (3, n_voxels)
@@ -112,15 +113,15 @@
     verbose : boolean
         If True, 'get_roiflag' outputs verbose message
 
     Returns
     -------
     roi_flag : array, shape = (n_rois, n_voxels)
         ROI flag array
-    """
+    '''
 
     epi_voxel_size = epi_xyz_array.shape[1]
 
     if verbose:
         print("EPI num voxels: %d" % epi_voxel_size)
 
     roi_flag_array = np.zeros((len(roi_xyz_list), epi_voxel_size))
@@ -328,14 +329,15 @@
             tokens.append(tkn)
         else:
             # FIXME: dirty solution
             tkn = tkn.replace('"', '')
             tkn = tkn.replace("'", '')
             tkn_e = re.escape(tkn)
             tkn_e = tkn_e.replace('\*', '.*')
+            tkn_e = tkn_e + '$'  # To mimic `fullmatch` that is available in Python >= 3.4
 
             mks = [k for k in bdata.metadata.key if re.match(tkn_e, k)]
             if len(mks) == 0:
                 raise RuntimeError('ROI %s not found' % merge_expr)
             s = ' + '.join(mks)
             tokens.extend(s.split(' '))
 
@@ -379,7 +381,119 @@
     description = 'Merged ROI: %s' % ' '.join(tokens)
     bdata.add_metadata(roi_name, merged_roi_mv, description)
 
     num_voxels = np.nansum(merged_roi_mv).astype(int)
     print('Num voxels or vertexes: %d' % num_voxels)
 
     return bdata
+
+
+def add_hcp_rois(bdata, overwrite=False):
+    '''Add HCP ROIs in `bdata`.
+
+    Note
+    ----
+    This function assumes that the HCP ROIs (splitted by left and right) are
+    named as "hcp180_r_lh.L_{}__*_ROI" and "hcp180_r_rh.R_{}__*_ROI".
+    '''
+
+    hcp180_rois = [
+        '1', '10d', '10pp', '10r', '10v', '11l', '13l', '2', '23c', '23d',
+        '24dd', '24dv', '25', '31a', '31pd', '31pv', '33pr', '3a', '3b', '4',
+        '43', '44', '45', '46', '47l', '47m', '47s', '52', '55b', '5L', '5m',
+        '5mv', '6a', '6d', '6ma', '6mp', '6r', '6v', '7AL', '7Am', '7PC',
+        '7PL', '7Pm', '7m', '8Ad', '8Av', '8BL', '8BM', '8C', '9-46d', '9a',
+        '9m', '9p', 'A1', 'A4', 'A5', 'AAIC', 'AIP', 'AVI', 'DVT', 'EC',
+        'FEF', 'FFC', 'FOP1', 'FOP2', 'FOP3', 'FOP4', 'FOP5', 'FST', 'H',
+        'IFJa', 'IFJp', 'IFSa', 'IFSp', 'IP0', 'IP1', 'IP2', 'IPS1', 'Ig',
+        'LBelt', 'LIPd', 'LIPv', 'LO1', 'LO2', 'LO3', 'MBelt', 'MI', 'MIP',
+        'MST', 'MT', 'OFC', 'OP1', 'OP2-3', 'OP4', 'PBelt', 'PCV', 'PEF',
+        'PF', 'PFcm', 'PFm', 'PFop', 'PFt', 'PGi', 'PGp', 'PGs', 'PH', 'PHA1',
+        'PHA2', 'PHA3', 'PHT', 'PI', 'PIT', 'POS1', 'POS2', 'PSL', 'PeEc',
+        'Pir', 'PoI1', 'PoI2', 'PreS', 'ProS', 'RI', 'RSC', 'SCEF', 'SFL',
+        'STGa', 'STSda', 'STSdp', 'STSva', 'STSvp', 'STV', 'TA2', 'TE1a',
+        'TE1m', 'TE1p', 'TE2a', 'TE2p', 'TF', 'TGd', 'TGv', 'TPOJ1', 'TPOJ2',
+        'TPOJ3', 'V1', 'V2', 'V3', 'V3A', 'V3B', 'V3CD', 'V4', 'V4t', 'V6',
+        'V6A', 'V7', 'V8', 'VIP', 'VMV1', 'VMV2', 'VMV3', 'VVC', 'a10p',
+        'a24', 'a24pr', 'a32pr', 'a47r', 'a9-46v', 'd23ab', 'd32', 'i6-8',
+        'p10p', 'p24', 'p24pr', 'p32', 'p32pr', 'p47r', 'p9-46v', 'pOFC',
+        's32', 's6-8', 'v23ab'
+    ]
+
+    hcp_22_regions = {
+        'PVC': ['V1'],
+        'EVC': ['V2', 'V3', 'V4'],
+        'DSVC': ['V3A', 'V7', 'V3B', 'V6', 'V6A', 'IPS1'],
+        'VSVC': ['V8', 'VVC', 'VMV1', 'VMV2', 'VMV3', 'PIT', 'FFC'],
+        'MTcVA': ['V3CD', 'LO1', 'LO2', 'LO3', 'MT', 'MST', 'V4t', 'FST', 'PH'],
+        'SMC': ['4', '3a', '3b', '1', '2'],
+        'PCL_MCC': ['5L', '5m', '5mv', '24dd', '24dv', '6mp', '6ma', 'SCEF'],
+        'PMC': ['6a', '6d', 'FEF', 'PEF', '55b', '6v', '6r'],
+        'POC': ['43', 'FOP1', 'OP4', 'OP2-3', 'OP1', 'PFcm'],
+        'EAC': ['A1', 'MBelt', 'LBelt', 'PBelt', 'RI'],
+        'AAC': ['A4', 'A5', 'STSdp', 'STSda', 'STSvp', 'STSva', 'TA2', 'STGa'],
+        'IFOC': ['52', 'PI', 'Ig', 'PoI1', 'PoI2', 'FOP2', 'Pir', 'AAIC', 'MI', 'FOP3', 'FOP4', 'FOP5', 'AVI'],
+        'MTC': ['H', 'PreS', 'EC', 'PeEc', 'PHA1', 'PHA2', 'PHA3'],
+        'LTC': ['TGd', 'TGv', 'TF', 'TE2a', 'TE2p', 'TE1a', 'TE1m', 'TE1p', 'PHT'],
+        'TPOJ': ['TPOJ2', 'TPOJ3', 'TPOJ1', 'STV', 'PSL'],
+        'SPC': ['MIP', 'LIPv', 'VIP', 'LIPd', 'AIP', '7PC', '7Am', '7AL', '7Pm', '7PL'],
+        'IPC': ['PGp', 'IP0', 'IP1', 'IP2', 'PF', 'PFt', 'PFop', 'PFm', 'PGi', 'PGs'],
+        'PCC': ['DVT', 'ProS', 'POS2', 'POS1', 'RSC', '7m', 'PCV', 'v23ab', 'd23ab', '31pv', '31pd', '31a', '23c', '23d'],
+        'ACC_mPFC': ['33pr', 'a24pr', 'p24pr', 'p24', 'a24', 'p32pr', 'a32pr', 'd32', 'p32', 's32', '8BM', '9m', '10r', '10v', '25'],
+        'OPFC': ['OFC', 'pOFC', '13l', '11l', '47s', '47m', 'a47r', '10pp', 'a10p', 'p10p', '10d'],
+        'IFC': ['44', '45', '47l', 'IFJp', 'IFJa', 'IFSp', 'IFSa', 'p47r'],
+        'DLPFC': ['SFL', 's6-8', 'i6-8', '8BL', '8Ad', '8Av', '8C', '9p', '9a', '9-46d', 'a9-46v', 'p9-46v', '46'],
+    }
+
+    # Merge left and right ROIs
+    for roi in hcp180_rois:
+        name = 'hcp180_{}'.format(roi)
+        if not overwrite and name in bdata.metadata.key:
+            continue
+        select = '"hcp180_r_lh.L_{}_ROI" + "hcp180_r_rh.R_{}_ROI"'.format(roi, roi)
+        print('{}: {}'.format(name, select))
+        bdata = merge_rois(bdata, name, select)
+
+    # Merge HCP ROI groups
+    # See "Supplementary Neuroanatomical Results" of Glasser et al. (2016)
+    # https://www.nature.com/articles/nature18933
+    for name, rois in hcp_22_regions.items():
+        name = 'hcp180_reg_{}'.format(name)
+        if not overwrite and name in bdata.metadata.key:
+            continue
+        select = ' + '.join(['"hcp180_{}"'.format(a) for a in rois])
+        bdata = merge_rois(bdata, name, select)
+
+    return bdata
+
+
+def add_hcp_visual_cortex(bdata, overwrite=False):
+    '''Add HCP-based visual cortex in `bdata`.'''
+
+    # Whole VC
+    vc_rois = [
+        'V1', 'V2', 'V3', 'V4',
+        'V3A', 'V3B', 'V6', 'V6A', 'V7', 'IPS1',
+        'V8', 'VVC', 'VMV1', 'VMV2', 'VMV3', 'PIT', 'FFC',
+        'V3CD', 'LO1', 'LO2', 'LO3', 'MT', 'MST', 'V4t', 'FST', 'PH',
+        'MIP', 'AIP', 'VIP', 'LIPv', 'LIPd', '7PC', '7Am', '7AL', '7Pm', '7PL',
+        'PHA1', 'PHA2', 'PHA3',
+        'IP0', 'IP1', 'IP2', 'PGp',
+        'TPOJ2', 'TPOJ3',
+        'DVT', 'ProS', 'POS1', 'POS2'
+    ]
+    select = ' + '.join(['"hcp180_{}"'.format(a) for a in vc_rois])
+
+    if overwrite or 'hcp180_hcpVC' not in bdata.metadata.key:
+        bdata = merge_rois(bdata, 'hcp180_hcpVC', select)
+
+    # Early, Ventral, Dorsal, and MT VC
+    if overwrite or 'hcp180_EarlyVC' not in bdata.metadata.key:
+        bdata = merge_rois(bdata, 'hcp180_EarlyVC',   'hcp180_V1 + hcp180_V2 + hcp180_V3')
+    if overwrite or 'hcp180_MTVC' not in bdata.metadata.key:
+        bdata = merge_rois(bdata, 'hcp180_MTVC',      'hcp180_reg_MTcVA + hcp180_TPOJ2 + hcp180_TPOJ3 - hcp180_EarlyVC')
+    if overwrite or 'hcp180_VentralVC' not in bdata.metadata.key:
+        bdata = merge_rois(bdata, 'hcp180_VentralVC', 'hcp180_V4 + hcp180_reg_VSVC + hcp180_PHA1 + hcp180_PHA2 + hcp180_PHA3 - hcp180_EarlyVC - hcp180_MTVC')
+    if overwrite or 'hcp180_DorsalVC' not in bdata.metadata.key:
+        bdata = merge_rois(bdata, 'hcp180_DorsalVC',  'hcp180_reg_DSVC + hcp180_reg_SPC + hcp180_PGp + hcp180_IP0 + hcp180_IP1 + hcp180_IP2 - hcp180_EarlyVC - hcp180_MTVC - hcp180_VentralVC')
+
+    return bdata
```

## bdpy/preproc/preprocessor.py

```diff
@@ -197,15 +197,15 @@
         regressor = regressor[ind, :]
 
         n_smp = x.shape[0]
 
         dc_cmp = np.ones((n_smp, 1))  # DC component (mean)
 
         if linear_detrend:
-            ln_cmp = np.c_[(np.arange(n_smp) + 1) / np.float(n_smp)]
+            ln_cmp = np.c_[(np.arange(n_smp) + 1) / float(n_smp)]
             regmat = np.hstack([dc_cmp, ln_cmp, regressor])
         else:
             regmat = np.hstack([dc_cmp, regressor])
 
         try:
             w = np.linalg.solve(np.dot(regmat.T, regmat),  np.dot(regmat.T, x))
         except:
```

## bdpy/preproc/select_top.py

```diff
@@ -34,14 +34,15 @@
     """
 
     if verbose:
         print_start_msg()
 
     num_elem = data.shape[axis]
 
+    value = np.array([-np.inf if np.isnan(a) else a for a in value])
     sorted_index = np.argsort(value)[::-1]
 
     rank = np.zeros(num_elem, dtype=np.int)
     rank[sorted_index] = np.array(range(0, num_elem))
 
     selected_index_bool = rank < num
```

## bdpy/recon/torch/icnn.py

```diff
@@ -333,15 +333,15 @@
             if generator_preproc is not None:
                 ft = generator_preproc(ft)
 
             xt = generator.forward(ft)
             # xt.retain_grad()
 
             if generator_postproc is not None:
-                xt = generator.postproc(xt)
+                xt = generator_postproc(xt)
 
             # Crop the generated image
             if crop_generator_output:
                 gen_image_size = (xt.shape[2], xt.shape[3])
                 top_left = ((gen_image_size[0] - image_size[0]) // 2,
                             (gen_image_size[1] - image_size[1]) // 2)
```

## bdpy/util/info.py

```diff
@@ -2,14 +2,15 @@
 import hashlib
 import os
 import sys
 import time
 import uuid
 import warnings
 import yaml
+import pwd
 
 
 def dump_info(output_dir, script=None, parameters=None, info_file='info.yaml'):
     '''Dump runtime information.'''
 
     if script is not None:
         script_path = os.path.abspath(script)
@@ -30,15 +31,15 @@
         'run_time'   : run_time,
         'time_stamp' : datetime.datetime.fromtimestamp(run_time).strftime('%Y-%m-%d %H:%M:%S'),
         'host'       : os.uname()[1],
         'hardware'   : os.uname()[4],
         'os'         : os.uname()[0],
         'os_release' : os.uname()[2],
         'os_version' : os.uname()[3],
-        'user'       : os.getlogin(),
+        'user'       : pwd.getpwuid(os.geteuid())[0],
         'script_path': script_path,
         'script_txt' : script_txt,
         'script_md5' : script_md5,
         }
 
     if parameters is not None:
         parameters_fixed = {}
```

## Comparing `bdpy-0.18.dist-info/LICENSE` & `bdpy-0.19.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `bdpy-0.18.dist-info/METADATA` & `bdpy-0.19.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,20 +1,21 @@
 Metadata-Version: 2.1
 Name: bdpy
-Version: 0.18
+Version: 0.19
 Summary: Brain decoder toolbox for Python
 Home-page: https://github.com/KamitaniLab/bdpy
 Author: Shuntaro C. Aoki
 Author-email: brainliner-admin@atr.jp
 Maintainer: Shuntaro C. Aoki
 Maintainer-email: brainliner-admin@atr.jp
 License: MIT
 Keywords: neuroscience,neuroimaging,brain decoding,fmri,machine learning
 Platform: UNKNOWN
 Description-Content-Type: text/markdown
+License-File: LICENSE
 Requires-Dist: numpy
 Requires-Dist: scipy
 Requires-Dist: scikit-learn
 Requires-Dist: h5py
 Requires-Dist: hdf5storage
 Requires-Dist: pyyaml
```

## Comparing `bdpy-0.18.dist-info/RECORD` & `bdpy-0.19.dist-info/RECORD`

 * *Files 19% similar despite different names*

```diff
@@ -1,63 +1,63 @@
 bdpy/__init__.py,sha256=8pTJrp8ZVa1cbOZlfnMYQ6wWAUaSpHDjUzDgpVB8cf8,277
 bdpy/bdata/__init__.py,sha256=0nWdE9l8gbQJDFrLK4A1YRabT1aw1NWQ37G-dEpaRXs,142
-bdpy/bdata/bdata.py,sha256=jWnGDLUgATrKgdunM6SIqCOZxy_6MOLJ9g_DS_k5xAk,27841
+bdpy/bdata/bdata.py,sha256=XTXaWtsxf879qNw7a-njLrEEFHEwKKxX8tieD_jTMZo,27829
 bdpy/bdata/featureselector.py,sha256=Hzfr2Mgo9mRCN9uDt5d8Bx-fpnqshcTOj16pP74bihE,3142
-bdpy/bdata/metadata.py,sha256=JtXmfHnE4-i7X3PATJYk3RdDpgNVvufi_1qBZv2J9PY,3959
-bdpy/bdata/utils.py,sha256=xkU92X6L-sI_PTCjqVes6WpLfQPU_2sRdHpVld73S3A,6016
+bdpy/bdata/metadata.py,sha256=LWwwYNfMJ-aKj94NchaROWNM271KkHgwu8hqKvKeNc4,3953
+bdpy/bdata/utils.py,sha256=xiT_Dn0lMqWsraTbciywdwCmpFUJVp9h-ViVPn3yOvg,8555
 bdpy/dataform/__init__.py,sha256=nOBTPke1z9a_fKrJZZASfbkln1JS44gbF9LjEdbvJ5w,155
-bdpy/dataform/datastore.py,sha256=EsamOZYVuhJAg5XyEY2HNsB-x1PILU5L61tc6Dbni8Q,8031
-bdpy/dataform/features.py,sha256=eblCQt7CN1DTGsOPU_kpA78p0rLhLBvel5aN7hFWTVw,5687
+bdpy/dataform/datastore.py,sha256=LfcuzZFfyka2Xo8MqnRQC-vXMGXfh_g51MOYW9fBSN8,7238
+bdpy/dataform/features.py,sha256=6oR-0S7qxQWKvxjp46gQk9nTWZNhiXoBv0eklFNlX4M,15415
 bdpy/dataform/pd.py,sha256=ztbHmo343VFdkC3V771rCEn9tm0v0rK27LyY9be18iM,835
 bdpy/dataform/sparse.py,sha256=Nm3mDKwgDnA8oU5LRIvywh2u4CRIGXfJnEx4j9JDWAM,3915
 bdpy/distcomp/__init__.py,sha256=f1aKub4bsM5URiYLegjeFIIpwcWl2rMAPvr9NoiIRM8,97
 bdpy/distcomp/distcomp.py,sha256=Wk1mobLwQwElR4-SRcv_jrp7y1rowWtgwoLI-sJKDQw,4552
 bdpy/dl/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 bdpy/dl/caffe.py,sha256=kKFvDcN1lgFD7kTRtQ4YwMwBWSoA06bLXmx-0C4AtDU,4095
 bdpy/dl/torch/__init__.py,sha256=rxBCd2bxd8gxOiWwATXcondxKF0MuaKNVYH_0AFEhCA,50
-bdpy/dl/torch/models.py,sha256=qK7YpPAguToP8sMRO769Az8Z3tEVR8v6netNoFFwt78,9020
-bdpy/dl/torch/torch.py,sha256=vlJvmpAGz0SIOSFpU4pCbSLLueacSRIMvIpNCksOBBs,5602
+bdpy/dl/torch/models.py,sha256=F9WF2Z_J9p8yOAWUY_PWZryh7y6dxvbI_t9DvL0Uq_s,13557
+bdpy/dl/torch/torch.py,sha256=1v6v6aUKVIixtaJAGv_Oz-yusngvSZaTEw0OtoKIAoI,8112
 bdpy/evals/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-bdpy/evals/metrics.py,sha256=rImx-veKs6XPBpmLD1lAntHAN9an1fttrV-nkWoHm78,3317
+bdpy/evals/metrics.py,sha256=2P780u1osQs-EYe77jwObvPH-2G8vdyu-0U9Kw1LKIY,4016
 bdpy/feature/__init__.py,sha256=4713ux7p16BRIc6tGs5pBAQ9ZhBsL7-29UW3iOHUX4I,74
 bdpy/feature/feature.py,sha256=hTCKz_83BSG7vXM4cLAeB2g89c34kNLXfKIlRFe41RA,2436
 bdpy/fig/__init__.py,sha256=jhcGGygWWuMNQq7hfT7G8J1h4jJwT8zBKCnSxZi8TE8,200
 bdpy/fig/draw_group_image_set.py,sha256=_1dNVlvg8RJNMBKevDGhlCAfG1KWPNWJx1kC5dItOb8,8007
 bdpy/fig/fig.py,sha256=Yxug5sQ4OoE8PJ-WgOb-sUR8rPyU-XHcRmKSxanKalk,4967
-bdpy/fig/makeplots.py,sha256=TccLw9SvRZApNJqfx9qyaAb3PX4Z9hWAsNWBBLLpjRs,25224
+bdpy/fig/makeplots.py,sha256=DWYJDC9Fd_dl5rz9pD8pVxBL21j_cYvOwOhIkD_vYu4,23906
 bdpy/fig/tile_images.py,sha256=LQoPb1J8xwnxUm1PJZX9xFJa-pyOQ1JWmTzTVXqL7Ow,6572
-bdpy/ml/__init__.py,sha256=f5vohekdzA58tWo3TagWb3PoSpSL8Ij93Ijy7ahgdPw,342
-bdpy/ml/crossvalidation.py,sha256=DHtpSdWL43pBITYvzWzu2hgneIPB7ZwW1uhUqW9uFng,3627
+bdpy/ml/__init__.py,sha256=LnFHCJPAPhVBCoA9jNM92UimbOCYz_E1Bb96WFUEnVA,366
+bdpy/ml/crossvalidation.py,sha256=ERIOOQJY_MqwibwbB_FtQ85uCU8ro21fgYw8k0zWTho,5920
 bdpy/ml/ensemble.py,sha256=_z50GOAVRJmlV4AmT44LiVw4VC0gL3NWLVjFy5TX_NQ,981
 bdpy/ml/learning.py,sha256=bU6nEXP9jfPpeHkj85KDUPmOjhsy9cykoBqBRVc3nus,18935
 bdpy/ml/regress.py,sha256=AIWbc_21kLq3nMVXn-mo7SxdaQ3CRfSjwXMEaoKRxCQ,695
 bdpy/ml/searchlight.py,sha256=VB6Ft_ptsKUCTht9TODd7dGwmtzT6Kzgi0po-EN6n7Y,1217
-bdpy/mri/__init__.py,sha256=n77_E9OZOqhaf8G_qo26ptwh0ptlnUc8Gk8pD6OnTyg,368
-bdpy/mri/fmriprep.py,sha256=XajrIroFePlLviC-Kt9abUwaHLM9M2_bpwyR9TYvO0g,37172
+bdpy/mri/__init__.py,sha256=UxdLtw3q4B-igZsHw-wENpq90o8Kp-JYY1lPxvfn9Lg,405
+bdpy/mri/fmriprep.py,sha256=sqXT9ylELtZDSB3rQdq_cReaB-XkRGzIh_S_NpiPuiU,35073
 bdpy/mri/glm.py,sha256=yiouwVB9_RK9gIKfOC_lQ0dY1jE3Bfe_1PH1Sr0Htms,3037
 bdpy/mri/image.py,sha256=l0DYBUdxsKI1jy_KPCcpi6GYheh-F4TEJP3ebLqbIOE,1516
 bdpy/mri/load_epi.py,sha256=NysL8R1deBF5_QRpHiT6qplGcy8NzRa6Zxb2D41kW_Y,1890
 bdpy/mri/load_mri.py,sha256=nC-NCEpBPArtBXi0URd0gu9uQlzkKEwKdMd1Ig3ptoU,1064
-bdpy/mri/roi.py,sha256=io-nFaps75hO8jZZSfmDIMM8rAjum5mezzfR9yxsi_Q,12800
+bdpy/mri/roi.py,sha256=1BFPbYzQSXFnhc1a3suuzoLj51wekAl7nGgKNREcK1I,18772
 bdpy/mri/spm.py,sha256=eEv73K3JlK7dm3pJ1JEnCtm1CmKCOqUE6GuhkJ2pj90,11280
 bdpy/opendata/__init__.py,sha256=aDvUQwGVuGko2xlSrfLU4b81J48WUW4X9V0B2spv-kU,32
 bdpy/opendata/openneuro.py,sha256=0CprRK4Ap7YhPzKOwhtvXPDj7qNsHSw6mmLXbdSijCE,11226
 bdpy/preproc/__init__.py,sha256=k1SdZNTTadsskADTlCYno-Y9Xux8hbH6vaZ-8REMFwo,159
 bdpy/preproc/interface.py,sha256=9rieGmZx9mbcGomEISabM23e_XjtTVNGMMBm9ETN674,5050
-bdpy/preproc/preprocessor.py,sha256=cqb8RJxoP0KPcGinMQWEZ3zcBN6bJ3JxQkC8BvvDI4Q,6217
-bdpy/preproc/select_top.py,sha256=RD2NobzP7nluBaUKPIPZlixQunk7VJ3vE2WyoN0-9VE,1294
+bdpy/preproc/preprocessor.py,sha256=rQ_MJavkNv7t3nDHb3LmjoAH4O91yW4VtHEf50f2-aE,6214
+bdpy/preproc/select_top.py,sha256=WnZI9WPGHal34hIm4a3uA92wSeriv9_m5QTT13qEMJQ,1363
 bdpy/preproc/util.py,sha256=BheeFgVLYYn-RnkMkqLm_Nej5MmjYP0H2O1T-BGV4cU,467
 bdpy/recon/__init__.py,sha256=AbpHGcgLb-kRsJGnwFEktk7uzpZOCcBY74-YBdrKVGs,1
 bdpy/recon/utils.py,sha256=kL4Hr3TQK9RabYTmblVIlPZrreFcmtJaefFAxLT7EQI,5657
 bdpy/recon/torch/__init__.py,sha256=uRo7xeIGTQJaiuEiVVyWlI7FtIldU9gJxLZrcaSRUTM,13
-bdpy/recon/torch/icnn.py,sha256=rALWjudZdhcDrqYiUlVoXXvTQpTfWKh4AQkJfwJyWqU,15319
+bdpy/recon/torch/icnn.py,sha256=UrH-XeFscAMRfvkPLS0QP9oOm4aWCZYZtNHuecPXtHY,15319
 bdpy/stats/__init__.py,sha256=4pfN3Zor0OZNXUEnoojaxSj0pWMBSbVGqT4seAUwASk,229
 bdpy/stats/corr.py,sha256=pgpBDBgV-NZUeY9mXAz9RE-OEbVURF0pBD72nfiO9HM,2548
 bdpy/util/__init__.py,sha256=xJlqJeCydx9VVqD-iXgPxhq_g7lxSvHEzuvMIGkJtsE,147
-bdpy/util/info.py,sha256=xWNqTrYT-ZoQOZuxa7Ww8NEkYBBVCdpGdZcVf-C7MDY,2262
+bdpy/util/info.py,sha256=pQe4LuQW3-S5FDLSwqGEeC9GVRbjO9j7LWs4cOQqadM,2289
 bdpy/util/math.py,sha256=FuqVkYw-lDmbTTNI-2FBbYnfQy31ks4vPHL7_hRz2DA,586
 bdpy/util/utils.py,sha256=stmO2tdOwr62YcBOHKuPNFRz_YmMzVE3BKDGsYHd7FU,3375
-bdpy-0.18.dist-info/LICENSE,sha256=e3nExGsL_0Ty_UoeqnVVGYY1Hg7c-PVMJSHmGcITSu4,1074
-bdpy-0.18.dist-info/METADATA,sha256=BV6_NN9EfCJ-mgZ5MlEGXN6Y-aH298gTHix42ndD8XA,2769
-bdpy-0.18.dist-info/WHEEL,sha256=ADKeyaGyKF5DwBNE0sRE5pvW-bSkFMJfBuhzZ3rceP4,110
-bdpy-0.18.dist-info/top_level.txt,sha256=miAsXBStOHP6mieTJSH8MK5I6enzfdrhJPVC7uzE4Ls,5
-bdpy-0.18.dist-info/RECORD,,
+bdpy-0.19.dist-info/LICENSE,sha256=e3nExGsL_0Ty_UoeqnVVGYY1Hg7c-PVMJSHmGcITSu4,1074
+bdpy-0.19.dist-info/METADATA,sha256=31ml76pXaVQP-1m0EMVoqKo1wjWFrtxBDu-jtxJPnDg,2791
+bdpy-0.19.dist-info/WHEEL,sha256=z9j0xAa_JmUKMpmz72K0ZGALSM_n-wQVmGbleXx2VHg,110
+bdpy-0.19.dist-info/top_level.txt,sha256=miAsXBStOHP6mieTJSH8MK5I6enzfdrhJPVC7uzE4Ls,5
+bdpy-0.19.dist-info/RECORD,,
```

